{
    "uuid": "25ffe76f-9274-56ea-b07f-4086da57bb65",
    "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 Datasets and Benchmarks Poster",
    "bibtex": "@inproceedings{\nyang2023intercode,\ntitle={InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback},\nauthor={John Yang and Akshara Prabhakar and Karthik R Narasimhan and Shunyu Yao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2023},\nurl={https://openreview.net/forum?id=fvKaLF1ns8}\n}",
    "authors": [
        "John Yang",
        "Akshara Prabhakar",
        "Karthik R Narasimhan",
        "Shunyu Yao"
    ],
    "pdf_url": "https://openreview.net/pdf/4bcd04423df03086069a51deb24dd49fd50a036e.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/25ffe76f-9274-56ea-b07f-4086da57bb65.pdf",
    "num_pages": 29,
    "abstract": "Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCodeâ€™s viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages.",
    "tldr": "We introduce a lightweight, flexible, and easy-to-use framework for designing interactive code environments.",
    "tags": [
        "natural language processing",
        "code generation",
        "interactive machine learning"
    ]
}