{
    "uuid": "00c1d25f-b86c-5903-862c-184c70969e85",
    "title": "Certifying LLM Safety against Adversarial Prompting",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2025,
    "authors": [
        "Aounon Kumar",
        "Chirag Agarwal",
        "Suraj Srinivas",
        "Aaron Jiaxun Li",
        "Soheil Feizi",
        "Himabindu Lakkaraju"
    ],
    "pdf_url": "http://arxiv.org/pdf/2309.02705v4",
    "pdf_path": "data/dataset/airqa/papers/arxiv2025/00c1d25f-b86c-5903-862c-184c70969e85.pdf",
    "bibtex": "@misc{kumar2025certifyingllmsafetyagainstadversarial,\n    title = {Certifying LLM Safety against Adversarial Prompting},\n    author = {Aounon Kumar and Chirag Agarwal and Suraj Srinivas and Aaron Jiaxun Li and Soheil Feizi and Himabindu Lakkaraju},\n    year = {2025},\n    eprint = {2309.02705},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2309.02705},\n}",
    "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add\nmalicious tokens to an input prompt to bypass the safety guardrails of an LLM\nand cause it to produce harmful content. In this work, we introduce\nerase-and-check, the first framework for defending against adversarial prompts\nwith certifiable safety guarantees. Given a prompt, our procedure erases tokens\nindividually and inspects the resulting subsequences using a safety filter. Our\nsafety certificate guarantees that harmful prompts are not mislabeled as safe\ndue to an adversarial attack up to a certain size. We implement the safety\nfilter in two ways, using Llama 2 and DistilBERT, and compare the performance\nof erase-and-check for the two cases. We defend against three attack modes: i)\nadversarial suffix, where an adversarial sequence is appended at the end of a\nharmful prompt; ii) adversarial insertion, where the adversarial sequence is\ninserted anywhere in the middle of the prompt; and iii) adversarial infusion,\nwhere adversarial tokens are inserted at arbitrary positions in the prompt, not\nnecessarily as a contiguous block. Our experimental results demonstrate that\nthis procedure can obtain strong certified safety guarantees on harmful prompts\nwhile maintaining good empirical performance on safe prompts. Additionally, we\npropose three efficient empirical defenses: i) RandEC, a randomized subsampling\nversion of erase-and-check; ii) GreedyEC, which greedily erases tokens that\nmaximize the softmax score of the harmful class; and iii) GradEC, which uses\ngradient information to optimize tokens to erase. We demonstrate their\neffectiveness against adversarial prompts generated by the Greedy Coordinate\nGradient (GCG) attack algorithm. The code for our experiments is available at\nhttps://github.com/aounon/certified-llm-safety.",
    "num_pages": 32,
    "tldr": "Introduces erase-and-check framework to certify LLM safety from adversarial prompts.",
    "tags": [
        "Adversarial Attacks",
        "LLM Safety",
        "Erase-and-Check",
        "Certified Safety",
        "Prompt Defense"
    ]
}