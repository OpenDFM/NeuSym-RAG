{
    "uuid": "b1038525-40b7-5470-a47c-13470cd55625",
    "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Guozheng Li",
        "Peng Wang",
        "Wenjun Ke"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.05028v4",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/b1038525-40b7-5470-a47c-13470cd55625.pdf",
    "bibtex": "@misc{li2023revisitinglargelanguagemodelsas,\n    title = {Revisiting Large Language Models as Zero-shot Relation Extractors},\n    author = {Guozheng Li and Peng Wang and Wenjun Ke},\n    year = {2023},\n    eprint = {2310.05028},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2310.05028},\n}",
    "abstract": "Relation extraction (RE) consistently involves a certain degree of labeled or\nunlabeled data even if under zero-shot setting. Recent studies have shown that\nlarge language models (LLMs) transfer well to new tasks out-of-the-box simply\ngiven a natural language prompt, which provides the possibility of extracting\nrelations from text without any data and parameter tuning. This work focuses on\nthe study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors.\nOn the one hand, we analyze the drawbacks of existing RE prompts and attempt to\nincorporate recent prompt techniques such as chain-of-thought (CoT) to improve\nzero-shot RE. We propose the summarize-and-ask (\\textsc{SumAsk}) prompting, a\nsimple prompt recursively using LLMs to transform RE inputs to the effective\nquestion answering (QA) format. On the other hand, we conduct comprehensive\nexperiments on various benchmarks and settings to investigate the capabilities\nof LLMs on zero-shot RE. Specifically, we have the following findings: (i)\n\\textsc{SumAsk} consistently and significantly improves LLMs performance on\ndifferent model sizes, benchmarks and settings; (ii) Zero-shot prompting with\nChatGPT achieves competitive or superior results compared with zero-shot and\nfully supervised methods; (iii) LLMs deliver promising performance in\nextracting overlapping relations; (iv) The performance varies greatly regarding\ndifferent relations. Different from small language models, LLMs are effective\nin handling challenge none-of-the-above (NoTA) relation.",
    "num_pages": 16,
    "tldr": "LLMs, like ChatGPT, excel in zero-shot relation extraction using improved prompts.",
    "tags": [
        "zero-shot relation extraction",
        "large language models",
        "prompt engineering",
        "chain-of-thought",
        "ChatGPT"
    ]
}