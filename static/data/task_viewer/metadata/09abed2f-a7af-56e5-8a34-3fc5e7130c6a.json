{
    "uuid": "09abed2f-a7af-56e5-8a34-3fc5e7130c6a",
    "title": "Probing Language Models for Pre-training Data Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2024-probing,\n    title = \"Probing Language Models for Pre-training Data Detection\",\n    author = \"Liu, Zhenhua  and\n      Zhu, Tong  and\n      Tan, Chuanyuan  and\n      Liu, Bing  and\n      Lu, Haonan  and\n      Chen, Wenliang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.86\",\n    doi = \"10.18653/v1/2024.acl-long.86\",\n    pages = \"1576--1587\",\n    abstract = \"Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model{'}s internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy.\",\n}\n",
    "authors": [
        "Zhenhua Liu",
        "Tong Zhu",
        "Chuanyuan Tan",
        "Bing Liu",
        "Haonan Lu",
        "Wenliang Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.86.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/09abed2f-a7af-56e5-8a34-3fc5e7130c6a.pdf",
    "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the modelâ€™s internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy.",
    "num_pages": 12,
    "tldr": "Detects LLM pre-training data using internal activations, outperforming baselines.",
    "tags": [
        "Language Models",
        "Pre-training Data Detection",
        "Probing Technique",
        "Data Contamination",
        "Benchmark Datasets"
    ]
}