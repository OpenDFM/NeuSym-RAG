{
    "uuid": "09d48a2a-4ad0-5a7f-84ec-557ac57f5830",
    "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nwang2024mint,\ntitle={{MINT}: Evaluating {LLM}s in Multi-turn Interaction with Tools and Language Feedback},\nauthor={Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=jp3gWrMuIZ}\n}",
    "authors": [
        "Xingyao Wang",
        "Zihan Wang",
        "Jiateng Liu",
        "Yangyi Chen",
        "Lifan Yuan",
        "Hao Peng",
        "Heng Ji"
    ],
    "pdf_url": "https://openreview.net/pdf/d41eb66c94971e1ba5e5477008f9f7ed1d13e05b.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/09d48a2a-4ad0-5a7f-84ec-557ac57f5830.pdf",
    "num_pages": 35,
    "abstract": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases.\nWe introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4.\nWe repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation.\nOur analysis of 20 open- and closed-source LLMs offers intriguing findings.\n(a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1--8% for each turn of tool use and 2--17% with natural language feedback.\n(b) Better single-turn performance does not guarantee better multi-turn performance.\n(c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities.\nWe expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
    "tldr": "MINT benchmark evaluates LLMs in multi-turn interactions with tools and feedback.",
    "tags": [
        "large language model",
        "multi-turn interaction",
        "learning from feedback",
        "reinforcement learning from human feedback",
        "instruction tuning"
    ]
}