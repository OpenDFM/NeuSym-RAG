{
    "uuid": "2d0a9f7f-6c7d-571d-90f4-8bafdbb97ce3",
    "title": "Context Length Extension via Generalized Extrapolation Scale",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-huaping-2024-context,\n    title = \"Context Length Extension via Generalized Extrapolation Scale\",\n    author = \"Li, Linhan  and\n      Huaping, Zhang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.249\",\n    doi = \"10.18653/v1/2024.findings-acl.249\",\n    pages = \"4211--4218\",\n}\n",
    "authors": [
        "Linhan Li",
        "Zhang Huaping"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.249.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2d0a9f7f-6c7d-571d-90f4-8bafdbb97ce3.pdf",
    "abstract": "Context length expansion of transformer models is considered a key challenge, especially when handling context beyond the training length during inference stage. In this paper, we propose Generalized extrapolatioN scalE (GeNE), a straightforward and effective method applied to the interpolate function of positional embeddings to achieve training short, test long. Experimental results show that GeNE notably improves long context language modeling. By randomly scaling the extrapolation ratio during the finetuning, GeNE achieves stable extrapolation on 64k contexts by training on 16k length. Further, the instruction following Llama2 model based on GeNE achieved competitive results compared with other open-source models of the same parameter scale. Our code is available at https: //github.com/LhLi-QED/GeNE.",
    "num_pages": 8,
    "tldr": "\"GeNE extends transformer context length effectively, improving long context handling.\"",
    "tags": [
        "transformer models",
        "context length expansion",
        "positional embeddings",
        "language modeling",
        "extrapolation techniques"
    ]
}