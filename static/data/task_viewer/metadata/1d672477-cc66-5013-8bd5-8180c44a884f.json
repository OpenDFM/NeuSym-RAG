{
    "uuid": "1d672477-cc66-5013-8bd5-8180c44a884f",
    "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 Track Datasets and Benchmarks Poster",
    "bibtex": "@inproceedings{\nliu2024mmdu,\ntitle={{MMDU}: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for {LVLM}s},\nauthor={Ziyu Liu and Tao Chu and Yuhang Zang and Xilin Wei and Xiaoyi Dong and Pan Zhang and Zijian Liang and Yuanjun Xiong and Yu Qiao and Dahua Lin and Jiaqi Wang},\nbooktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2024},\nurl={https://openreview.net/forum?id=s8h2jSN6a6}\n}",
    "authors": [
        "Ziyu Liu",
        "Tao Chu",
        "Yuhang Zang",
        "Xilin Wei",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Zijian Liang",
        "Yuanjun Xiong",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "pdf_url": "https://openreview.net/pdf/37ae7ea4f4be3963d883f3e43a1277d504054df6.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/1d672477-cc66-5013-8bd5-8180c44a884f.pdf",
    "num_pages": 36,
    "abstract": "Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model.\nMMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5x longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data.\nWe demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. The links to MMDU, and MMDU-45k are available in the supplementary material.",
    "tldr": "Introducing MMDU, a benchmark to enhance LVLMs in multi-turn, multi-image dialog tasks.",
    "tags": [
        "LVLM benchmark",
        "LVLM instruction-tuning dataset"
    ]
}