{
    "uuid": "20d260fa-1e33-5f05-b84a-132458d61695",
    "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Andy Zou",
        "Zifan Wang",
        "Nicholas Carlini",
        "Milad Nasr",
        "J. Zico Kolter",
        "Matt Fredrikson"
    ],
    "pdf_url": "http://arxiv.org/pdf/2307.15043v2",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/20d260fa-1e33-5f05-b84a-132458d61695.pdf",
    "bibtex": "@misc{zou2023universalandtransferableadversarialattacks,\n    title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},\n    author = {Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},\n    year = {2023},\n    eprint = {2307.15043},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2307.15043},\n}",
    "abstract": "Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.",
    "num_pages": 31,
    "tldr": "Efficient attack method generates objectionable content in aligned LLMs.",
    "tags": [
        "Adversarial Attacks",
        "Aligned Language Models",
        "Transferable Prompts",
        "Objectionable Content",
        "Automatic Prompt Generation"
    ]
}