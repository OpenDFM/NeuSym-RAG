{
    "uuid": "0a2c3d8b-dc16-570c-b354-11797aebe290",
    "title": "Dialogue Summarization with Mixture of Experts based on Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{tian-etal-2024-dialogue,\n    title = \"Dialogue Summarization with Mixture of Experts based on Large Language Models\",\n    author = \"Tian, Yuanhe  and\n      Xia, Fei  and\n      Song, Yan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.385\",\n    doi = \"10.18653/v1/2024.acl-long.385\",\n    pages = \"7143--7155\",\n    abstract = \"Dialogue summarization is an important task that requires to generate highlights for a conversation from different aspects (e.g., content of various speakers). While several studies successfully employ large language models (LLMs) and achieve satisfying results, they are limited by using one model at a time or treat it as a black box, which makes it hard to discriminatively learn essential content in a dialogue from different aspects, therefore may lead to anticipation bias and potential loss of information in the produced summaries. In this paper, we propose an LLM-based approach with role-oriented routing and fusion generation to utilize mixture of experts (MoE) for dialogue summarization. Specifically, the role-oriented routing is an LLM-based module that selects appropriate experts to process different information; fusion generation is another LLM-based module to locate salient information and produce finalized dialogue summaries. The proposed approach offers an alternative solution to employing multiple LLMs for dialogue summarization by leveraging their capabilities of in-context processing and generation in an effective manner. We run experiments on widely used benchmark datasets for this task, where the results demonstrate the superiority of our approach in producing informative and accurate dialogue summarization.\",\n}\n",
    "authors": [
        "Yuanhe Tian",
        "Fei Xia",
        "Yan Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.385.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0a2c3d8b-dc16-570c-b354-11797aebe290.pdf",
    "abstract": "Dialogue summarization is an important task that requires to generate highlights for a conversation from different aspects (e.g., content of various speakers). While several studies successfully employ large language models (LLMs) and achieve satisfying results, they are limited by using one model at a time or treat it as a black box, which makes it hard to discriminatively learn essential content in a dialogue from different aspects, therefore may lead to anticipation bias and potential loss of information in the produced summaries. In this paper, we propose an LLM-based approach with role-oriented routing and fusion generation to utilize mixture of experts (MoE) for dialogue summarization. Specifically, the role-oriented routing is an LLM-based module that selects appropriate experts to process different information; fusion generation is another LLM-based module to locate salient information and produce finalized dialogue summaries. The proposed approach offers an alternative solution to employing multiple LLMs for dialogue summarization by leveraging their capabilities of in-context processing and generation in an effective manner. We run experiments on widely used benchmark datasets for this task, where the results demonstrate the superiority of our approach in producing informative and accurate dialogue summarization.",
    "num_pages": 13,
    "tldr": "Proposed MoE approach enhances dialogue summarization using multiple LLMs.",
    "tags": [
        "Dialogue Summarization",
        "Large Language Models",
        "Mixture of Experts",
        "Role-oriented Routing",
        "Fusion Generation"
    ]
}