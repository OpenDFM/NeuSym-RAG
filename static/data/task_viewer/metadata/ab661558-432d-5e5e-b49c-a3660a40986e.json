{
    "uuid": "ab661558-432d-5e5e-b49c-a3660a40986e",
    "title": "Language Models (Mostly) Know What They Know",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Saurav Kadavath",
        "Tom Conerly",
        "Amanda Askell",
        "Tom Henighan",
        "Dawn Drain",
        "Ethan Perez",
        "Nicholas Schiefer",
        "Zac Hatfield-Dodds",
        "Nova DasSarma",
        "Eli Tran-Johnson",
        "Scott Johnston",
        "Sheer El-Showk",
        "Andy Jones",
        "Nelson Elhage",
        "Tristan Hume",
        "Anna Chen",
        "Yuntao Bai",
        "Sam Bowman",
        "Stanislav Fort",
        "Deep Ganguli",
        "Danny Hernandez",
        "Josh Jacobson",
        "Jackson Kernion",
        "Shauna Kravec",
        "Liane Lovitt",
        "Kamal Ndousse",
        "Catherine Olsson",
        "Sam Ringer",
        "Dario Amodei",
        "Tom Brown",
        "Jack Clark",
        "Nicholas Joseph",
        "Ben Mann",
        "Sam McCandlish",
        "Chris Olah",
        "Jared Kaplan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2207.05221v4",
    "pdf_path": "data/dataset/airqa/papers/arxiv2022/ab661558-432d-5e5e-b49c-a3660a40986e.pdf",
    "bibtex": "@misc{kadavath2022languagemodelsmostlyknowwhat,\n    title = {Language Models (Mostly) Know What They Know},\n    author = {Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zac Hatfield-Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},\n    year = {2022},\n    eprint = {2207.05221},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2207.05221},\n}",
    "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
    "num_pages": 43,
    "tldr": "Language models can self-evaluate and predict their knowledge effectively.",
    "tags": [
        "language models",
        "self-evaluation",
        "calibration",
        "open-ended tasks",
        "prediction accuracy"
    ]
}