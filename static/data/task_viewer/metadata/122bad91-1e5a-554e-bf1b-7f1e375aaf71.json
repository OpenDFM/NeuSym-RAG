{
    "uuid": "122bad91-1e5a-554e-bf1b-7f1e375aaf71",
    "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{guo-etal-2023-analyzing,\n    title = \"Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast\",\n    author = \"Guo, Yiduo  and\n      Liang, Yaobo  and\n      Zhao, Dongyan  and\n      Liu, Bing  and\n      Duan, Nan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.221\",\n    doi = \"10.18653/v1/2023.acl-long.221\",\n    pages = \"4002--4017\",\n    abstract = \"Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.\",\n}\n",
    "authors": [
        "Yiduo Guo",
        "Yaobo Liang",
        "Dongyan Zhao",
        "Bing Liu",
        "Nan Duan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.221.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/122bad91-1e5a-554e-bf1b-7f1e375aaf71.pdf",
    "abstract": "Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.",
    "num_pages": 16,
    "tldr": "Method reduces cross-lingual performance gap in multilingual fine-tuning.",
    "tags": [
        "cross-lingual transfer",
        "multilingual models",
        "fine-tuning",
        "performance gap",
        "language model adaptation"
    ]
}