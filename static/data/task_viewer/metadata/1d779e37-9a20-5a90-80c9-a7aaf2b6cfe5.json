{
    "uuid": "1d779e37-9a20-5a90-80c9-a7aaf2b6cfe5",
    "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Todor Mihaylov",
        "Peter Clark",
        "Tushar Khot",
        "Ashish Sabharwal"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.02789v1",
    "pdf_path": "data/dataset/airqa/papers/arxiv2018/1d779e37-9a20-5a90-80c9-a7aaf2b6cfe5.pdf",
    "bibtex": "@misc{mihaylov2018canasuitofarmor,\n    title = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},\n    author = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},\n    year = {2018},\n    eprint = {1809.02789},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1809.02789},\n}",
    "abstract": "We present a new kind of question answering dataset, OpenBookQA, modeled\nafter open book exams for assessing human understanding of a subject. The open\nbook that comes with our questions is a set of 1329 elementary level science\nfacts. Roughly 6000 questions probe an understanding of these facts and their\napplication to novel situations. This requires combining an open book fact\n(e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of\narmor is made of metal) obtained from other sources. While existing QA datasets\nover documents or knowledge bases, being generally self-contained, focus on\nlinguistic understanding, OpenBookQA probes a deeper understanding of both the\ntopic---in the context of common knowledge---and the language it is expressed\nin. Human performance on OpenBookQA is close to 92%, but many state-of-the-art\npre-trained QA methods perform surprisingly poorly, worse than several simple\nneural baselines we develop. Our oracle experiments designed to circumvent the\nknowledge retrieval bottleneck demonstrate the value of both the open book and\nadditional facts. We leave it as a challenge to solve the retrieval problem in\nthis multi-hop setting and to close the large gap to human performance.",
    "num_pages": 14,
    "tldr": "New QA dataset tests fact application and common knowledge comprehension.",
    "tags": [
        "open book exams",
        "question answering",
        "dataset",
        "common knowledge",
        "knowledge retrieval"
    ]
}