{
    "uuid": "0fe6d2d4-00e7-596b-a80c-ffe5a6d88b97",
    "title": "Benchmarking Data Science Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-benchmarking-data,\n    title = \"Benchmarking Data Science Agents\",\n    author = \"Zhang, Yuge  and\n      Jiang, Qiyang  and\n      XingyuHan, XingyuHan  and\n      Chen, Nan  and\n      Yang, Yuqing  and\n      Ren, Kan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.308\",\n    doi = \"10.18653/v1/2024.acl-long.308\",\n    pages = \"5677--5700\",\n    abstract = \"In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval {--} a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.\",\n}\n",
    "authors": [
        "Yuge Zhang",
        "Qiyang Jiang",
        "XingyuHan XingyuHan",
        "Nan Chen",
        "Yuqing Yang",
        "Kan Ren"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.308.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0fe6d2d4-00e7-596b-a80c-ffe5a6d88b97.pdf",
    "abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval â€“ a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
    "num_pages": 24,
    "tldr": "Introducing DSEval: a new paradigm for evaluating data science agents.",
    "tags": [
        "Data Science Agents",
        "Large Language Models",
        "Benchmarking",
        "Evaluation Paradigm",
        "Data Analysis Lifecycle"
    ]
}