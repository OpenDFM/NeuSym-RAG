{
    "uuid": "3e409d3a-1045-575f-b4ad-f4923916080a",
    "title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Adam Liška",
        "Tomáš Kočiský",
        "Elena Gribovskaya",
        "Tayfun Terzi",
        "Eren Sezener",
        "Devang Agrawal",
        "Cyprien de Masson d'Autume",
        "Tim Scholtes",
        "Manzil Zaheer",
        "Susannah Young",
        "Ellen Gilsenan-McMahon",
        "Sophia Austin",
        "Phil Blunsom",
        "Angeliki Lazaridou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.11388v1",
    "pdf_path": "data/dataset/airqa/papers/arxiv2022/3e409d3a-1045-575f-b4ad-f4923916080a.pdf",
    "bibtex": "@misc{lika2022streamingqaabenchmarkforadaptation,\n    title = {StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models},\n    author = {Adam Liška and Tomáš Kočiský and Elena Gribovskaya and Tayfun Terzi and Eren Sezener and Devang Agrawal and Cyprien de Masson d'Autume and Tim Scholtes and Manzil Zaheer and Susannah Young and Ellen Gilsenan-McMahon and Sophia Austin and Phil Blunsom and Angeliki Lazaridou},\n    year = {2022},\n    eprint = {2205.11388},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.11388},\n}",
    "abstract": "Knowledge and language understanding of models evaluated through question\nanswering (QA) has been usually studied on static snapshots of knowledge, like\nWikipedia. However, our world is dynamic, evolves over time, and our models'\nknowledge becomes outdated. To study how semi-parametric QA models and their\nunderlying parametric language models (LMs) adapt to evolving knowledge, we\nconstruct a new large-scale dataset, StreamingQA, with human written and\ngenerated questions asked on a given date, to be answered from 14 years of\ntime-stamped news articles. We evaluate our models quarterly as they read new\narticles not seen in pre-training. We show that parametric models can be\nupdated without full retraining, while avoiding catastrophic forgetting. For\nsemi-parametric models, adding new articles into the search space allows for\nrapid adaptation, however, models with an outdated underlying LM under-perform\nthose with a retrained LM. For questions about higher-frequency named entities,\nparametric updates are particularly beneficial. In our dynamic world, the\nStreamingQA dataset enables a more realistic evaluation of QA models, and our\nexperiments highlight several promising directions for future research.",
    "num_pages": 19,
    "tldr": "Benchmark for QA models adapting to evolving knowledge over time.",
    "tags": [
        "Question Answering",
        "Dynamic Knowledge",
        "Semi-Parametric Models",
        "Language Models",
        "StreamingQA Dataset"
    ]
}