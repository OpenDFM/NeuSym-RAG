{
    "uuid": "2bb974d2-1630-54c3-9eac-4ada100f56ec",
    "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Michael M. Bronstein",
        "Joan Bruna",
        "Taco Cohen",
        "Petar Veličković"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13478v2",
    "pdf_path": "data/dataset/airqa/papers/arxiv2021/2bb974d2-1630-54c3-9eac-4ada100f56ec.pdf",
    "bibtex": "@misc{bronstein2021geometricdeeplearninggridsgroups,\n    title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},\n    author = {Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},\n    year = {2021},\n    eprint = {2104.13478},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2104.13478},\n}",
    "abstract": "The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.",
    "num_pages": 160,
    "tldr": "Geometric deep learning unifies neural networks using geometric principles.",
    "tags": [
        "geometric deep learning",
        "neural networks",
        "representation learning",
        "mathematical framework",
        "architectural principles"
    ]
}