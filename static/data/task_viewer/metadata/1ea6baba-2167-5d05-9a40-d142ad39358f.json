{
    "uuid": "1ea6baba-2167-5d05-9a40-d142ad39358f",
    "title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2024,
    "volume": "NeurIPS 2024 poster",
    "bibtex": "@inproceedings{\nzhang2024knowgpt,\ntitle={Know{GPT}: Knowledge Graph based Prompting for Large Language Models},\nauthor={Qinggang Zhang and Junnan Dong and Hao Chen and Daochen Zha and Zailiang Yu and Xiao Huang},\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\nyear={2024},\nurl={https://openreview.net/forum?id=PacBluO5m7}\n}",
    "authors": [
        "Qinggang Zhang",
        "Junnan Dong",
        "Hao Chen",
        "Daochen Zha",
        "Zailiang Yu",
        "Xiao Huang"
    ],
    "pdf_url": "https://openreview.net/pdf/4ec9739895ff72a71118e7b64bb98e28f109616b.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2024/1ea6baba-2167-5d05-9a40-d142ad39358f.pdf",
    "num_pages": 29,
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in many real-world applications. Nonetheless, LLMs are often criticized for their tendency to produce hallucinations, wherein the models fabricate incorrect statements on tasks beyond their knowledge and perception. To alleviate this issue, graph retrieval-augmented generation (GraphRAG) has been extensively explored which leverages the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles. However, most state-of-the-art LLMs are closed-source, making it challenging to develop a prompting framework that can efficiently and effectively integrate KGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs usually suffer from three critical issues, including huge search space, high API costs, and laborious prompt engineering, that impede their widespread application in practice. To this end, we introduce a novel **Know**ledge **Gr**aph based **P**romp**T**ing framework, namely **KnowGPT**, to enhance LLMs with domain knowledge. KnowGPT contains a knowledge extraction module to extract the most informative knowledge from KGs, and a context-aware prompt construction module to automatically convert extracted knowledge into effective prompts. Experiments on three benchmarks demonstrate that KnowGPT significantly outperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on OpenbookQA leaderboard, comparable to human-level performance.",
    "tldr": "We propse a novel Knowledge Graph based Prompting framework, namely KnowGPT, to leverage the factual knowledge in knowledge graphs (KGs) to ground the LLM's responses in established facts and principles.",
    "tags": [
        "Large language models",
        "knowledge graphs",
        "reasoning"
    ]
}