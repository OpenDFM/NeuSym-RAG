{
    "uuid": "d6b892b8-cf43-5b62-bde9-48c070c2e5dc",
    "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bai-etal-2024-longbench,\n    title = \"{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding\",\n    author = \"Bai, Yushi  and\n      Lv, Xin  and\n      Zhang, Jiajie  and\n      Lyu, Hongchang  and\n      Tang, Jiankai  and\n      Huang, Zhidian  and\n      Du, Zhengxiao  and\n      Liu, Xiao  and\n      Zeng, Aohan  and\n      Hou, Lei  and\n      Dong, Yuxiao  and\n      Tang, Jie  and\n      Li, Juanzi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.172\",\n    doi = \"10.18653/v1/2024.acl-long.172\",\n    pages = \"3119--3137\",\n    abstract = \"Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs{'} long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.\",\n}\n",
    "authors": [
        "Yushi Bai",
        "Xin Lv",
        "Jiajie Zhang",
        "Hongchang Lyu",
        "Jiankai Tang",
        "Zhidian Huang",
        "Zhengxiao Du",
        "Xiao Liu",
        "Aohan Zeng",
        "Lei Hou",
        "Yuxiao Dong",
        "Jie Tang",
        "Juanzi Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.172.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d6b892b8-cf43-5b62-bde9-48c070c2e5dc.pdf",
    "abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMsâ€™ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
    "num_pages": 19,
    "tldr": "LongBench: A bilingual benchmark for evaluating long context understanding in LLMs.",
    "tags": [
        "long context understanding",
        "bilingual benchmark",
        "multitask evaluation",
        "large language models",
        "context compression techniques"
    ]
}