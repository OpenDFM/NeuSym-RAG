{
    "uuid": "46cca6ed-363d-5bcf-8b04-6e8f56b1debb",
    "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.12597v3",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/46cca6ed-363d-5bcf-8b04-6e8f56b1debb.pdf",
    "bibtex": "@misc{li2023blip2bootstrappinglanguageimagepretrainingwith,\n    title = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},\n    author = {Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},\n    year = {2023},\n    eprint = {2301.12597},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2301.12597},\n}",
    "abstract": "The cost of vision-and-language pre-training has become increasingly\nprohibitive due to end-to-end training of large-scale models. This paper\nproposes BLIP-2, a generic and efficient pre-training strategy that bootstraps\nvision-language pre-training from off-the-shelf frozen pre-trained image\nencoders and frozen large language models. BLIP-2 bridges the modality gap with\na lightweight Querying Transformer, which is pre-trained in two stages. The\nfirst stage bootstraps vision-language representation learning from a frozen\nimage encoder. The second stage bootstraps vision-to-language generative\nlearning from a frozen language model. BLIP-2 achieves state-of-the-art\nperformance on various vision-language tasks, despite having significantly\nfewer trainable parameters than existing methods. For example, our model\noutperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable\nparameters. We also demonstrate the model's emerging capabilities of zero-shot\nimage-to-text generation that can follow natural language instructions.",
    "num_pages": 13,
    "tldr": "BLIP-2 efficiently pre-trains vision-language models using frozen encoders and models.",
    "tags": [
        "vision-language pre-training",
        "image encoders",
        "large language models",
        "Querying Transformer",
        "zero-shot image-to-text generation"
    ]
}