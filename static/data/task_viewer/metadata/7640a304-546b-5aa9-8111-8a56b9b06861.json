{
    "uuid": "7640a304-546b-5aa9-8111-8a56b9b06861",
    "title": "Measuring Retrieval Complexity in Question Answering Systems",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{gabburo-etal-2024-measuring,\n    title = \"Measuring Retrieval Complexity in Question Answering Systems\",\n    author = \"Gabburo, Matteo  and\n      Jedema, Nicolaas  and\n      Garg, Siddhant  and\n      Ribeiro, Leonardo  and\n      Moschitti, Alessandro\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.872\",\n    doi = \"10.18653/v1/2024.findings-acl.872\",\n    pages = \"14636--14650\",\n    abstract = \"In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system.Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks. Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty.Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions. Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets.\",\n}\n",
    "authors": [
        "Matteo Gabburo",
        "Nicolaas Jedema",
        "Siddhant Garg",
        "Leonardo Ribeiro",
        "Alessandro Moschitti"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.872.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7640a304-546b-5aa9-8111-8a56b9b06861.pdf",
    "abstract": "In this paper, we investigate which questions are challenging for retrieval-based Question Answering (QA). We (i) propose retrieval complexity (RC), a novel metric conditioned on the completeness of retrieved documents, which measures the difficulty of answering questions, and (ii) propose an unsupervised pipeline to measure RC given an arbitrary retrieval system.Our proposed pipeline measures RC more accurately than alternative estimators, including LLMs, on six challenging QA benchmarks. Further investigation reveals that RC scores strongly correlate with both QA performance and expert judgment across five of the six studied benchmarks, indicating that RC is an effective measure of question difficulty.Subsequent categorization of high-RC questions shows that they span a broad set of question shapes, including multi-hop, compositional, and temporal QA, indicating that RC scores can categorize a new subset of complex questions. Our system can also have a major impact on retrieval-based systems by helping to identify more challenging questions on existing datasets.",
    "num_pages": 15,
    "tldr": "Introduces retrieval complexity metric to assess QA question difficulty.",
    "tags": [
        "retrieval complexity",
        "question answering",
        "unsupervised pipeline",
        "question difficulty",
        "complex questions"
    ]
}