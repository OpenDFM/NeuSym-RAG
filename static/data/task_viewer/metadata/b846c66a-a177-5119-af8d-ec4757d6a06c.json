{
    "uuid": "b846c66a-a177-5119-af8d-ec4757d6a06c",
    "title": "Code Llama: Open Foundation Models for Code",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Baptiste Rozière",
        "Jonas Gehring",
        "Fabian Gloeckle",
        "Sten Sootla",
        "Itai Gat",
        "Xiaoqing Ellen Tan",
        "Yossi Adi",
        "Jingyu Liu",
        "Romain Sauvestre",
        "Tal Remez",
        "Jérémy Rapin",
        "Artyom Kozhevnikov",
        "Ivan Evtimov",
        "Joanna Bitton",
        "Manish Bhatt",
        "Cristian Canton Ferrer",
        "Aaron Grattafiori",
        "Wenhan Xiong",
        "Alexandre Défossez",
        "Jade Copet",
        "Faisal Azhar",
        "Hugo Touvron",
        "Louis Martin",
        "Nicolas Usunier",
        "Thomas Scialom",
        "Gabriel Synnaeve"
    ],
    "pdf_url": "http://arxiv.org/pdf/2308.12950v3",
    "pdf_path": "data/dataset/airqa/papers/arxiv2024/b846c66a-a177-5119-af8d-ec4757d6a06c.pdf",
    "bibtex": "@misc{rozire2024codellamaopenfoundationmodels,\n    title = {Code Llama: Open Foundation Models for Code},\n    author = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},\n    year = {2024},\n    eprint = {2308.12950},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2308.12950},\n}",
    "abstract": "We release Code Llama, a family of large language models for code based on\nLlama 2 providing state-of-the-art performance among open models, infilling\ncapabilities, support for large input contexts, and zero-shot instruction\nfollowing ability for programming tasks. We provide multiple flavors to cover a\nwide range of applications: foundation models (Code Llama), Python\nspecializations (Code Llama - Python), and instruction-following models (Code\nLlama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are\ntrained on sequences of 16k tokens and show improvements on inputs with up to\n100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants\nsupport infilling based on surrounding content. Code Llama reaches\nstate-of-the-art performance among open models on several code benchmarks, with\nscores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code\nLlama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our\nmodels outperform every other publicly available model on MultiPL-E. We release\nCode Llama under a permissive license that allows for both research and\ncommercial use.",
    "num_pages": 48,
    "tldr": "Code Llama: State-of-the-art open models for code with versatile capabilities.",
    "tags": [
        "large language models",
        "code generation",
        "infilling capabilities",
        "Python specialization",
        "open-source models"
    ]
}