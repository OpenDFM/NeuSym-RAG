{
    "uuid": "2bf8095c-81d0-5988-858b-a5155e0cc985",
    "title": "Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{tan-etal-2024-towards,\n    title = \"Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop {QA} Dataset and Pseudo-Instruction Tuning\",\n    author = \"Tan, Qingyu  and\n      Ng, Hwee Tou  and\n      Bing, Lidong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.374\",\n    doi = \"10.18653/v1/2024.findings-acl.374\",\n    pages = \"6272--6286\",\n    abstract = \"Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering (TQA) did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs{'} performance on temporal QA benchmarks by significant margins.\",\n}\n",
    "authors": [
        "Qingyu Tan",
        "Hwee Tou Ng",
        "Lidong Bing"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.374.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2bf8095c-81d0-5988-858b-a5155e0cc985.pdf",
    "abstract": "Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering (TQA) did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMsâ€™ performance on temporal QA benchmarks by significant margins.",
    "num_pages": 15,
    "tldr": "New dataset and strategy enhance LLMs' multi-hop temporal reasoning skills.",
    "tags": [
        "Temporal Reasoning",
        "Large Language Models",
        "Multi-Hop Question Answering",
        "Temporal QA Dataset",
        "Data Augmentation Strategy"
    ]
}