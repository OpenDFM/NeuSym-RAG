{
    "uuid": "357ecfc8-7a31-50d8-93ca-7aaf3e2ec1b1",
    "title": "Designing Toxic Content Classification for a Diversity of Perspectives",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Deepak Kumar",
        "Patrick Gage Kelley",
        "Sunny Consolvo",
        "Joshua Mason",
        "Elie Bursztein",
        "Zakir Durumeric",
        "Kurt Thomas",
        "Michael Bailey"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04511v1",
    "pdf_path": "data/dataset/airqa/papers/arxiv2021/357ecfc8-7a31-50d8-93ca-7aaf3e2ec1b1.pdf",
    "bibtex": "@misc{kumar2021designingtoxiccontentclassificationfor,\n    title = {Designing Toxic Content Classification for a Diversity of Perspectives},\n    author = {Deepak Kumar and Patrick Gage Kelley and Sunny Consolvo and Joshua Mason and Elie Bursztein and Zakir Durumeric and Kurt Thomas and Michael Bailey},\n    year = {2021},\n    eprint = {2106.04511},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.SI},\n    url = {http://arxiv.org/abs/2106.04511},\n}",
    "abstract": "In this work, we demonstrate how existing classifiers for identifying toxic\ncomments online fail to generalize to the diverse concerns of Internet users.\nWe survey 17,280 participants to understand how user expectations for what\nconstitutes toxic content differ across demographics, beliefs, and personal\nexperiences. We find that groups historically at-risk of harassment - such as\npeople who identify as LGBTQ+ or young adults - are more likely to to flag a\nrandom comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who\nhave personally experienced harassment in the past. Based on our findings, we\nshow how current one-size-fits-all toxicity classification algorithms, like the\nPerspective API from Jigsaw, can improve in accuracy by 86% on average through\npersonalized model tuning. Ultimately, we highlight current pitfalls and new\ndesign directions that can improve the equity and efficacy of toxic content\nclassifiers for all users.",
    "num_pages": 24,
    "tldr": "Toxic content classifiers can improve by 86% with personalized tuning for diverse users.",
    "tags": [
        "toxic content classification",
        "diversity of perspectives",
        "personalized model tuning",
        "online harassment",
        "classifier accuracy improvement"
    ]
}