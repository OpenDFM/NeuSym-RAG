{
    "uuid": "16945bc4-e1c2-55bf-8bcd-7203262db0aa",
    "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 Datasets and Benchmarks Poster",
    "bibtex": "@inproceedings{\nbai2023benchmarking,\ntitle={Benchmarking Foundation Models with Language-Model-as-an-Examiner},\nauthor={Yushi Bai and Jiahao Ying and Yixin Cao and Xin Lv and Yuze He and Xiaozhi Wang and Jifan Yu and Kaisheng Zeng and Yijia Xiao and Haozhe Lyu and Jiayin Zhang and Juanzi Li and Lei Hou},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2023},\nurl={https://openreview.net/forum?id=IiRHQ7gvnq}\n}",
    "authors": [
        "Yushi Bai",
        "Jiahao Ying",
        "Yixin Cao",
        "Xin Lv",
        "Yuze He",
        "Xiaozhi Wang",
        "Jifan Yu",
        "Kaisheng Zeng",
        "Yijia Xiao",
        "Haozhe Lyu",
        "Jiayin Zhang",
        "Juanzi Li",
        "Lei Hou"
    ],
    "pdf_url": "https://openreview.net/pdf/a3eb5cfdd673b0a9fde157f65195edb64c7ceb9e.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/16945bc4-e1c2-55bf-8bcd-7203262db0aa.pdf",
    "num_pages": 26,
    "abstract": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans.\nMost of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: http://lmexam.xlore.cn.",
    "tldr": "we propose Language-Model-as-an-Examiner, a novel benchmarking method that utilizes an LM as a knowledgeable examiner to construct dataset and evaluate other models.",
    "tags": [
        "Question answering",
        "Automatic evaluation",
        "Decentralized evaluation"
    ]
}