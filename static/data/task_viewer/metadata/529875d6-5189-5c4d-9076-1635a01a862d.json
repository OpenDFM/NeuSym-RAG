{
    "uuid": "529875d6-5189-5c4d-9076-1635a01a862d",
    "title": "Won’t Get Fooled Again: Answering Questions with False Premises",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{hu-etal-2023-wont,\n    title = \"Won{'}t Get Fooled Again: Answering Questions with False Premises\",\n    author = \"Hu, Shengding  and\n      Luo, Yifan  and\n      Wang, Huadong  and\n      Cheng, Xingyi  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.309\",\n    doi = \"10.18653/v1/2023.acl-long.309\",\n    pages = \"5626--5643\",\n    abstract = \"Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as {``}How many eyes does the sun have?{''}. Such frailties of PLMs often allude to the lack of knowledge within them. In this paper, we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge. To systematize this observation, we investigate the PLMs{'} responses to one kind of tricky questions, i.e., the false premises questions (FPQs). We annotate a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions. Using FalseQA, we discover that PLMs are capable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256) of examples. PLMs also generate reasonable explanations for the false premise, which serve as rebuttals. Further replaying a few general questions during training allows PLMs to excel on FPQs and general questions simultaneously. Our work suggests that once the rebuttal ability is stimulated, knowledge inside the PLMs can be effectively utilized to handle FPQs, which incentivizes the research on PLM-based QA systems. The FalseQA dataset and code are available at \\url{https://github.com/thunlp/FalseQA} .\",\n}\n",
    "authors": [
        "Shengding Hu",
        "Yifan Luo",
        "Huadong Wang",
        "Xingyi Cheng",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.309.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/529875d6-5189-5c4d-9076-1635a01a862d.pdf",
    "abstract": "Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them. In this paper, we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge. To systematize this observation, we investigate the PLMs’ responses to one kind of tricky questions, i.e., the false premises questions (FPQs). We annotate a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions. Using FalseQA, we discover that PLMs are capable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256) of examples. PLMs also generate reasonable explanations for the false premise, which serve as rebuttals. Further replaying a few general questions during training allows PLMs to excel on FPQs and general questions simultaneously. Our work suggests that once the rebuttal ability is stimulated, knowledge inside the PLMs can be effectively utilized to handle FPQs, which incentivizes the research on PLM-based QA systems. The FalseQA dataset and code are available at https://github.com/thunlp/FalseQA .",
    "num_pages": 18,
    "tldr": "PLMs can handle false premise questions by fine-tuning with FalseQA dataset.",
    "tags": [
        "pre-trained language models",
        "question-answering systems",
        "false premises",
        "dataset annotation",
        "knowledge activation"
    ]
}