{
    "uuid": "ca763ccd-4ec8-5b90-9067-ada1af33f8be",
    "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yiheng Xu",
        "Tengchao Lv",
        "Lei Cui",
        "Guoxin Wang",
        "Yijuan Lu",
        "Dinei Florencio",
        "Cha Zhang",
        "Furu Wei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08836v3",
    "pdf_path": "data/dataset/airqa/papers/arxiv2021/ca763ccd-4ec8-5b90-9067-ada1af33f8be.pdf",
    "bibtex": "@misc{xu2021layoutxlmmultimodalpretrainingformultilingual,\n    title = {LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding},\n    author = {Yiheng Xu and Tengchao Lv and Lei Cui and Guoxin Wang and Yijuan Lu and Dinei Florencio and Cha Zhang and Furu Wei},\n    year = {2021},\n    eprint = {2104.08836},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08836},\n}",
    "abstract": "Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.",
    "num_pages": 10,
    "tldr": "LayoutXLM excels in multilingual, multimodal document understanding with a new benchmark.",
    "tags": [
        "multimodal pre-training",
        "multilingual document understanding",
        "visually-rich documents",
        "cross-lingual model",
        "benchmark dataset"
    ]
}