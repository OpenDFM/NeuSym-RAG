{
    "uuid": "3107f6a8-1939-5af0-b3d8-06d7aa66158d",
    "title": "On the Compositional Generalization in Versatile Open-domain Dialogue",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fu-etal-2023-compositional,\n    title = \"On the Compositional Generalization in Versatile Open-domain Dialogue\",\n    author = \"Fu, Tingchen  and\n      Zhao, Xueliang  and\n      Liu, Lemao  and\n      Yan, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.760\",\n    doi = \"10.18653/v1/2023.acl-long.760\",\n    pages = \"13585--13605\",\n    abstract = \"Previous research has demonstrated the potential of multi-task learning to foster a conversational agent{'}s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse knowledge and skills learned from other datasets. In contrast to previous works, we develop a sparsely activated modular network: (1) We propose a well-rounded set of operators and instantiate each operator with an independent module; (2) We formulate dialogue generation as the execution of a generated programme which recursively composes and assembles modules. Extensive experiments on 9 datasets verify the efficacy of our methods through automatic evaluation and human evaluation. Notably, our model outperforms state-of-the-art supervised approaches on 4 datasets with only 10{\\%} training data thanks to the modular architecture and multi-task learning.\",\n}\n",
    "authors": [
        "Tingchen Fu",
        "Xueliang Zhao",
        "Lemao Liu",
        "Rui Yan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.760.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3107f6a8-1939-5af0-b3d8-06d7aa66158d.pdf",
    "abstract": "Previous research has demonstrated the potential of multi-task learning to foster a conversational agentâ€™s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse knowledge and skills learned from other datasets. In contrast to previous works, we develop a sparsely activated modular network: (1) We propose a well-rounded set of operators and instantiate each operator with an independent module; (2) We formulate dialogue generation as the execution of a generated programme which recursively composes and assembles modules. Extensive experiments on 9 datasets verify the efficacy of our methods through automatic evaluation and human evaluation. Notably, our model outperforms state-of-the-art supervised approaches on 4 datasets with only 10% training data thanks to the modular architecture and multi-task learning.",
    "num_pages": 21,
    "tldr": "Modular network improves dialogue generation with minimal training data.",
    "tags": [
        "compositional generalization",
        "open-domain dialogue",
        "multi-task learning",
        "modular network",
        "negative transfer"
    ]
}