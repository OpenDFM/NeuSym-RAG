{
    "uuid": "58fb66aa-41bf-59a9-8e27-d9effa1f81aa",
    "title": "Masked Structural Growth for 2x Faster Language Model Pre-training",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nyao2024masked,\ntitle={Masked Structural Growth for 2x Faster Language Model Pre-training},\nauthor={Yiqun Yao and Zheng Zhang and Jing Li and Yequan Wang},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=rL7xsg1aRn}\n}",
    "authors": [
        "Yiqun Yao",
        "Zheng Zhang",
        "Jing Li",
        "Yequan Wang"
    ],
    "pdf_url": "https://openreview.net/pdf/bbb7c09286b80a46c623073658efbf3411c35198.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/58fb66aa-41bf-59a9-8e27-d9effa1f81aa.pdf",
    "num_pages": 18,
    "abstract": "Accelerating large language model pre-training is a critical issue in present research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth: determining the optimal growth schedule, and designing efficient growth operators. In terms of growth schedule, the impact of each single dimension on a scheduleâ€™s efficiency is underexplored by existing work. Regarding the growth operators, existing methods rely on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further improvements on training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including (i) growth schedules involving all possible dimensions and (ii) strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is significantly faster than related work: we achieve up to 2.2x speedup in pre-training different types of language models while maintaining comparable or better downstream performances. Code is publicly available at https://github.com/cofe-ai/MSG.",
    "tldr": "We grow up language models in pre-training with efficient schedules and function-preserving operators that yields 2x speedup.",
    "tags": [
        "Progressive Learning",
        "Large Language Models",
        "Model Growth"
    ]
}