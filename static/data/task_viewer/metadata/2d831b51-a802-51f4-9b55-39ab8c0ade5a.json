{
    "uuid": "2d831b51-a802-51f4-9b55-39ab8c0ade5a",
    "title": "Efficient Streaming Language Models with Attention Sinks",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nxiao2024efficient,\ntitle={Efficient Streaming Language Models with Attention Sinks},\nauthor={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=NG7sS51zVF}\n}",
    "authors": [
        "Guangxuan Xiao",
        "Yuandong Tian",
        "Beidi Chen",
        "Song Han",
        "Mike Lewis"
    ],
    "pdf_url": "https://openreview.net/pdf/5e9b035b5224d38807ae1ff0a27d972cf3fd10a8.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/2d831b51-a802-51f4-9b55-39ab8c0ade5a.pdf",
    "num_pages": 21,
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\nFirstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.\nSecondly, popular LLMs cannot generalize to longer texts than the training sequence length.\nWindow attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size.\nWe observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.\nBased on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.\nWe show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\nIn addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2$\\times$ speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
    "tldr": "We enable LLMs to work on infinite-length texts without compromising efficiency and performance.",
    "tags": [
        "Large Language Models",
        "Length Extrapolation",
        "Efficiency"
    ]
}