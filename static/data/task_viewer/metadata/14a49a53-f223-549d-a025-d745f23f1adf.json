{
    "uuid": "14a49a53-f223-549d-a025-d745f23f1adf",
    "title": "Compressive Transformers for Long-Range Sequence Modelling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Jack W. Rae",
        "Anna Potapenko",
        "Siddhant M. Jayakumar",
        "Timothy P. Lillicrap"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.05507v1",
    "pdf_path": "data/dataset/airqa/papers/arxiv2019/14a49a53-f223-549d-a025-d745f23f1adf.pdf",
    "bibtex": "@misc{rae2019compressivetransformersforlongrangesequence,\n    title = {Compressive Transformers for Long-Range Sequence Modelling},\n    author = {Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Timothy P. Lillicrap},\n    year = {2019},\n    eprint = {1911.05507},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1911.05507},\n}",
    "abstract": "We present the Compressive Transformer, an attentive sequence model which\ncompresses past memories for long-range sequence learning. We find the\nCompressive Transformer obtains state-of-the-art language modelling results in\nthe WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc\nrespectively. We also find it can model high-frequency speech effectively and\ncan be used as a memory mechanism for RL, demonstrated on an object matching\ntask. To promote the domain of long-range sequence learning, we propose a new\nopen-vocabulary language modelling benchmark derived from books, PG-19.",
    "num_pages": 19,
    "tldr": "Compressive Transformer excels in long-range sequence and language modeling.",
    "tags": [
        "Compressive Transformers",
        "long-range sequence learning",
        "language modelling",
        "memory mechanism",
        "speech modelling"
    ]
}