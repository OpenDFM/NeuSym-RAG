{
    "uuid": "67f92ccd-ca66-5cd5-b6ac-3852a53255e2",
    "title": "Llemma: An Open Language Model for Mathematics",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nazerbayev2024llemma,\ntitle={Llemma: An Open Language Model for Mathematics},\nauthor={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=4WnqRR915j}\n}",
    "authors": [
        "Zhangir Azerbayev",
        "Hailey Schoelkopf",
        "Keiran Paster",
        "Marco Dos Santos",
        "Stephen Marcus McAleer",
        "Albert Q. Jiang",
        "Jia Deng",
        "Stella Biderman",
        "Sean Welleck"
    ],
    "pdf_url": "https://openreview.net/pdf/be75740b06066f002fed0867925b737ec1f8757f.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/67f92ccd-ca66-5cd5-b6ac-3852a53255e2.pdf",
    "num_pages": 28,
    "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
    "tldr": "Llemma is a superior open math language model with robust theorem proving capabilities.",
    "tags": [
        "reasoning",
        "language models",
        "pretraining"
    ]
}