{
    "uuid": "f398b6e5-0ff2-59e4-9f26-eba5cea5b48c",
    "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Fei Xia",
        "Ed Chi",
        "Quoc Le",
        "Denny Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.11903v6",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/f398b6e5-0ff2-59e4-9f26-eba5cea5b48c.pdf",
    "bibtex": "@misc{wei2023chainofthoughtpromptingelicitsreasoningin,\n    title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},\n    author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},\n    year = {2023},\n    eprint = {2201.11903},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.11903},\n}",
    "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.",
    "num_pages": 43,
    "tldr": "Chain-of-thought prompting enhances reasoning in large language models.",
    "tags": [
        "Chain-of-thought prompting",
        "reasoning in language models",
        "arithmetic reasoning",
        "commonsense reasoning",
        "symbolic reasoning"
    ]
}