{
    "uuid": "4badd0e5-53ce-5044-b6c6-abec723c34aa",
    "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Qinkai Zheng",
        "Xiao Xia",
        "Xu Zou",
        "Yuxiao Dong",
        "Shan Wang",
        "Yufei Xue",
        "Zihan Wang",
        "Lei Shen",
        "Andi Wang",
        "Yang Li",
        "Teng Su",
        "Zhilin Yang",
        "Jie Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.17568v2",
    "pdf_path": "data/dataset/airqa/papers/arxiv2024/4badd0e5-53ce-5044-b6c6-abec723c34aa.pdf",
    "bibtex": "@misc{zheng2024codegeexapretrainedmodelfor,\n    title = {CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},\n    author = {Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},\n    year = {2024},\n    eprint = {2303.17568},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2303.17568},\n}",
    "abstract": "Large pre-trained code generation models, such as OpenAI Codex, can generate\nsyntax- and function-correct code, making the coding of programmers more\nproductive and our pursuit of artificial general intelligence closer. In this\npaper, we introduce CodeGeeX, a multilingual model with 13 billion parameters\nfor code generation. CodeGeeX is pre-trained on 850 billion tokens of 23\nprogramming languages as of June 2022. Our extensive experiments suggest that\nCodeGeeX outperforms multilingual code models of similar scale for both the\ntasks of code generation and translation on HumanEval-X. Building upon\nHumanEval (Python only), we develop the HumanEval-X benchmark for evaluating\nmultilingual models by hand-writing the solutions in C++, Java, JavaScript, and\nGo. In addition, we build CodeGeeX-based extensions on Visual Studio Code,\nJetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of\nthousands of active users per week. Our user study demonstrates that CodeGeeX\ncan help to increase coding efficiency for 83.4% of its users. Finally,\nCodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code,\nmodel weights (the version of 850B tokens), API, extensions, and HumanEval-X at\nhttps://github.com/THUDM/CodeGeeX.",
    "num_pages": 30,
    "tldr": "CodeGeeX: A multilingual code generation model improving coding efficiency and open-sourced.",
    "tags": [
        "Code generation",
        "Multilingual model",
        "HumanEval-X",
        "Pre-trained model",
        "Programming languages"
    ]
}