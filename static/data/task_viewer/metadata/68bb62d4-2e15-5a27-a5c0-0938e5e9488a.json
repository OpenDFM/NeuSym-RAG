{
    "uuid": "68bb62d4-2e15-5a27-a5c0-0938e5e9488a",
    "title": "AdaMerging: Adaptive Model Merging for Multi-Task Learning",
    "conference_full": "International Conference on Learning Representations",
    "conference": "ICLR",
    "year": 2024,
    "volume": "ICLR 2024 poster",
    "bibtex": "@inproceedings{\nyang2024adamerging,\ntitle={AdaMerging: Adaptive Model Merging for Multi-Task Learning},\nauthor={Enneng Yang and Zhenyi Wang and Li Shen and Shiwei Liu and Guibing Guo and Xingwei Wang and Dacheng Tao},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=nZP6NgD3QY}\n}",
    "authors": [
        "Enneng Yang",
        "Zhenyi Wang",
        "Li Shen",
        "Shiwei Liu",
        "Guibing Guo",
        "Xingwei Wang",
        "Dacheng Tao"
    ],
    "pdf_url": "https://openreview.net/pdf/3e0e893b9ff9066c7f5cbb919559f855594e638c.pdf",
    "pdf_path": "data/dataset/airqa/papers/iclr2024/68bb62d4-2e15-5a27-a5c0-0938e5e9488a.pdf",
    "num_pages": 21,
    "abstract": "Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging method operates as an automatic, unsupervised task arithmetic scheme. It leverages entropy minimization on unlabeled test samples from the multi-task setup as a surrogate objective function to iteratively refine the merging coefficients of the multiple models. Our experimental findings across eight tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared to the current state-of-the-art (SOTA) task arithmetic merging scheme, AdaMerging showcases a remarkable 11\\% improvement in performance. Notably, AdaMerging also exhibits superior generalization capabilities when applied to unseen downstream tasks. Furthermore, it displays a significantly enhanced robustness to data distribution shifts that may occur during the testing phase.",
    "tldr": "This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging) to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data.",
    "tags": [
        "Multi-task Learning",
        "Model Editing",
        "Task Arithmetic"
    ]
}