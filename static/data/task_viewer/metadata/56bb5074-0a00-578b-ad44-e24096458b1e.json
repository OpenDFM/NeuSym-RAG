{
    "uuid": "56bb5074-0a00-578b-ad44-e24096458b1e",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Maarten Sap",
        "Ronan LeBras",
        "Daniel Fried",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.13312v2",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/56bb5074-0a00-578b-ad44-e24096458b1e.pdf",
    "bibtex": "@misc{sap2023neuraltheoryofmindonthelimits,\n    title = {Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},\n    author = {Maarten Sap and Ronan LeBras and Daniel Fried and Yejin Choi},\n    year = {2023},\n    eprint = {2210.13312},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.13312},\n}",
    "abstract": "Social intelligence and Theory of Mind (ToM), i.e., the ability to reason\nabout the different mental states, intents, and reactions of all people\ninvolved, allow humans to effectively navigate and understand everyday social\ninteractions. As NLP systems are used in increasingly complex social\nsituations, their ability to grasp social dynamics becomes crucial. In this\nwork, we examine the open question of social intelligence and Theory of Mind in\nmodern NLP systems from an empirical and theory-based perspective. We show that\none of today's largest language models (GPT-3; Brown et al., 2020) lacks this\nkind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et\nal., 2019), which measures models' ability to understand intents and reactions\nof participants of social interactions, and ToMi (Le et al., 2019), which\nmeasures whether models can infer mental states and realities of participants\nof situations. Our results show that models struggle substantially at these\nTheory of Mind tasks, with well-below-human accuracies of 55% and 60% on\nSocialIQa and ToMi, respectively. To conclude, we draw on theories from\npragmatics to contextualize this shortcoming of large language models, by\nexamining the limitations stemming from their data, neural architecture, and\ntraining paradigms. Challenging the prevalent narrative that only scale is\nneeded, we posit that person-centric NLP approaches might be more effective\ntowards neural Theory of Mind.\n  In our updated version, we also analyze newer instruction tuned and RLFH\nmodels for neural ToM. We find that even ChatGPT and GPT-4 do not display\nemergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on\nthe ToMi questions related to mental states and realities.",
    "num_pages": 23,
    "tldr": "Large LMs struggle with Theory of Mind, showing limited social intelligence.",
    "tags": [
        "Theory of Mind",
        "Social intelligence",
        "Large language models",
        "Neural architectures",
        "Natural Language Processing"
    ]
}