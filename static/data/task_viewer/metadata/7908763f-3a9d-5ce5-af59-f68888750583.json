{
    "uuid": "7908763f-3a9d-5ce5-af59-f68888750583",
    "title": "Finetuned Language Models Are Zero-Shot Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Vincent Y. Zhao",
        "Kelvin Guu",
        "Adams Wei Yu",
        "Brian Lester",
        "Nan Du",
        "Andrew M. Dai",
        "Quoc V. Le"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.01652v5",
    "pdf_path": "data/dataset/airqa/papers/arxiv2022/7908763f-3a9d-5ce5-af59-f68888750583.pdf",
    "bibtex": "@misc{wei2022finetunedlanguagemodelsarezeroshot,\n    title = {Finetuned Language Models Are Zero-Shot Learners},\n    author = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},\n    year = {2022},\n    eprint = {2109.01652},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.01652},\n}",
    "abstract": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.",
    "num_pages": 46,
    "tldr": "Instruction-tuning boosts zero-shot performance in language models like FLAN.",
    "tags": [
        "Zero-shot learning",
        "Language models",
        "Instruction tuning",
        "NLP tasks",
        "Performance evaluation"
    ]
}