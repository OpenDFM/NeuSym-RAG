{
    "uuid": "7b9d2080-37d8-593b-93e9-abfcb2aead4a",
    "title": "Reflexion: language agents with verbal reinforcement learning",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\nshinn2023reflexion,\ntitle={Reflexion: language agents with verbal reinforcement learning},\nauthor={Noah Shinn and Federico Cassano and Ashwin Gopinath and Karthik R Narasimhan and Shunyu Yao},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=vAElhFcKW6}\n}",
    "authors": [
        "Noah Shinn",
        "Federico Cassano",
        "Ashwin Gopinath",
        "Karthik R Narasimhan",
        "Shunyu Yao"
    ],
    "pdf_url": "https://openreview.net/pdf/4e4b27c12dee4aa2e8102061a1882b04f1753477.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/7b9d2080-37d8-593b-93e9-abfcb2aead4a.pdf",
    "num_pages": 19,
    "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose \\emph{Reflexion}, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. We release all code, demos, and datasets at \\url{https://github.com/noahshinn024/reflexion}.",
    "tldr": "Reflexion is a framework that reinforces language agents by updating language rather than model weights.",
    "tags": [
        "language model",
        "reasoning",
        "decision making",
        "programming"
    ]
}