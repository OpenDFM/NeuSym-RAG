{
    "uuid": "6a224ba5-c711-5435-b425-9bacbcd552a6",
    "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yuzhen Huang",
        "Yuzhuo Bai",
        "Zhihao Zhu",
        "Junlei Zhang",
        "Jinghan Zhang",
        "Tangjun Su",
        "Junteng Liu",
        "Chuancheng Lv",
        "Yikai Zhang",
        "Jiayi Lei",
        "Yao Fu",
        "Maosong Sun",
        "Junxian He"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.08322v3",
    "pdf_path": "data/dataset/airqa/papers/arxiv2023/6a224ba5-c711-5435-b425-9bacbcd552a6.pdf",
    "bibtex": "@misc{huang2023cevalamultilevelmultidisciplinechinese,\n    title = {C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models},\n    author = {Yuzhen Huang and Yuzhuo Bai and Zhihao Zhu and Junlei Zhang and Jinghan Zhang and Tangjun Su and Junteng Liu and Chuancheng Lv and Yikai Zhang and Jiayi Lei and Yao Fu and Maosong Sun and Junxian He},\n    year = {2023},\n    eprint = {2305.08322},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.08322},\n}",
    "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.",
    "num_pages": 20,
    "tldr": "C-Eval: A Chinese evaluation suite for foundation models across 52 disciplines.",
    "tags": [
        "Chinese NLP",
        "evaluation suite",
        "foundation models",
        "multi-discipline assessment",
        "large language models"
    ]
}