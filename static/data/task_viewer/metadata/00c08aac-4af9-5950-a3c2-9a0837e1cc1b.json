{
    "uuid": "00c08aac-4af9-5950-a3c2-9a0837e1cc1b",
    "title": "Diverse Conventions for Human-AI Collaboration",
    "conference_full": "Conference on Neural Information Processing Systems",
    "conference": "NeurIPS",
    "year": 2023,
    "volume": "NeurIPS 2023 poster",
    "bibtex": "@inproceedings{\nsarkar2023diverse,\ntitle={Diverse Conventions for Human-{AI} Collaboration},\nauthor={Bidipta Sarkar and Andy Shih and Dorsa Sadigh},\nbooktitle={Thirty-seventh Conference on Neural Information Processing Systems},\nyear={2023},\nurl={https://openreview.net/forum?id=MljeRycu9s}\n}",
    "authors": [
        "Bidipta Sarkar",
        "Andy Shih",
        "Dorsa Sadigh"
    ],
    "pdf_url": "https://openreview.net/pdf/fea0ad613ee9182b8a36c9cb009bc0236fa5c739.pdf",
    "pdf_path": "data/dataset/airqa/papers/neurips2023/00c08aac-4af9-5950-a3c2-9a0837e1cc1b.pdf",
    "num_pages": 25,
    "abstract": "Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce mixed-play, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.",
    "tldr": "We present CoMeDi, a technique for generating a diverse set of good-faith conventions in multi-agent coordination games, and demonstrate its utility for human-AI collaboration.",
    "tags": [
        "Multi-Agent RL",
        "Multi-Agent Coordination",
        "Human-AI Coordination"
    ]
}