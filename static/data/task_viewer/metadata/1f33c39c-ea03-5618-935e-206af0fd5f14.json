{
    "uuid": "1f33c39c-ea03-5618-935e-206af0fd5f14",
    "title": "Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bhardwaj-etal-2024-language,\n    title = \"Language Models are {H}omer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic\",\n    author = \"Bhardwaj, Rishabh  and\n      Do, Duc Anh  and\n      Poria, Soujanya\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.762\",\n    doi = \"10.18653/v1/2024.acl-long.762\",\n    pages = \"14138--14149\",\n    abstract = \"We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6{\\%} to 5.1{\\%} and from 9.2{\\%} to 1.5{\\%} in parameter-efficient and full fine-tuning, respectively, while maintaining most of the model{'}s performance on the task. We release the source codes at: https://github.com/declare-lab/resta.\",\n}\n",
    "authors": [
        "Rishabh Bhardwaj",
        "Duc Anh Do",
        "Soujanya Poria"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.762.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1f33c39c-ea03-5618-935e-206af0fd5f14.pdf",
    "abstract": "We propose RESTA to perform LLM realignment towards safety, which gets compromised due to downstream task fine-tuning. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full fine-tuning, respectively, while maintaining most of the modelâ€™s performance on the task. We release the source codes at: https://github.com/declare-lab/resta.",
    "num_pages": 12,
    "tldr": "RESTA improves LLM safety post-fine-tuning using a simple safety vector addition.",
    "tags": [
        "safety alignment",
        "fine-tuned language models",
        "task arithmetic",
        "harmful content mitigation",
        "multilingual benchmarks"
    ]
}