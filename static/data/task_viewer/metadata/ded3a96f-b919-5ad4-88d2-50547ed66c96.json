{
    "uuid": "ded3a96f-b919-5ad4-88d2-50547ed66c96",
    "title": "TinyLlama: An Open-Source Small Language Model",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Peiyuan Zhang",
        "Guangtao Zeng",
        "Tianduo Wang",
        "Wei Lu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2401.02385v2",
    "pdf_path": "data/dataset/airqa/papers/arxiv2024/ded3a96f-b919-5ad4-88d2-50547ed66c96.pdf",
    "bibtex": "@misc{zhang2024tinyllamaanopensourcesmalllanguage,\n    title = {TinyLlama: An Open-Source Small Language Model},\n    author = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},\n    year = {2024},\n    eprint = {2401.02385},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2401.02385},\n}",
    "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention and Lit-GPT), achieving better\ncomputational efficiency. Despite its relatively small size, TinyLlama\ndemonstrates remarkable performance in a series of downstream tasks. It\nsignificantly outperforms existing open-source language models with comparable\nsizes. Our model checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.",
    "num_pages": 10,
    "tldr": "TinyLlama: 1.1B model excels in tasks, outperforming similar open-source models.",
    "tags": [
        "TinyLlama",
        "language model",
        "open-source",
        "computational efficiency",
        "downstream tasks"
    ]
}