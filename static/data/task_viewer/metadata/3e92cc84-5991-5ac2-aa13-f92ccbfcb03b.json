{
    "uuid": "3e92cc84-5991-5ac2-aa13-f92ccbfcb03b",
    "title": "Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hu-etal-2024-continual,\n    title = \"Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition\",\n    author = \"Hu, Jianpeng  and\n      Tan, Chengxiang  and\n      Xu, JiaCheng  and\n      XiangyunKong, XiangyunKong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.702\",\n    doi = \"10.18653/v1/2024.findings-acl.702\",\n    pages = \"11805--11816\",\n    abstract = \"Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting. In this paper, we propose the concept of the following degree from the perspective of instability to analyze catastrophic forgetting and design a novel method based on adaptive gradient correction and knowledge decomposition to alleviate catastrophic forgetting. Specifically, the adaptive gradient correction algorithm is designed to limit the instability of embeddings, which adaptively constrains the current gradient to be orthogonal to the embedding space learned from previous tasks. To reduce the instability between samples and prototypes, the knowledge decomposition module decomposes knowledge into general and task-related knowledge from the perspective of model architecture, which is asynchronously optimized during training. Experimental results on two standard benchmarks show that our method outperforms the state-of-the-art CFRE model and effectively improves the following degree of embeddings.\",\n}\n",
    "authors": [
        "Jianpeng Hu",
        "Chengxiang Tan",
        "JiaCheng Xu",
        "XiangyunKong XiangyunKong"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.702.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3e92cc84-5991-5ac2-aa13-f92ccbfcb03b.pdf",
    "abstract": "Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting. In this paper, we propose the concept of the following degree from the perspective of instability to analyze catastrophic forgetting and design a novel method based on adaptive gradient correction and knowledge decomposition to alleviate catastrophic forgetting. Specifically, the adaptive gradient correction algorithm is designed to limit the instability of embeddings, which adaptively constrains the current gradient to be orthogonal to the embedding space learned from previous tasks. To reduce the instability between samples and prototypes, the knowledge decomposition module decomposes knowledge into general and task-related knowledge from the perspective of model architecture, which is asynchronously optimized during training. Experimental results on two standard benchmarks show that our method outperforms the state-of-the-art CFRE model and effectively improves the following degree of embeddings.",
    "num_pages": 12,
    "tldr": "Improves continual few-shot relation extraction with adaptive gradient correction.",
    "tags": [
        "Continual Learning",
        "Few-shot Learning",
        "Relation Extraction",
        "Catastrophic Forgetting",
        "Gradient Correction"
    ]
}