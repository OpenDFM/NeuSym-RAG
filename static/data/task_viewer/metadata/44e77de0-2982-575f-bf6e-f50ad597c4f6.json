{
    "uuid": "44e77de0-2982-575f-bf6e-f50ad597c4f6",
    "title": "Mixtral of Experts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Albert Q. Jiang",
        "Alexandre Sablayrolles",
        "Antoine Roux",
        "Arthur Mensch",
        "Blanche Savary",
        "Chris Bamford",
        "Devendra Singh Chaplot",
        "Diego de las Casas",
        "Emma Bou Hanna",
        "Florian Bressand",
        "Gianna Lengyel",
        "Guillaume Bour",
        "Guillaume Lample",
        "Lélio Renard Lavaud",
        "Lucile Saulnier",
        "Marie-Anne Lachaux",
        "Pierre Stock",
        "Sandeep Subramanian",
        "Sophia Yang",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Théophile Gervet",
        "Thibaut Lavril",
        "Thomas Wang",
        "Timothée Lacroix",
        "William El Sayed"
    ],
    "pdf_url": "http://arxiv.org/pdf/2401.04088v1",
    "pdf_path": "data/dataset/airqa/papers/arxiv2024/44e77de0-2982-575f-bf6e-f50ad597c4f6.pdf",
    "bibtex": "@misc{jiang2024mixtralofexperts,\n    title = {Mixtral of Experts},\n    author = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},\n    year = {2024},\n    eprint = {2401.04088},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2401.04088},\n}",
    "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
    "num_pages": 13,
    "tldr": "Mixtral 8x7B excels in language tasks, outperforming larger models.",
    "tags": [
        "Sparse Mixture of Experts",
        "language model",
        "performance benchmarks",
        "model fine-tuning",
        "Apache 2.0 license"
    ]
}