{
    "uuid": "58bd1994-d7e2-55c9-a194-9daf63eb3e6c",
    "title": "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
    "conference_full": "The 2024 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2024,
    "volume": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{chen-etal-2024-towards-injecting,\n    title = \"Towards Injecting Medical Visual Knowledge into Multimodal {LLM}s at Scale\",\n    author = \"Chen, Junying  and\n      Gui, Chi  and\n      Ouyang, Ruyi  and\n      Gao, Anningzhe  and\n      Chen, Shunian  and\n      Chen, Guiming Hardy  and\n      Wang, Xidong  and\n      Cai, Zhenyang  and\n      Ji, Ke  and\n      Wan, Xiang  and\n      Wang, Benyou\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.418\",\n    doi = \"10.18653/v1/2024.emnlp-main.418\",\n    pages = \"7346--7370\",\n    abstract = \"The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed{'}s large-scale, de-identified medical image-text pairs to address these limitations, they often fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an {`}unblinded{'} capacity to denoise and reformat the data, resulting in the creation of the **PubMedVision** dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of MLLMs, showing significant improvement in benchmarks including the MMMU Health {\\&} Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM **HuatuoGPT-Vision**, which shows superior performance in medical multimodal scenarios among open-source MLLMs. Our code and data are available at https://github.com/FreedomIntelligence/HuatuoGPT-Vision.\",\n}\n",
    "authors": [
        "Junying Chen",
        "Chi Gui",
        "Ruyi Ouyang",
        "Anningzhe Gao",
        "Shunian Chen",
        "Guiming Hardy Chen",
        "Xidong Wang",
        "Zhenyang Cai",
        "Ke Ji",
        "Xiang Wan",
        "Benyou Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.emnlp-main.418.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2024/58bd1994-d7e2-55c9-a194-9daf63eb3e6c.pdf",
    "num_pages": 25,
    "abstract": "The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed’s large-scale, de-identified medical image-text pairs to address these limitations, they often fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an ‘unblinded’ capacity to denoise and reformat the data, resulting in the creation of the **PubMedVision** dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of MLLMs, showing significant improvement in benchmarks including the MMMU Health & Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM **HuatuoGPT-Vision**, which shows superior performance in medical multimodal scenarios among open-source MLLMs. Our code and data are available at https://github.com/FreedomIntelligence/HuatuoGPT-Vision.",
    "tldr": "PubMedVision improves medical MLLMs' performance with refined image-text data.",
    "tags": [
        "medical multimodal models",
        "PubMedVision dataset",
        "medical image-text pairs",
        "GPT-4V",
        "data denoising techniques"
    ]
}