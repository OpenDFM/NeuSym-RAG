{
    "uuid": "2c222763-f33b-5cfa-8897-5d217aaf9142",
    "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
    "conference_full": "The 2023 Conference on Empirical Methods in Natural Language Processing",
    "conference": "EMNLP",
    "year": 2023,
    "volume": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    "bibtex": "@inproceedings{kim-etal-2023-visually,\n    title = \"Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models\",\n    author = \"Kim, Geewook  and\n      Lee, Hodong  and\n      Kim, Daehee  and\n      Jung, Haeji  and\n      Park, Sanghee  and\n      Kim, Yoonsik  and\n      Yun, Sangdoo  and\n      Kil, Taeho  and\n      Lee, Bado  and\n      Park, Seunghyun\",\n    editor = \"Bouamor, Houda  and\n      Pino, Juan  and\n      Bali, Kalika\",\n    booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\",\n    month = dec,\n    year = \"2023\",\n    address = \"Singapore\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.emnlp-main.735\",\n    doi = \"10.18653/v1/2023.emnlp-main.735\",\n    pages = \"11989--12010\",\n    abstract = \"Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream.\",\n}\n",
    "authors": [
        "Geewook Kim",
        "Hodong Lee",
        "Daehee Kim",
        "Haeji Jung",
        "Sanghee Park",
        "Yoonsik Kim",
        "Sangdoo Yun",
        "Taeho Kil",
        "Bado Lee",
        "Seunghyun Park"
    ],
    "pdf_url": "https://aclanthology.org/2023.emnlp-main.735.pdf",
    "pdf_path": "data/dataset/airqa/papers/emnlp2023/2c222763-f33b-5cfa-8897-5d217aaf9142.pdf",
    "num_pages": 22,
    "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream.",
    "tldr": "Introducing Cream: a model enhancing LLMs for better visual-language understanding.",
    "tags": [
        "Natural Language Understanding",
        "Large Language Models",
        "Visual Document Understanding",
        "Contrastive Reading Model",
        "Vision-Language Integration"
    ]
}