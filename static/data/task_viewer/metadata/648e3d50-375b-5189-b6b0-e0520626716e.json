{
    "uuid": "648e3d50-375b-5189-b6b0-e0520626716e",
    "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-zhang-2024-look,\n    title = \"You Only Look at Screens: Multimodal Chain-of-Action Agents\",\n    author = \"Zhang, Zhuosheng  and\n      Zhang, Aston\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.186\",\n    doi = \"10.18653/v1/2024.findings-acl.186\",\n    pages = \"3132--3149\",\n    abstract = \"Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique{---}leveraging a series of intermediate previous action histories and future action plans{---}to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30$K$ unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90{\\%} and an overall action success rate of 74{\\%}. Code is publicly available at https://github.com/cooelf/Auto-GUI.\",\n}\n",
    "authors": [
        "Zhuosheng Zhang",
        "Aston Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.186.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/648e3d50-375b-5189-b6b0-e0520626716e.pdf",
    "abstract": "Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique—leveraging a series of intermediate previous action histories and future action plans—to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-GUI.",
    "num_pages": 18,
    "tldr": "Auto-GUI enhances GUI automation with multimodal action prediction accuracy.",
    "tags": [
        "Autonomous GUI agents",
        "Multimodal interaction",
        "Chain-of-action technique",
        "Task automation",
        "Large language models"
    ]
}