{
    "uuid": "333cbb4d-64c4-5497-a942-650407cd4044",
    "question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What specific adaptation does the model proposed in the paper employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?",
    "answer_format": "You should answer the question in a short-answer form. Do not provide long answers.",
    "tags": [
        "multiple",
        "table",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "4a71c642-ce6a-51d3-bd55-bc5764d46bb8"
    ],
    "reference_pdf": [
        "49b4cdbd-7730-5f6c-9be1-e404ddf81009",
        "88ac1798-77b6-575e-bb56-686a270f2b90",
        "565281f7-0639-57ef-8326-b0a5736453ec",
        "1ad849ac-0315-597e-9953-3dc6a95ebc07",
        "b978ad55-c35e-5206-931e-7722a286eb77",
        "1d8acec7-e518-5232-876e-1239dd9798a2",
        "020081a0-ba8e-58cd-a3e4-92898afdbcdd",
        "3d59acd4-6d28-53c7-97fb-59c76ada4070",
        "b678de9e-7ab0-5414-b09d-e1b5330c2124",
        "34ea827e-75b7-5db5-8f5f-635ac55b4a5e",
        "c628e6e6-81bd-5601-8cef-2499ac25f581",
        "9048f37a-221a-5252-a6ff-2e7feb5258b3",
        "d9298bb6-d0eb-5b7b-a81a-47c4ae28352a",
        "0c590283-50e8-5f8a-b1b5-a0ba83a07b07",
        "626f7394-4e6e-551d-9786-dc5d0949561d",
        "83c3e134-0000-581c-a6f2-d311645b274b",
        "f09c794e-86fa-5594-90f7-3c9301329a5e",
        "fa3e6c04-231f-5c2b-9ae1-20f91f081351",
        "d0859a0a-4bc5-5b3e-828d-f5f818fd0227",
        "dc908cce-31ca-5bef-a3b1-552c1cb2e887",
        "cdef8abf-e135-5090-b146-94b4a3840672",
        "ca763ccd-4ec8-5b90-9067-ada1af33f8be",
        "097d0250-9e1c-51bf-acd7-0cb0d6baa5a4",
        "332fef4a-3940-5d3a-aba6-2c27ce658f5d",
        "6ea15eac-4060-5f1d-8629-76330e0b67c5",
        "e45656e2-9cd5-5c57-95dc-45d0e35c23d6",
        "10ff59fb-9509-502b-b27a-1cba18082292",
        "fc1799fc-b2be-559a-81ad-5851732795be",
        "91f68f07-6cb0-53d1-98a1-3f3061d6ef44",
        "60ae5f99-3a2c-5936-b306-3333c1463463",
        "2996caf3-f7a5-515a-ba60-091b02f7c9e5",
        "819d0208-b342-5a31-a2ab-da64c204544e",
        "3a6897a4-2ea0-5070-a155-1d8c36764d68",
        "29ca8367-936e-562d-a161-00e163499a28",
        "05f9c218-644f-5c4c-81b2-a66f192586c8",
        "ac1b0430-6781-5539-9d45-5067c0c6ff3e"
    ],
    "conference": [],
    "reasoning_steps": [],
    "evaluator": {
        "eval_func": "eval_m3sciqa",
        "eval_kwargs": {
            "question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What specific adaptation does the model proposed in the paper employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?",
            "reference_answer": "To prevent the language-specific pre-processing, LayoutXLM decides to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, it calculates the bounding box of each token by merging the bounding boxes of all characters it contains, efficiently unifying the multilingual multimodal inputs."
        }
    },
    "state": {},
    "annotator": "m3sciqa",
    "anchor_image": [
        "data/dataset/m3sciqa/images/2310.11016/comparison_table.png"
    ]
}