{
    "uuid": "95236a53-f0c4-5644-b094-924a6970ac26",
    "question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. What specific strategy does the model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?",
    "answer_format": "You should answer the question in a short-answer form. Do not provide long answers.",
    "tags": [
        "multiple",
        "table",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "00ae9127-82dd-5bbd-9d0f-c2a7a58d65fa"
    ],
    "reference_pdf": [
        "7e44dc95-9f92-5653-9246-572cc25c5d22",
        "7bc3753a-e1cb-5635-805e-bdb98255c704",
        "91f2e795-bf7f-52df-bfae-2b6525a56d60",
        "fdb748a6-d777-5a99-8537-5c0d524d277d",
        "193b12c4-871c-53cf-9497-523742859b8d",
        "34ea827e-75b7-5db5-8f5f-635ac55b4a5e",
        "f32f540f-cb2f-51b7-9bb9-89934eb68916",
        "e8b94d55-54d3-5c7e-ab41-e6af7c9b8b03",
        "058b61e1-dbec-5ca7-9603-d53c1e14e733",
        "77146f7f-edd6-57d3-a190-c37c96243ea3",
        "8fbd30bf-37b1-5741-8aaa-e4fdbb0468f5",
        "30a603dc-f798-5bef-bc07-78a6882b1cff",
        "e45897f5-4429-5750-a8fb-dcfa9a904b5f",
        "c76b968a-995a-5109-a4eb-f329fa710f26",
        "2292ac5f-ddf9-5ed5-8009-b4f7a69a8ec9",
        "4f312f60-4fb7-53a0-b6d7-7810cd39eb43",
        "26e45afd-1c91-5f0f-bb47-33707acec072",
        "9ada7bff-c684-55ab-ae9b-04f836247ddc",
        "afaf79d2-ff40-538d-9a27-d932a5d41d8e",
        "c811c5a5-c22a-5fd6-9f1c-1ffa1bde7c47",
        "ae10df12-cb06-58ac-a746-6f941ee929e3",
        "ac1b0430-6781-5539-9d45-5067c0c6ff3e"
    ],
    "conference": [],
    "reasoning_steps": [],
    "evaluator": {
        "eval_func": "eval_m3sciqa",
        "eval_kwargs": {
            "question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. What specific strategy does the model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?",
            "reference_answer": "BERT uses a mixed strategy for masking the target tokens during the Masked LM pre-training. The probabilities associated with each part of this strategy are as follows:\n- 80% of the time, the word is replaced with the [MASK] token.\n- 10% of the time, the word is replaced with a random word.\n- 10% of the time, the original word is kept unchanged."
        }
    },
    "state": {},
    "annotator": "m3sciqa",
    "anchor_image": [
        "data/dataset/m3sciqa/images/2310.18343/result_table.png"
    ]
}