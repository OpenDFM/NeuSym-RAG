{
    "uuid": "f8566398-9ed0-577d-9f9c-78c1b182920b",
    "title": "Enriching and Controlling Global Semantics for Text Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Thong Nguyen",
        "Anh Tuan Luu",
        "Truc Lu",
        "Tho Quan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10616v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f8566398-9ed0-577d-9f9c-78c1b182920b.pdf",
    "bibtex": "@misc{nguyen2021enrichingandcontrollingglobalsemantics,\n    title = {Enriching and Controlling Global Semantics for Text Summarization},\n    author = {Thong Nguyen and Anh Tuan Luu and Truc Lu and Tho Quan},\n    year = {2021},\n    eprint = {2109.10616},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.10616},\n}",
    "abstract": "Recently, Transformer-based models have been proven effective in the\nabstractive summarization task by creating fluent and informative summaries.\nNevertheless, these models still suffer from the short-range dependency\nproblem, causing them to produce summaries that miss the key points of\ndocument. In this paper, we attempt to address this issue by introducing a\nneural topic model empowered with normalizing flow to capture the global\nsemantics of the document, which are then integrated into the summarization\nmodel. In addition, to avoid the overwhelming effect of global semantics on\ncontextualized representation, we introduce a mechanism to control the amount\nof global semantics supplied to the text generation module. Our method\noutperforms state-of-the-art summarization models on five common text\nsummarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and\nPubMed.",
    "num_pages": 14
}