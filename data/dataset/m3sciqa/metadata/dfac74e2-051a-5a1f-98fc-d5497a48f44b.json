{
    "uuid": "dfac74e2-051a-5a1f-98fc-d5497a48f44b",
    "title": "Neural Machine Translation of Rare Words with Subword Units",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
    ],
    "pdf_url": "http://arxiv.org/pdf/1508.07909v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\dfac74e2-051a-5a1f-98fc-d5497a48f44b.pdf",
    "bibtex": "@misc{sennrich2016neuralmachinetranslationofrare,\n    title = {Neural Machine Translation of Rare Words with Subword Units},\n    author = {Rico Sennrich and Barry Haddow and Alexandra Birch},\n    year = {2016},\n    eprint = {1508.07909},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1508.07909},\n}",
    "abstract": "Neural machine translation (NMT) models typically operate with a fixed\nvocabulary, but translation is an open-vocabulary problem. Previous work\naddresses the translation of out-of-vocabulary words by backing off to a\ndictionary. In this paper, we introduce a simpler and more effective approach,\nmaking the NMT model capable of open-vocabulary translation by encoding rare\nand unknown words as sequences of subword units. This is based on the intuition\nthat various word classes are translatable via smaller units than words, for\ninstance names (via character copying or transliteration), compounds (via\ncompositional translation), and cognates and loanwords (via phonological and\nmorphological transformations). We discuss the suitability of different word\nsegmentation techniques, including simple character n-gram models and a\nsegmentation based on the byte pair encoding compression algorithm, and\nempirically show that subword models improve over a back-off dictionary\nbaseline for the WMT 15 translation tasks English-German and English-Russian by\n1.1 and 1.3 BLEU, respectively.",
    "num_pages": 11
}