{
    "uuid": "c6401b44-ad7a-5948-88ad-b180075d7dbe",
    "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yan Song",
        "Tong Zhang",
        "Yonggang Wang",
        "Kai-Fu Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.01279v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\c6401b44-ad7a-5948-88ad-b180075d7dbe.pdf",
    "bibtex": "@misc{song2021zen20continuetrainingand,\n    title = {ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders},\n    author = {Yan Song and Tong Zhang and Yonggang Wang and Kai-Fu Lee},\n    year = {2021},\n    eprint = {2105.01279},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2105.01279},\n}",
    "abstract": "Pre-trained text encoders have drawn sustaining attention in natural language\nprocessing (NLP) and shown their capability in obtaining promising results in\ndifferent tasks. Recent studies illustrated that external self-supervised\nsignals (or knowledge extracted by unsupervised learning, such as n-grams) are\nbeneficial to provide useful semantic evidence for understanding languages such\nas Chinese, so as to improve the performance on various downstream tasks\naccordingly. To further enhance the encoders, in this paper, we propose to\npre-train n-gram-enhanced encoders with a large volume of data and advanced\ntechniques for training. Moreover, we try to extend the encoder to different\nlanguages as well as different domains, where it is confirmed that the same\narchitecture is applicable to these varying circumstances and new\nstate-of-the-art performance is observed from a long list of NLP tasks across\nlanguages and domains.",
    "num_pages": 13
}