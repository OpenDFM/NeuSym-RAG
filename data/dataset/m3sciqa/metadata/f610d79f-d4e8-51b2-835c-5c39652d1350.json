{
    "uuid": "f610d79f-d4e8-51b2-835c-5c39652d1350",
    "title": "Improving Unsupervised Question Answering via Summarization-Informed Question Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Chenyang Lyu",
        "Lifeng Shang",
        "Yvette Graham",
        "Jennifer Foster",
        "Xin Jiang",
        "Qun Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.07954v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f610d79f-d4e8-51b2-835c-5c39652d1350.pdf",
    "bibtex": "@misc{lyu2021improvingunsupervisedquestionansweringvia,\n    title = {Improving Unsupervised Question Answering via Summarization-Informed Question Generation},\n    author = {Chenyang Lyu and Lifeng Shang and Yvette Graham and Jennifer Foster and Xin Jiang and Qun Liu},\n    year = {2021},\n    eprint = {2109.07954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.07954},\n}",
    "abstract": "Question Generation (QG) is the task of generating a plausible question for a\ngiven <passage, answer> pair. Template-based QG uses linguistically-informed\nheuristics to transform declarative sentences into interrogatives, whereas\nsupervised QG uses existing Question Answering (QA) datasets to train a system\nto generate a question given a passage and an answer. A disadvantage of the\nheuristic approach is that the generated questions are heavily tied to their\ndeclarative counterparts. A disadvantage of the supervised approach is that\nthey are heavily tied to the domain/language of the QA dataset used as training\ndata. In order to overcome these shortcomings, we propose an unsupervised QG\nmethod which uses questions generated heuristically from summaries as a source\nof training data for a QG system. We make use of freely available news summary\ndata, transforming declarative summary sentences into appropriate questions\nusing heuristics informed by dependency parsing, named entity recognition and\nsemantic role labeling. The resulting questions are then combined with the\noriginal news articles to train an end-to-end neural QG model. We extrinsically\nevaluate our approach using unsupervised QA: our QG model is used to generate\nsynthetic QA pairs for training a QA model. Experimental results show that,\ntrained with only 20k English Wikipedia-based synthetic QA pairs, the QA model\nsubstantially outperforms previous unsupervised models on three in-domain\ndatasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain\ndatasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the\napproach.",
    "num_pages": 15
}