{
    "uuid": "3134099b-d3ac-56d3-898d-c77c7a99370e",
    "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Zhenzhong Lan",
        "Mingda Chen",
        "Sebastian Goodman",
        "Kevin Gimpel",
        "Piyush Sharma",
        "Radu Soricut"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.11942v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\3134099b-d3ac-56d3-898d-c77c7a99370e.pdf",
    "bibtex": "@misc{lan2020albertalitebertfor,\n    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},\n    author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},\n    year = {2020},\n    eprint = {1909.11942},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.11942},\n}",
    "abstract": "Increasing model size when pretraining natural language representations often\nresults in improved performance on downstream tasks. However, at some point\nfurther model increases become harder due to GPU/TPU memory limitations and\nlonger training times. To address these problems, we present two\nparameter-reduction techniques to lower memory consumption and increase the\ntraining speed of BERT. Comprehensive empirical evidence shows that our\nproposed methods lead to models that scale much better compared to the original\nBERT. We also use a self-supervised loss that focuses on modeling\ninter-sentence coherence, and show it consistently helps downstream tasks with\nmulti-sentence inputs. As a result, our best model establishes new\nstate-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having\nfewer parameters compared to BERT-large. The code and the pretrained models are\navailable at https://github.com/google-research/ALBERT.",
    "num_pages": 17
}