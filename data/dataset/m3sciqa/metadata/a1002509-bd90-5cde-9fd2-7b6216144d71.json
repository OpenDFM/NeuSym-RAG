{
    "uuid": "a1002509-bd90-5cde-9fd2-7b6216144d71",
    "title": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Minghao Xu",
        "Xinyu Yuan",
        "Santiago Miret",
        "Jian Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.12040v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\a1002509-bd90-5cde-9fd2-7b6216144d71.pdf",
    "bibtex": "@misc{xu2023protstmultimodalitylearningofprotein,\n    title = {ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts},\n    author = {Minghao Xu and Xinyu Yuan and Santiago Miret and Jian Tang},\n    year = {2023},\n    eprint = {2301.12040},\n    archivePrefix = {arXiv},\n    primaryClass = {q-bio.BM},\n    url = {http://arxiv.org/abs/2301.12040},\n}",
    "abstract": "Current protein language models (PLMs) learn protein representations mainly\nbased on their sequences, thereby well capturing co-evolutionary information,\nbut they are unable to explicitly acquire protein functions, which is the end\ngoal of protein representation learning. Fortunately, for many proteins, their\ntextual property descriptions are available, where their various functions are\nalso described. Motivated by this fact, we first build the ProtDescribe dataset\nto augment protein sequences with text descriptions of their functions and\nother important properties. Based on this dataset, we propose the ProtST\nframework to enhance Protein Sequence pre-training and understanding by\nbiomedical Texts. During pre-training, we design three types of tasks, i.e.,\nunimodal mask prediction, multimodal representation alignment and multimodal\nmask prediction, to enhance a PLM with protein property information with\ndifferent granularities and, at the same time, preserve the PLM's original\nrepresentation power. On downstream tasks, ProtST enables both supervised\nlearning and zero-shot prediction. We verify the superiority of ProtST-induced\nPLMs over previous ones on diverse representation learning benchmarks. Under\nthe zero-shot setting, we show the effectiveness of ProtST on zero-shot protein\nclassification, and ProtST also enables functional protein retrieval from a\nlarge-scale database without any function annotation.",
    "num_pages": 19
}