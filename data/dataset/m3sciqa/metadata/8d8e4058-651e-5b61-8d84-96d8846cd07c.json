{
    "uuid": "8d8e4058-651e-5b61-8d84-96d8846cd07c",
    "title": "Factuality Enhanced Language Models for Open-Ended Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Nayeon Lee",
        "Wei Ping",
        "Peng Xu",
        "Mostofa Patwary",
        "Pascale Fung",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.04624v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\8d8e4058-651e-5b61-8d84-96d8846cd07c.pdf",
    "bibtex": "@misc{lee2023factualityenhancedlanguagemodelsfor,\n    title = {Factuality Enhanced Language Models for Open-Ended Text Generation},\n    author = {Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},\n    year = {2023},\n    eprint = {2206.04624},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2206.04624},\n}",
    "abstract": "Pretrained language models (LMs) are susceptible to generate text with\nnonfactual information. In this work, we measure and improve the factual\naccuracy of large-scale LMs for open-ended text generation. We design the\nFactualityPrompts test set and metrics to measure the factuality of LM\ngenerations. Based on that, we study the factual accuracy of LMs with parameter\nsizes ranging from 126M to 530B. Interestingly, we find that larger LMs are\nmore factual than smaller ones, although a previous study suggests that larger\nLMs can be less truthful in terms of misconceptions. In addition, popular\nsampling algorithms (e.g., top-p) in open-ended text generation can harm the\nfactuality due to the ''uniform randomness'' introduced at every sampling step.\nWe propose the factual-nucleus sampling algorithm that dynamically adapts the\nrandomness to improve the factuality of generation while maintaining quality.\nFurthermore, we analyze the inefficiencies of the standard training method in\nlearning correct associations between entities from factual text corpus (e.g.,\nWikipedia). We propose a factuality-enhanced training method that uses\nTopicPrefix for better awareness of facts and sentence completion as the\ntraining objective, which can vastly reduce the factual errors. We release our\ncode and FactualityPrompts benchmark at:\nhttps://github.com/nayeon7lee/FactualityPrompt.",
    "num_pages": 24
}