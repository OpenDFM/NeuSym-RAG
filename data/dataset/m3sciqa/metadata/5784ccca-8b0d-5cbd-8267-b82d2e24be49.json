{
    "uuid": "5784ccca-8b0d-5cbd-8267-b82d2e24be49",
    "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Da Yin",
        "Li Dong",
        "Hao Cheng",
        "Xiaodong Liu",
        "Kai-Wei Chang",
        "Furu Wei",
        "Jianfeng Gao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.08772v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\5784ccca-8b0d-5cbd-8267-b82d2e24be49.pdf",
    "bibtex": "@misc{yin2022asurveyofknowledgeintensivenlp,\n    title = {A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models},\n    author = {Da Yin and Li Dong and Hao Cheng and Xiaodong Liu and Kai-Wei Chang and Furu Wei and Jianfeng Gao},\n    year = {2022},\n    eprint = {2202.08772},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2202.08772},\n}",
    "abstract": "With the increasing of model capacity brought by pre-trained language models,\nthere emerges boosting needs for more knowledgeable natural language processing\n(NLP) models with advanced functionalities including providing and making\nflexible use of encyclopedic and commonsense knowledge. The mere pre-trained\nlanguage models, however, lack the capacity of handling such\nknowledge-intensive NLP tasks alone. To address this challenge, large numbers\nof pre-trained language models augmented with external knowledge sources are\nproposed and in rapid development. In this paper, we aim to summarize the\ncurrent progress of pre-trained language model-based knowledge-enhanced models\n(PLMKEs) by dissecting their three vital elements: knowledge sources,\nknowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we\npresent the challenges of PLMKEs based on the discussion regarding the three\nelements and attempt to provide NLP practitioners with potential directions for\nfurther research.",
    "num_pages": 8
}