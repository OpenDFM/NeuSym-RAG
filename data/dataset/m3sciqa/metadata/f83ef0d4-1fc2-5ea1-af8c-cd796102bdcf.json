{
    "uuid": "f83ef0d4-1fc2-5ea1-af8c-cd796102bdcf",
    "title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tianduo Wang",
        "Wei Lu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.01707v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f83ef0d4-1fc2-5ea1-af8c-cd796102bdcf.pdf",
    "bibtex": "@misc{wang2023learningmultistepreasoningbysolving,\n    title = {Learning Multi-Step Reasoning by Solving Arithmetic Tasks},\n    author = {Tianduo Wang and Wei Lu},\n    year = {2023},\n    eprint = {2306.01707},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.01707},\n}",
    "abstract": "Mathematical reasoning is regarded as a necessary ability for Language Models\n(LMs). Recent works demonstrate large LMs' impressive performance in solving\nmath problems. The success is attributed to their Chain-of-Thought (CoT)\nreasoning abilities, i.e., the ability to decompose complex questions into\nstep-by-step reasoning chains, but such ability seems only to emerge from\nmodels with abundant parameters. This work investigates how to incorporate\nrelatively small LMs with the capabilities of multi-step reasoning. We propose\nto inject such abilities by continually pre-training LMs on a synthetic dataset\nMsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four\nmath word problem datasets show the effectiveness of the proposed method in\nenhancing LMs' math reasoning abilities.",
    "num_pages": 8
}