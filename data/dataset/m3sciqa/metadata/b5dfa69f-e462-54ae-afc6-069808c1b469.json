{
    "uuid": "b5dfa69f-e462-54ae-afc6-069808c1b469",
    "title": "Unsupervised Opinion Summarization as Copycat-Review Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Arthur Bražinskas",
        "Mirella Lapata",
        "Ivan Titov"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.02247v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\b5dfa69f-e462-54ae-afc6-069808c1b469.pdf",
    "bibtex": "@misc{brainskas2020unsupervisedopinionsummarizationascopycatreview,\n    title = {Unsupervised Opinion Summarization as Copycat-Review Generation},\n    author = {Arthur Bražinskas and Mirella Lapata and Ivan Titov},\n    year = {2020},\n    eprint = {1911.02247},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1911.02247},\n}",
    "abstract": "Opinion summarization is the task of automatically creating summaries that\nreflect subjective information expressed in multiple documents, such as product\nreviews. While the majority of previous work has focused on the extractive\nsetting, i.e., selecting fragments from input reviews to produce a summary, we\nlet the model generate novel sentences and hence produce abstractive summaries.\nRecent progress in summarization has seen the development of supervised models\nwhich rely on large quantities of document-summary pairs. Since such training\ndata is expensive to acquire, we instead consider the unsupervised setting, in\nother words, we do not use any summaries in training. We define a generative\nmodel for a review collection which capitalizes on the intuition that when\ngenerating a new review given a set of other reviews of a product, we should be\nable to control the \"amount of novelty\" going into the new review or,\nequivalently, vary the extent to which it deviates from the input. At test\ntime, when generating summaries, we force the novelty to be minimal, and\nproduce a text reflecting consensus opinions. We capture this intuition by\ndefining a hierarchical variational autoencoder model. Both individual reviews\nand the products they correspond to are associated with stochastic latent\ncodes, and the review generator (\"decoder\") has direct access to the text of\ninput reviews through the pointer-generator mechanism. Experiments on Amazon\nand Yelp datasets, show that setting at test time the review's latent code to\nits mean, allows the model to produce fluent and coherent summaries reflecting\ncommon opinions.",
    "num_pages": 19
}