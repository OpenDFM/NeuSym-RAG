{
    "uuid": "adf24566-930f-5a42-8118-6aeb65e4f6f4",
    "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zequn Liu",
        "Wei Zhang",
        "Yingce Xia",
        "Lijun Wu",
        "Shufang Xie",
        "Tao Qin",
        "Ming Zhang",
        "Tie-Yan Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.10688v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\adf24566-930f-5a42-8118-6aeb65e4f6f4.pdf",
    "bibtex": "@misc{liu2023molxptwrappingmoleculeswithtext,\n    title = {MolXPT: Wrapping Molecules with Text for Generative Pre-training},\n    author = {Zequn Liu and Wei Zhang and Yingce Xia and Lijun Wu and Shufang Xie and Tao Qin and Ming Zhang and Tie-Yan Liu},\n    year = {2023},\n    eprint = {2305.10688},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.10688},\n}",
    "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success\nin natural language processing and related techniques have been adapted into\nmolecular modeling. Considering that text is the most important record for\nscientific discovery, in this paper, we propose MolXPT, a unified language\nmodel of text and molecules pre-trained on SMILES (a sequence representation of\nmolecules) wrapped by text. Briefly, we detect the molecule names in each\nsequence and replace them to the corresponding SMILES. In this way, the SMILES\ncould leverage the information from surrounding text, and vice versa. The above\nwrapped sequences, text sequences from PubMed and SMILES sequences from PubChem\nare all fed into a language model for pre-training. Experimental results\ndemonstrate that MolXPT outperforms strong baselines of molecular property\nprediction on MoleculeNet, performs comparably to the best model in\ntext-molecule translation while using less than half of its parameters, and\nenables zero-shot molecular generation without finetuning.",
    "num_pages": 9
}