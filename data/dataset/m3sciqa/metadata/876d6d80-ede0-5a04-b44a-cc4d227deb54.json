{
    "uuid": "876d6d80-ede0-5a04-b44a-cc4d227deb54",
    "title": "Accelerating Deep Learning by Focusing on the Biggest Losers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Angela H. Jiang",
        "Daniel L. -K. Wong",
        "Giulio Zhou",
        "David G. Andersen",
        "Jeffrey Dean",
        "Gregory R. Ganger",
        "Gauri Joshi",
        "Michael Kaminksy",
        "Michael Kozuch",
        "Zachary C. Lipton",
        "Padmanabhan Pillai"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00762v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\876d6d80-ede0-5a04-b44a-cc4d227deb54.pdf",
    "bibtex": "@misc{jiang2019acceleratingdeeplearningbyfocusing,\n    title = {Accelerating Deep Learning by Focusing on the Biggest Losers},\n    author = {Angela H. Jiang and Daniel L. -K. Wong and Giulio Zhou and David G. Andersen and Jeffrey Dean and Gregory R. Ganger and Gauri Joshi and Michael Kaminksy and Michael Kozuch and Zachary C. Lipton and Padmanabhan Pillai},\n    year = {2019},\n    eprint = {1910.00762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1910.00762},\n}",
    "abstract": "This paper introduces Selective-Backprop, a technique that accelerates the\ntraining of deep neural networks (DNNs) by prioritizing examples with high loss\nat each iteration. Selective-Backprop uses the output of a training example's\nforward pass to decide whether to use that example to compute gradients and\nupdate parameters, or to skip immediately to the next example. By reducing the\nnumber of computationally-expensive backpropagation steps performed,\nSelective-Backprop accelerates training. Evaluation on CIFAR10, CIFAR100, and\nSVHN, across a variety of modern image models, shows that Selective-Backprop\nconverges to target error rates up to 3.5x faster than with standard SGD and\nbetween 1.02--1.8x faster than a state-of-the-art importance sampling approach.\nFurther acceleration of 26% can be achieved by using stale forward pass results\nfor selection, thus also skipping forward passes of low priority examples.",
    "num_pages": 14
}