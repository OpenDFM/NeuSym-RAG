{
    "uuid": "e44ba5fc-2846-5b06-923b-eb11cc9b2e51",
    "title": "An Interpretable Reasoning Network for Multi-Relation Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Mantong Zhou",
        "Minlie Huang",
        "Xiaoyan Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1801.04726v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\e44ba5fc-2846-5b06-923b-eb11cc9b2e51.pdf",
    "bibtex": "@misc{zhou2018aninterpretablereasoningnetworkfor,\n    title = {An Interpretable Reasoning Network for Multi-Relation Question Answering},\n    author = {Mantong Zhou and Minlie Huang and Xiaoyan Zhu},\n    year = {2018},\n    eprint = {1801.04726},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1801.04726},\n}",
    "abstract": "Multi-relation Question Answering is a challenging task, due to the\nrequirement of elaborated analysis on questions and reasoning over multiple\nfact triples in knowledge base. In this paper, we present a novel model called\nInterpretable Reasoning Network that employs an interpretable, hop-by-hop\nreasoning process for question answering. The model dynamically decides which\npart of an input question should be analyzed at each hop; predicts a relation\nthat corresponds to the current parsed results; utilizes the predicted relation\nto update the question representation and the state of the reasoning process;\nand then drives the next-hop reasoning. Experiments show that our model yields\nstate-of-the-art results on two datasets. More interestingly, the model can\noffer traceable and observable intermediate predictions for reasoning analysis\nand failure diagnosis, thereby allowing manual manipulation in predicting the\nfinal answer.",
    "num_pages": 13
}