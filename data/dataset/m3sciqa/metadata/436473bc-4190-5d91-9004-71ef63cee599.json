{
    "uuid": "436473bc-4190-5d91-9004-71ef63cee599",
    "title": "Probing Pretrained Language Models for Lexical Semantics",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Ivan Vulić",
        "Edoardo Maria Ponti",
        "Robert Litschko",
        "Goran Glavaš",
        "Anna Korhonen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.05731v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\436473bc-4190-5d91-9004-71ef63cee599.pdf",
    "bibtex": "@misc{vuli2020probingpretrainedlanguagemodelsfor,\n    title = {Probing Pretrained Language Models for Lexical Semantics},\n    author = {Ivan Vulić and Edoardo Maria Ponti and Robert Litschko and Goran Glavaš and Anna Korhonen},\n    year = {2020},\n    eprint = {2010.05731},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.05731},\n}",
    "abstract": "The success of large pretrained language models (LMs) such as BERT and\nRoBERTa has sparked interest in probing their representations, in order to\nunveil what types of knowledge they implicitly capture. While prior research\nfocused on morphosyntactic, semantic, and world knowledge, it remains unclear\nto which extent LMs also derive lexical type-level knowledge from words in\ncontext. In this work, we present a systematic empirical analysis across six\ntypologically diverse languages and five different lexical tasks, addressing\nthe following questions: 1) How do different lexical knowledge extraction\nstrategies (monolingual versus multilingual source LM, out-of-context versus\nin-context encoding, inclusion of special tokens, and layer-wise averaging)\nimpact performance? How consistent are the observed effects across tasks and\nlanguages? 2) Is lexical knowledge stored in few parameters, or is it scattered\nthroughout the network? 3) How do these representations fare against\ntraditional static word vectors in lexical tasks? 4) Does the lexical\ninformation emerging from independently trained monolingual LMs display latent\nsimilarities? Our main results indicate patterns and best practices that hold\nuniversally, but also point to prominent variations across languages and tasks.\nMoreover, we validate the claim that lower Transformer layers carry more\ntype-level lexical knowledge, but also show that this knowledge is distributed\nacross multiple layers.",
    "num_pages": 19
}