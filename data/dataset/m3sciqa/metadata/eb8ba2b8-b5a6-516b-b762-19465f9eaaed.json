{
    "uuid": "eb8ba2b8-b5a6-516b-b762-19465f9eaaed",
    "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Moin Nadeem",
        "Anna Bethke",
        "Siva Reddy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.09456v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\eb8ba2b8-b5a6-516b-b762-19465f9eaaed.pdf",
    "bibtex": "@misc{nadeem2020stereosetmeasuringstereotypicalbiasin,\n    title = {StereoSet: Measuring stereotypical bias in pretrained language models},\n    author = {Moin Nadeem and Anna Bethke and Siva Reddy},\n    year = {2020},\n    eprint = {2004.09456},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.09456},\n}",
    "abstract": "A stereotype is an over-generalized belief about a particular group of\npeople, e.g., Asians are good at math or Asians are bad drivers. Such beliefs\n(biases) are known to hurt target groups. Since pretrained language models are\ntrained on large real world data, they are known to capture stereotypical\nbiases. In order to assess the adverse effects of these models, it is important\nto quantify the bias captured in them. Existing literature on quantifying bias\nevaluates pretrained language models on a small set of artificially constructed\nbias-assessing sentences. We present StereoSet, a large-scale natural dataset\nin English to measure stereotypical biases in four domains: gender, profession,\nrace, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and\nXLNet on our dataset and show that these models exhibit strong stereotypical\nbiases. We also present a leaderboard with a hidden test set to track the bias\nof future language models at https://stereoset.mit.edu",
    "num_pages": 15
}