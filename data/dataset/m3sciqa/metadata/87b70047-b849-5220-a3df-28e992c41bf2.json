{
    "uuid": "87b70047-b849-5220-a3df-28e992c41bf2",
    "title": "A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Neeraj Varshney",
        "Wenlin Yao",
        "Hongming Zhang",
        "Jianshu Chen",
        "Dong Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2307.03987v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\87b70047-b849-5220-a3df-28e992c41bf2.pdf",
    "bibtex": "@misc{varshney2023astitchintimesaves,\n    title = {A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation},\n    author = {Neeraj Varshney and Wenlin Yao and Hongming Zhang and Jianshu Chen and Dong Yu},\n    year = {2023},\n    eprint = {2307.03987},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2307.03987},\n}",
    "abstract": "Recently developed large language models have achieved remarkable success in\ngenerating fluent and coherent text. However, these models often tend to\n'hallucinate' which critically hampers their reliability. In this work, we\naddress this crucial problem and propose an approach that actively detects and\nmitigates hallucinations during the generation process. Specifically, we first\nidentify the candidates of potential hallucination leveraging the model's logit\noutput values, check their correctness through a validation procedure, mitigate\nthe detected hallucinations, and then continue with the generation process.\nThrough extensive experiments with GPT-3.5 (text-davinci-003) on the 'article\ngeneration task', we first demonstrate the individual efficacy of our detection\nand mitigation techniques. Specifically, the detection technique achieves a\nrecall of ~88% and the mitigation technique successfully mitigates 57.6% of the\ncorrectly detected hallucinations. Importantly, our mitigation technique does\nnot introduce new hallucinations even in the case of incorrectly detected\nhallucinations, i.e., false positives. Then, we show that the proposed active\ndetection and mitigation approach successfully reduces the hallucinations of\nthe GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the\neffectiveness and wide applicability of our approach through additional studies\nincluding performance on different types of questions (multi-hop and false\npremise questions) and with another LLM from a different model family (Vicuna).\nIn summary, our work contributes to improving the reliability and\ntrustworthiness of large language models, a crucial step en route to enabling\ntheir widespread adoption in real-world applications.",
    "num_pages": 23
}