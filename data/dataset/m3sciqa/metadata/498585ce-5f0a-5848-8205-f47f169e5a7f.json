{
    "uuid": "498585ce-5f0a-5848-8205-f47f169e5a7f",
    "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Subhabrata Mukherjee",
        "Arindam Mitra",
        "Ganesh Jawahar",
        "Sahaj Agarwal",
        "Hamid Palangi",
        "Ahmed Awadallah"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.02707v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\498585ce-5f0a-5848-8205-f47f169e5a7f.pdf",
    "bibtex": "@misc{mukherjee2023orcaprogressivelearningfromcomplex,\n    title = {Orca: Progressive Learning from Complex Explanation Traces of GPT-4},\n    author = {Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n    year = {2023},\n    eprint = {2306.02707},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.02707},\n}",
    "abstract": "Recent research has focused on enhancing the capability of smaller models\nthrough imitation learning, drawing on the outputs generated by large\nfoundation models (LFMs). A number of issues impact the quality of these\nmodels, ranging from limited imitation signals from shallow LFM outputs; small\nscale homogeneous training data; and most notably a lack of rigorous evaluation\nresulting in overestimating the small model's capability as they tend to learn\nto imitate the style, but not the reasoning process of LFMs. To address these\nchallenges, we develop Orca (We are working with our legal team to publicly\nrelease a diff of the model weights in accordance with LLaMA's release policy\nto be published at https://aka.ms/orca-lm), a 13-billion parameter model that\nlearns to imitate the reasoning process of LFMs. Orca learns from rich signals\nfrom GPT-4 including explanation traces; step-by-step thought processes; and\nother complex instructions, guided by teacher assistance from ChatGPT. To\npromote this progressive learning, we tap into large-scale and diverse\nimitation data with judicious sampling and selection. Orca surpasses\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B by\nmore than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard\n(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH\nbenchmark and shows competitive performance (4 pts gap with optimized system\nmessage) in professional and academic examinations like the SAT, LSAT, GRE, and\nGMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our\nresearch indicates that learning from step-by-step explanations, whether these\nare generated by humans or more advanced AI models, is a promising direction to\nimprove model capabilities and skills.",
    "num_pages": 51
}