{
    "uuid": "4291f117-a24f-5a67-ada9-37cba4c696da",
    "title": "Deep Learning and the Information Bottleneck Principle",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Naftali Tishby",
        "Noga Zaslavsky"
    ],
    "pdf_url": "http://arxiv.org/pdf/1503.02406v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\4291f117-a24f-5a67-ada9-37cba4c696da.pdf",
    "bibtex": "@misc{tishby2015deeplearningandtheinformation,\n    title = {Deep Learning and the Information Bottleneck Principle},\n    author = {Naftali Tishby and Noga Zaslavsky},\n    year = {2015},\n    eprint = {1503.02406},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1503.02406},\n}",
    "abstract": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the\ninformation bottleneck (IB) principle. We first show that any DNN can be\nquantified by the mutual information between the layers and the input and\noutput variables. Using this representation we can calculate the optimal\ninformation theoretic limits of the DNN and obtain finite sample generalization\nbounds. The advantage of getting closer to the theoretical limit is\nquantifiable both by the generalization bound and by the network's simplicity.\nWe argue that both the optimal architecture, number of layers and\nfeatures/connections at each layer, are related to the bifurcation points of\nthe information bottleneck tradeoff, namely, relevant compression of the input\nlayer with respect to the output layer. The hierarchical representations at the\nlayered network naturally correspond to the structural phase transitions along\nthe information curve. We believe that this new insight can lead to new\noptimality bounds and deep learning algorithms.",
    "num_pages": 5
}