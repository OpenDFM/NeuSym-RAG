{
    "uuid": "a2032bdb-4133-5b23-9fa8-44eca0a97d8d",
    "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Omar Khattab",
        "Keshav Santhanam",
        "Xiang Lisa Li",
        "David Hall",
        "Percy Liang",
        "Christopher Potts",
        "Matei Zaharia"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.14024v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\a2032bdb-4133-5b23-9fa8-44eca0a97d8d.pdf",
    "bibtex": "@misc{khattab2023demonstratesearchpredictcomposingretrievalandlanguage,\n    title = {Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP},\n    author = {Omar Khattab and Keshav Santhanam and Xiang Lisa Li and David Hall and Percy Liang and Christopher Potts and Matei Zaharia},\n    year = {2023},\n    eprint = {2212.14024},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.14024},\n}",
    "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-intensive tasks using frozen language models (LM) and\nretrieval models (RM). Existing work has combined these in simple\n\"retrieve-then-read\" pipelines in which the RM retrieves passages that are\ninserted into the LM prompt. To begin to fully realize the potential of frozen\nLMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that\nrelies on passing natural language texts in sophisticated pipelines between an\nLM and an RM. DSP can express high-level programs that bootstrap pipeline-aware\ndemonstrations, search for relevant passages, and generate grounded\npredictions, systematically breaking down problems into small transformations\nthat the LM and RM can handle more reliably. We have written novel DSP programs\nfor answering questions in open-domain, multi-hop, and conversational settings,\nestablishing in early evaluations new state-of-the-art in-context learning\nresults and delivering 37-120%, 8-39%, and 80-290% relative gains against the\nvanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a\ncontemporaneous self-ask pipeline, respectively. We release DSP at\nhttps://github.com/stanfordnlp/dsp",
    "num_pages": 15
}