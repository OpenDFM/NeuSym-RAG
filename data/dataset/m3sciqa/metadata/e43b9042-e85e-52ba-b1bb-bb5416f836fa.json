{
    "uuid": "e43b9042-e85e-52ba-b1bb-bb5416f836fa",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ronen Eldan",
        "Yuanzhi Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.07759v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\e43b9042-e85e-52ba-b1bb-bb5416f836fa.pdf",
    "bibtex": "@misc{eldan2023tinystorieshowsmallcanlanguage,\n    title = {TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},\n    author = {Ronen Eldan and Yuanzhi Li},\n    year = {2023},\n    eprint = {2305.07759},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.07759},\n}",
    "abstract": "Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.",
    "num_pages": 27
}