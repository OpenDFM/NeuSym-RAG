{
    "uuid": "1e5af690-d43f-573a-9426-eae1c86f4c12",
    "title": "Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Philippe Laban",
        "Tobias Schnabel",
        "Paul Bennett",
        "Marti A. Hearst"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.03444v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\1e5af690-d43f-573a-9426-eae1c86f4c12.pdf",
    "bibtex": "@misc{laban2021keepitsimpleunsupervisedsimplification,\n    title = {Keep it Simple: Unsupervised Simplification of Multi-Paragraph Text},\n    author = {Philippe Laban and Tobias Schnabel and Paul Bennett and Marti A. Hearst},\n    year = {2021},\n    eprint = {2107.03444},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2107.03444},\n}",
    "abstract": "This work presents Keep it Simple (KiS), a new approach to unsupervised text\nsimplification which learns to balance a reward across three properties:\nfluency, salience and simplicity. We train the model with a novel algorithm to\noptimize the reward (k-SCST), in which the model proposes several candidate\nsimplifications, computes each candidate's reward, and encourages candidates\nthat outperform the mean reward. Finally, we propose a realistic text\ncomprehension task as an evaluation method for text simplification. When tested\non the English news domain, the KiS model outperforms strong supervised\nbaselines by more than 4 SARI points, and can help people complete a\ncomprehension task an average of 18% faster while retaining accuracy, when\ncompared to the original text. Code available:\nhttps://github.com/tingofurro/keep_it_simple",
    "num_pages": 14
}