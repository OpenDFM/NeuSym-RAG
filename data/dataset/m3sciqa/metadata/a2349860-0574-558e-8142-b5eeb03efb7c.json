{
    "uuid": "a2349860-0574-558e-8142-b5eeb03efb7c",
    "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Kaitao Song",
        "Xu Tan",
        "Tao Qin",
        "Jianfeng Lu",
        "Tie-Yan Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.02450v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\a2349860-0574-558e-8142-b5eeb03efb7c.pdf",
    "bibtex": "@misc{song2019massmaskedsequencetosequence,\n    title = {MASS: Masked Sequence to Sequence Pre-training for Language Generation},\n    author = {Kaitao Song and Xu Tan and Tao Qin and Jianfeng Lu and Tie-Yan Liu},\n    year = {2019},\n    eprint = {1905.02450},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1905.02450},\n}",
    "abstract": "Pre-training and fine-tuning, e.g., BERT, have achieved great success in\nlanguage understanding by transferring knowledge from rich-resource\npre-training task to the low/zero-resource downstream tasks. Inspired by the\nsuccess of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for\nthe encoder-decoder based language generation tasks. MASS adopts the\nencoder-decoder framework to reconstruct a sentence fragment given the\nremaining part of the sentence: its encoder takes a sentence with randomly\nmasked fragment (several consecutive tokens) as input, and its decoder tries to\npredict this masked fragment. In this way, MASS can jointly train the encoder\nand decoder to develop the capability of representation extraction and language\nmodeling. By further fine-tuning on a variety of zero/low-resource language\ngeneration tasks, including neural machine translation, text summarization and\nconversational response generation (3 tasks and totally 8 datasets), MASS\nachieves significant improvements over the baselines without pre-training or\nwith other pre-training methods. Specially, we achieve the state-of-the-art\naccuracy (37.5 in terms of BLEU score) on the unsupervised English-French\ntranslation, even beating the early attention-based supervised model.",
    "num_pages": 11
}