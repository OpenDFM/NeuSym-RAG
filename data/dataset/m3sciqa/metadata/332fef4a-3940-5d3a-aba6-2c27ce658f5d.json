{
    "uuid": "332fef4a-3940-5d3a-aba6-2c27ce658f5d",
    "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yang Xu",
        "Yiheng Xu",
        "Tengchao Lv",
        "Lei Cui",
        "Furu Wei",
        "Guoxin Wang",
        "Yijuan Lu",
        "Dinei Florencio",
        "Cha Zhang",
        "Wanxiang Che",
        "Min Zhang",
        "Lidong Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14740v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\332fef4a-3940-5d3a-aba6-2c27ce658f5d.pdf",
    "bibtex": "@misc{xu2022layoutlmv2multimodalpretrainingforvisuallyrich,\n    title = {LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},\n    author = {Yang Xu and Yiheng Xu and Tengchao Lv and Lei Cui and Furu Wei and Guoxin Wang and Yijuan Lu and Dinei Florencio and Cha Zhang and Wanxiang Che and Min Zhang and Lidong Zhou},\n    year = {2022},\n    eprint = {2012.14740},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.14740},\n}",
    "abstract": "Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$\n0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520),\nRVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our\nmodel and code publicly available at \\url{https://aka.ms/layoutlmv2}.",
    "num_pages": 13
}