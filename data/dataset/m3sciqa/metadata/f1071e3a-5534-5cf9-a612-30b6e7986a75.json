{
    "uuid": "f1071e3a-5534-5cf9-a612-30b6e7986a75",
    "title": "Deceiving Google's Perspective API Built for Detecting Toxic Comments",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Hossein Hosseini",
        "Sreeram Kannan",
        "Baosen Zhang",
        "Radha Poovendran"
    ],
    "pdf_url": "http://arxiv.org/pdf/1702.08138v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\f1071e3a-5534-5cf9-a612-30b6e7986a75.pdf",
    "bibtex": "@misc{hosseini2017deceivinggooglesperspectiveapibuilt,\n    title = {Deceiving Google's Perspective API Built for Detecting Toxic Comments},\n    author = {Hossein Hosseini and Sreeram Kannan and Baosen Zhang and Radha Poovendran},\n    year = {2017},\n    eprint = {1702.08138},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1702.08138},\n}",
    "abstract": "Social media platforms provide an environment where people can freely engage\nin discussions. Unfortunately, they also enable several problems, such as\nonline harassment. Recently, Google and Jigsaw started a project called\nPerspective, which uses machine learning to automatically detect toxic\nlanguage. A demonstration website has been also launched, which allows anyone\nto type a phrase in the interface and instantaneously see the toxicity score\n[1]. In this paper, we propose an attack on the Perspective toxic detection\nsystem based on the adversarial examples. We show that an adversary can subtly\nmodify a highly toxic phrase in a way that the system assigns significantly\nlower toxicity score to it. We apply the attack on the sample phrases provided\nin the Perspective website and show that we can consistently reduce the\ntoxicity scores to the level of the non-toxic phrases. The existence of such\nadversarial examples is very harmful for toxic detection systems and seriously\nundermines their usability.",
    "num_pages": 4
}