{
    "uuid": "67f97fa1-4d75-5346-8e7f-4701de843e11",
    "title": "TRUE: Re-evaluating Factual Consistency Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Or Honovich",
        "Roee Aharoni",
        "Jonathan Herzig",
        "Hagai Taitelbaum",
        "Doron Kukliansy",
        "Vered Cohen",
        "Thomas Scialom",
        "Idan Szpektor",
        "Avinatan Hassidim",
        "Yossi Matias"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.04991v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\67f97fa1-4d75-5346-8e7f-4701de843e11.pdf",
    "bibtex": "@misc{honovich2022truereevaluatingfactualconsistencyevaluation,\n    title = {TRUE: Re-evaluating Factual Consistency Evaluation},\n    author = {Or Honovich and Roee Aharoni and Jonathan Herzig and Hagai Taitelbaum and Doron Kukliansy and Vered Cohen and Thomas Scialom and Idan Szpektor and Avinatan Hassidim and Yossi Matias},\n    year = {2022},\n    eprint = {2204.04991},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.04991},\n}",
    "abstract": "Grounded text generation systems often generate text that contains factual\ninconsistencies, hindering their real-world applicability. Automatic factual\nconsistency evaluation may help alleviate this limitation by accelerating\nevaluation cycles, filtering inconsistent outputs and augmenting training data.\nWhile attracting increasing attention, such evaluation metrics are usually\ndeveloped and evaluated in silo for a single task or dataset, slowing their\nadoption. Moreover, previous meta-evaluation protocols focused on system-level\ncorrelations with human annotations, which leave the example-level accuracy of\nsuch metrics unclear. In this work, we introduce TRUE: a comprehensive survey\nand assessment of factual consistency metrics on a standardized collection of\nexisting texts from diverse tasks, manually annotated for factual consistency.\nOur standardization enables an example-level meta-evaluation protocol that is\nmore actionable and interpretable than previously reported correlations,\nyielding clearer quality measures. Across diverse state-of-the-art metrics and\n11 datasets we find that large-scale NLI and question\ngeneration-and-answering-based approaches achieve strong and complementary\nresults. We recommend those methods as a starting point for model and metric\ndevelopers, and hope TRUE will foster progress towards even better evaluation\nmethods.",
    "num_pages": 16
}