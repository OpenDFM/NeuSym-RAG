{
    "uuid": "7d73d518-86e4-5000-95a5-50c8c4daf0e7",
    "title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Esin Durmus",
        "He He",
        "Mona Diab"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.03754v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\7d73d518-86e4-5000-95a5-50c8c4daf0e7.pdf",
    "bibtex": "@misc{durmus2020feqaaquestionansweringevaluation,\n    title = {FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization},\n    author = {Esin Durmus and He He and Mona Diab},\n    year = {2020},\n    eprint = {2005.03754},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.03754},\n}",
    "abstract": "Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries.",
    "num_pages": 16
}