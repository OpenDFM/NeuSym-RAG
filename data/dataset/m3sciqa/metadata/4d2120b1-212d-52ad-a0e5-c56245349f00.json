{
    "uuid": "4d2120b1-212d-52ad-a0e5-c56245349f00",
    "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zhen Wang",
        "Rameswar Panda",
        "Leonid Karlinsky",
        "Rogerio Feris",
        "Huan Sun",
        "Yoon Kim"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.02861v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4d2120b1-212d-52ad-a0e5-c56245349f00.pdf",
    "bibtex": "@misc{wang2023multitaskprompttuningenablesparameterefficient,\n    title = {Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning},\n    author = {Zhen Wang and Rameswar Panda and Leonid Karlinsky and Rogerio Feris and Huan Sun and Yoon Kim},\n    year = {2023},\n    eprint = {2303.02861},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.02861},\n}",
    "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via\nconditioning on learned prompt vectors, has emerged as a promising approach for\nefficiently adapting large language models to multiple downstream tasks.\nHowever, existing methods typically learn soft prompt vectors from scratch, and\nit has not been clear how to exploit the rich cross-task knowledge with prompt\nvectors in a multitask learning setting. We propose multitask prompt tuning\n(MPT), which first learns a single transferable prompt by distilling knowledge\nfrom multiple task-specific source prompts. We then learn multiplicative low\nrank updates to this shared prompt to efficiently adapt it to each downstream\ntarget task. Extensive experiments on 23 NLP datasets demonstrate that our\nproposed approach outperforms the state-of-the-art methods, including the full\nfinetuning baseline in some cases, despite only tuning 0.035% as many\ntask-specific parameters.",
    "num_pages": 16
}