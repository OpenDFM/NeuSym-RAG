{
    "uuid": "e7feb350-d59f-5df8-bcaa-3b0de755d8bc",
    "title": "Knowledge Neurons in Pretrained Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Damai Dai",
        "Li Dong",
        "Yaru Hao",
        "Zhifang Sui",
        "Baobao Chang",
        "Furu Wei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08696v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e7feb350-d59f-5df8-bcaa-3b0de755d8bc.pdf",
    "bibtex": "@misc{dai2022knowledgeneuronsinpretrainedtransformers,\n    title = {Knowledge Neurons in Pretrained Transformers},\n    author = {Damai Dai and Li Dong and Yaru Hao and Zhifang Sui and Baobao Chang and Furu Wei},\n    year = {2022},\n    eprint = {2104.08696},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08696},\n}",
    "abstract": "Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.",
    "num_pages": 10
}