{
    "uuid": "7b79244f-3cea-5a12-a1bf-1e4609a1c0f2",
    "title": "Representation Learning with Contrastive Predictive Coding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Aaron van den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
    ],
    "pdf_url": "http://arxiv.org/pdf/1807.03748v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\7b79244f-3cea-5a12-a1bf-1e4609a1c0f2.pdf",
    "bibtex": "@misc{oord2019representationlearningwithcontrastivepredictive,\n    title = {Representation Learning with Contrastive Predictive Coding},\n    author = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},\n    year = {2019},\n    eprint = {1807.03748},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1807.03748},\n}",
    "abstract": "While supervised learning has enabled great progress in many applications,\nunsupervised learning has not seen such widespread adoption, and remains an\nimportant and challenging endeavor for artificial intelligence. In this work,\nwe propose a universal unsupervised learning approach to extract useful\nrepresentations from high-dimensional data, which we call Contrastive\nPredictive Coding. The key insight of our model is to learn such\nrepresentations by predicting the future in latent space by using powerful\nautoregressive models. We use a probabilistic contrastive loss which induces\nthe latent space to capture information that is maximally useful to predict\nfuture samples. It also makes the model tractable by using negative sampling.\nWhile most prior work has focused on evaluating representations for a\nparticular modality, we demonstrate that our approach is able to learn useful\nrepresentations achieving strong performance on four distinct domains: speech,\nimages, text and reinforcement learning in 3D environments.",
    "num_pages": 13
}