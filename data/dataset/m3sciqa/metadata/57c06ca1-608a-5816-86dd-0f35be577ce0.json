{
    "uuid": "57c06ca1-608a-5816-86dd-0f35be577ce0",
    "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xiao Liu",
        "Hanyu Lai",
        "Hao Yu",
        "Yifan Xu",
        "Aohan Zeng",
        "Zhengxiao Du",
        "Peng Zhang",
        "Yuxiao Dong",
        "Jie Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.07906v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\57c06ca1-608a-5816-86dd-0f35be577ce0.pdf",
    "bibtex": "@misc{liu2023webglmtowardsanefficientwebenhanced,\n    title = {WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},\n    author = {Xiao Liu and Hanyu Lai and Hao Yu and Yifan Xu and Aohan Zeng and Zhengxiao Du and Peng Zhang and Yuxiao Dong and Jie Tang},\n    year = {2023},\n    eprint = {2306.07906},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.07906},\n}",
    "abstract": "We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\n\\url{https://github.com/THUDM/WebGLM}.",
    "num_pages": 42
}