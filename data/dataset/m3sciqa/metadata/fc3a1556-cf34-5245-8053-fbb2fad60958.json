{
    "uuid": "fc3a1556-cf34-5245-8053-fbb2fad60958",
    "title": "Context-faithful Prompting for Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Wenxuan Zhou",
        "Sheng Zhang",
        "Hoifung Poon",
        "Muhao Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.11315v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\fc3a1556-cf34-5245-8053-fbb2fad60958.pdf",
    "bibtex": "@misc{zhou2023contextfaithfulpromptingforlargelanguage,\n    title = {Context-faithful Prompting for Large Language Models},\n    author = {Wenxuan Zhou and Sheng Zhang and Hoifung Poon and Muhao Chen},\n    year = {2023},\n    eprint = {2303.11315},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.11315},\n}",
    "abstract": "Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts. Code and data are released\nat https://github.com/wzhouad/context-faithful-llm.",
    "num_pages": 13
}