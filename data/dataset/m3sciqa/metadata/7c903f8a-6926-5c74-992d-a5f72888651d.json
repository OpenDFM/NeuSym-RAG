{
    "uuid": "7c903f8a-6926-5c74-992d-a5f72888651d",
    "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xianjun Yang",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Xiaoman Pan",
        "Linda Petzold",
        "Dong Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.09233v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\7c903f8a-6926-5c74-992d-a5f72888651d.pdf",
    "bibtex": "@misc{yang2023oasumlargescaleopendomainaspectbased,\n    title = {OASum: Large-Scale Open Domain Aspect-based Summarization},\n    author = {Xianjun Yang and Kaiqiang Song and Sangwoo Cho and Xiaoyang Wang and Xiaoman Pan and Linda Petzold and Dong Yu},\n    year = {2023},\n    eprint = {2212.09233},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.09233},\n}",
    "abstract": "Aspect or query-based summarization has recently caught more attention, as it\ncan generate differentiated summaries based on users' interests. However, the\ncurrent dataset for aspect or query-based summarization either focuses on\nspecific domains, contains relatively small-scale instances, or includes only a\nfew aspect types. Such limitations hinder further explorations in this\ndirection. In this work, we take advantage of crowd-sourcing knowledge on\nWikipedia.org and automatically create a high-quality, large-scale open-domain\naspect-based summarization dataset named OASum, which contains more than 3.7\nmillion instances with around 1 million different aspects on 2 million\nWikipedia pages. We provide benchmark results on OASum and demonstrate its\nability for diverse aspect-based summarization generation. To overcome the data\nscarcity problem on specific domains, we also perform zero-shot, few-shot, and\nfine-tuning on seven downstream datasets. Specifically, zero/few-shot and\nfine-tuning results show that the model pre-trained on our corpus demonstrates\na strong aspect or query-focused generation ability compared with the backbone\nmodel. Our dataset and pre-trained checkpoints are publicly available.",
    "num_pages": 19
}