{
    "uuid": "c89e402d-2dd4-57dc-883b-be5c04e54b3a",
    "title": "Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Baijun Ji",
        "Tong Zhang",
        "Yicheng Zou",
        "Bojie Hu",
        "Si Shen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.08478v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c89e402d-2dd4-57dc-883b-be5c04e54b3a.pdf",
    "bibtex": "@misc{ji2022increasingvisualawarenessinmultimodal,\n    title = {Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective},\n    author = {Baijun Ji and Tong Zhang and Yicheng Zou and Bojie Hu and Si Shen},\n    year = {2022},\n    eprint = {2210.08478},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2210.08478},\n}",
    "abstract": "Multimodal machine translation (MMT) aims to improve translation quality by\nequipping the source sentence with its corresponding image. Despite the\npromising performance, MMT models still suffer the problem of input\ndegradation: models focus more on textual information while visual information\nis generally overlooked. In this paper, we endeavor to improve MMT performance\nby increasing visual awareness from an information theoretic perspective. In\ndetail, we decompose the informative visual signals into two parts:\nsource-specific information and target-specific information. We use mutual\ninformation to quantify them and propose two methods for objective optimization\nto better leverage visual signals. Experiments on two datasets demonstrate that\nour approach can effectively enhance the visual awareness of MMT model and\nachieve superior results against strong baselines.",
    "num_pages": 10
}