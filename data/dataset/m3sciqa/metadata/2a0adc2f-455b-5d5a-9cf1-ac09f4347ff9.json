{
    "uuid": "2a0adc2f-455b-5d5a-9cf1-ac09f4347ff9",
    "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Peng Wang",
        "An Yang",
        "Rui Men",
        "Junyang Lin",
        "Shuai Bai",
        "Zhikang Li",
        "Jianxin Ma",
        "Chang Zhou",
        "Jingren Zhou",
        "Hongxia Yang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.03052v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\2a0adc2f-455b-5d5a-9cf1-ac09f4347ff9.pdf",
    "bibtex": "@misc{wang2022ofaunifyingarchitecturestasksand,\n    title = {OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},\n    author = {Peng Wang and An Yang and Rui Men and Junyang Lin and Shuai Bai and Zhikang Li and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},\n    year = {2022},\n    eprint = {2202.03052},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2202.03052},\n}",
    "abstract": "In this work, we pursue a unified paradigm for multimodal pretraining to\nbreak the scaffolds of complex task/modality-specific customization. We propose\nOFA, a Task-Agnostic and Modality-Agnostic framework that supports Task\nComprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks,\nincluding image generation, visual grounding, image captioning, image\nclassification, language modeling, etc., in a simple sequence-to-sequence\nlearning framework. OFA follows the instruction-based learning in both\npretraining and finetuning stages, requiring no extra task-specific layers for\ndownstream tasks. In comparison with the recent state-of-the-art vision &\nlanguage models that rely on extremely large cross-modal datasets, OFA is\npretrained on only 20M publicly available image-text pairs. Despite its\nsimplicity and relatively small-scale training data, OFA achieves new SOTAs in\na series of cross-modal tasks while attaining highly competitive performances\non uni-modal tasks. Our further analysis indicates that OFA can also\neffectively transfer to unseen tasks and unseen domains. Our code and models\nare publicly available at https://github.com/OFA-Sys/OFA.",
    "num_pages": 26
}