{
    "uuid": "ee1dfb4c-7988-5a4c-bb1c-a26a7a03b8bf",
    "title": "MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Wenqiao Zhang",
        "Haochen Shi",
        "Jiannan Guo",
        "Shengyu Zhang",
        "Qingpeng Cai",
        "Juncheng Li",
        "Sihui Luo",
        "Yueting Zhuang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06558v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ee1dfb4c-7988-5a4c-bb1c-a26a7a03b8bf.pdf",
    "bibtex": "@misc{zhang2022magicmultimodalrelationalgraphadversarial,\n    title = {MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-based Image Captioning},\n    author = {Wenqiao Zhang and Haochen Shi and Jiannan Guo and Shengyu Zhang and Qingpeng Cai and Juncheng Li and Sihui Luo and Yueting Zhuang},\n    year = {2022},\n    eprint = {2112.06558},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2112.06558},\n}",
    "abstract": "Text-based image captioning (TextCap) requires simultaneous comprehension of\nvisual content and reading the text of images to generate a natural language\ndescription. Although a task can teach machines to understand the complex human\nenvironment further given that text is omnipresent in our daily surroundings,\nit poses additional challenges in normal captioning. A text-based image\nintuitively contains abundant and complex multimodal relational content, that\nis, image details can be described diversely from multiview rather than a\nsingle caption. Certainly, we can introduce additional paired training data to\nshow the diversity of images' descriptions, this process is labor-intensive and\ntime-consuming for TextCap pair annotations with extra texts. Based on the\ninsight mentioned above, we investigate how to generate diverse captions that\nfocus on different image parts using an unpaired training paradigm. We propose\nthe Multimodal relAtional Graph adversarIal inferenCe (MAGIC) framework for\ndiverse and unpaired TextCap. This framework can adaptively construct multiple\nmultimodal relational graphs of images and model complex relationships among\ngraphs to represent descriptive diversity. Moreover, a cascaded generative\nadversarial network is developed from modeled graphs to infer the unpaired\ncaption generation in image-sentence feature alignment and linguistic coherence\nlevels. We validate the effectiveness of MAGIC in generating diverse captions\nfrom different relational information items of an image. Experimental results\nshow that MAGIC can generate very promising outcomes without using any\nimage-caption training pairs.",
    "num_pages": 9
}