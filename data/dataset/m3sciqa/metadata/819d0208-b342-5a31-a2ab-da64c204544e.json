{
    "uuid": "819d0208-b342-5a31-a2ab-da64c204544e",
    "title": "ERNIE: Enhanced Language Representation with Informative Entities",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Zhengyan Zhang",
        "Xu Han",
        "Zhiyuan Liu",
        "Xin Jiang",
        "Maosong Sun",
        "Qun Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.07129v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\819d0208-b342-5a31-a2ab-da64c204544e.pdf",
    "bibtex": "@misc{zhang2019ernieenhancedlanguagerepresentationwith,\n    title = {ERNIE: Enhanced Language Representation with Informative Entities},\n    author = {Zhengyan Zhang and Xu Han and Zhiyuan Liu and Xin Jiang and Maosong Sun and Qun Liu},\n    year = {2019},\n    eprint = {1905.07129},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1905.07129},\n}",
    "abstract": "Neural language representation models such as BERT pre-trained on large-scale\ncorpora can well capture rich semantic patterns from plain text, and be\nfine-tuned to consistently improve the performance of various NLP tasks.\nHowever, the existing pre-trained language models rarely consider incorporating\nknowledge graphs (KGs), which can provide rich structured knowledge facts for\nbetter language understanding. We argue that informative entities in KGs can\nenhance language representation with external knowledge. In this paper, we\nutilize both large-scale textual corpora and KGs to train an enhanced language\nrepresentation model (ERNIE), which can take full advantage of lexical,\nsyntactic, and knowledge information simultaneously. The experimental results\nhave demonstrated that ERNIE achieves significant improvements on various\nknowledge-driven tasks, and meanwhile is comparable with the state-of-the-art\nmodel BERT on other common NLP tasks. The source code of this paper can be\nobtained from https://github.com/thunlp/ERNIE.",
    "num_pages": 11
}