{
    "uuid": "38c42ad1-4e5f-5ebe-ae2d-683ef661bbfe",
    "title": "Scaling Instruction-Finetuned Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Hyung Won Chung",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma",
        "Albert Webson",
        "Shixiang Shane Gu",
        "Zhuyun Dai",
        "Mirac Suzgun",
        "Xinyun Chen",
        "Aakanksha Chowdhery",
        "Alex Castro-Ros",
        "Marie Pellat",
        "Kevin Robinson",
        "Dasha Valter",
        "Sharan Narang",
        "Gaurav Mishra",
        "Adams Yu",
        "Vincent Zhao",
        "Yanping Huang",
        "Andrew Dai",
        "Hongkun Yu",
        "Slav Petrov",
        "Ed H. Chi",
        "Jeff Dean",
        "Jacob Devlin",
        "Adam Roberts",
        "Denny Zhou",
        "Quoc V. Le",
        "Jason Wei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.11416v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\38c42ad1-4e5f-5ebe-ae2d-683ef661bbfe.pdf",
    "bibtex": "@misc{chung2022scalinginstructionfinetunedlanguagemodels,\n    title = {Scaling Instruction-Finetuned Language Models},\n    author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},\n    year = {2022},\n    eprint = {2210.11416},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2210.11416},\n}",
    "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.",
    "num_pages": 54
}