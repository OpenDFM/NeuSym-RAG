{
    "uuid": "c16ad2d8-be0c-5eed-ae16-5950a83e34bc",
    "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Jingqing Zhang",
        "Yao Zhao",
        "Mohammad Saleh",
        "Peter J. Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.08777v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\c16ad2d8-be0c-5eed-ae16-5950a83e34bc.pdf",
    "bibtex": "@misc{zhang2020pegasuspretrainingwithextractedgapsentences,\n    title = {PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},\n    author = {Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},\n    year = {2020},\n    eprint = {1912.08777},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1912.08777},\n}",
    "abstract": "Recent work pre-training Transformers with self-supervised objectives on\nlarge text corpora has shown great success when fine-tuned on downstream NLP\ntasks including text summarization. However, pre-training objectives tailored\nfor abstractive text summarization have not been explored. Furthermore there is\na lack of systematic evaluation across diverse domains. In this work, we\npropose pre-training large Transformer-based encoder-decoder models on massive\ntext corpora with a new self-supervised objective. In PEGASUS, important\nsentences are removed/masked from an input document and are generated together\nas one output sequence from the remaining sentences, similar to an extractive\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\ntasks spanning news, science, stories, instructions, emails, patents, and\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\nalso shows surprising performance on low-resource summarization, surpassing\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\nFinally we validated our results using human evaluation and show that our model\nsummaries achieve human performance on multiple datasets.",
    "num_pages": 55
}