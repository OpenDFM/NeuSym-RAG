{
    "uuid": "1e6bbab0-102d-51b6-bd9c-79b6f266eb8a",
    "title": "SPICE: Semantic Propositional Image Caption Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Peter Anderson",
        "Basura Fernando",
        "Mark Johnson",
        "Stephen Gould"
    ],
    "pdf_url": "http://arxiv.org/pdf/1607.08822v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\1e6bbab0-102d-51b6-bd9c-79b6f266eb8a.pdf",
    "bibtex": "@misc{anderson2016spicesemanticpropositionalimagecaption,\n    title = {SPICE: Semantic Propositional Image Caption Evaluation},\n    author = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},\n    year = {2016},\n    eprint = {1607.08822},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1607.08822},\n}",
    "abstract": "There is considerable interest in the task of automatically generating image\ncaptions. However, evaluation is challenging. Existing automatic evaluation\nmetrics are primarily sensitive to n-gram overlap, which is neither necessary\nnor sufficient for the task of simulating human judgment. We hypothesize that\nsemantic propositional content is an important component of human caption\nevaluation, and propose a new automated caption evaluation metric defined over\nscene graphs coined SPICE. Extensive evaluations across a range of models and\ndatasets indicate that SPICE captures human judgments over model-generated\ncaptions better than other automatic metrics (e.g., system-level correlation of\n0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and\n0.53 for METEOR). Furthermore, SPICE can answer questions such as `which\ncaption-generator best understands colors?' and `can caption-generators count?'",
    "num_pages": 17
}