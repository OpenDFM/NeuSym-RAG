{
    "uuid": "0504eb73-304a-58fe-b1cb-9ec94e383a34",
    "title": "MEGA: Multilingual Evaluation of Generative AI",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Kabir Ahuja",
        "Harshita Diddee",
        "Rishav Hada",
        "Millicent Ochieng",
        "Krithika Ramesh",
        "Prachi Jain",
        "Akshay Nambi",
        "Tanuja Ganu",
        "Sameer Segal",
        "Maxamed Axmed",
        "Kalika Bali",
        "Sunayana Sitaram"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.12528v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\0504eb73-304a-58fe-b1cb-9ec94e383a34.pdf",
    "bibtex": "@misc{ahuja2023megamultilingualevaluationofgenerative,\n    title = {MEGA: Multilingual Evaluation of Generative AI},\n    author = {Kabir Ahuja and Harshita Diddee and Rishav Hada and Millicent Ochieng and Krithika Ramesh and Prachi Jain and Akshay Nambi and Tanuja Ganu and Sameer Segal and Maxamed Axmed and Kalika Bali and Sunayana Sitaram},\n    year = {2023},\n    eprint = {2303.12528},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.12528},\n}",
    "abstract": "Generative AI models have shown impressive performance on many Natural\nLanguage Processing tasks such as language understanding, reasoning, and\nlanguage generation. An important question being asked by the AI community\ntoday is about the capabilities and limits of these models, and it is clear\nthat evaluating generative AI is very challenging. Most studies on generative\nLLMs have been restricted to English and it is unclear how capable these models\nare at understanding and generating text in other languages. We present the\nfirst comprehensive benchmarking of generative LLMs - MEGA, which evaluates\nmodels on standard NLP benchmarks, covering 16 NLP datasets across 70\ntypologically diverse languages. We compare the performance of generative LLMs\nincluding Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive\nmodels on these tasks to determine how well generative models perform compared\nto the previous generation of LLMs. We present a thorough analysis of the\nperformance of models across languages and tasks and discuss challenges in\nimproving the performance of generative LLMs on low-resource languages. We\ncreate a framework for evaluating generative LLMs in the multilingual setting\nand provide directions for future progress in the field.",
    "num_pages": 36
}