{
    "uuid": "fca2d60a-027e-5d54-92aa-497b8d9161e7",
    "title": "Variational Temporal Abstraction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Taesup Kim",
        "Sungjin Ahn",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.00775v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\fca2d60a-027e-5d54-92aa-497b8d9161e7.pdf",
    "bibtex": "@misc{kim2019variationaltemporalabstraction,\n    title = {Variational Temporal Abstraction},\n    author = {Taesup Kim and Sungjin Ahn and Yoshua Bengio},\n    year = {2019},\n    eprint = {1910.00775},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1910.00775},\n}",
    "abstract": "We introduce a variational approach to learning and inference of temporally\nhierarchical structure and representation for sequential data. We propose the\nVariational Temporal Abstraction (VTA), a hierarchical recurrent state space\nmodel that can infer the latent temporal structure and thus perform the\nstochastic state transition hierarchically. We also propose to apply this model\nto implement the jumpy-imagination ability in imagination-augmented\nagent-learning in order to improve the efficiency of the imagination. In\nexperiments, we demonstrate that our proposed method can model 2D and 3D visual\nsequence datasets with interpretable temporal structure discovery and that its\napplication to jumpy imagination enables more efficient agent-learning in a 3D\nnavigation task.",
    "num_pages": 14
}