{
    "uuid": "bc29c966-1055-531c-93fc-2d629e8fc3a8",
    "title": "Non-Autoregressive Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Jiatao Gu",
        "James Bradbury",
        "Caiming Xiong",
        "Victor O. K. Li",
        "Richard Socher"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.02281v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\bc29c966-1055-531c-93fc-2d629e8fc3a8.pdf",
    "bibtex": "@misc{gu2018nonautoregressiveneuralmachinetranslation,\n    title = {Non-Autoregressive Neural Machine Translation},\n    author = {Jiatao Gu and James Bradbury and Caiming Xiong and Victor O. K. Li and Richard Socher},\n    year = {2018},\n    eprint = {1711.02281},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1711.02281},\n}",
    "abstract": "Existing approaches to neural machine translation condition each output word\non previously generated outputs. We introduce a model that avoids this\nautoregressive property and produces its outputs in parallel, allowing an order\nof magnitude lower latency during inference. Through knowledge distillation,\nthe use of input token fertilities as a latent variable, and policy gradient\nfine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative\nto the autoregressive Transformer network used as a teacher. We demonstrate\nsubstantial cumulative improvements associated with each of the three aspects\nof our training strategy, and validate our approach on IWSLT 2016\nEnglish-German and two WMT language pairs. By sampling fertilities in parallel\nat inference time, our non-autoregressive model achieves near-state-of-the-art\nperformance of 29.8 BLEU on WMT 2016 English-Romanian.",
    "num_pages": 13
}