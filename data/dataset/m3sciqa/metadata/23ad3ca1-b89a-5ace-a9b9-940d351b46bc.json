{
    "uuid": "23ad3ca1-b89a-5ace-a9b9-940d351b46bc",
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Tim Dettmers",
        "Mike Lewis",
        "Younes Belkada",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2208.07339v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\23ad3ca1-b89a-5ace-a9b9-940d351b46bc.pdf",
    "bibtex": "@misc{dettmers2022llmint88bitmatrixmultiplicationfor,\n    title = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},\n    author = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},\n    year = {2022},\n    eprint = {2208.07339},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2208.07339},\n}",
    "abstract": "Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.",
    "num_pages": 20
}