{
    "uuid": "6d8e0fc1-a6bb-5260-874f-59723d196b81",
    "title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Kai Sheng Tai",
        "Richard Socher",
        "Christopher D. Manning"
    ],
    "pdf_url": "http://arxiv.org/pdf/1503.00075v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\6d8e0fc1-a6bb-5260-874f-59723d196b81.pdf",
    "bibtex": "@misc{tai2015improvedsemanticrepresentationsfromtreestructured,\n    title = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},\n    author = {Kai Sheng Tai and Richard Socher and Christopher D. Manning},\n    year = {2015},\n    eprint = {1503.00075},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1503.00075},\n}",
    "abstract": "Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).",
    "num_pages": 11
}