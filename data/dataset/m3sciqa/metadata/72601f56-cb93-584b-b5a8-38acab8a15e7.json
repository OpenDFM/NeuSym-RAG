{
    "uuid": "72601f56-cb93-584b-b5a8-38acab8a15e7",
    "title": "Interpretable Unified Language Checking",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tianhua Zhang",
        "Hongyin Luo",
        "Yung-Sung Chuang",
        "Wei Fang",
        "Luc Gaitskell",
        "Thomas Hartvigsen",
        "Xixin Wu",
        "Danny Fox",
        "Helen Meng",
        "James Glass"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.03728v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\72601f56-cb93-584b-b5a8-38acab8a15e7.pdf",
    "bibtex": "@misc{zhang2023interpretableunifiedlanguagechecking,\n    title = {Interpretable Unified Language Checking},\n    author = {Tianhua Zhang and Hongyin Luo and Yung-Sung Chuang and Wei Fang and Luc Gaitskell and Thomas Hartvigsen and Xixin Wu and Danny Fox and Helen Meng and James Glass},\n    year = {2023},\n    eprint = {2304.03728},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.03728},\n}",
    "abstract": "Despite recent concerns about undesirable behaviors generated by large\nlanguage models (LLMs), including non-factual, biased, and hateful language, we\nfind LLMs are inherent multi-task language checkers based on their latent\nrepresentations of natural and social knowledge. We present an interpretable,\nunified, language checking (UniLC) method for both human and machine-generated\nlanguage that aims to check if language input is factual and fair. While\nfairness and fact-checking tasks have been handled separately with dedicated\nmodels, we find that LLMs can achieve high performance on a combination of\nfact-checking, stereotype detection, and hate speech detection tasks with a\nsimple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task\nlanguage checking method proposed in this work, the GPT3.5-turbo model\noutperforms fully supervised baselines on several language tasks. The simple\napproach and results suggest that based on strong latent knowledge\nrepresentations, an LLM can be an adaptive and explainable tool for detecting\nmisinformation, stereotypes, and hate speech.",
    "num_pages": 16
}