{
    "uuid": "4f312f60-4fb7-53a0-b6d7-7810cd39eb43",
    "title": "Evaluating historical text normalization systems: How well do they generalize?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Alexander Robertson",
        "Sharon Goldwater"
    ],
    "pdf_url": "http://arxiv.org/pdf/1804.02545v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\4f312f60-4fb7-53a0-b6d7-7810cd39eb43.pdf",
    "bibtex": "@misc{robertson2018evaluatinghistoricaltextnormalizationsystems,\n    title = {Evaluating historical text normalization systems: How well do they generalize?},\n    author = {Alexander Robertson and Sharon Goldwater},\n    year = {2018},\n    eprint = {1804.02545},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1804.02545},\n}",
    "abstract": "We highlight several issues in the evaluation of historical text\nnormalization systems that make it hard to tell how well these systems would\nactually work in practice---i.e., for new datasets or languages; in comparison\nto more na\\\"ive systems; or as a preprocessing step for downstream NLP tools.\nWe illustrate these issues and exemplify our proposed evaluation practices by\ncomparing two neural models against a na\\\"ive baseline system. We show that the\nneural models generalize well to unseen words in tests on five languages;\nnevertheless, they provide no clear benefit over the na\\\"ive baseline for\ndownstream POS tagging of an English historical collection. We conclude that\nfuture work should include more rigorous evaluation, including both intrinsic\nand extrinsic measures where possible.",
    "num_pages": 6
}