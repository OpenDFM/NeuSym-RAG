{
    "uuid": "1784e68d-a499-59ab-a942-14c7a55861db",
    "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant",
        "Aditya Barua",
        "Colin Raffel"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.11934v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\1784e68d-a499-59ab-a942-14c7a55861db.pdf",
    "bibtex": "@misc{xue2021mt5amassivelymultilingualpretrained,\n    title = {mT5: A massively multilingual pre-trained text-to-text transformer},\n    author = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},\n    year = {2021},\n    eprint = {2010.11934},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.11934},\n}",
    "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified\ntext-to-text format and scale to attain state-of-the-art results on a wide\nvariety of English-language NLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained on a new Common Crawl-based\ndataset covering 101 languages. We detail the design and modified training of\nmT5 and demonstrate its state-of-the-art performance on many multilingual\nbenchmarks. We also describe a simple technique to prevent \"accidental\ntranslation\" in the zero-shot setting, where a generative model chooses to\n(partially) translate its prediction into the wrong language. All of the code\nand model checkpoints used in this work are publicly available.",
    "num_pages": 17
}