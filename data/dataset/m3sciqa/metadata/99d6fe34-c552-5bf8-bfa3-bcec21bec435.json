{
    "uuid": "99d6fe34-c552-5bf8-bfa3-bcec21bec435",
    "title": "Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chao Feng",
        "Xinyu Zhang",
        "Zichu Fei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2309.03118v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\99d6fe34-c552-5bf8-bfa3-bcec21bec435.pdf",
    "bibtex": "@misc{feng2023knowledgesolverteachingllmsto,\n    title = {Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs},\n    author = {Chao Feng and Xinyu Zhang and Zichu Fei},\n    year = {2023},\n    eprint = {2309.03118},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2309.03118},\n}",
    "abstract": "Large language models (LLMs), such as ChatGPT and GPT-4, are versatile and\ncan solve different tasks due to their emergent ability and generalizability.\nHowever, LLMs sometimes lack domain-specific knowledge to perform tasks, which\nwould also cause hallucination during inference. In some previous works,\nadditional modules like graph neural networks (GNNs) are trained on retrieved\nknowledge from external knowledge bases, aiming to mitigate the problem of\nlacking domain-specific knowledge. However, incorporating additional modules:\n1) would need retraining additional modules when encountering novel domains; 2)\nwould become a bottleneck since LLMs' strong abilities are not fully utilized\nfor retrieval. In this paper, we propose a paradigm, termed Knowledge Solver\n(KSL), to teach LLMs to search for essential knowledge from external knowledge\nbases by harnessing their own strong generalizability. Specifically, we design\na simple yet effective prompt to transform retrieval into a multi-hop decision\nsequence, which empowers LLMs with searching knowledge ability in zero-shot\nmanner. Additionally, KSL is able to provide complete retrieval paths and\ntherefore increase explainability of LLMs' reasoning processes. We conduct\nexperiments on three datasets: CommonsenseQA, OpenbookQA, and MedQA-USMLE, and\nfound that our approach improves LLM baseline performance by a relatively large\nmargin.",
    "num_pages": 13
}