{
    "uuid": "d67339f8-e834-5f1e-8dec-a4967099e092",
    "title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Piotr Rybak",
        "Robert Mroczkowski",
        "Janusz Tracz",
        "Ireneusz Gawlik"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00630v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\d67339f8-e834-5f1e-8dec-a4967099e092.pdf",
    "bibtex": "@misc{rybak2020klejcomprehensivebenchmarkforpolish,\n    title = {KLEJ: Comprehensive Benchmark for Polish Language Understanding},\n    author = {Piotr Rybak and Robert Mroczkowski and Janusz Tracz and Ireneusz Gawlik},\n    year = {2020},\n    eprint = {2005.00630},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.00630},\n}",
    "abstract": "In recent years, a series of Transformer-based models unlocked major\nimprovements in general natural language understanding (NLU) tasks. Such a fast\npace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks\nare available only for a handful of languages. To alleviate this issue, we\nintroduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse\nset of tasks, adopted from existing datasets for named entity recognition,\nquestion-answering, textual entailment, and others. We also introduce a new\nsentiment analysis task for the e-commerce domain, named Allegro Reviews (AR).\nTo ensure a common evaluation scheme and promote models that generalize to\ndifferent NLU tasks, the benchmark includes datasets from varying domains and\napplications. Additionally, we release HerBERT, a Transformer-based model\ntrained specifically for the Polish language, which has the best average\nperformance and obtains the best results for three out of nine tasks. Finally,\nwe provide an extensive evaluation, including several standard baselines and\nrecently proposed, multilingual Transformer-based models.",
    "num_pages": 11
}