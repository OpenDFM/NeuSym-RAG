{
    "uuid": "3eb1e55f-5293-52ea-8b6f-1d2ff896bf3c",
    "title": "Continual Learning of Natural Language Processing Tasks: A Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zixuan Ke",
        "Bing Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.12701v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\3eb1e55f-5293-52ea-8b6f-1d2ff896bf3c.pdf",
    "bibtex": "@misc{ke2023continuallearningofnaturallanguage,\n    title = {Continual Learning of Natural Language Processing Tasks: A Survey},\n    author = {Zixuan Ke and Bing Liu},\n    year = {2023},\n    eprint = {2211.12701},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.12701},\n}",
    "abstract": "Continual learning (CL) is a learning paradigm that emulates the human\ncapability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the learned\nknowledge to help learn new tasks better. This survey presents a comprehensive\nreview and analysis of the recent progress of CL in NLP, which has significant\ndifferences from CL in computer vision and machine learning. It covers (1) all\nCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting\n(CF) prevention, (3) knowledge transfer (KT), which is particularly important\nfor NLP tasks; and (4) some theory and the hidden challenge of inter-task class\nseparation (ICS). (1), (3) and (4) have not been included in the existing\nsurvey. Finally, a list of future directions is discussed.",
    "num_pages": 16
}