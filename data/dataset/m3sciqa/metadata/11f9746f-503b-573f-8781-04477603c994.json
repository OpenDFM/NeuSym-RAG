{
    "uuid": "11f9746f-503b-573f-8781-04477603c994",
    "title": "Improving language models by retrieving from trillions of tokens",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Jordan Hoffmann",
        "Trevor Cai",
        "Eliza Rutherford",
        "Katie Millican",
        "George van den Driessche",
        "Jean-Baptiste Lespiau",
        "Bogdan Damoc",
        "Aidan Clark",
        "Diego de Las Casas",
        "Aurelia Guy",
        "Jacob Menick",
        "Roman Ring",
        "Tom Hennigan",
        "Saffron Huang",
        "Loren Maggiore",
        "Chris Jones",
        "Albin Cassirer",
        "Andy Brock",
        "Michela Paganini",
        "Geoffrey Irving",
        "Oriol Vinyals",
        "Simon Osindero",
        "Karen Simonyan",
        "Jack W. Rae",
        "Erich Elsen",
        "Laurent Sifre"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.04426v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\11f9746f-503b-573f-8781-04477603c994.pdf",
    "bibtex": "@misc{borgeaud2022improvinglanguagemodelsbyretrieving,\n    title = {Improving language models by retrieving from trillions of tokens},\n    author = {Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and Tom Hennigan and Saffron Huang and Loren Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and Karen Simonyan and Jack W. Rae and Erich Elsen and Laurent Sifre},\n    year = {2022},\n    eprint = {2112.04426},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2112.04426},\n}",
    "abstract": "We enhance auto-regressive language models by conditioning on document chunks\nretrieved from a large corpus, based on local similarity with preceding tokens.\nWith a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite\nusing 25$\\times$ fewer parameters. After fine-tuning, RETRO performance\ntranslates to downstream knowledge-intensive tasks such as question answering.\nRETRO combines a frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order of magnitude more\ndata than what is typically consumed during training. We typically train RETRO\nfrom scratch, yet can also rapidly RETROfit pre-trained transformers with\nretrieval and still achieve good performance. Our work opens up new avenues for\nimproving language models through explicit memory at unprecedented scale.",
    "num_pages": 43
}