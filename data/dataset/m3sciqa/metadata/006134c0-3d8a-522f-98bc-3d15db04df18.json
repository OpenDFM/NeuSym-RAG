{
    "uuid": "006134c0-3d8a-522f-98bc-3d15db04df18",
    "title": "Boosting Few-Shot Learning With Adaptive Margin Loss",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Aoxue Li",
        "Weiran Huang",
        "Xu Lan",
        "Jiashi Feng",
        "Zhenguo Li",
        "Liwei Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13826v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\006134c0-3d8a-522f-98bc-3d15db04df18.pdf",
    "bibtex": "@misc{li2020boostingfewshotlearningwithadaptive,\n    title = {Boosting Few-Shot Learning With Adaptive Margin Loss},\n    author = {Aoxue Li and Weiran Huang and Xu Lan and Jiashi Feng and Zhenguo Li and Liwei Wang},\n    year = {2020},\n    eprint = {2005.13826},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2005.13826},\n}",
    "abstract": "Few-shot learning (FSL) has attracted increasing attention in recent years\nbut remains challenging, due to the intrinsic difficulty in learning to\ngeneralize from a few examples. This paper proposes an adaptive margin\nprinciple to improve the generalization ability of metric-based meta-learning\napproaches for few-shot learning problems. Specifically, we first develop a\nclass-relevant additive margin loss, where semantic similarity between each\npair of classes is considered to separate samples in the feature embedding\nspace from similar classes. Further, we incorporate the semantic context among\nall classes in a sampled training task and develop a task-relevant additive\nmargin loss to better distinguish samples from different classes. Our adaptive\nmargin method can be easily extended to a more realistic generalized FSL\nsetting. Extensive experiments demonstrate that the proposed method can boost\nthe performance of current metric-based meta-learning approaches, under both\nthe standard FSL and generalized FSL settings.",
    "num_pages": 9
}