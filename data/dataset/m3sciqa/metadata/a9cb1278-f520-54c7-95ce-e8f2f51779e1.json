{
    "uuid": "a9cb1278-f520-54c7-95ce-e8f2f51779e1",
    "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chengcheng Han",
        "Xiaowei Du",
        "Che Zhang",
        "Yixin Lian",
        "Xiang Li",
        "Ming Gao",
        "Baoyuan Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.05074v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\a9cb1278-f520-54c7-95ce-e8f2f51779e1.pdf",
    "bibtex": "@misc{han2023dialcotmeetsppodecomposingand,\n    title = {DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models},\n    author = {Chengcheng Han and Xiaowei Du and Che Zhang and Yixin Lian and Xiang Li and Ming Gao and Baoyuan Wang},\n    year = {2023},\n    eprint = {2310.05074},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.05074},\n}",
    "abstract": "Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the\nreasoning capabilities of Large Language Models (LLMs) with at least 100\nbillion parameters. However, it is ineffective or even detrimental when applied\nto reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion\nparameters. To address this limitation, we introduce Dialogue-guided\nChain-of-Thought (DialCoT) which employs a dialogue format to generate\nintermediate reasoning steps, guiding the model toward the final answer.\nAdditionally, we optimize the model's reasoning path selection using the\nProximal Policy Optimization (PPO) algorithm, further enhancing its reasoning\ncapabilities. Our method offers several advantages compared to previous\napproaches. Firstly, we transform the process of solving complex reasoning\nquestions by breaking them down into a series of simpler sub-questions,\nsignificantly reducing the task difficulty and making it more suitable for\nSLMs. Secondly, we optimize the model's reasoning path selection through the\nPPO algorithm. We conduct comprehensive experiments on four arithmetic\nreasoning datasets, demonstrating that our method achieves significant\nperformance improvements compared to state-of-the-art competitors.",
    "num_pages": 14
}