{
    "uuid": "6bc1e005-7ec2-5a8b-b8b1-e6b71c3f0693",
    "title": "Generalization through Memorization: Nearest Neighbor Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Urvashi Khandelwal",
        "Omer Levy",
        "Dan Jurafsky",
        "Luke Zettlemoyer",
        "Mike Lewis"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00172v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\6bc1e005-7ec2-5a8b-b8b1-e6b71c3f0693.pdf",
    "bibtex": "@misc{khandelwal2020generalizationthroughmemorizationnearestneighbor,\n    title = {Generalization through Memorization: Nearest Neighbor Language Models},\n    author = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},\n    year = {2020},\n    eprint = {1911.00172},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1911.00172},\n}",
    "abstract": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM)\nby linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The\nnearest neighbors are computed according to distance in the pre-trained LM\nembedding space, and can be drawn from any text collection, including the\noriginal LM training data. Applying this augmentation to a strong Wikitext-103\nLM, with neighbors drawn from the original training set, our $k$NN-LM achieves\na new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no\nadditional training. We also show that this approach has implications for\nefficiently scaling up to larger training sets and allows for effective domain\nadaptation, by simply varying the nearest neighbor datastore, again without\nfurther training. Qualitatively, the model is particularly helpful in\npredicting rare patterns, such as factual knowledge. Together, these results\nstrongly suggest that learning similarity between sequences of text is easier\nthan predicting the next word, and that nearest neighbor search is an effective\napproach for language modeling in the long tail.",
    "num_pages": 13
}