{
    "uuid": "b0b8f08f-5cc3-51bd-9834-5c7e50ae692e",
    "title": "Recursive Top-Down Production for Sentence Generation with Latent Trees",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Shawn Tan",
        "Yikang Shen",
        "Timothy J. O'Donnell",
        "Alessandro Sordoni",
        "Aaron Courville"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04704v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\b0b8f08f-5cc3-51bd-9834-5c7e50ae692e.pdf",
    "bibtex": "@misc{tan2020recursivetopdownproductionforsentence,\n    title = {Recursive Top-Down Production for Sentence Generation with Latent Trees},\n    author = {Shawn Tan and Yikang Shen and Timothy J. O'Donnell and Alessandro Sordoni and Aaron Courville},\n    year = {2020},\n    eprint = {2010.04704},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.04704},\n}",
    "abstract": "We model the recursive production property of context-free grammars for\nnatural and synthetic languages. To this end, we present a dynamic programming\nalgorithm that marginalises over latent binary tree structures with $N$ leaves,\nallowing us to compute the likelihood of a sequence of $N$ tokens under a\nlatent tree model, which we maximise to train a recursive neural function. We\ndemonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017),\nwhere it outperforms previous models on the LENGTH split, and English question\nformation (McCoy et al., 2020), where it performs comparably to decoders with\nthe ground-truth tree structure. We also present experimental results on\nGerman-English translation on the Multi30k dataset (Elliott et al., 2016), and\nqualitatively analyse the induced tree structures our model learns for the SCAN\ntasks and the German-English translation task.",
    "num_pages": 17
}