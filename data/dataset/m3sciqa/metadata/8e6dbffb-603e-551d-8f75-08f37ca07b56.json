{
    "uuid": "8e6dbffb-603e-551d-8f75-08f37ca07b56",
    "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Laurent Sartran",
        "Samuel Barrett",
        "Adhiguna Kuncoro",
        "Miloš Stanojević",
        "Phil Blunsom",
        "Chris Dyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.00633v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\8e6dbffb-603e-551d-8f75-08f37ca07b56.pdf",
    "bibtex": "@misc{sartran2022transformergrammarsaugmentingtransformerlanguage,\n    title = {Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale},\n    author = {Laurent Sartran and Samuel Barrett and Adhiguna Kuncoro and Miloš Stanojević and Phil Blunsom and Chris Dyer},\n    year = {2022},\n    eprint = {2203.00633},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.00633},\n}",
    "abstract": "We introduce Transformer Grammars (TGs), a novel class of Transformer\nlanguage models that combine (i) the expressive power, scalability, and strong\nperformance of Transformers and (ii) recursive syntactic compositions, which\nhere are implemented through a special attention mask and deterministic\ntransformation of the linearized tree. We find that TGs outperform various\nstrong baselines on sentence-level language modeling perplexity, as well as on\nmultiple syntax-sensitive language modeling evaluation metrics. Additionally,\nwe find that the recursive syntactic composition bottleneck which represents\neach sentence as a single vector harms perplexity on document-level language\nmodeling, providing evidence that a different kind of memory mechanism -- one\nthat is independent of composed syntactic representations -- plays an important\nrole in current successful models of long text.",
    "num_pages": 17
}