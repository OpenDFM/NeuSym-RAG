{
    "uuid": "6cc62946-4f47-57f5-b109-b923f5840e29",
    "title": "Variational Recurrent Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Jinsong Su",
        "Shan Wu",
        "Deyi Xiong",
        "Yaojie Lu",
        "Xianpei Han",
        "Biao Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1801.05119v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\6cc62946-4f47-57f5-b109-b923f5840e29.pdf",
    "bibtex": "@misc{su2018variationalrecurrentneuralmachinetranslation,\n    title = {Variational Recurrent Neural Machine Translation},\n    author = {Jinsong Su and Shan Wu and Deyi Xiong and Yaojie Lu and Xianpei Han and Biao Zhang},\n    year = {2018},\n    eprint = {1801.05119},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1801.05119},\n}",
    "abstract": "Partially inspired by successful applications of variational recurrent neural\nnetworks, we propose a novel variational recurrent neural machine translation\n(VRNMT) model in this paper. Different from the variational NMT, VRNMT\nintroduces a series of latent random variables to model the translation\nprocedure of a sentence in a generative way, instead of a single latent\nvariable. Specifically, the latent random variables are included into the\nhidden states of the NMT decoder with elements from the variational\nautoencoder. In this way, these variables are recurrently generated, which\nenables them to further capture strong and complex dependencies among the\noutput translations at different timesteps. In order to deal with the\nchallenges in performing efficient posterior inference and large-scale training\nduring the incorporation of latent variables, we build a neural posterior\napproximator, and equip it with a reparameterization technique to estimate the\nvariational lower bound. Experiments on Chinese-English and English-German\ntranslation tasks demonstrate that the proposed model achieves significant\nimprovements over both the conventional and variational NMT models.",
    "num_pages": 8
}