{
    "uuid": "f23baff0-753e-5787-b392-d67f27d48d2c",
    "title": "The Knowref Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Ali Emami",
        "Paul Trichelair",
        "Adam Trischler",
        "Kaheer Suleman",
        "Hannes Schulz",
        "Jackie Chi Kit Cheung"
    ],
    "pdf_url": "http://arxiv.org/pdf/1811.01747v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\f23baff0-753e-5787-b392-d67f27d48d2c.pdf",
    "bibtex": "@misc{emami2019theknowrefcoreferencecorpusremoving,\n    title = {The Knowref Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution},\n    author = {Ali Emami and Paul Trichelair and Adam Trischler and Kaheer Suleman and Hannes Schulz and Jackie Chi Kit Cheung},\n    year = {2019},\n    eprint = {1811.01747},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1811.01747},\n}",
    "abstract": "We introduce a new benchmark for coreference resolution and NLI, Knowref,\nthat targets common-sense understanding and world knowledge. Previous\ncoreference resolution tasks can largely be solved by exploiting the number and\ngender of the antecedents, or have been handcrafted and do not reflect the\ndiversity of naturally occurring text. We present a corpus of over 8,000\nannotated text passages with ambiguous pronominal anaphora. These instances are\nboth challenging and realistic. We show that various coreference systems,\nwhether rule-based, feature-rich, or neural, perform significantly worse on the\ntask than humans, who display high inter-annotator agreement. To explain this\nperformance gap, we show empirically that state-of-the art models often fail to\ncapture context, instead relying on the gender or number of candidate\nantecedents to make a decision. We then use problem-specific insights to\npropose a data-augmentation trick called antecedent switching to alleviate this\ntendency in models. Finally, we show that antecedent switching yields promising\nresults on other tasks as well: we use it to achieve state-of-the-art results\non the GAP coreference task.",
    "num_pages": 11
}