{
    "uuid": "3327fb1f-7d25-5b78-9328-a1a471faecd3",
    "title": "Improving Variational Inference with Inverse Autoregressive Flow",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Diederik P. Kingma",
        "Tim Salimans",
        "Rafal Jozefowicz",
        "Xi Chen",
        "Ilya Sutskever",
        "Max Welling"
    ],
    "pdf_url": "http://arxiv.org/pdf/1606.04934v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\3327fb1f-7d25-5b78-9328-a1a471faecd3.pdf",
    "bibtex": "@misc{kingma2017improvingvariationalinferencewithinverse,\n    title = {Improving Variational Inference with Inverse Autoregressive Flow},\n    author = {Diederik P. Kingma and Tim Salimans and Rafal Jozefowicz and Xi Chen and Ilya Sutskever and Max Welling},\n    year = {2017},\n    eprint = {1606.04934},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1606.04934},\n}",
    "abstract": "The framework of normalizing flows provides a general strategy for flexible\nvariational inference of posteriors over latent variables. We propose a new\ntype of normalizing flow, inverse autoregressive flow (IAF), that, in contrast\nto earlier published flows, scales well to high-dimensional latent spaces. The\nproposed flow consists of a chain of invertible transformations, where each\ntransformation is based on an autoregressive neural network. In experiments, we\nshow that IAF significantly improves upon diagonal Gaussian approximate\nposteriors. In addition, we demonstrate that a novel type of variational\nautoencoder, coupled with IAF, is competitive with neural autoregressive models\nin terms of attained log-likelihood on natural images, while allowing\nsignificantly faster synthesis.",
    "num_pages": 16
}