{
    "uuid": "ffdb65b1-741c-5b1f-8468-2f3601aa347f",
    "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Zhiheng Huang",
        "Wei Xu",
        "Kai Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1508.01991v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\ffdb65b1-741c-5b1f-8468-2f3601aa347f.pdf",
    "bibtex": "@misc{huang2015bidirectionallstmcrfmodelsforsequence,\n    title = {Bidirectional LSTM-CRF Models for Sequence Tagging},\n    author = {Zhiheng Huang and Wei Xu and Kai Yu},\n    year = {2015},\n    eprint = {1508.01991},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1508.01991},\n}",
    "abstract": "In this paper, we propose a variety of Long Short-Term Memory (LSTM) based\nmodels for sequence tagging. These models include LSTM networks, bidirectional\nLSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer\n(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is\nthe first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to\nNLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model\ncan efficiently use both past and future input features thanks to a\nbidirectional LSTM component. It can also use sentence level tag information\nthanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or\nclose to) accuracy on POS, chunking and NER data sets. In addition, it is\nrobust and has less dependence on word embedding as compared to previous\nobservations.",
    "num_pages": 10
}