{
    "uuid": "93091dfb-5e24-551a-9651-6c000af2f432",
    "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jack W. Rae",
        "Sebastian Borgeaud",
        "Trevor Cai",
        "Katie Millican",
        "Jordan Hoffmann",
        "Francis Song",
        "John Aslanides",
        "Sarah Henderson",
        "Roman Ring",
        "Susannah Young",
        "Eliza Rutherford",
        "Tom Hennigan",
        "Jacob Menick",
        "Albin Cassirer",
        "Richard Powell",
        "George van den Driessche",
        "Lisa Anne Hendricks",
        "Maribeth Rauh",
        "Po-Sen Huang",
        "Amelia Glaese",
        "Johannes Welbl",
        "Sumanth Dathathri",
        "Saffron Huang",
        "Jonathan Uesato",
        "John Mellor",
        "Irina Higgins",
        "Antonia Creswell",
        "Nat McAleese",
        "Amy Wu",
        "Erich Elsen",
        "Siddhant Jayakumar",
        "Elena Buchatskaya",
        "David Budden",
        "Esme Sutherland",
        "Karen Simonyan",
        "Michela Paganini",
        "Laurent Sifre",
        "Lena Martens",
        "Xiang Lorraine Li",
        "Adhiguna Kuncoro",
        "Aida Nematzadeh",
        "Elena Gribovskaya",
        "Domenic Donato",
        "Angeliki Lazaridou",
        "Arthur Mensch",
        "Jean-Baptiste Lespiau",
        "Maria Tsimpoukelli",
        "Nikolai Grigorev",
        "Doug Fritz",
        "Thibault Sottiaux",
        "Mantas Pajarskas",
        "Toby Pohlen",
        "Zhitao Gong",
        "Daniel Toyama",
        "Cyprien de Masson d'Autume",
        "Yujia Li",
        "Tayfun Terzi",
        "Vladimir Mikulik",
        "Igor Babuschkin",
        "Aidan Clark",
        "Diego de Las Casas",
        "Aurelia Guy",
        "Chris Jones",
        "James Bradbury",
        "Matthew Johnson",
        "Blake Hechtman",
        "Laura Weidinger",
        "Iason Gabriel",
        "William Isaac",
        "Ed Lockhart",
        "Simon Osindero",
        "Laura Rimell",
        "Chris Dyer",
        "Oriol Vinyals",
        "Kareem Ayoub",
        "Jeff Stanway",
        "Lorrayne Bennett",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Geoffrey Irving"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.11446v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\93091dfb-5e24-551a-9651-6c000af2f432.pdf",
    "bibtex": "@misc{rae2022scalinglanguagemodelsmethodsanalysis,\n    title = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},\n    author = {Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},\n    year = {2022},\n    eprint = {2112.11446},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2112.11446},\n}",
    "abstract": "Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.",
    "num_pages": 120
}