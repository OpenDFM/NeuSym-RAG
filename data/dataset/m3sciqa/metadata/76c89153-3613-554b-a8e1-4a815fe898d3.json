{
    "uuid": "76c89153-3613-554b-a8e1-4a815fe898d3",
    "title": "CTC-based Non-autoregressive Speech Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chen Xu",
        "Xiaoqian Liu",
        "Xiaowen Liu",
        "Qingxuan Sun",
        "Yuhao Zhang",
        "Murun Yang",
        "Qianqian Dong",
        "Tom Ko",
        "Mingxuan Wang",
        "Tong Xiao",
        "Anxiang Ma",
        "Jingbo Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.17358v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\76c89153-3613-554b-a8e1-4a815fe898d3.pdf",
    "bibtex": "@misc{xu2023ctcbasednonautoregressivespeechtranslation,\n    title = {CTC-based Non-autoregressive Speech Translation},\n    author = {Chen Xu and Xiaoqian Liu and Xiaowen Liu and Qingxuan Sun and Yuhao Zhang and Murun Yang and Qianqian Dong and Tom Ko and Mingxuan Wang and Tong Xiao and Anxiang Ma and Jingbo Zhu},\n    year = {2023},\n    eprint = {2305.17358},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.17358},\n}",
    "abstract": "Combining end-to-end speech translation (ST) and non-autoregressive (NAR)\ngeneration is promising in language and speech processing for their advantages\nof less error propagation and low latency. In this paper, we investigate the\npotential of connectionist temporal classification (CTC) for non-autoregressive\nspeech translation (NAST). In particular, we develop a model consisting of two\nencoders that are guided by CTC to predict the source and target texts,\nrespectively. Introducing CTC into NAST on both language sides has obvious\nchallenges: 1) the conditional independent generation somewhat breaks the\ninterdependency among tokens, and 2) the monotonic alignment assumption in\nstandard CTC does not hold in translation tasks. In response, we develop a\nprediction-aware encoding approach and a cross-layer attention approach to\naddress these issues. We also use curriculum learning to improve convergence of\ntraining. Experiments on the MuST-C ST benchmarks show that our NAST model\nachieves an average BLEU score of 29.5 with a speed-up of 5.67$\\times$, which\nis comparable to the autoregressive counterpart and even outperforms the\nprevious best result of 0.9 BLEU points.",
    "num_pages": 17
}