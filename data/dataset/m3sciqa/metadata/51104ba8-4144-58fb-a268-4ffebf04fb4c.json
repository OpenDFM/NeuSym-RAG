{
    "uuid": "51104ba8-4144-58fb-a268-4ffebf04fb4c",
    "title": "Carbon Emissions and Large Neural Network Training",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "David Patterson",
        "Joseph Gonzalez",
        "Quoc Le",
        "Chen Liang",
        "Lluis-Miquel Munguia",
        "Daniel Rothchild",
        "David So",
        "Maud Texier",
        "Jeff Dean"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.10350v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\51104ba8-4144-58fb-a268-4ffebf04fb4c.pdf",
    "bibtex": "@misc{patterson2021carbonemissionsandlargeneural,\n    title = {Carbon Emissions and Large Neural Network Training},\n    author = {David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},\n    year = {2021},\n    eprint = {2104.10350},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2104.10350},\n}",
    "abstract": "The computation demand for machine learning (ML) has grown rapidly recently,\nwhich comes with a number of costs. Estimating the energy cost helps measure\nits environmental impact and finding greener strategies, yet it is challenging\nwithout detailed information. We calculate the energy use and carbon footprint\nof several recent large models-T5, Meena, GShard, Switch Transformer, and\nGPT-3-and refine earlier estimates for the neural architecture search that\nfound Evolved Transformer. We highlight the following opportunities to improve\nenergy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely\nactivated DNNs can consume <1/10th the energy of large, dense DNNs without\nsacrificing accuracy despite using as many or even more parameters. Geographic\nlocation matters for ML workload scheduling since the fraction of carbon-free\nenergy and resulting CO2e vary ~5X-10X, even within the same country and the\nsame organization. We are now optimizing where and when large models are\ntrained. Specific datacenter infrastructure matters, as Cloud datacenters can\nbe ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented\naccelerators inside them can be ~2-5X more effective than off-the-shelf\nsystems. Remarkably, the choice of DNN, datacenter, and processor can reduce\nthe carbon footprint up to ~100-1000X. These large factors also make\nretroactive estimates of energy cost difficult. To avoid miscalculations, we\nbelieve ML papers requiring large computational resources should make energy\nconsumption and CO2e explicit when practical. We are working to be more\ntransparent about energy use and CO2e in our future research. To help reduce\nthe carbon footprint of ML, we believe energy usage and CO2e should be a key\nmetric in evaluating models, and we are collaborating with MLPerf developers to\ninclude energy usage during training and inference in this industry standard\nbenchmark.",
    "num_pages": 22
}