{
    "uuid": "b7315180-e811-5a4a-882f-d4a8e0ffe866",
    "title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Haoyu Wang",
        "Shuo Wang",
        "Yukun Yan",
        "Xujia Wang",
        "Zhiyu Yang",
        "Yuzhuang Xu",
        "Zhenghao Liu",
        "Liner Yang",
        "Ning Ding",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "pdf_url": "http://arxiv.org/pdf/2402.04588v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\b7315180-e811-5a4a-882f-d4a8e0ffe866.pdf",
    "bibtex": "@misc{wang2024ultralinkanopensourceknowledgeenhancedmultilingual,\n    title = {UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset},\n    author = {Haoyu Wang and Shuo Wang and Yukun Yan and Xujia Wang and Zhiyu Yang and Yuzhuang Xu and Zhenghao Liu and Liner Yang and Ning Ding and Xu Han and Zhiyuan Liu and Maosong Sun},\n    year = {2024},\n    eprint = {2402.04588},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2402.04588},\n}",
    "abstract": "Open-source large language models (LLMs) have gained significant strength\nacross diverse fields. Nevertheless, the majority of studies primarily\nconcentrate on English, with only limited exploration into the realm of\nmultilingual abilities. In this work, we therefore construct an open-source\nmultilingual supervised fine-tuning dataset. Different from previous works that\nsimply translate English instructions, we consider both the language-specific\nand language-agnostic abilities of LLMs. Firstly, we introduce a\nknowledge-grounded data augmentation approach to elicit more language-specific\nknowledge of LLMs, improving their ability to serve users from different\ncountries. Moreover, we find modern LLMs possess strong cross-lingual transfer\ncapabilities, thus repeatedly learning identical content in various languages\nis not necessary. Consequently, we can substantially prune the\nlanguage-agnostic supervised fine-tuning (SFT) data without any performance\ndegradation, making multilingual SFT more efficient. The resulting UltraLink\ndataset comprises approximately 1 million samples across five languages (i.e.,\nEn, Zh, Ru, Fr, Es), and the proposed data construction method can be easily\nextended to other languages. UltraLink-LM, which is trained on UltraLink,\noutperforms several representative baselines across many tasks.",
    "num_pages": 10
}