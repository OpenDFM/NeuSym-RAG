{
    "uuid": "9e0baaab-f75d-5b52-b965-a5b427196392",
    "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho So",
        "Jaewoo Kang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.08746v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\9e0baaab-f75d-5b52-b965-a5b427196392.pdf",
    "bibtex": "@misc{lee2019biobertapretrainedbiomedicallanguage,\n    title = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},\n    author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},\n    year = {2019},\n    eprint = {1901.08746},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1901.08746},\n}",
    "abstract": "Biomedical text mining is becoming increasingly important as the number of\nbiomedical documents rapidly grows. With the progress in natural language\nprocessing (NLP), extracting valuable information from biomedical literature\nhas gained popularity among researchers, and deep learning has boosted the\ndevelopment of effective biomedical text mining models. However, directly\napplying the advancements in NLP to biomedical text mining often yields\nunsatisfactory results due to a word distribution shift from general domain\ncorpora to biomedical corpora. In this article, we investigate how the recently\nintroduced pre-trained language model BERT can be adapted for biomedical\ncorpora. We introduce BioBERT (Bidirectional Encoder Representations from\nTransformers for Biomedical Text Mining), which is a domain-specific language\nrepresentation model pre-trained on large-scale biomedical corpora. With almost\nthe same architecture across tasks, BioBERT largely outperforms BERT and\nprevious state-of-the-art models in a variety of biomedical text mining tasks\nwhen pre-trained on biomedical corpora. While BERT obtains performance\ncomparable to that of previous state-of-the-art models, BioBERT significantly\noutperforms them on the following three representative biomedical text mining\ntasks: biomedical named entity recognition (0.62% F1 score improvement),\nbiomedical relation extraction (2.80% F1 score improvement) and biomedical\nquestion answering (12.24% MRR improvement). Our analysis results show that\npre-training BERT on biomedical corpora helps it to understand complex\nbiomedical texts. We make the pre-trained weights of BioBERT freely available\nat https://github.com/naver/biobert-pretrained, and the source code for\nfine-tuning BioBERT available at https://github.com/dmis-lab/biobert.",
    "num_pages": 7
}