{
    "uuid": "e5036d4f-b1eb-5a28-b5e0-4887f14183d2",
    "title": "What do RNN Language Models Learn about Filler-Gap Dependencies?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Ethan Wilcox",
        "Roger Levy",
        "Takashi Morita",
        "Richard Futrell"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.00042v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\e5036d4f-b1eb-5a28-b5e0-4887f14183d2.pdf",
    "bibtex": "@misc{wilcox2018whatdornnlanguagemodels,\n    title = {What do RNN Language Models Learn about Filler-Gap Dependencies?},\n    author = {Ethan Wilcox and Roger Levy and Takashi Morita and Richard Futrell},\n    year = {2018},\n    eprint = {1809.00042},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1809.00042},\n}",
    "abstract": "RNN language models have achieved state-of-the-art perplexity results and\nhave proven useful in a suite of NLP tasks, but it is as yet unclear what\nsyntactic generalizations they learn. Here we investigate whether\nstate-of-the-art RNN language models represent long-distance filler-gap\ndependencies and constraints on them. Examining RNN behavior on experimentally\ncontrolled sentences designed to expose filler-gap dependencies, we show that\nRNNs can represent the relationship in multiple syntactic positions and over\nlarge spans of text. Furthermore, we show that RNNs learn a subset of the known\nrestrictions on filler-gap dependencies, known as island constraints: RNNs show\nevidence for wh-islands, adjunct islands, and complex NP islands. These studies\ndemonstrates that state-of-the-art RNN models are able to learn and generalize\nabout empty syntactic positions.",
    "num_pages": 11
}