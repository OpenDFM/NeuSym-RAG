{
    "uuid": "7efa89b4-4460-5eed-b6f0-62238a690c9b",
    "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Victor Sanh",
        "Lysandre Debut",
        "Julien Chaumond",
        "Thomas Wolf"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01108v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\7efa89b4-4460-5eed-b6f0-62238a690c9b.pdf",
    "bibtex": "@misc{sanh2020distilbertadistilledversionof,\n    title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n    author = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n    year = {2020},\n    eprint = {1910.01108},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1910.01108},\n}",
    "abstract": "As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.",
    "num_pages": 5
}