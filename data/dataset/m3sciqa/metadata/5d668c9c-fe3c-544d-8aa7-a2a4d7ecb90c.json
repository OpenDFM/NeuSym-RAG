{
    "uuid": "5d668c9c-fe3c-544d-8aa7-a2a4d7ecb90c",
    "title": "Wasserstein Auto-Encoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Ilya Tolstikhin",
        "Olivier Bousquet",
        "Sylvain Gelly",
        "Bernhard Schoelkopf"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.01558v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\5d668c9c-fe3c-544d-8aa7-a2a4d7ecb90c.pdf",
    "bibtex": "@misc{tolstikhin2019wassersteinautoencoders,\n    title = {Wasserstein Auto-Encoders},\n    author = {Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},\n    year = {2019},\n    eprint = {1711.01558},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1711.01558},\n}",
    "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score.",
    "num_pages": 20
}