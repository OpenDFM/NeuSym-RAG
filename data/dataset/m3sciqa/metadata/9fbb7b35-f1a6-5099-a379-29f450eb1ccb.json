{
    "uuid": "9fbb7b35-f1a6-5099-a379-29f450eb1ccb",
    "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yongming Rao",
        "Wenliang Zhao",
        "Benlin Liu",
        "Jiwen Lu",
        "Jie Zhou",
        "Cho-Jui Hsieh"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.02034v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\9fbb7b35-f1a6-5099-a379-29f450eb1ccb.pdf",
    "bibtex": "@misc{rao2021dynamicvitefficientvisiontransformerswith,\n    title = {DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},\n    author = {Yongming Rao and Wenliang Zhao and Benlin Liu and Jiwen Lu and Jie Zhou and Cho-Jui Hsieh},\n    year = {2021},\n    eprint = {2106.02034},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2106.02034},\n}",
    "abstract": "Attention is sparse in vision transformers. We observe the final prediction\nin vision transformers is only based on a subset of most informative tokens,\nwhich is sufficient for accurate image recognition. Based on this observation,\nwe propose a dynamic token sparsification framework to prune redundant tokens\nprogressively and dynamically based on the input. Specifically, we devise a\nlightweight prediction module to estimate the importance score of each token\ngiven the current features. The module is added to different layers to prune\nredundant tokens hierarchically. To optimize the prediction module in an\nend-to-end manner, we propose an attention masking strategy to differentiably\nprune a token by blocking its interactions with other tokens. Benefiting from\nthe nature of self-attention, the unstructured sparse tokens are still hardware\nfriendly, which makes our framework easy to achieve actual speed-up. By\nhierarchically pruning 66% of the input tokens, our method greatly reduces\n31%~37% FLOPs and improves the throughput by over 40% while the drop of\naccuracy is within 0.5% for various vision transformers. Equipped with the\ndynamic token sparsification framework, DynamicViT models can achieve very\ncompetitive complexity/accuracy trade-offs compared to state-of-the-art CNNs\nand vision transformers on ImageNet. Code is available at\nhttps://github.com/raoyongming/DynamicViT",
    "num_pages": 15
}