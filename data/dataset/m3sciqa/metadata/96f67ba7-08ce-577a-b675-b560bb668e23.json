{
    "uuid": "96f67ba7-08ce-577a-b675-b560bb668e23",
    "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Hardy",
        "Andreas Vlachos"
    ],
    "pdf_url": "http://arxiv.org/pdf/1808.09160v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\96f67ba7-08ce-577a-b675-b560bb668e23.pdf",
    "bibtex": "@misc{hardy2018guidedneurallanguagegenerationfor,\n    title = {Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation},\n    author = {Hardy and Andreas Vlachos},\n    year = {2018},\n    eprint = {1808.09160},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1808.09160},\n}",
    "abstract": "Recent work on abstractive summarization has made progress with neural\nencoder-decoder architectures. However, such models are often challenged due to\ntheir lack of explicit semantic modeling of the source document and its\nsummary. In this paper, we extend previous work on abstractive summarization\nusing Abstract Meaning Representation (AMR) with a neural language generation\nstage which we guide using the source document. We demonstrate that this\nguidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using\ngold standard AMR parses and parses obtained from an off-the-shelf parser\nrespectively. We also find that the summarization performance using the latter\nis 2 ROUGE-2 points higher than that of a well-established neural\nencoder-decoder approach trained on a larger dataset. Code is available at\n\\url{https://github.com/sheffieldnlp/AMR2Text-summ}",
    "num_pages": 6
}