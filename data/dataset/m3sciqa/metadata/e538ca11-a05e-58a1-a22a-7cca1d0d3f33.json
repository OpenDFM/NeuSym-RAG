{
    "uuid": "e538ca11-a05e-58a1-a22a-7cca1d0d3f33",
    "title": "Mix and Match: Learning-free Controllable Text Generation using Energy Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Fatemehsadat Mireshghallah",
        "Kartik Goyal",
        "Taylor Berg-Kirkpatrick"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.13299v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e538ca11-a05e-58a1-a22a-7cca1d0d3f33.pdf",
    "bibtex": "@misc{mireshghallah2022mixandmatchlearningfreecontrollable,\n    title = {Mix and Match: Learning-free Controllable Text Generation using Energy Language Models},\n    author = {Fatemehsadat Mireshghallah and Kartik Goyal and Taylor Berg-Kirkpatrick},\n    year = {2022},\n    eprint = {2203.13299},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.13299},\n}",
    "abstract": "Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.",
    "num_pages": 16
}