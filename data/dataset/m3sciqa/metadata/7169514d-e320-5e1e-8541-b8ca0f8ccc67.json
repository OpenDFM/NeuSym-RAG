{
    "uuid": "7169514d-e320-5e1e-8541-b8ca0f8ccc67",
    "title": "Learning to Revise References for Faithful Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Griffin Adams",
        "Han-Chin Shing",
        "Qing Sun",
        "Christopher Winestock",
        "Kathleen McKeown",
        "Noémie Elhadad"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.10290v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\7169514d-e320-5e1e-8541-b8ca0f8ccc67.pdf",
    "bibtex": "@misc{adams2022learningtorevisereferencesfor,\n    title = {Learning to Revise References for Faithful Summarization},\n    author = {Griffin Adams and Han-Chin Shing and Qing Sun and Christopher Winestock and Kathleen McKeown and Noémie Elhadad},\n    year = {2022},\n    eprint = {2204.10290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.10290},\n}",
    "abstract": "In real-world scenarios with naturally occurring datasets, reference\nsummaries are noisy and may contain information that cannot be inferred from\nthe source text. On large news corpora, removing low quality samples has been\nshown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora,\nfiltering is detrimental to performance. To improve reference quality while\nretaining all data, we propose a new approach: to selectively re-write\nunsupported reference sentences to better reflect source data. We automatically\ngenerate a synthetic dataset of positive and negative revisions by corrupting\nsupported sentences and learn to revise reference sentences with contrastive\nlearning. The intensity of revisions is treated as a controllable attribute so\nthat, at inference, diverse candidates can be over-generated-then-rescored to\nbalance faithfulness and abstraction. To test our methods, we extract noisy\nreferences from publicly available MIMIC-III discharge summaries for the task\nof hospital-course summarization, and vary the data on which models are\ntrained. According to metrics and human evaluation, models trained on revised\nclinical references are much more faithful, informative, and fluent than models\ntrained on original or filtered data.",
    "num_pages": 19
}