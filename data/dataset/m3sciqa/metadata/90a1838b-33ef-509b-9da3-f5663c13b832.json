{
    "uuid": "90a1838b-33ef-509b-9da3-f5663c13b832",
    "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Michael J. Q. Zhang",
        "Eunsol Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.14824v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\90a1838b-33ef-509b-9da3-f5663c13b832.pdf",
    "bibtex": "@misc{zhang2024mitigatingtemporalmisalignmentbydiscarding,\n    title = {Mitigating Temporal Misalignment by Discarding Outdated Facts},\n    author = {Michael J. Q. Zhang and Eunsol Choi},\n    year = {2024},\n    eprint = {2305.14824},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.14824},\n}",
    "abstract": "While large language models are able to retain vast amounts of world\nknowledge seen during pretraining, such knowledge is prone to going out of date\nand is nontrivial to update. Furthermore, these models are often used under\ntemporal misalignment, tasked with answering questions about the present,\ndespite having only been trained on data collected in the past. To mitigate the\neffects of temporal misalignment, we propose fact duration prediction: the task\nof predicting how long a given fact will remain true. In our experiments, we\ndemonstrate that identifying which facts are prone to rapid change can help\nmodels avoid reciting outdated information and determine which predictions\nrequire seeking out up-to-date knowledge sources. We also show how modeling\nfact duration improves calibration for knowledge-intensive tasks, such as\nopen-retrieval question answering, under temporal misalignment, by discarding\nvolatile facts. Our data and code are released publicly at\nhttps://github.com/mikejqzhang/mitigating_misalignment.",
    "num_pages": 14
}