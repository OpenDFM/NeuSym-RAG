{
    "uuid": "e5c41e78-a697-56ef-8a46-b3d30e9a9c86",
    "title": "Continual Lifelong Learning in Natural Language Processing: A Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Magdalena Biesialska",
        "Katarzyna Biesialska",
        "Marta R. Costa-jussà"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09823v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\e5c41e78-a697-56ef-8a46-b3d30e9a9c86.pdf",
    "bibtex": "@misc{biesialska2020continuallifelonglearninginnatural,\n    title = {Continual Lifelong Learning in Natural Language Processing: A Survey},\n    author = {Magdalena Biesialska and Katarzyna Biesialska and Marta R. Costa-jussà},\n    year = {2020},\n    eprint = {2012.09823},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.09823},\n}",
    "abstract": "Continual learning (CL) aims to enable information systems to learn from a\ncontinuous data stream across time. However, it is difficult for existing deep\nlearning architectures to learn a new task without largely forgetting\npreviously acquired knowledge. Furthermore, CL is particularly challenging for\nlanguage learning, as natural language is ambiguous: it is discrete,\ncompositional, and its meaning is context-dependent. In this work, we look at\nthe problem of CL through the lens of various NLP tasks. Our survey discusses\nmajor challenges in CL and current methods applied in neural network models. We\nalso provide a critical review of the existing CL evaluation methods and\ndatasets in NLP. Finally, we present our outlook on future research directions.",
    "num_pages": 19
}