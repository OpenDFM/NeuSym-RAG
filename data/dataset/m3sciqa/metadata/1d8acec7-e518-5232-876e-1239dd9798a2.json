{
    "uuid": "1d8acec7-e518-5232-876e-1239dd9798a2",
    "title": "Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jianlin Su",
        "Ahmed Murtadha",
        "Shengfeng Pan",
        "Jing Hou",
        "Jun Sun",
        "Wanwei Huang",
        "Bo Wen",
        "Yunfeng Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2208.03054v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\1d8acec7-e518-5232-876e-1239dd9798a2.pdf",
    "bibtex": "@misc{su2022globalpointernovelefficientspanbased,\n    title = {Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition},\n    author = {Jianlin Su and Ahmed Murtadha and Shengfeng Pan and Jing Hou and Jun Sun and Wanwei Huang and Bo Wen and Yunfeng Liu},\n    year = {2022},\n    eprint = {2208.03054},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2208.03054},\n}",
    "abstract": "Named entity recognition (NER) task aims at identifying entities from a piece\nof text that belong to predefined semantic types such as person, location,\norganization, etc. The state-of-the-art solutions for flat entities NER\ncommonly suffer from capturing the fine-grained semantic information in\nunderlying texts. The existing span-based approaches overcome this limitation,\nbut the computation time is still a concern. In this work, we propose a novel\nspan-based NER framework, namely Global Pointer (GP), that leverages the\nrelative positions through a multiplicative attention mechanism. The ultimate\ngoal is to enable a global view that considers the beginning and the end\npositions to predict the entity. To this end, we design two modules to identify\nthe head and the tail of a given entity to enable the inconsistency between the\ntraining and inference processes. Moreover, we introduce a novel classification\nloss function to address the imbalance label problem. In terms of parameters,\nwe introduce a simple but effective approximate method to reduce the training\nparameters. We extensively evaluate GP on various benchmark datasets. Our\nextensive experiments demonstrate that GP can outperform the existing solution.\nMoreover, the experimental results show the efficacy of the introduced loss\nfunction compared to softmax and entropy alternatives.",
    "num_pages": 12
}