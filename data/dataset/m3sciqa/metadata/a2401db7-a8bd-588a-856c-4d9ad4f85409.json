{
    "uuid": "a2401db7-a8bd-588a-856c-4d9ad4f85409",
    "title": "PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Minghao Xu",
        "Zuobai Zhang",
        "Jiarui Lu",
        "Zhaocheng Zhu",
        "Yangtian Zhang",
        "Chang Ma",
        "Runcheng Liu",
        "Jian Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.02096v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\a2401db7-a8bd-588a-856c-4d9ad4f85409.pdf",
    "bibtex": "@misc{xu2022peeracomprehensiveandmultitask,\n    title = {PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding},\n    author = {Minghao Xu and Zuobai Zhang and Jiarui Lu and Zhaocheng Zhu and Yangtian Zhang and Chang Ma and Runcheng Liu and Jian Tang},\n    year = {2022},\n    eprint = {2206.02096},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2206.02096},\n}",
    "abstract": "We are now witnessing significant progress of deep learning methods in a\nvariety of tasks (or datasets) of proteins. However, there is a lack of a\nstandard benchmark to evaluate the performance of different methods, which\nhinders the progress of deep learning in this field. In this paper, we propose\nsuch a benchmark called PEER, a comprehensive and multi-task benchmark for\nProtein sEquence undERstanding. PEER provides a set of diverse protein\nunderstanding tasks including protein function prediction, protein localization\nprediction, protein structure prediction, protein-protein interaction\nprediction, and protein-ligand interaction prediction. We evaluate different\ntypes of sequence-based methods for each task including traditional feature\nengineering approaches, different sequence encoding methods as well as\nlarge-scale pre-trained protein language models. In addition, we also\ninvestigate the performance of these methods under the multi-task learning\nsetting. Experimental results show that large-scale pre-trained protein\nlanguage models achieve the best performance for most individual tasks, and\njointly training multiple tasks further boosts the performance. The datasets\nand source codes of this benchmark are all available at\nhttps://github.com/DeepGraphLearning/PEER_Benchmark",
    "num_pages": 19
}