{
    "uuid": "ef1fd71c-6764-5364-8f8f-18d147a69e6d",
    "title": "Dynamic Context-guided Capsule Network for Multimodal Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Huan Lin",
        "Fandong Meng",
        "Jinsong Su",
        "Yongjing Yin",
        "Zhengyuan Yang",
        "Yubin Ge",
        "Jie Zhou",
        "Jiebo Luo"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.02016v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\ef1fd71c-6764-5364-8f8f-18d147a69e6d.pdf",
    "bibtex": "@misc{lin2020dynamiccontextguidedcapsulenetworkfor,\n    title = {Dynamic Context-guided Capsule Network for Multimodal Machine Translation},\n    author = {Huan Lin and Fandong Meng and Jinsong Su and Yongjing Yin and Zhengyuan Yang and Yubin Ge and Jie Zhou and Jiebo Luo},\n    year = {2020},\n    eprint = {2009.02016},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.02016},\n}",
    "abstract": "Multimodal machine translation (MMT), which mainly focuses on enhancing\ntext-only translation with visual features, has attracted considerable\nattention from both computer vision and natural language processing\ncommunities. Most current MMT models resort to attention mechanism, global\ncontext modeling or multimodal joint representation learning to utilize visual\nfeatures. However, the attention mechanism lacks sufficient semantic\ninteractions between modalities while the other two provide fixed visual\ncontext, which is unsuitable for modeling the observed variability when\ngenerating translation. To address the above issues, in this paper, we propose\na novel Dynamic Context-guided Capsule Network (DCCN) for MMT. Specifically, at\neach timestep of decoding, we first employ the conventional source-target\nattention to produce a timestep-specific source-side context vector. Next, DCCN\ntakes this vector as input and uses it to guide the iterative extraction of\nrelated visual features via a context-guided dynamic routing mechanism.\nParticularly, we represent the input image with global and regional visual\nfeatures, we introduce two parallel DCCNs to model multimodal context vectors\nwith visual features at different granularities. Finally, we obtain two\nmultimodal context vectors, which are fused and incorporated into the decoder\nfor the prediction of the target word. Experimental results on the Multi30K\ndataset of English-to-German and English-to-French translation demonstrate the\nsuperiority of DCCN. Our code is available on\nhttps://github.com/DeepLearnXMU/MM-DCCN.",
    "num_pages": 10
}