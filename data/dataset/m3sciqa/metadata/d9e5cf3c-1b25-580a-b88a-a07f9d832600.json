{
    "uuid": "d9e5cf3c-1b25-580a-b88a-a07f9d832600",
    "title": "Human-like Summarization Evaluation with ChatGPT",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Mingqi Gao",
        "Jie Ruan",
        "Renliang Sun",
        "Xunjian Yin",
        "Shiping Yang",
        "Xiaojun Wan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.02554v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\d9e5cf3c-1b25-580a-b88a-a07f9d832600.pdf",
    "bibtex": "@misc{gao2023humanlikesummarizationevaluationwithchatgpt,\n    title = {Human-like Summarization Evaluation with ChatGPT},\n    author = {Mingqi Gao and Jie Ruan and Renliang Sun and Xunjian Yin and Shiping Yang and Xiaojun Wan},\n    year = {2023},\n    eprint = {2304.02554},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.02554},\n}",
    "abstract": "Evaluating text summarization is a challenging problem, and existing\nevaluation metrics are far from satisfactory. In this study, we explored\nChatGPT's ability to perform human-like summarization evaluation using four\nhuman evaluation methods on five datasets. We found that ChatGPT was able to\ncomplete annotations relatively smoothly using Likert scale scoring, pairwise\ncomparison, Pyramid, and binary factuality evaluation. Additionally, it\noutperformed commonly used automatic evaluation metrics on some datasets.\nFurthermore, we discussed the impact of different prompts, compared its\nperformance with that of human evaluation, and analyzed the generated\nexplanations and invalid responses.",
    "num_pages": 9
}