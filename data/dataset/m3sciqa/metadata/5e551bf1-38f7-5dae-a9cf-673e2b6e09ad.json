{
    "uuid": "5e551bf1-38f7-5dae-a9cf-673e2b6e09ad",
    "title": "PMET: Precise Model Editing in a Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Xiaopeng Li",
        "Shasha Li",
        "Shezheng Song",
        "Jing Yang",
        "Jun Ma",
        "Jie Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2308.08742v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\5e551bf1-38f7-5dae-a9cf-673e2b6e09ad.pdf",
    "bibtex": "@misc{li2024pmetprecisemodeleditingin,\n    title = {PMET: Precise Model Editing in a Transformer},\n    author = {Xiaopeng Li and Shasha Li and Shezheng Song and Jing Yang and Jun Ma and Jie Yu},\n    year = {2024},\n    eprint = {2308.08742},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2308.08742},\n}",
    "abstract": "Model editing techniques modify a minor proportion of knowledge in Large\nLanguage Models (LLMs) at a relatively low cost, which have demonstrated\nnotable success. Existing methods assume Transformer Layer (TL) hidden states\nare values of key-value memories of the Feed-Forward Network (FFN). They\nusually optimize the TL hidden states to memorize target knowledge and use it\nto update the weights of the FFN in LLMs. However, the information flow of TL\nhidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,\nand residual connections. Existing methods neglect the fact that the TL hidden\nstates contains information not specifically required for FFN. Consequently,\nthe performance of model editing decreases. To achieve more precise model\nediting, we analyze hidden states of MHSA and FFN, finding that MHSA encodes\ncertain general knowledge extraction patterns. This implies that MHSA weights\ndo not require updating when new knowledge is introduced. Based on above\nfindings, we introduce PMET, which simultaneously optimizes Transformer\nComponent (TC, namely MHSA and FFN) hidden states, while only using the\noptimized TC hidden states of FFN to precisely update FFN weights. Our\nexperiments demonstrate that PMET exhibits state-of-the-art performance on both\nthe COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the\neffectiveness of our enhancements, further reinforcing the finding that the\nMHSA encodes certain general knowledge extraction patterns and indicating its\nstorage of a small amount of factual knowledge. Our code is available at\nhttps://github.com/xpq-tech/PMET.",
    "num_pages": 11
}