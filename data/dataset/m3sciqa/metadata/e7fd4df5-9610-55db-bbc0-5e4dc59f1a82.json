{
    "uuid": "e7fd4df5-9610-55db-bbc0-5e4dc59f1a82",
    "title": "CoNT: Contrastive Neural Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chenxin An",
        "Jiangtao Feng",
        "Kai Lv",
        "Lingpeng Kong",
        "Xipeng Qiu",
        "Xuanjing Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.14690v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\e7fd4df5-9610-55db-bbc0-5e4dc59f1a82.pdf",
    "bibtex": "@misc{an2023contcontrastiveneuraltextgeneration,\n    title = {CoNT: Contrastive Neural Text Generation},\n    author = {Chenxin An and Jiangtao Feng and Kai Lv and Lingpeng Kong and Xipeng Qiu and Xuanjing Huang},\n    year = {2023},\n    eprint = {2205.14690},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.14690},\n}",
    "abstract": "Recently, contrastive learning attracts increasing interests in neural text\ngeneration as a new solution to alleviate the exposure bias problem. It\nintroduces a sequence-level training signal which is crucial to generation\ntasks that always rely on auto-regressive decoding. However, previous methods\nusing contrastive learning in neural text generation usually lead to inferior\nperformance. In this paper, we analyse the underlying reasons and propose a new\nContrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks\nthat prevent contrastive learning from being widely adopted in generation tasks\nfrom three aspects -- the construction of contrastive examples, the choice of\nthe contrastive loss, and the strategy in decoding. We validate CoNT on five\ngeneration tasks with ten benchmarks, including machine translation,\nsummarization, code comment generation, data-to-text generation and commonsense\ngeneration. Experimental results show that CoNT clearly outperforms the\nconventional training framework on all the ten benchmarks with a convincing\nmargin. Especially, CoNT surpasses previous the most competitive contrastive\nlearning method for text generation, by 1.50 BLEU on machine translation and\n1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art\non summarization, code comment generation (without external data) and\ndata-to-text generation.",
    "num_pages": 21
}