{
    "uuid": "2f767215-ca92-5449-9a65-f9628c071a18",
    "title": "One-shot and few-shot learning of word embeddings",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Andrew K. Lampinen",
        "James L. McClelland"
    ],
    "pdf_url": "http://arxiv.org/pdf/1710.10280v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\2f767215-ca92-5449-9a65-f9628c071a18.pdf",
    "bibtex": "@misc{lampinen2018oneshotandfewshotlearningof,\n    title = {One-shot and few-shot learning of word embeddings},\n    author = {Andrew K. Lampinen and James L. McClelland},\n    year = {2018},\n    eprint = {1710.10280},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1710.10280},\n}",
    "abstract": "Standard deep learning systems require thousands or millions of examples to\nlearn a concept, and cannot integrate new concepts easily. By contrast, humans\nhave an incredible ability to do one-shot or few-shot learning. For instance,\nfrom just hearing a word used in a sentence, humans can infer a great deal\nabout it, by leveraging what the syntax and semantics of the surrounding words\ntells us. Here, we draw inspiration from this to highlight a simple technique\nby which deep recurrent networks can similarly exploit their prior knowledge to\nlearn a useful representation for a new word from little data. This could make\nnatural language processing systems much more flexible, by allowing them to\nlearn continually from the new words they encounter.",
    "num_pages": 15
}