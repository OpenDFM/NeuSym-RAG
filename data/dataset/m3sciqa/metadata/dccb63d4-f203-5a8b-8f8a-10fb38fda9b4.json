{
    "uuid": "dccb63d4-f203-5a8b-8f8a-10fb38fda9b4",
    "title": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Devendra Singh Sachan",
        "Siva Reddy",
        "William Hamilton",
        "Chris Dyer",
        "Dani Yogatama"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05346v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\dccb63d4-f203-5a8b-8f8a-10fb38fda9b4.pdf",
    "bibtex": "@misc{sachan2021endtoendtrainingofmultidocumentreader,\n    title = {End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering},\n    author = {Devendra Singh Sachan and Siva Reddy and William Hamilton and Chris Dyer and Dani Yogatama},\n    year = {2021},\n    eprint = {2106.05346},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.05346},\n}",
    "abstract": "We present an end-to-end differentiable training method for\nretrieval-augmented open-domain question answering systems that combine\ninformation from multiple retrieved documents when generating answers. We model\nretrieval decisions as latent variables over sets of relevant documents. Since\nmarginalizing over sets of retrieved documents is computationally hard, we\napproximate this using an expectation-maximization algorithm. We iteratively\nestimate the value of our latent variable (the set of relevant documents for a\ngiven question) and then use this estimate to update the retriever and reader\nparameters. We hypothesize that such end-to-end training allows training\nsignals to flow to the reader and then to the retriever better than staged-wise\ntraining. This results in a retriever that is able to select more relevant\ndocuments for a question and a reader that is trained on more accurate\ndocuments to generate an answer. Experiments on three benchmark datasets\ndemonstrate that our proposed method outperforms all existing approaches of\ncomparable size by 2-3% absolute exact match points, achieving new\nstate-of-the-art results. Our results also demonstrate the feasibility of\nlearning to retrieve to improve answer generation without explicit supervision\nof retrieval decisions.",
    "num_pages": 22
}