{
    "uuid": "46fd03d6-7a66-5072-b2ab-61e072e5131f",
    "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Zhilin Yang",
        "Peng Qi",
        "Saizheng Zhang",
        "Yoshua Bengio",
        "William W. Cohen",
        "Ruslan Salakhutdinov",
        "Christopher D. Manning"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.09600v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\46fd03d6-7a66-5072-b2ab-61e072e5131f.pdf",
    "bibtex": "@misc{yang2018hotpotqaadatasetfordiverse,\n    title = {HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n    author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},\n    year = {2018},\n    eprint = {1809.09600},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1809.09600},\n}",
    "abstract": "Existing question answering (QA) datasets fail to train QA systems to perform\ncomplex reasoning and provide explanations for answers. We introduce HotpotQA,\na new dataset with 113k Wikipedia-based question-answer pairs with four key\nfeatures: (1) the questions require finding and reasoning over multiple\nsupporting documents to answer; (2) the questions are diverse and not\nconstrained to any pre-existing knowledge bases or knowledge schemas; (3) we\nprovide sentence-level supporting facts required for reasoning, allowing QA\nsystems to reason with strong supervision and explain the predictions; (4) we\noffer a new type of factoid comparison questions to test QA systems' ability to\nextract relevant facts and perform necessary comparison. We show that HotpotQA\nis challenging for the latest QA systems, and the supporting facts enable\nmodels to improve performance and make explainable predictions.",
    "num_pages": 12
}