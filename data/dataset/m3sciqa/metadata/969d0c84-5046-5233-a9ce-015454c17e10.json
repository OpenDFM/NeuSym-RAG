{
    "uuid": "969d0c84-5046-5233-a9ce-015454c17e10",
    "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Xiaofeng Zhang",
        "Yikang Shen",
        "Zeyu Huang",
        "Jie Zhou",
        "Wenge Rong",
        "Zhang Xiong"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.05144v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\969d0c84-5046-5233-a9ce-015454c17e10.pdf",
    "bibtex": "@misc{zhang2022mixtureofattentionheadsselecting,\n    title = {Mixture of Attention Heads: Selecting Attention Heads Per Token},\n    author = {Xiaofeng Zhang and Yikang Shen and Zeyu Huang and Jie Zhou and Wenge Rong and Zhang Xiong},\n    year = {2022},\n    eprint = {2210.05144},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.05144},\n}",
    "abstract": "Mixture-of-Experts (MoE) networks have been proposed as an efficient way to\nscale up model capacity and implement conditional computing. However, the study\nof MoE components mostly focused on the feedforward layer in Transformer\narchitecture. This paper proposes the Mixture of Attention Heads (MoA), a new\narchitecture that combines multi-head attention with the MoE mechanism. MoA\nincludes a set of attention heads that each has its own set of parameters.\nGiven an input, a router dynamically selects a subset of $k$ attention heads\nper token. This conditional computation schema allows MoA to achieve stronger\nperformance than the standard multi-head attention layer. Furthermore, the\nsparsely gated MoA can easily scale up the number of attention heads and the\nnumber of parameters while preserving computational efficiency. In addition to\nthe performance improvements, MoA also automatically differentiates heads'\nutilities, providing a new perspective to discuss the model's interpretability.\nWe conducted experiments on several important tasks, including Machine\nTranslation and Masked Language Modeling. Experiments have shown promising\nresults on several tasks against strong baselines that involve large and very\ndeep models.",
    "num_pages": 13
}