{
    "uuid": "28428662-2973-5032-9217-2aa27a59f6de",
    "title": "STaR: Bootstrapping Reasoning With Reasoning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Eric Zelikman",
        "Yuhuai Wu",
        "Jesse Mu",
        "Noah D. Goodman"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.14465v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\28428662-2973-5032-9217-2aa27a59f6de.pdf",
    "bibtex": "@misc{zelikman2022starbootstrappingreasoningwithreasoning,\n    title = {STaR: Bootstrapping Reasoning With Reasoning},\n    author = {Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},\n    year = {2022},\n    eprint = {2203.14465},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2203.14465},\n}",
    "abstract": "Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.",
    "num_pages": 30
}