{
    "uuid": "83c3e134-0000-581c-a6f2-d311645b274b",
    "title": "StrucTexT: Structured Text Understanding with Multi-Modal Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yulin Li",
        "Yuxi Qian",
        "Yuchen Yu",
        "Xiameng Qin",
        "Chengquan Zhang",
        "Yan Liu",
        "Kun Yao",
        "Junyu Han",
        "Jingtuo Liu",
        "Errui Ding"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.02923v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\83c3e134-0000-581c-a6f2-d311645b274b.pdf",
    "bibtex": "@misc{li2021structextstructuredtextunderstandingwith,\n    title = {StrucTexT: Structured Text Understanding with Multi-Modal Transformers},\n    author = {Yulin Li and Yuxi Qian and Yuchen Yu and Xiameng Qin and Chengquan Zhang and Yan Liu and Kun Yao and Junyu Han and Jingtuo Liu and Errui Ding},\n    year = {2021},\n    eprint = {2108.02923},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2108.02923},\n}",
    "abstract": "Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.",
    "num_pages": 9
}