{
    "uuid": "d625e278-c235-5992-bfb7-2e447d365a2e",
    "title": "Expression Snippet Transformer for Robust Video-based Facial Expression Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yuanyuan Liu",
        "Wenbin Wang",
        "Chuanxu Feng",
        "Haoyu Zhang",
        "Zhe Chen",
        "Yibing Zhan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08409v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\d625e278-c235-5992-bfb7-2e447d365a2e.pdf",
    "bibtex": "@misc{liu2021expressionsnippettransformerforrobust,\n    title = {Expression Snippet Transformer for Robust Video-based Facial Expression Recognition},\n    author = {Yuanyuan Liu and Wenbin Wang and Chuanxu Feng and Haoyu Zhang and Zhe Chen and Yibing Zhan},\n    year = {2021},\n    eprint = {2109.08409},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2109.08409},\n}",
    "abstract": "The recent success of Transformer has provided a new direction to various\nvisual understanding tasks, including video-based facial expression recognition\n(FER). By modeling visual relations effectively, Transformer has shown its\npower for describing complicated patterns. However, Transformer still performs\nunsatisfactorily to notice subtle facial expression movements, because the\nexpression movements of many videos can be too small to extract meaningful\nspatial-temporal relations and achieve robust performance. To this end, we\npropose to decompose each video into a series of expression snippets, each of\nwhich contains a small number of facial movements, and attempt to augment the\nTransformer's ability for modeling intra-snippet and inter-snippet visual\nrelations, respectively, obtaining the Expression snippet Transformer (EST). In\nparticular, for intra-snippet modeling, we devise an attention-augmented\nsnippet feature extractor (AA-SFE) to enhance the encoding of subtle facial\nmovements of each snippet by gradually attending to more salient information.\nIn addition, for inter-snippet modeling, we introduce a shuffled snippet order\nprediction (SSOP) head and a corresponding loss to improve the modeling of\nsubtle motion changes across subsequent snippets by training the Transformer to\nidentify shuffled snippet orders. Extensive experiments on four challenging\ndatasets (i.e., BU-3DFE, MMI, AFEW, and DFEW) demonstrate that our EST is\nsuperior to other CNN-based methods, obtaining state-of-the-art performance.",
    "num_pages": 18
}