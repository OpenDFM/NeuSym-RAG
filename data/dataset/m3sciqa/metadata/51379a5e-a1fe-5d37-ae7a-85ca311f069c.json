{
    "uuid": "51379a5e-a1fe-5d37-ae7a-85ca311f069c",
    "title": "Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Alexander Ku",
        "Peter Anderson",
        "Roma Patel",
        "Eugene Ie",
        "Jason Baldridge"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07954v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\51379a5e-a1fe-5d37-ae7a-85ca311f069c.pdf",
    "bibtex": "@misc{ku2020roomacrossroommultilingualvisionandlanguagenavigationwith,\n    title = {Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding},\n    author = {Alexander Ku and Peter Anderson and Roma Patel and Eugene Ie and Jason Baldridge},\n    year = {2020},\n    eprint = {2010.07954},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2010.07954},\n}",
    "abstract": "We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation\n(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger\n(more paths and instructions) than other VLN datasets. It emphasizes the role\nof language in VLN by addressing known biases in paths and eliciting more\nreferences to visible entities. Furthermore, each word in an instruction is\ntime-aligned to the virtual poses of instruction creators and validators. We\nestablish baseline scores for monolingual and multilingual settings and\nmultitask learning when including Room-to-Room annotations. We also provide\nresults for a model that learns from synchronized pose traces by focusing only\non portions of the panorama attended to in human demonstrations. The size,\nscope and detail of RxR dramatically expands the frontier for research on\nembodied language agents in simulated, photo-realistic environments.",
    "num_pages": 21
}