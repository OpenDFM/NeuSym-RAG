{
    "uuid": "f1a59d96-65d9-5a8a-a47e-747a32a51865",
    "title": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Shamsuddeen Hassan Muhammad",
        "David Ifeoluwa Adelani",
        "Sebastian Ruder",
        "Ibrahim Said Ahmad",
        "Idris Abdulmumin",
        "Bello Shehu Bello",
        "Monojit Choudhury",
        "Chris Chinenye Emezue",
        "Saheed Salahudeen Abdullahi",
        "Anuoluwapo Aremu",
        "Alipio Jeorge",
        "Pavel Brazdil"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.08277v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\f1a59d96-65d9-5a8a-a47e-747a32a51865.pdf",
    "bibtex": "@misc{muhammad2022naijasentianigeriantwittersentiment,\n    title = {NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis},\n    author = {Shamsuddeen Hassan Muhammad and David Ifeoluwa Adelani and Sebastian Ruder and Ibrahim Said Ahmad and Idris Abdulmumin and Bello Shehu Bello and Monojit Choudhury and Chris Chinenye Emezue and Saheed Salahudeen Abdullahi and Anuoluwapo Aremu and Alipio Jeorge and Pavel Brazdil},\n    year = {2022},\n    eprint = {2201.08277},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.08277},\n}",
    "abstract": "Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.",
    "num_pages": 13
}