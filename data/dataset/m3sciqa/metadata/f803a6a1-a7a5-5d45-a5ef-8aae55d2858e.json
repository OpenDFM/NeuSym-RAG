{
    "uuid": "f803a6a1-a7a5-5d45-a5ef-8aae55d2858e",
    "title": "Hash Layers For Large Sparse Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Stephen Roller",
        "Sainbayar Sukhbaatar",
        "Arthur Szlam",
        "Jason Weston"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04426v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f803a6a1-a7a5-5d45-a5ef-8aae55d2858e.pdf",
    "bibtex": "@misc{roller2021hashlayersforlargesparse,\n    title = {Hash Layers For Large Sparse Models},\n    author = {Stephen Roller and Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston},\n    year = {2021},\n    eprint = {2106.04426},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2106.04426},\n}",
    "abstract": "We investigate the training of sparse layers that use different parameters\nfor different inputs based on hashing in large Transformer models.\nSpecifically, we modify the feedforward layer to hash to different sets of\nweights depending on the current token, over all tokens in the sequence. We\nshow that this procedure either outperforms or is competitive with\nlearning-to-route mixture-of-expert methods such as Switch Transformers and\nBASE Layers, while requiring no routing parameters or extra terms in the\nobjective function such as a load balancing loss, and no sophisticated\nassignment algorithm. We study the performance of different hashing techniques,\nhash sizes and input features, and show that balanced and random hashes focused\non the most local features work best, compared to either learning clusters or\nusing longer-range context. We show our approach works well both on large\nlanguage modeling and dialogue tasks, and on downstream fine-tuning tasks.",
    "num_pages": 16
}