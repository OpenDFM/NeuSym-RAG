{
    "uuid": "e013d77b-3c7f-5bb6-9836-f7c3b0d6991b",
    "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Angeliki Lazaridou",
        "Elena Gribovskaya",
        "Wojciech Stokowiec",
        "Nikolai Grigorev"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.05115v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e013d77b-3c7f-5bb6-9836-f7c3b0d6991b.pdf",
    "bibtex": "@misc{lazaridou2022internetaugmentedlanguagemodelsthroughfewshot,\n    title = {Internet-augmented language models through few-shot prompting for open-domain question answering},\n    author = {Angeliki Lazaridou and Elena Gribovskaya and Wojciech Stokowiec and Nikolai Grigorev},\n    year = {2022},\n    eprint = {2203.05115},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.05115},\n}",
    "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of\nlarge-scale language models (LSLMs) to overcome some of their challenges with\nrespect to grounding to factual and up-to-date information. Motivated by\nsemi-parametric language models (LMs), which ground their decisions in external\nretrieved evidence, we use few-shot prompting to learn to condition LMs on\ninformation returned from the web using Google Search, a broad and constantly\nupdated knowledge source. Our approach does not involve fine-tuning or learning\nadditional parameters, thus making it applicable to any LM, offering therefore\na strong baseline. Indeed, we find that LMs conditioned on the web surpass\nperformance of closed-book models of similar, or even larger, model sizes in\nopen-domain question answering. Finally, we find that increasing the\ninference-time compute of models, achieved via using multiple retrieved\nevidences to generate multiple answers followed by a reranking stage that uses\nscores generated by the same LMs, leads to better performance and alleviates\nlower performance of smaller few-shot LMs. All in all, our findings suggest\nthat it might be beneficial to slow down the race towards the biggest model and\ninstead shift attention towards finding more effective ways to use models,\nincluding but not limited to, better prompting or increasing inference-time\ncompute.",
    "num_pages": 20
}