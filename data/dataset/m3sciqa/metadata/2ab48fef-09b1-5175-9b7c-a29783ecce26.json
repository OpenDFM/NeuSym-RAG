{
    "uuid": "2ab48fef-09b1-5175-9b7c-a29783ecce26",
    "title": "Random Features Strengthen Graph Neural Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Ryoma Sato",
        "Makoto Yamada",
        "Hisashi Kashima"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.03155v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\2ab48fef-09b1-5175-9b7c-a29783ecce26.pdf",
    "bibtex": "@misc{sato2021randomfeaturesstrengthengraphneural,\n    title = {Random Features Strengthen Graph Neural Networks},\n    author = {Ryoma Sato and Makoto Yamada and Hisashi Kashima},\n    year = {2021},\n    eprint = {2002.03155},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2002.03155},\n}",
    "abstract": "Graph neural networks (GNNs) are powerful machine learning models for various\ngraph learning tasks. Recently, the limitations of the expressive power of\nvarious GNN models have been revealed. For example, GNNs cannot distinguish\nsome non-isomorphic graphs and they cannot learn efficient graph algorithms. In\nthis paper, we demonstrate that GNNs become powerful just by adding a random\nfeature to each node. We prove that the random features enable GNNs to learn\nalmost optimal polynomial-time approximation algorithms for the minimum\ndominating set problem and maximum matching problem in terms of approximation\nratios. The main advantage of our method is that it can be combined with\noff-the-shelf GNN models with slight modifications. Through experiments, we\nshow that the addition of random features enables GNNs to solve various\nproblems that normal GNNs, including the graph convolutional networks (GCNs)\nand graph isomorphism networks (GINs), cannot solve.",
    "num_pages": 13
}