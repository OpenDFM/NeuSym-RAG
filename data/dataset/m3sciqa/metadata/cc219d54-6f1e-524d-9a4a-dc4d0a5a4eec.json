{
    "uuid": "cc219d54-6f1e-524d-9a4a-dc4d0a5a4eec",
    "title": "I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yang Liu",
        "Zequn Sun",
        "Guangyao Li",
        "Wei Hu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2208.09828v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\cc219d54-6f1e-524d-9a4a-dc4d0a5a4eec.pdf",
    "bibtex": "@misc{liu2022iknowwhatyoudo,\n    title = {I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning},\n    author = {Yang Liu and Zequn Sun and Guangyao Li and Wei Hu},\n    year = {2022},\n    eprint = {2208.09828},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2208.09828},\n}",
    "abstract": "Knowledge graph (KG) embedding seeks to learn vector representations for\nentities and relations. Conventional models reason over graph structures, but\nthey suffer from the issues of graph incompleteness and long-tail entities.\nRecent studies have used pre-trained language models to learn embeddings based\non the textual information of entities and relations, but they cannot take\nadvantage of graph structures. In the paper, we show empirically that these two\nkinds of features are complementary for KG embedding. To this end, we propose\nCoLE, a Co-distillation Learning method for KG Embedding that exploits the\ncomplementarity of graph structures and text information. Its graph embedding\nmodel employs Transformer to reconstruct the representation of an entity from\nits neighborhood subgraph. Its text embedding model uses a pre-trained language\nmodel to generate entity representations from the soft prompts of their names,\ndescriptions, and relational neighbors. To let the two model promote each\nother, we propose co-distillation learning that allows them to distill\nselective knowledge from each other's prediction logits. In our co-distillation\nlearning, each model serves as both a teacher and a student. Experiments on\nbenchmark datasets demonstrate that the two models outperform their related\nbaselines, and the ensemble method CoLE with co-distillation learning advances\nthe state-of-the-art of KG embedding.",
    "num_pages": 10
}