{
    "uuid": "6677d3c0-7aea-5795-a934-d93933e25157",
    "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados",
        "Leonardo Neves",
        "Luis Espinosa-Anke"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.12421v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\6677d3c0-7aea-5795-a934-d93933e25157.pdf",
    "bibtex": "@misc{barbieri2020tweetevalunifiedbenchmarkandcomparative,\n    title = {TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification},\n    author = {Francesco Barbieri and Jose Camacho-Collados and Leonardo Neves and Luis Espinosa-Anke},\n    year = {2020},\n    eprint = {2010.12421},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.12421},\n}",
    "abstract": "The experimental landscape in natural language processing for social media is\ntoo fragmented. Each year, new shared tasks and datasets are proposed, ranging\nfrom classics like sentiment analysis to irony detection or emoji prediction.\nTherefore, it is unclear what the current state of the art is, as there is no\nstandardized evaluation protocol, neither a strong set of baselines trained on\nsuch domain-specific data. In this paper, we propose a new evaluation framework\n(TweetEval) consisting of seven heterogeneous Twitter-specific classification\ntasks. We also provide a strong set of baselines as starting point, and compare\ndifferent language modeling pre-training strategies. Our initial experiments\nshow the effectiveness of starting off with existing pre-trained generic\nlanguage models, and continue training them on Twitter corpora.",
    "num_pages": 7
}