{
    "uuid": "29da882a-5b65-5c20-88d1-51cda121e0f6",
    "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Lihua Qian",
        "Hao Zhou",
        "Yu Bao",
        "Mingxuan Wang",
        "Lin Qiu",
        "Weinan Zhang",
        "Yong Yu",
        "Lei Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07905v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\29da882a-5b65-5c20-88d1-51cda121e0f6.pdf",
    "bibtex": "@misc{qian2021glancingtransformerfornonautoregressiveneural,\n    title = {Glancing Transformer for Non-Autoregressive Neural Machine Translation},\n    author = {Lihua Qian and Hao Zhou and Yu Bao and Mingxuan Wang and Lin Qiu and Weinan Zhang and Yong Yu and Lei Li},\n    year = {2021},\n    eprint = {2008.07905},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2008.07905},\n}",
    "abstract": "Recent work on non-autoregressive neural machine translation (NAT) aims at\nimproving the efficiency by parallel decoding without sacrificing the quality.\nHowever, existing NAT methods are either inferior to Transformer or require\nmultiple decoding passes, leading to reduced speedup. We propose the Glancing\nLanguage Model (GLM), a method to learn word interdependency for single-pass\nparallel generation models. With GLM, we develop Glancing Transformer (GLAT)\nfor machine translation. With only single-pass parallel decoding, GLAT is able\nto generate high-quality translation with 8-15 times speedup. Experiments on\nmultiple WMT language directions show that GLAT outperforms all previous single\npass non-autoregressive methods, and is nearly comparable to Transformer,\nreducing the gap to 0.25-0.9 BLEU points.",
    "num_pages": 10
}