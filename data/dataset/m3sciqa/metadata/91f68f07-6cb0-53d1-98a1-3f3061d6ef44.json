{
    "uuid": "91f68f07-6cb0-53d1-98a1-3f3061d6ef44",
    "title": "LayoutLM: Pre-training of Text and Layout for Document Image Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yiheng Xu",
        "Minghao Li",
        "Lei Cui",
        "Shaohan Huang",
        "Furu Wei",
        "Ming Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.13318v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\91f68f07-6cb0-53d1-98a1-3f3061d6ef44.pdf",
    "bibtex": "@misc{xu2020layoutlmpretrainingoftextand,\n    title = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},\n    author = {Yiheng Xu and Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou},\n    year = {2020},\n    eprint = {1912.13318},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1912.13318},\n}",
    "abstract": "Pre-training techniques have been verified successfully in a variety of NLP\ntasks in recent years. Despite the widespread use of pre-training models for\nNLP applications, they almost exclusively focus on text-level manipulation,\nwhile neglecting layout and style information that is vital for document image\nunderstanding. In this paper, we propose the \\textbf{LayoutLM} to jointly model\ninteractions between text and layout information across scanned document\nimages, which is beneficial for a great number of real-world document image\nunderstanding tasks such as information extraction from scanned documents.\nFurthermore, we also leverage image features to incorporate words' visual\ninformation into LayoutLM. To the best of our knowledge, this is the first time\nthat text and layout are jointly learned in a single framework for\ndocument-level pre-training. It achieves new state-of-the-art results in\nseveral downstream tasks, including form understanding (from 70.72 to 79.27),\nreceipt understanding (from 94.02 to 95.24) and document image classification\n(from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly\navailable at \\url{https://aka.ms/layoutlm}.",
    "num_pages": 9
}