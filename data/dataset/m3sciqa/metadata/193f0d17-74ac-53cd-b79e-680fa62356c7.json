{
    "uuid": "193f0d17-74ac-53cd-b79e-680fa62356c7",
    "title": "NICE: Non-linear Independent Components Estimation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Laurent Dinh",
        "David Krueger",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1410.8516v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\193f0d17-74ac-53cd-b79e-680fa62356c7.pdf",
    "bibtex": "@misc{dinh2015nicenonlinearindependentcomponentsestimation,\n    title = {NICE: Non-linear Independent Components Estimation},\n    author = {Laurent Dinh and David Krueger and Yoshua Bengio},\n    year = {2015},\n    eprint = {1410.8516},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1410.8516},\n}",
    "abstract": "We propose a deep learning framework for modeling complex high-dimensional\ndensities called Non-linear Independent Component Estimation (NICE). It is\nbased on the idea that a good representation is one in which the data has a\ndistribution that is easy to model. For this purpose, a non-linear\ndeterministic transformation of the data is learned that maps it to a latent\nspace so as to make the transformed data conform to a factorized distribution,\ni.e., resulting in independent latent variables. We parametrize this\ntransformation so that computing the Jacobian determinant and inverse transform\nis trivial, yet we maintain the ability to learn complex non-linear\ntransformations, via a composition of simple building blocks, each based on a\ndeep neural network. The training criterion is simply the exact log-likelihood,\nwhich is tractable. Unbiased ancestral sampling is also easy. We show that this\napproach yields good generative models on four image datasets and can be used\nfor inpainting.",
    "num_pages": 13
}