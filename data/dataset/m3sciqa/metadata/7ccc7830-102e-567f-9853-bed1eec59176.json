{
    "uuid": "7ccc7830-102e-567f-9853-bed1eec59176",
    "title": "X-Linear Attention Networks for Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yingwei Pan",
        "Ting Yao",
        "Yehao Li",
        "Tao Mei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.14080v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\7ccc7830-102e-567f-9853-bed1eec59176.pdf",
    "bibtex": "@misc{pan2020xlinearattentionnetworksforimage,\n    title = {X-Linear Attention Networks for Image Captioning},\n    author = {Yingwei Pan and Ting Yao and Yehao Li and Tao Mei},\n    year = {2020},\n    eprint = {2003.14080},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2003.14080},\n}",
    "abstract": "Recent progress on fine-grained visual recognition and visual question\nanswering has featured Bilinear Pooling, which effectively models the 2$^{nd}$\norder interactions across multi-modal inputs. Nevertheless, there has not been\nevidence in support of building such interactions concurrently with attention\nmechanism for image captioning. In this paper, we introduce a unified attention\nblock -- X-Linear attention block, that fully employs bilinear pooling to\nselectively capitalize on visual information or perform multi-modal reasoning.\nTechnically, X-Linear attention block simultaneously exploits both the spatial\nand channel-wise bilinear attention distributions to capture the 2$^{nd}$ order\ninteractions between the input single-modal or multi-modal features. Higher and\neven infinity order feature interactions are readily modeled through stacking\nmultiple X-Linear attention blocks and equipping the block with Exponential\nLinear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we\npresent X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates\nX-Linear attention block(s) into image encoder and sentence decoder of image\ncaptioning model to leverage higher order intra- and inter-modal interactions.\nThe experiments on COCO benchmark demonstrate that our X-LAN obtains to-date\nthe best published CIDEr performance of 132.0% on COCO Karpathy test split.\nWhen further endowing Transformer with X-Linear attention blocks, CIDEr is\nboosted up to 132.8%. Source code is available at\n\\url{https://github.com/Panda-Peter/image-captioning}.",
    "num_pages": 10
}