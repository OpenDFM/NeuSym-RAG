{
    "uuid": "c5833e1d-8813-56da-99d1-bf61d9ac33df",
    "title": "Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Jia-Chen Gu",
        "Tianda Li",
        "Quan Liu",
        "Zhen-Hua Ling",
        "Zhiming Su",
        "Si Wei",
        "Xiaodan Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.03588v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\c5833e1d-8813-56da-99d1-bf61d9ac33df.pdf",
    "bibtex": "@misc{gu2020speakerawarebertformultiturnresponse,\n    title = {Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots},\n    author = {Jia-Chen Gu and Tianda Li and Quan Liu and Zhen-Hua Ling and Zhiming Su and Si Wei and Xiaodan Zhu},\n    year = {2020},\n    eprint = {2004.03588},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.03588},\n}",
    "abstract": "In this paper, we study the problem of employing pre-trained language models\nfor multi-turn response selection in retrieval-based chatbots. A new model,\nnamed Speaker-Aware BERT (SA-BERT), is proposed in order to make the model\naware of the speaker change information, which is an important and intrinsic\nproperty of multi-turn dialogues. Furthermore, a speaker-aware disentanglement\nstrategy is proposed to tackle the entangled dialogues. This strategy selects a\nsmall number of most important utterances as the filtered context according to\nthe speakers' information in them. Finally, domain adaptation is performed to\nincorporate the in-domain knowledge into pre-trained language models.\nExperiments on five public datasets show that our proposed model outperforms\nthe present models on all metrics by large margins and achieves new\nstate-of-the-art performances for multi-turn response selection.",
    "num_pages": 7
}