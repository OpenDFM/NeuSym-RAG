{
    "uuid": "52e249a0-cbea-5812-8143-0bbf71491d92",
    "title": "R-GCN: The R Could Stand for Random",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Vic Degraeve",
        "Gilles Vandewiele",
        "Femke Ongenae",
        "Sofie Van Hoecke"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.02424v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\52e249a0-cbea-5812-8143-0bbf71491d92.pdf",
    "bibtex": "@misc{degraeve2022rgcnthercouldstand,\n    title = {R-GCN: The R Could Stand for Random},\n    author = {Vic Degraeve and Gilles Vandewiele and Femke Ongenae and Sofie Van Hoecke},\n    year = {2022},\n    eprint = {2203.02424},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2203.02424},\n}",
    "abstract": "The inception of the Relational Graph Convolutional Network (R-GCN) marked a\nmilestone in the Semantic Web domain as a widely cited method that generalises\nend-to-end hierarchical representation learning to Knowledge Graphs (KGs).\nR-GCNs generate representations for nodes of interest by repeatedly aggregating\nparameterised, relation-specific transformations of their neighbours. However,\nin this paper, we argue that the the R-GCN's main contribution lies in this\n\"message passing\" paradigm, rather than the learned weights. To this end, we\nintroduce the \"Random Relational Graph Convolutional Network\" (RR-GCN), which\nleaves all parameters untrained and thus constructs node embeddings by\naggregating randomly transformed random representations from neighbours, i.e.,\nwith no learned parameters. We empirically show that RR-GCNs can compete with\nfully trained R-GCNs in both node classification and link prediction settings.",
    "num_pages": 18
}