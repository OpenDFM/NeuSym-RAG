{
    "uuid": "620e5c6e-997e-55e4-aaf9-57f2ae520ff2",
    "title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Zhongli Li",
        "Wenxuan Zhang",
        "Chao Yan",
        "Qingyu Zhou",
        "Chao Li",
        "Hongzhi Liu",
        "Yunbo Cao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08464v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\620e5c6e-997e-55e4-aaf9-57f2ae520ff2.pdf",
    "bibtex": "@misc{li2022seekingpatternsnotjustmemorizing,\n    title = {Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems},\n    author = {Zhongli Li and Wenxuan Zhang and Chao Yan and Qingyu Zhou and Chao Li and Hongzhi Liu and Yunbo Cao},\n    year = {2022},\n    eprint = {2110.08464},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.08464},\n}",
    "abstract": "Math Word Problem (MWP) solving needs to discover the quantitative\nrelationships over natural language narratives. Recent work shows that existing\nmodels memorize procedures from context and rely on shallow heuristics to solve\nMWPs. In this paper, we look at this issue and argue that the cause is a lack\nof overall understanding of MWP patterns. We first investigate how a neural\nnetwork understands patterns only from semantics, and observe that, if the\nprototype equations are the same, most problems get closer representations and\nthose representations apart from them or close to other prototypes tend to\nproduce wrong solutions. Inspired by it, we propose a contrastive learning\napproach, where the neural network perceives the divergence of patterns. We\ncollect contrastive examples by converting the prototype equation into a tree\nand seeking similar tree structures. The solving model is trained with an\nauxiliary objective on the collected examples, resulting in the representations\nof problems with similar prototypes being pulled closer. We conduct experiments\non the Chinese dataset Math23k and the English dataset MathQA. Our method\ngreatly improves the performance in monolingual and multilingual settings.",
    "num_pages": 11
}