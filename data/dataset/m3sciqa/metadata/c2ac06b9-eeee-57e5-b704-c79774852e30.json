{
    "uuid": "c2ac06b9-eeee-57e5-b704-c79774852e30",
    "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Dan Hendrycks",
        "Xiaoyuan Liu",
        "Eric Wallace",
        "Adam Dziedzic",
        "Rishabh Krishnan",
        "Dawn Song"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.06100v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\c2ac06b9-eeee-57e5-b704-c79774852e30.pdf",
    "bibtex": "@misc{hendrycks2020pretrainedtransformersimproveoutofdistributionrobustness,\n    title = {Pretrained Transformers Improve Out-of-Distribution Robustness},\n    author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and Rishabh Krishnan and Dawn Song},\n    year = {2020},\n    eprint = {2004.06100},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.06100},\n}",
    "abstract": "Although pretrained Transformers such as BERT achieve high accuracy on\nin-distribution examples, do they generalize to new distributions? We\nsystematically measure out-of-distribution (OOD) generalization for seven NLP\ndatasets by constructing a new robustness benchmark with realistic distribution\nshifts. We measure the generalization of previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show that pretrained Transformers'\nperformance declines are substantially smaller. Pretrained transformers are\nalso more effective at detecting anomalous or OOD examples, while many previous\nmodels are frequently worse than chance. We examine which factors affect\nrobustness, finding that larger models are not necessarily more robust,\ndistillation can be harmful, and more diverse pretraining data can enhance\nrobustness. Finally, we show where future work can improve OOD robustness.",
    "num_pages": 11
}