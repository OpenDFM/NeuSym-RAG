{
    "uuid": "5768d951-ac45-5015-8e79-e8373551d34d",
    "title": "Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ben Zhou",
        "Kyle Richardson",
        "Xiaodong Yu",
        "Dan Roth"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.16865v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\5768d951-ac45-5015-8e79-e8373551d34d.pdf",
    "bibtex": "@misc{zhou2022learningtodecomposehypotheticalquestion,\n    title = {Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts},\n    author = {Ben Zhou and Kyle Richardson and Xiaodong Yu and Dan Roth},\n    year = {2022},\n    eprint = {2210.16865},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.16865},\n}",
    "abstract": "Explicit decomposition modeling, which involves breaking down complex tasks\ninto more straightforward and often more interpretable sub-tasks, has long been\na central theme in developing robust and interpretable NLU systems. However,\ndespite the many datasets and resources built as part of this effort, the\nmajority have small-scale annotations and limited scope, which is insufficient\nto solve general decomposition tasks. In this paper, we look at large-scale\nintermediate pre-training of decomposition-based transformers using distant\nsupervision from comparable texts, particularly large-scale parallel news. We\nshow that with such intermediate pre-training, developing robust\ndecomposition-based models for a diverse range of tasks becomes more feasible.\nFor example, on semantic parsing, our model, DecompT5, improves 20% to 30% on\ntwo datasets, Overnight and TORQUE, over the baseline language model. We\nfurther use DecompT5 to build a novel decomposition-based QA system named\nDecompEntail, improving over state-of-the-art models, including GPT-3, on both\nHotpotQA and StrategyQA by 8% and 4%, respectively.",
    "num_pages": 13
}