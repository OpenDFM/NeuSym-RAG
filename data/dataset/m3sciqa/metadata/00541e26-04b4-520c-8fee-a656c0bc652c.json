{
    "uuid": "00541e26-04b4-520c-8fee-a656c0bc652c",
    "title": "Saturated Transformers are Constant-Depth Threshold Circuits",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "William Merrill",
        "Ashish Sabharwal",
        "Noah A. Smith"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.16213v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\00541e26-04b4-520c-8fee-a656c0bc652c.pdf",
    "bibtex": "@misc{merrill2022saturatedtransformersareconstantdepththreshold,\n    title = {Saturated Transformers are Constant-Depth Threshold Circuits},\n    author = {William Merrill and Ashish Sabharwal and Noah A. Smith},\n    year = {2022},\n    eprint = {2106.16213},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.16213},\n}",
    "abstract": "Transformers have become a standard neural network architecture for many NLP\nproblems, motivating theoretical analysis of their power in terms of formal\nlanguages. Recent work has shown that transformers with hard attention are\nquite limited in power (Hahn, 2020), as they can be simulated by constant-depth\nAND/OR circuits (Hao et al. 2021). However, hard attention is a strong\nassumption, which may complicate the relevance of these results in practice. In\nthis work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We first show that\nsaturated transformers transcend the known limitations of hard-attention\ntransformers. We then prove saturated transformers with floating-point values\ncan be simulated by constant-depth threshold circuits, giving the class\n$\\mathsf{TC}^0$ as an upper bound on the formal languages they recognize.",
    "num_pages": 14
}