{
    "uuid": "a87515b4-ac31-5ecb-a261-cc3c8d5f4c8b",
    "title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Torsten Scholak",
        "Nathan Schucher",
        "Dzmitry Bahdanau"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05093v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\a87515b4-ac31-5ecb-a261-cc3c8d5f4c8b.pdf",
    "bibtex": "@misc{scholak2021picardparsingincrementallyforconstrained,\n    title = {PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models},\n    author = {Torsten Scholak and Nathan Schucher and Dzmitry Bahdanau},\n    year = {2021},\n    eprint = {2109.05093},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.05093},\n}",
    "abstract": "Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.",
    "num_pages": 7
}