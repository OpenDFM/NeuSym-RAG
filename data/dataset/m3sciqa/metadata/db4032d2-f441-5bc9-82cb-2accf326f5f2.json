{
    "uuid": "db4032d2-f441-5bc9-82cb-2accf326f5f2",
    "title": "Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Haw-Shiuan Chang",
        "Zonghai Yao",
        "Alolika Gon",
        "Hong Yu",
        "Andrew McCallum"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.12289v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\db4032d2-f441-5bc9-82cb-2accf326f5f2.pdf",
    "bibtex": "@misc{chang2023revisitingthearchitectureslikepointer,\n    title = {Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond},\n    author = {Haw-Shiuan Chang and Zonghai Yao and Alolika Gon and Hong Yu and Andrew McCallum},\n    year = {2023},\n    eprint = {2305.12289},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.12289},\n}",
    "abstract": "Is the output softmax layer, which is adopted by most language models (LMs),\nalways the best way to compute the next word probability? Given so many\nattention layers in a modern transformer-based LM, are the pointer networks\nredundant nowadays? In this study, we discover that the answers to both\nquestions are no. This is because the softmax bottleneck sometimes prevents the\nLMs from predicting the desired distribution and the pointer networks can be\nused to break the bottleneck efficiently. Based on the finding, we propose\nseveral softmax alternatives by simplifying the pointer networks and\naccelerating the word-by-word rerankers. In GPT-2, our proposals are\nsignificantly better and more efficient than mixture of softmax, a\nstate-of-the-art softmax alternative. In summarization experiments, without\nsignificantly decreasing its training/testing speed, our best method based on\nT5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and\nimproves MAUVE scores by 30% in BookSum paragraph-level dataset.",
    "num_pages": 22
}