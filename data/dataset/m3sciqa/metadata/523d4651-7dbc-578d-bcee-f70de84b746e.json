{
    "uuid": "523d4651-7dbc-578d-bcee-f70de84b746e",
    "title": "HyperNetworks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "David Ha",
        "Andrew Dai",
        "Quoc V. Le"
    ],
    "pdf_url": "http://arxiv.org/pdf/1609.09106v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\523d4651-7dbc-578d-bcee-f70de84b746e.pdf",
    "bibtex": "@misc{ha2016hypernetworks,\n    title = {HyperNetworks},\n    author = {David Ha and Andrew Dai and Quoc V. Le},\n    year = {2016},\n    eprint = {1609.09106},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1609.09106},\n}",
    "abstract": "This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters.",
    "num_pages": 29
}