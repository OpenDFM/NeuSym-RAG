{
    "uuid": "3a2ae924-5f60-5905-abab-631f660f7bb6",
    "title": "Translation between Molecules and Natural Language",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Carl Edwards",
        "Tuan Lai",
        "Kevin Ros",
        "Garrett Honke",
        "Kyunghyun Cho",
        "Heng Ji"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.11817v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\3a2ae924-5f60-5905-abab-631f660f7bb6.pdf",
    "bibtex": "@misc{edwards2022translationbetweenmoleculesandnatural,\n    title = {Translation between Molecules and Natural Language},\n    author = {Carl Edwards and Tuan Lai and Kevin Ros and Garrett Honke and Kyunghyun Cho and Heng Ji},\n    year = {2022},\n    eprint = {2204.11817},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.11817},\n}",
    "abstract": "We present $\\textbf{MolT5}$ $-$ a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. $\\textbf{MolT5}$ allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Since\n$\\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome the\nchemistry domain shortcoming of data scarcity. Furthermore, we consider several\nmetrics, including a new cross-modal embedding-based metric, to evaluate the\ntasks of molecule captioning and text-based molecule generation. Our results\nshow that $\\textbf{MolT5}$-based models are able to generate outputs, both\nmolecules and captions, which in many cases are high quality.",
    "num_pages": 39
}