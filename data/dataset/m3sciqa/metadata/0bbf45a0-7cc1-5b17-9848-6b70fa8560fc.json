{
    "uuid": "0bbf45a0-7cc1-5b17-9848-6b70fa8560fc",
    "title": "Large Language Models are Built-in Autoregressive Search Engines",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Noah Ziems",
        "Wenhao Yu",
        "Zhihan Zhang",
        "Meng Jiang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.09612v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\0bbf45a0-7cc1-5b17-9848-6b70fa8560fc.pdf",
    "bibtex": "@misc{ziems2023largelanguagemodelsarebuiltin,\n    title = {Large Language Models are Built-in Autoregressive Search Engines},\n    author = {Noah Ziems and Wenhao Yu and Zhihan Zhang and Meng Jiang},\n    year = {2023},\n    eprint = {2305.09612},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.09612},\n}",
    "abstract": "Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n  Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.",
    "num_pages": 11
}