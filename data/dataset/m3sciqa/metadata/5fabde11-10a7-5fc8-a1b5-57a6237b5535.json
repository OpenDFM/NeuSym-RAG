{
    "uuid": "5fabde11-10a7-5fc8-a1b5-57a6237b5535",
    "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Soravit Changpinyo",
        "Piyush Sharma",
        "Nan Ding",
        "Radu Soricut"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.08981v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\5fabde11-10a7-5fc8-a1b5-57a6237b5535.pdf",
    "bibtex": "@misc{changpinyo2021conceptual12mpushingwebscaleimagetext,\n    title = {Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},\n    author = {Soravit Changpinyo and Piyush Sharma and Nan Ding and Radu Soricut},\n    year = {2021},\n    eprint = {2102.08981},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2102.08981},\n}",
    "abstract": "The availability of large-scale image captioning and visual question\nanswering datasets has contributed significantly to recent successes in\nvision-and-language pre-training. However, these datasets are often collected\nwith overrestrictive requirements inherited from their original target tasks\n(e.g., image caption generation), which limit the resulting dataset scale and\ndiversity. We take a step further in pushing the limits of vision-and-language\npre-training data by relaxing the data collection pipeline used in Conceptual\nCaptions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M\n(CC12M), a dataset with 12 million image-text pairs specifically meant to be\nused for vision-and-language pre-training. We perform an analysis of this\ndataset and benchmark its effectiveness against CC3M on multiple downstream\ntasks with an emphasis on long-tail visual recognition. Our results clearly\nillustrate the benefit of scaling up pre-training data for vision-and-language\ntasks, as indicated by the new state-of-the-art results on both the nocaps and\nConceptual Captions benchmarks.",
    "num_pages": 16
}