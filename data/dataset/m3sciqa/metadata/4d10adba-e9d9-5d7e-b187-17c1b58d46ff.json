{
    "uuid": "4d10adba-e9d9-5d7e-b187-17c1b58d46ff",
    "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tiedong Liu",
        "Bryan Kian Hsiang Low"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.14201v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4d10adba-e9d9-5d7e-b187-17c1b58d46ff.pdf",
    "bibtex": "@misc{liu2023goatfinetunedllamaoutperformsgpt4,\n    title = {Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks},\n    author = {Tiedong Liu and Bryan Kian Hsiang Low},\n    year = {2023},\n    eprint = {2305.14201},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2305.14201},\n}",
    "abstract": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\nGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\ndataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\nsub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\naccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\nnear-perfect accuracy on large-number addition and subtraction through\nsupervised fine-tuning only, which is almost impossible with previous\npretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\nGoat's exceptional performance to LLaMA's consistent tokenization of numbers.\nTo tackle more challenging tasks like large-number multiplication and division,\nwe propose an approach that classifies tasks based on their learnability, and\nsubsequently decomposes unlearnable tasks, such as multi-digit multiplication\nand division, into a series of learnable tasks by leveraging basic arithmetic\nprinciples. We thoroughly examine the performance of our model, offering a\ncomprehensive evaluation of the effectiveness of our proposed decomposition\nsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\nGPU, facilitating reproducibility for other researchers. We release our model,\ndataset, and the Python script for dataset generation.",
    "num_pages": 15
}