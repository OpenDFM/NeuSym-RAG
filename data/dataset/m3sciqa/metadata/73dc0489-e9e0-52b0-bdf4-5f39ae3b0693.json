{
    "uuid": "73dc0489-e9e0-52b0-bdf4-5f39ae3b0693",
    "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Fangyu Liu",
        "Ivan Vulić",
        "Anna Korhonen",
        "Nigel Collier"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08027v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\73dc0489-e9e0-52b0-bdf4-5f39ae3b0693.pdf",
    "bibtex": "@misc{liu2021fasteffectiveandselfsupervisedtransforming,\n    title = {Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders},\n    author = {Fangyu Liu and Ivan Vulić and Anna Korhonen and Nigel Collier},\n    year = {2021},\n    eprint = {2104.08027},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08027},\n}",
    "abstract": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.",
    "num_pages": 18
}