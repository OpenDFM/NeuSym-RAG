{
    "uuid": "725c8996-76d4-5ed3-953b-caff7892c741",
    "title": "Aligned Cross Entropy for Non-Autoregressive Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Marjan Ghazvininejad",
        "Vladimir Karpukhin",
        "Luke Zettlemoyer",
        "Omer Levy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01655v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\725c8996-76d4-5ed3-953b-caff7892c741.pdf",
    "bibtex": "@misc{ghazvininejad2020alignedcrossentropyfornonautoregressive,\n    title = {Aligned Cross Entropy for Non-Autoregressive Machine Translation},\n    author = {Marjan Ghazvininejad and Vladimir Karpukhin and Luke Zettlemoyer and Omer Levy},\n    year = {2020},\n    eprint = {2004.01655},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.01655},\n}",
    "abstract": "Non-autoregressive machine translation models significantly speed up decoding\nby allowing for parallel prediction of the entire target sequence. However,\nmodeling word order is more challenging due to the lack of autoregressive\nfactors in the model. This difficultly is compounded during training with cross\nentropy loss, which can highly penalize small shifts in word order. In this\npaper, we propose aligned cross entropy (AXE) as an alternative loss function\nfor training of non-autoregressive models. AXE uses a differentiable dynamic\nprogram to assign loss based on the best possible monotonic alignment between\ntarget tokens and model predictions. AXE-based training of conditional masked\nlanguage models (CMLMs) substantially improves performance on major WMT\nbenchmarks, while setting a new state of the art for non-autoregressive models.",
    "num_pages": 9
}