{
    "uuid": "9906dc9c-eae7-5352-82ca-533b23d498a1",
    "title": "ToViLaG: Your Visual-Language Generative Model is Also An Evildoer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xinpeng Wang",
        "Xiaoyuan Yi",
        "Han Jiang",
        "Shanlin Zhou",
        "Zhihua Wei",
        "Xing Xie"
    ],
    "pdf_url": "http://arxiv.org/pdf/2312.11523v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\9906dc9c-eae7-5352-82ca-533b23d498a1.pdf",
    "bibtex": "@misc{wang2023tovilagyourvisuallanguagegenerativemodel,\n    title = {ToViLaG: Your Visual-Language Generative Model is Also An Evildoer},\n    author = {Xinpeng Wang and Xiaoyuan Yi and Han Jiang and Shanlin Zhou and Zhihua Wei and Xing Xie},\n    year = {2023},\n    eprint = {2312.11523},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2312.11523},\n}",
    "abstract": "Warning: this paper includes model outputs showing offensive content. Recent\nlarge-scale Visual-Language Generative Models (VLGMs) have achieved\nunprecedented improvement in multimodal image/text generation. However, these\nmodels might also generate toxic content, e.g., offensive text and pornography\nimages, raising significant ethical risks. Despite exhaustive studies on toxic\ndegeneration of language models, this problem remains largely unexplored within\nthe context of visual-language generation. This work delves into the propensity\nfor toxicity generation and susceptibility to toxic data across various VLGMs.\nFor this purpose, we built ToViLaG, a dataset comprising 32K\nco-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that\ntends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity\nmetric tailored to visual-language generation, which theoretically reflects\ndifferent aspects of toxicity considering both input and output. On such a\nbasis, we benchmarked the toxicity of a diverse spectrum of VLGMs and\ndiscovered that some models do more evil than expected while some are more\nvulnerable to infection, underscoring the necessity of VLGMs detoxification.\nTherefore, we develop an innovative bottleneck-based detoxification method. Our\nmethod could reduce toxicity while maintaining comparable generation quality,\nproviding a promising initial solution to this line of research.",
    "num_pages": 25
}