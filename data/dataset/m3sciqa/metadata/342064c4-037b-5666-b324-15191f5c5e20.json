{
    "uuid": "342064c4-037b-5666-b324-15191f5c5e20",
    "title": "Root Mean Square Layer Normalization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Biao Zhang",
        "Rico Sennrich"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.07467v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\342064c4-037b-5666-b324-15191f5c5e20.pdf",
    "bibtex": "@misc{zhang2019rootmeansquarelayernormalization,\n    title = {Root Mean Square Layer Normalization},\n    author = {Biao Zhang and Rico Sennrich},\n    year = {2019},\n    eprint = {1910.07467},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1910.07467},\n}",
    "abstract": "Layer normalization (LayerNorm) has been successfully applied to various deep\nneural networks to help stabilize training and boost model convergence because\nof its capability in handling re-centering and re-scaling of both inputs and\nweight matrix. However, the computational overhead introduced by LayerNorm\nmakes these improvements expensive and significantly slows the underlying\nnetwork, e.g. RNN in particular. In this paper, we hypothesize that\nre-centering invariance in LayerNorm is dispensable and propose root mean\nsquare layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs\nto a neuron in one layer according to root mean square (RMS), giving the model\nre-scaling invariance property and implicit learning rate adaptation ability.\nRMSNorm is computationally simpler and thus more efficient than LayerNorm. We\nalso present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of\nthe summed inputs without breaking the above properties. Extensive experiments\non several tasks using diverse network architectures show that RMSNorm achieves\ncomparable performance against LayerNorm but reduces the running time by 7%~64%\non different models. Source code is available at\nhttps://github.com/bzhangGo/rmsnorm.",
    "num_pages": 14
}