{
    "uuid": "f32f540f-cb2f-51b7-9bb9-89934eb68916",
    "title": "DocFormer: End-to-End Transformer for Document Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Srikar Appalaraju",
        "Bhavan Jasani",
        "Bhargava Urala Kota",
        "Yusheng Xie",
        "R. Manmatha"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11539v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f32f540f-cb2f-51b7-9bb9-89934eb68916.pdf",
    "bibtex": "@misc{appalaraju2021docformerendtoendtransformerfordocument,\n    title = {DocFormer: End-to-End Transformer for Document Understanding},\n    author = {Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and Yusheng Xie and R. Manmatha},\n    year = {2021},\n    eprint = {2106.11539},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2106.11539},\n}",
    "abstract": "We present DocFormer -- a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).",
    "num_pages": 22
}