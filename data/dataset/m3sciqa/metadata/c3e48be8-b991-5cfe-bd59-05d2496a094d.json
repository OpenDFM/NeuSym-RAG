{
    "uuid": "c3e48be8-b991-5cfe-bd59-05d2496a094d",
    "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Mor Geva",
        "Avi Caciularu",
        "Kevin Ro Wang",
        "Yoav Goldberg"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.14680v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c3e48be8-b991-5cfe-bd59-05d2496a094d.pdf",
    "bibtex": "@misc{geva2022transformerfeedforwardlayersbuildpredictions,\n    title = {Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},\n    author = {Mor Geva and Avi Caciularu and Kevin Ro Wang and Yoav Goldberg},\n    year = {2022},\n    eprint = {2203.14680},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.14680},\n}",
    "abstract": "Transformer-based language models (LMs) are at the core of modern NLP, but\ntheir internal prediction construction process is opaque and largely not\nunderstood. In this work, we make a substantial step towards unveiling this\nunderlying prediction process, by reverse-engineering the operation of the\nfeed-forward network (FFN) layers, one of the building blocks of transformer\nmodels. We view the token representation as a changing distribution over the\nvocabulary, and the output from each FFN layer as an additive update to that\ndistribution. Then, we analyze the FFN updates in the vocabulary space, showing\nthat each update can be decomposed to sub-updates corresponding to single FFN\nparameter vectors, each promoting concepts that are often human-interpretable.\nWe then leverage these findings for controlling LM predictions, where we reduce\nthe toxicity of GPT2 by almost 50%, and for improving computation efficiency\nwith a simple early exit rule, saving 20% of computation on average.",
    "num_pages": 16
}