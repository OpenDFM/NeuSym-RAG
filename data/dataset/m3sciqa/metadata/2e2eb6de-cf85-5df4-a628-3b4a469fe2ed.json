{
    "uuid": "2e2eb6de-cf85-5df4-a628-3b4a469fe2ed",
    "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.13586v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\2e2eb6de-cf85-5df4-a628-3b4a469fe2ed.pdf",
    "bibtex": "@misc{liu2021pretrainpromptandpredicta,\n    title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},\n    author = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},\n    year = {2021},\n    eprint = {2107.13586},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2107.13586},\n}",
    "abstract": "This paper surveys and organizes research works in a new paradigm in natural\nlanguage processing, which we dub \"prompt-based learning\". Unlike traditional\nsupervised learning, which trains a model to take in an input x and predict an\noutput y as P(y|x), prompt-based learning is based on language models that\nmodel the probability of text directly. To use these models to perform\nprediction tasks, the original input x is modified using a template into a\ntextual string prompt x' that has some unfilled slots, and then the language\nmodel is used to probabilistically fill the unfilled information to obtain a\nfinal string x, from which the final output y can be derived. This framework is\npowerful and attractive for a number of reasons: it allows the language model\nto be pre-trained on massive amounts of raw text, and by defining a new\nprompting function the model is able to perform few-shot or even zero-shot\nlearning, adapting to new scenarios with few or no labeled data. In this paper\nwe introduce the basics of this promising paradigm, describe a unified set of\nmathematical notations that can cover a wide variety of existing work, and\norganize existing work along several dimensions, e.g.the choice of pre-trained\nmodels, prompts, and tuning strategies. To make the field more accessible to\ninterested beginners, we not only make a systematic review of existing works\nand a highly structured typology of prompt-based concepts, but also release\nother resources, e.g., a website http://pretrain.nlpedia.ai/ including\nconstantly-updated survey, and paperlist.",
    "num_pages": 46
}