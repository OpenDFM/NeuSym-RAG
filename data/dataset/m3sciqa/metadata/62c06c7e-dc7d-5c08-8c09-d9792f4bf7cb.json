{
    "uuid": "62c06c7e-dc7d-5c08-8c09-d9792f4bf7cb",
    "title": "A Review on Language Models as Knowledge Bases",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Badr AlKhamissi",
        "Millicent Li",
        "Asli Celikyilmaz",
        "Mona Diab",
        "Marjan Ghazvininejad"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.06031v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\62c06c7e-dc7d-5c08-8c09-d9792f4bf7cb.pdf",
    "bibtex": "@misc{alkhamissi2022areviewonlanguagemodels,\n    title = {A Review on Language Models as Knowledge Bases},\n    author = {Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona Diab and Marjan Ghazvininejad},\n    year = {2022},\n    eprint = {2204.06031},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.06031},\n}",
    "abstract": "Recently, there has been a surge of interest in the NLP community on the use\nof pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have\nshown that LMs trained on a sufficiently large (web) corpus will encode a\nsignificant amount of knowledge implicitly in its parameters. The resulting LM\ncan be probed for different kinds of knowledge and thus acting as a KB. This\nhas a major advantage over traditional KBs in that this method requires no\nhuman supervision. In this paper, we present a set of aspects that we deem a LM\nshould have to fully act as a KB, and review the recent literature with respect\nto those aspects.",
    "num_pages": 21
}