{
    "uuid": "99dd3a08-02e8-501f-9353-8153b5764ad3",
    "title": "InfoVAE: Information Maximizing Variational Autoencoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Shengjia Zhao",
        "Jiaming Song",
        "Stefano Ermon"
    ],
    "pdf_url": "http://arxiv.org/pdf/1706.02262v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\99dd3a08-02e8-501f-9353-8153b5764ad3.pdf",
    "bibtex": "@misc{zhao2018infovaeinformationmaximizingvariationalautoencoders,\n    title = {InfoVAE: Information Maximizing Variational Autoencoders},\n    author = {Shengjia Zhao and Jiaming Song and Stefano Ermon},\n    year = {2018},\n    eprint = {1706.02262},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1706.02262},\n}",
    "abstract": "A key advance in learning generative models is the use of amortized inference\ndistributions that are jointly trained with the models. We find that existing\ntraining objectives for variational autoencoders can lead to inaccurate\namortized inference distributions and, in some cases, improving the objective\nprovably degrades the inference quality. In addition, it has been observed that\nvariational autoencoders tend to ignore the latent variables when combined with\na decoding distribution that is too flexible. We again identify the cause in\nexisting training criteria and propose a new class of objectives (InfoVAE) that\nmitigate these problems. We show that our model can significantly improve the\nquality of the variational posterior and can make effective use of the latent\nfeatures regardless of the flexibility of the decoding distribution. Through\nextensive qualitative and quantitative analyses, we demonstrate that our models\noutperform competing approaches on multiple performance metrics.",
    "num_pages": 14
}