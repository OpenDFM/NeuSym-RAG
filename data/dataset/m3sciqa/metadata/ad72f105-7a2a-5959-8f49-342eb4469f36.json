{
    "uuid": "ad72f105-7a2a-5959-8f49-342eb4469f36",
    "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.10592v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ad72f105-7a2a-5959-8f49-342eb4469f36.pdf",
    "bibtex": "@misc{zhu2023minigpt4enhancingvisionlanguageunderstandingwith,\n    title = {MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},\n    author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},\n    year = {2023},\n    eprint = {2304.10592},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2304.10592},\n}",
    "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model's generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.",
    "num_pages": 15
}