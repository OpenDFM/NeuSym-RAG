{
    "uuid": "6bf39f15-4505-5fec-a157-0d7269604c8c",
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Thomas L. Griffiths",
        "Yuan Cao",
        "Karthik Narasimhan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.10601v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\6bf39f15-4505-5fec-a157-0d7269604c8c.pdf",
    "bibtex": "@misc{yao2023treeofthoughtsdeliberateproblem,\n    title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\n    author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},\n    year = {2023},\n    eprint = {2305.10601},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.10601},\n}",
    "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.",
    "num_pages": 14
}