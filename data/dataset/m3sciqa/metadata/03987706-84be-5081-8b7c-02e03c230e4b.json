{
    "uuid": "03987706-84be-5081-8b7c-02e03c230e4b",
    "title": "Learning Sparse Prototypes for Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Junxian He",
        "Taylor Berg-Kirkpatrick",
        "Graham Neubig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16336v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\03987706-84be-5081-8b7c-02e03c230e4b.pdf",
    "bibtex": "@misc{he2020learningsparseprototypesfortext,\n    title = {Learning Sparse Prototypes for Text Generation},\n    author = {Junxian He and Taylor Berg-Kirkpatrick and Graham Neubig},\n    year = {2020},\n    eprint = {2006.16336},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2006.16336},\n}",
    "abstract": "Prototype-driven text generation uses non-parametric models that first choose\nfrom a library of sentence \"prototypes\" and then modify the prototype to\ngenerate the output text. While effective, these methods are inefficient at\ntest time as a result of needing to store and index the entire training corpus.\nFurther, existing methods often require heuristics to identify which prototypes\nto reference at training time. In this paper, we propose a novel generative\nmodel that automatically learns a sparse prototype support set that,\nnonetheless, achieves strong language modeling performance. This is achieved by\n(1) imposing a sparsity-inducing prior on the prototype selection distribution,\nand (2) utilizing amortized variational inference to learn a prototype\nretrieval function. In experiments, our model outperforms previous\nprototype-driven language models while achieving up to a 1000x memory\nreduction, as well as a 1000x speed-up at test time. More interestingly, we\nshow that the learned prototypes are able to capture semantics and syntax at\ndifferent granularity as we vary the sparsity of prototype selection, and that\ncertain sentence attributes can be controlled by specifying the prototype for\ngeneration.",
    "num_pages": 15
}