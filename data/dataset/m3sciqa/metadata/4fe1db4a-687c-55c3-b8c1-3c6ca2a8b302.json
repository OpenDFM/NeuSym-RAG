{
    "uuid": "4fe1db4a-687c-55c3-b8c1-3c6ca2a8b302",
    "title": "Subregular Complexity and Deep Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Enes Avcu",
        "Chihiro Shibata",
        "Jeffrey Heinz"
    ],
    "pdf_url": "http://arxiv.org/pdf/1705.05940v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\4fe1db4a-687c-55c3-b8c1-3c6ca2a8b302.pdf",
    "bibtex": "@misc{avcu2017subregularcomplexityanddeeplearning,\n    title = {Subregular Complexity and Deep Learning},\n    author = {Enes Avcu and Chihiro Shibata and Jeffrey Heinz},\n    year = {2017},\n    eprint = {1705.05940},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1705.05940},\n}",
    "abstract": "This paper argues that the judicial use of formal language theory and\ngrammatical inference are invaluable tools in understanding how deep neural\nnetworks can and cannot represent and learn long-term dependencies in temporal\nsequences. Learning experiments were conducted with two types of Recurrent\nNeural Networks (RNNs) on six formal languages drawn from the Strictly Local\n(SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs\n(s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and\nSP classes are among the simplest in a mathematically well-understood hierarchy\nof subregular classes. They encode local and long-term dependencies,\nrespectively. The grammatical inference algorithm Regular Positive and Negative\nInference (RPNI) provided a baseline. According to earlier research, the LSTM\narchitecture should be capable of learning long-term dependencies and should\noutperform s-RNNs. The results of these experiments challenge this narrative.\nFirst, the LSTMs' performance was generally worse in the SP experiments than in\nthe SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP\nexperiment and performed comparably to them on the others.",
    "num_pages": 16
}