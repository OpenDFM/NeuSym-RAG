{
    "uuid": "cc3e6e4a-1cd0-5e58-a2de-aea4e04f9fa8",
    "title": "ViDeBERTa: A powerful pre-trained language model for Vietnamese",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Cong Dao Tran",
        "Nhut Huy Pham",
        "Anh Nguyen",
        "Truong Son Hy",
        "Tu Vu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.10439v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\cc3e6e4a-1cd0-5e58-a2de-aea4e04f9fa8.pdf",
    "bibtex": "@misc{tran2023videbertaapowerfulpretrainedlanguage,\n    title = {ViDeBERTa: A powerful pre-trained language model for Vietnamese},\n    author = {Cong Dao Tran and Nhut Huy Pham and Anh Nguyen and Truong Son Hy and Tu Vu},\n    year = {2023},\n    eprint = {2301.10439},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.10439},\n}",
    "abstract": "This paper presents ViDeBERTa, a new pre-trained monolingual language model\nfor Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and\nViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality\nand diverse Vietnamese texts using DeBERTa architecture. Although many\nsuccessful pre-trained language models based on Transformer have been widely\nproposed for the English language, there are still few pre-trained models for\nVietnamese, a low-resource language, that perform good results on downstream\ntasks, especially Question answering. We fine-tune and evaluate our model on\nthree important natural language downstream tasks, Part-of-speech tagging,\nNamed-entity recognition, and Question answering. The empirical results\ndemonstrate that ViDeBERTa with far fewer parameters surpasses the previous\nstate-of-the-art models on multiple Vietnamese-specific natural language\nunderstanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only\nabout 23% of PhoBERT_large with 370M parameters, still performs the same or\nbetter results than the previous state-of-the-art model. Our ViDeBERTa models\nare available at: https://github.com/HySonLab/ViDeBERTa.",
    "num_pages": 8
}