{
    "uuid": "1047803c-3229-546d-8a93-9027fcbe4df0",
    "title": "Continual Training of Language Models for Few-Shot Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Zixuan Ke",
        "Haowei Lin",
        "Yijia Shao",
        "Hu Xu",
        "Lei Shu",
        "Bing Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.05549v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\1047803c-3229-546d-8a93-9027fcbe4df0.pdf",
    "bibtex": "@misc{ke2022continualtrainingoflanguagemodels,\n    title = {Continual Training of Language Models for Few-Shot Learning},\n    author = {Zixuan Ke and Haowei Lin and Yijia Shao and Hu Xu and Lei Shu and Bing Liu},\n    year = {2022},\n    eprint = {2210.05549},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.05549},\n}",
    "abstract": "Recent work on applying large language models (LMs) achieves impressive\nperformance in many NLP applications. Adapting or posttraining an LM using an\nunlabeled domain corpus can produce even better performance for end-tasks in\nthe domain. This paper proposes the problem of continually extending an LM by\nincrementally post-train the LM with a sequence of unlabeled domain corpora to\nexpand its knowledge without forgetting its previous skills. The goal is to\nimprove the few-shot end-task learning in these domains. The resulting system\nis called CPT (Continual PostTraining), which to our knowledge, is the first\ncontinual post-training system. Experimental results verify its effectiveness.",
    "num_pages": 13
}