{
    "uuid": "f393f359-5002-5544-88cf-484f811f9de0",
    "title": "Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Na Li",
        "Hanane Kteich",
        "Zied Bouraoui",
        "Steven Schockaert"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.09785v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f393f359-5002-5544-88cf-484f811f9de0.pdf",
    "bibtex": "@misc{li2023distillingsemanticconceptembeddingsfrom,\n    title = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models},\n    author = {Na Li and Hanane Kteich and Zied Bouraoui and Steven Schockaert},\n    year = {2023},\n    eprint = {2305.09785},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.09785},\n}",
    "abstract": "Learning vectors that capture the meaning of concepts remains a fundamental\nchallenge. Somewhat surprisingly, perhaps, pre-trained language models have\nthus far only enabled modest improvements to the quality of such concept\nembeddings. Current strategies for using language models typically represent a\nconcept by averaging the contextualised representations of its mentions in some\ncorpus. This is potentially sub-optimal for at least two reasons. First,\ncontextualised word vectors have an unusual geometry, which hampers downstream\ntasks. Second, concept embeddings should capture the semantic properties of\nconcepts, whereas contextualised word vectors are also affected by other\nfactors. To address these issues, we propose two contrastive learning\nstrategies, based on the view that whenever two sentences reveal similar\nproperties, the corresponding contextualised vectors should also be similar.\nOne strategy is fully unsupervised, estimating the properties which are\nexpressed in a sentence from the neighbourhood structure of the contextualised\nword embeddings. The second strategy instead relies on a distant supervision\nsignal from ConceptNet. Our experimental results show that the resulting\nvectors substantially outperform existing concept embeddings in predicting the\nsemantic properties of concepts, with the ConceptNet-based strategy achieving\nthe best results. These findings are furthermore confirmed in a clustering task\nand in the downstream task of ontology completion.",
    "num_pages": 11
}