{
    "uuid": "c628e6e6-81bd-5601-8cef-2499ac25f581",
    "title": "LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jiapeng Wang",
        "Lianwen Jin",
        "Kai Ding"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.13669v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c628e6e6-81bd-5601-8cef-2499ac25f581.pdf",
    "bibtex": "@misc{wang2022liltasimpleyeteffective,\n    title = {LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding},\n    author = {Jiapeng Wang and Lianwen Jin and Kai Ding},\n    year = {2022},\n    eprint = {2202.13669},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2202.13669},\n}",
    "abstract": "Structured document understanding has attracted considerable attention and\nmade significant progress recently, owing to its crucial role in intelligent\ndocument processing. However, most existing related models can only deal with\nthe document data of specific language(s) (typically English) included in the\npre-training collection, which is extremely limited. To address this issue, we\npropose a simple yet effective Language-independent Layout Transformer (LiLT)\nfor structured document understanding. LiLT can be pre-trained on the\nstructured documents of a single language and then directly fine-tuned on other\nlanguages with the corresponding off-the-shelf monolingual/multilingual\npre-trained textual models. Experimental results on eight languages have shown\nthat LiLT can achieve competitive or even superior performance on diverse\nwidely-used downstream benchmarks, which enables language-independent benefit\nfrom the pre-training of document layout structure. Code and model are publicly\navailable at https://github.com/jpWang/LiLT.",
    "num_pages": 11
}