{
    "uuid": "69e18e42-778d-5f47-92ec-bdfe3b03ed37",
    "title": "Neural GPUs Learn Algorithms",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Łukasz Kaiser",
        "Ilya Sutskever"
    ],
    "pdf_url": "http://arxiv.org/pdf/1511.08228v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\69e18e42-778d-5f47-92ec-bdfe3b03ed37.pdf",
    "bibtex": "@misc{kaiser2016neuralgpuslearnalgorithms,\n    title = {Neural GPUs Learn Algorithms},\n    author = {Łukasz Kaiser and Ilya Sutskever},\n    year = {2016},\n    eprint = {1511.08228},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1511.08228},\n}",
    "abstract": "Learning an algorithm from examples is a fundamental problem that has been\nwidely studied. Recently it has been addressed using neural networks, in\nparticular by Neural Turing Machines (NTMs). These are fully differentiable\ncomputers that use backpropagation to learn their own programming. Despite\ntheir appeal NTMs have a weakness that is caused by their sequential nature:\nthey are not parallel and are are hard to train due to their large depth when\nunfolded.\n  We present a neural network architecture to address this problem: the Neural\nGPU. It is based on a type of convolutional gated recurrent unit and, like the\nNTM, is computationally universal. Unlike the NTM, the Neural GPU is highly\nparallel which makes it easier to train and efficient to run.\n  An essential property of algorithms is their ability to handle inputs of\narbitrary size. We show that the Neural GPU can be trained on short instances\nof an algorithmic task and successfully generalize to long instances. We\nverified it on a number of tasks including long addition and long\nmultiplication of numbers represented in binary. We train the Neural GPU on\nnumbers with upto 20 bits and observe no errors whatsoever while testing it,\neven on much longer numbers.\n  To achieve these results we introduce a technique for training deep recurrent\nnetworks: parameter sharing relaxation. We also found a small amount of dropout\nand gradient noise to have a large positive effect on learning and\ngeneralization.",
    "num_pages": 9
}