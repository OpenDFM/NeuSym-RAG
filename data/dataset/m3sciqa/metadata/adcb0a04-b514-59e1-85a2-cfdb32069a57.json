{
    "uuid": "adcb0a04-b514-59e1-85a2-cfdb32069a57",
    "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Mor Geva",
        "Jasmijn Bastings",
        "Katja Filippova",
        "Amir Globerson"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.14767v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\adcb0a04-b514-59e1-85a2-cfdb32069a57.pdf",
    "bibtex": "@misc{geva2023dissectingrecalloffactualassociations,\n    title = {Dissecting Recall of Factual Associations in Auto-Regressive Language Models},\n    author = {Mor Geva and Jasmijn Bastings and Katja Filippova and Amir Globerson},\n    year = {2023},\n    eprint = {2304.14767},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.14767},\n}",
    "abstract": "Transformer-based language models (LMs) are known to capture factual\nknowledge in their parameters. While previous work looked into where factual\nassociations are stored, only little is known about how they are retrieved\ninternally during inference. We investigate this question through the lens of\ninformation flow. Given a subject-relation query, we study how the model\naggregates information about the subject and relation to predict the correct\nattribute. With interventions on attention edges, we first identify two\ncritical points where information propagates to the prediction: one from the\nrelation positions followed by another from the subject positions. Next, by\nanalyzing the information at these points, we unveil a three-step internal\nmechanism for attribute extraction. First, the representation at the\nlast-subject position goes through an enrichment process, driven by the early\nMLP sublayers, to encode many subject-related attributes. Second, information\nfrom the relation propagates to the prediction. Third, the prediction\nrepresentation \"queries\" the enriched subject to extract the attribute. Perhaps\nsurprisingly, this extraction is typically done via attention heads, which\noften encode subject-attribute mappings in their parameters. Overall, our\nfindings introduce a comprehensive view of how factual associations are stored\nand extracted internally in LMs, facilitating future research on knowledge\nlocalization and editing.",
    "num_pages": 20
}