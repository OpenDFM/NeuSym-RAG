{
    "uuid": "e77a8699-a9b0-5923-9ee7-6b40ebe9c61f",
    "title": "Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Nicholas Egan",
        "Oleg Vasilyev",
        "John Bohannon"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.10918v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\e77a8699-a9b0-5923-9ee7-6b40ebe9c61f.pdf",
    "bibtex": "@misc{egan2021playtheshannongamewith,\n    title = {Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation},\n    author = {Nicholas Egan and Oleg Vasilyev and John Bohannon},\n    year = {2021},\n    eprint = {2103.10918},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2103.10918},\n}",
    "abstract": "The goal of a summary is to concisely state the most important information in\na document. With this principle in mind, we introduce new reference-free\nsummary evaluation metrics that use a pretrained language model to estimate the\ninformation content shared between a document and its summary. These metrics\nare a modern take on the Shannon Game, a method for summary quality scoring\nproposed decades ago, where we replace human annotators with language models.\nWe also view these metrics as an extension of BLANC, a recently proposed\napproach to summary quality measurement based on the performance of a language\nmodel with and without the help of a summary. Using transformer based language\nmodels, we empirically verify that our metrics achieve state-of-the-art\ncorrelation with human judgement of the summary quality dimensions of both\ncoherence and relevance, as well as competitive correlation with human\njudgement of consistency and fluency.",
    "num_pages": 16
}