{
    "uuid": "0cdbfd3b-33bb-558e-803c-ebc26aff80ea",
    "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zihan Zhang",
        "Meng Fang",
        "Ling Chen",
        "Mohammad-Reza Namazi-Rad",
        "Jun Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.07343v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\0cdbfd3b-33bb-558e-803c-ebc26aff80ea.pdf",
    "bibtex": "@misc{zhang2023howdolargelanguagemodels,\n    title = {How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances},\n    author = {Zihan Zhang and Meng Fang and Ling Chen and Mohammad-Reza Namazi-Rad and Jun Wang},\n    year = {2023},\n    eprint = {2310.07343},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.07343},\n}",
    "abstract": "Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms",
    "num_pages": 23
}