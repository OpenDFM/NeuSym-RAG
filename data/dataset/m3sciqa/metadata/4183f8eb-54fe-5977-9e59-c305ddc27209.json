{
    "uuid": "4183f8eb-54fe-5977-9e59-c305ddc27209",
    "title": "Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Kai Sun",
        "Dian Yu",
        "Jianshu Chen",
        "Dong Yu",
        "Claire Cardie"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.05831v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\4183f8eb-54fe-5977-9e59-c305ddc27209.pdf",
    "bibtex": "@misc{sun2020improvingmachinereadingcomprehensionwith,\n    title = {Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge},\n    author = {Kai Sun and Dian Yu and Jianshu Chen and Dong Yu and Claire Cardie},\n    year = {2020},\n    eprint = {2009.05831},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.05831},\n}",
    "abstract": "In this paper, we aim to extract commonsense knowledge to improve machine\nreading comprehension. We propose to represent relations implicitly by\nsituating structured knowledge in a context instead of relying on a pre-defined\nset of relations, and we call it contextualized knowledge. Each piece of\ncontextualized knowledge consists of a pair of interrelated verbal and\nnonverbal messages extracted from a script and the scene in which they occur as\ncontext to implicitly represent the relation between the verbal and nonverbal\nmessages, which are originally conveyed by different modalities within the\nscript. We propose a two-stage fine-tuning strategy to use the large-scale\nweakly-labeled data based on a single type of contextualized knowledge and\nemploy a teacher-student paradigm to inject multiple types of contextualized\nknowledge into a student machine reader. Experimental results demonstrate that\nour method outperforms a state-of-the-art baseline by a 4.3% improvement in\naccuracy on the machine reading comprehension dataset C^3, wherein most of the\nquestions require unstated prior knowledge.",
    "num_pages": 10
}