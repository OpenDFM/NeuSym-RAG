{
    "uuid": "5102db9f-2387-5fbe-8dfb-045a29ad21da",
    "title": "QuestEval: Summarization Asks for Fact-based Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Thomas Scialom",
        "Paul-Alexis Dray",
        "Patrick Gallinari",
        "Sylvain Lamprier",
        "Benjamin Piwowarski",
        "Jacopo Staiano",
        "Alex Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.12693v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\5102db9f-2387-5fbe-8dfb-045a29ad21da.pdf",
    "bibtex": "@misc{scialom2021questevalsummarizationasksforfactbased,\n    title = {QuestEval: Summarization Asks for Fact-based Evaluation},\n    author = {Thomas Scialom and Paul-Alexis Dray and Patrick Gallinari and Sylvain Lamprier and Benjamin Piwowarski and Jacopo Staiano and Alex Wang},\n    year = {2021},\n    eprint = {2103.12693},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2103.12693},\n}",
    "abstract": "Summarization evaluation remains an open research problem: current metrics\nsuch as ROUGE are known to be limited and to correlate poorly with human\njudgments. To alleviate this issue, recent work has proposed evaluation metrics\nwhich rely on question answering models to assess whether a summary contains\nall the relevant information in its source document. Though promising, the\nproposed approaches have so far failed to correlate better than ROUGE with\nhuman judgments.\n  In this paper, we extend previous approaches and propose a unified framework,\nnamed QuestEval. In contrast to established metrics such as ROUGE or BERTScore,\nQuestEval does not require any ground-truth reference. Nonetheless, QuestEval\nsubstantially improves the correlation with human judgments over four\nevaluation dimensions (consistency, coherence, fluency, and relevance), as\nshown in the extensive experiments we report.",
    "num_pages": 10
}