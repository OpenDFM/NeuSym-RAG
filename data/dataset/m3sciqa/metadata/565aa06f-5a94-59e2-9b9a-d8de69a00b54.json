{
    "uuid": "565aa06f-5a94-59e2-9b9a-d8de69a00b54",
    "title": "Systematic Generalization with Edge Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Leon Bergen",
        "Timothy J. O'Donnell",
        "Dzmitry Bahdanau"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.00578v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\565aa06f-5a94-59e2-9b9a-d8de69a00b54.pdf",
    "bibtex": "@misc{bergen2021systematicgeneralizationwithedgetransformers,\n    title = {Systematic Generalization with Edge Transformers},\n    author = {Leon Bergen and Timothy J. O'Donnell and Dzmitry Bahdanau},\n    year = {2021},\n    eprint = {2112.00578},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2112.00578},\n}",
    "abstract": "Recent research suggests that systematic generalization in natural language\nunderstanding remains a challenge for state-of-the-art neural models such as\nTransformers and Graph Neural Networks. To tackle this challenge, we propose\nEdge Transformer, a new model that combines inspiration from Transformers and\nrule-based symbolic AI. The first key idea in Edge Transformers is to associate\nvector states with every edge, that is, with every pair of input nodes -- as\nopposed to just every node, as it is done in the Transformer model. The second\nmajor innovation is a triangular attention mechanism that updates edge\nrepresentations in a way that is inspired by unification from logic\nprogramming. We evaluate Edge Transformer on compositional generalization\nbenchmarks in relational reasoning, semantic parsing, and dependency parsing.\nIn all three settings, the Edge Transformer outperforms Relation-aware,\nUniversal and classical Transformer baselines.",
    "num_pages": 15
}