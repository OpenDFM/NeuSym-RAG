{
    "uuid": "fdf999c8-5af6-5dc8-a845-250927fac543",
    "title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Peter Hase",
        "Mohit Bansal",
        "Been Kim",
        "Asma Ghandeharioun"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.04213v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\fdf999c8-5af6-5dc8-a845-250927fac543.pdf",
    "bibtex": "@misc{hase2023doeslocalizationinformeditingsurprising,\n    title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},\n    author = {Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},\n    year = {2023},\n    eprint = {2301.04213},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2301.04213},\n}",
    "abstract": "Language models learn a great quantity of factual information during\npretraining, and recent work localizes this information to specific model\nweights like mid-layer MLP weights. In this paper, we find that we can change\nhow a fact is stored in a model by editing weights that are in a different\nlocation than where existing methods suggest that the fact is stored. This is\nsurprising because we would expect that localizing facts to specific model\nparameters would tell us where to manipulate knowledge in models, and this\nassumption has motivated past work on model editing methods. Specifically, we\nshow that localization conclusions from representation denoising (also known as\nCausal Tracing) do not provide any insight into which model MLP layer would be\nbest to edit in order to override an existing stored fact with a new one. This\nfinding raises questions about how past work relies on Causal Tracing to select\nwhich model layers to edit. Next, we consider several variants of the editing\nproblem, including erasing and amplifying facts. For one of our editing\nproblems, editing performance does relate to localization results from\nrepresentation denoising, but we find that which layer we edit is a far better\npredictor of performance. Our results suggest, counterintuitively, that better\nmechanistic understanding of how pretrained language models work may not always\ntranslate to insights about how to best change their behavior. Our code is\navailable at https://github.com/google/belief-localization",
    "num_pages": 26
}