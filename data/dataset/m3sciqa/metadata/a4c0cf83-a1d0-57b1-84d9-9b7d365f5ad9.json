{
    "uuid": "a4c0cf83-a1d0-57b1-84d9-9b7d365f5ad9",
    "title": "Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jonathan Herzig",
        "Peter Shaw",
        "Ming-Wei Chang",
        "Kelvin Guu",
        "Panupong Pasupat",
        "Yuan Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07478v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\a4c0cf83-a1d0-57b1-84d9-9b7d365f5ad9.pdf",
    "bibtex": "@misc{herzig2021unlockingcompositionalgeneralizationinpretrained,\n    title = {Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations},\n    author = {Jonathan Herzig and Peter Shaw and Ming-Wei Chang and Kelvin Guu and Panupong Pasupat and Yuan Zhang},\n    year = {2021},\n    eprint = {2104.07478},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.07478},\n}",
    "abstract": "Sequence-to-sequence (seq2seq) models are prevalent in semantic parsing, but\nhave been found to struggle at out-of-distribution compositional\ngeneralization. While specialized model architectures and pre-training of\nseq2seq models have been proposed to address this issue, the former often comes\nat the cost of generality and the latter only shows limited success. In this\npaper, we study the impact of intermediate representations on compositional\ngeneralization in pre-trained seq2seq models, without changing the model\narchitecture at all, and identify key aspects for designing effective\nrepresentations. Instead of training to directly map natural language to an\nexecutable form, we map to a reversible or lossy intermediate representation\nthat has stronger structural correspondence with natural language. The\ncombination of our proposed intermediate representations and pre-trained models\nis surprisingly effective, where the best combinations obtain a new\nstate-of-the-art on CFQ (+14.8 accuracy points) and on the template-splits of\nthree text-to-SQL datasets (+15.0 to +19.4 accuracy points). This work\nhighlights that intermediate representations provide an important and\npotentially overlooked degree of freedom for improving the compositional\ngeneralization abilities of pre-trained seq2seq models.",
    "num_pages": 13
}