{
    "uuid": "6bb1f536-7f4c-5a26-adc9-11154e7174a0",
    "title": "UDALM: Unsupervised Domain Adaptation through Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Constantinos Karouzos",
        "Georgios Paraskevopoulos",
        "Alexandros Potamianos"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.07078v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\6bb1f536-7f4c-5a26-adc9-11154e7174a0.pdf",
    "bibtex": "@misc{karouzos2021udalmunsuperviseddomainadaptationthrough,\n    title = {UDALM: Unsupervised Domain Adaptation through Language Modeling},\n    author = {Constantinos Karouzos and Georgios Paraskevopoulos and Alexandros Potamianos},\n    year = {2021},\n    eprint = {2104.07078},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.07078},\n}",
    "abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained\nlanguage models for downstream tasks. We introduce UDALM, a fine-tuning\nprocedure, using a mixed classification and Masked Language Model loss, that\ncan adapt to the target domain distribution in a robust and sample efficient\nmanner. Our experiments show that performance of models trained with the mixed\nloss scales with the amount of available target data and the mixed loss can be\neffectively used as a stopping criterion during UDA training. Furthermore, we\ndiscuss the relationship between A-distance and the target error and explore\nsome limitations of the Domain Adversarial Training approach. Our method is\nevaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset,\nyielding $91.74\\%$ accuracy, which is an $1.11\\%$ absolute improvement over the\nstate-of-the-art.",
    "num_pages": 12
}