{
    "uuid": "ff105ef4-7a03-5e2b-8ea8-2dd70b86766c",
    "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "David Wingate",
        "Mohammad Shoeybi",
        "Taylor Sorensen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.03162v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ff105ef4-7a03-5e2b-8ea8-2dd70b86766c.pdf",
    "bibtex": "@misc{wingate2022promptcompressionandcontrastiveconditioning,\n    title = {Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models},\n    author = {David Wingate and Mohammad Shoeybi and Taylor Sorensen},\n    year = {2022},\n    eprint = {2210.03162},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.03162},\n}",
    "abstract": "We explore the idea of compressing the prompts used to condition language\nmodels, and show that compressed prompts can retain a substantive amount of\ninformation about the original prompt. For severely compressed prompts, while\nfine-grained information is lost, abstract information and general sentiments\ncan be retained with surprisingly few parameters, which can be useful in the\ncontext of decode-time algorithms for controllability and toxicity reduction.\nWe explore contrastive conditioning to steer language model generation towards\ndesirable text and away from undesirable text, and find that some complex\nprompts can be effectively compressed into a single token to guide generation.\nWe also show that compressed prompts are largely compositional, and can be\nconstructed such that they can be used to control independent aspects of\ngenerated text.",
    "num_pages": 14
}