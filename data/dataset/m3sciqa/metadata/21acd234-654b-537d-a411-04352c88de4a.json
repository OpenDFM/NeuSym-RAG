{
    "uuid": "21acd234-654b-537d-a411-04352c88de4a",
    "title": "A Survey of Knowledge Enhanced Pre-trained Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Linmei Hu",
        "Zeyi Liu",
        "Ziwang Zhao",
        "Lei Hou",
        "Liqiang Nie",
        "Juanzi Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.05994v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\21acd234-654b-537d-a411-04352c88de4a.pdf",
    "bibtex": "@misc{hu2023asurveyofknowledgeenhanced,\n    title = {A Survey of Knowledge Enhanced Pre-trained Language Models},\n    author = {Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},\n    year = {2023},\n    eprint = {2211.05994},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.05994},\n}",
    "abstract": "Pre-trained Language Models (PLMs) which are trained on large text corpus via\nself-supervised learning method, have yielded promising performance on various\ntasks in Natural Language Processing (NLP). However, though PLMs with huge\nparameters can effectively possess rich knowledge learned from massive training\ntext and benefit downstream tasks at the fine-tuning stage, they still have\nsome limitations such as poor reasoning ability due to the lack of external\nknowledge. Research has been dedicated to incorporating knowledge into PLMs to\ntackle these issues. In this paper, we present a comprehensive review of\nKnowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear\ninsight into this thriving field. We introduce appropriate taxonomies\nrespectively for Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) to highlight these two main tasks of NLP. For NLU, we divide\nthe types of knowledge into four categories: linguistic knowledge, text\nknowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are\ncategorized into KG-based and retrieval-based methods. Finally, we point out\nsome promising future directions of KE-PLMs.",
    "num_pages": 14
}