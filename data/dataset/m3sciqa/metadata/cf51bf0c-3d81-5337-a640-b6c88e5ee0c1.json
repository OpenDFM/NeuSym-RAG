{
    "uuid": "cf51bf0c-3d81-5337-a640-b6c88e5ee0c1",
    "title": "Do Neural Language Representations Learn Physical Commonsense?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Maxwell Forbes",
        "Ari Holtzman",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.02899v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\cf51bf0c-3d81-5337-a640-b6c88e5ee0c1.pdf",
    "bibtex": "@misc{forbes2019doneurallanguagerepresentationslearn,\n    title = {Do Neural Language Representations Learn Physical Commonsense?},\n    author = {Maxwell Forbes and Ari Holtzman and Yejin Choi},\n    year = {2019},\n    eprint = {1908.02899},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1908.02899},\n}",
    "abstract": "Humans understand language based on the rich background knowledge about how\nthe physical world works, which in turn allows us to reason about the physical\nworld through language. In addition to the properties of objects (e.g., boats\nrequire fuel) and their affordances, i.e., the actions that are applicable to\nthem (e.g., boats can be driven), we can also reason about if-then inferences\nbetween what properties of objects imply the kind of actions that are\napplicable to them (e.g., that if we can drive something then it likely\nrequires fuel).\n  In this paper, we investigate the extent to which state-of-the-art neural\nlanguage representations, trained on a vast amount of natural language text,\ndemonstrate physical commonsense reasoning. While recent advancements of neural\nlanguage models have demonstrated strong performance on various types of\nnatural language inference tasks, our study based on a dataset of over 200k\nnewly collected annotations suggests that neural language representations still\nonly learn associations that are explicitly written down.",
    "num_pages": 14
}