{
    "uuid": "efbbb801-169a-5be8-b60f-f9f1201099e3",
    "title": "Multilingual Twitter Sentiment Classification: The Role of Human Annotators",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Igor Mozetic",
        "Miha Grcar",
        "Jasmina Smailovic"
    ],
    "pdf_url": "http://arxiv.org/pdf/1602.07563v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\efbbb801-169a-5be8-b60f-f9f1201099e3.pdf",
    "bibtex": "@misc{mozetic2016multilingualtwittersentimentclassificationthe,\n    title = {Multilingual Twitter Sentiment Classification: The Role of Human Annotators},\n    author = {Igor Mozetic and Miha Grcar and Jasmina Smailovic},\n    year = {2016},\n    eprint = {1602.07563},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1602.07563},\n}",
    "abstract": "What are the limits of automated Twitter sentiment classification? We analyze\na large set of manually labeled tweets in different languages, use them as\ntraining data, and construct automated classification models. It turns out that\nthe quality of classification models depends much more on the quality and size\nof training data than on the type of the model trained. Experimental results\nindicate that there is no statistically significant difference between the\nperformance of the top classification models. We quantify the quality of\ntraining data by applying various annotator agreement measures, and identify\nthe weakest points of different datasets. We show that the model performance\napproaches the inter-annotator agreement when the size of the training set is\nsufficiently large. However, it is crucial to regularly monitor the self- and\ninter-annotator agreements since this improves the training datasets and\nconsequently the model performance. Finally, we show that there is strong\nevidence that humans perceive the sentiment classes (negative, neutral, and\npositive) as ordered.",
    "num_pages": 26
}