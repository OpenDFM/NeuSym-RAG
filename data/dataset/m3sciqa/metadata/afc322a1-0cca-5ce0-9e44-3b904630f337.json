{
    "uuid": "afc322a1-0cca-5ce0-9e44-3b904630f337",
    "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham",
        "Quoc V. Le",
        "Yunhsuan Sung",
        "Zhen Li",
        "Tom Duerig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.05918v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\afc322a1-0cca-5ce0-9e44-3b904630f337.pdf",
    "bibtex": "@misc{jia2021scalingupvisualandvisionlanguage,\n    title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},\n    author = {Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yunhsuan Sung and Zhen Li and Tom Duerig},\n    year = {2021},\n    eprint = {2102.05918},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2102.05918},\n}",
    "abstract": "Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.",
    "num_pages": 14
}