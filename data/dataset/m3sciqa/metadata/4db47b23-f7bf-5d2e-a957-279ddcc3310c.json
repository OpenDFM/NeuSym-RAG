{
    "uuid": "4db47b23-f7bf-5d2e-a957-279ddcc3310c",
    "title": "Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Hai Yu",
        "Chong Deng",
        "Qinglin Zhang",
        "Jiaqing Liu",
        "Qian Chen",
        "Wen Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.11772v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4db47b23-f7bf-5d2e-a957-279ddcc3310c.pdf",
    "bibtex": "@misc{yu2023improvinglongdocumenttopicsegmentation,\n    title = {Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling},\n    author = {Hai Yu and Chong Deng and Qinglin Zhang and Jiaqing Liu and Qian Chen and Wen Wang},\n    year = {2023},\n    eprint = {2310.11772},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.11772},\n}",
    "abstract": "Topic segmentation is critical for obtaining structured documents and\nimproving downstream tasks such as information retrieval. Due to its ability of\nautomatically exploring clues of topic shift from abundant labeled data, recent\nsupervised neural models have greatly promoted the development of long document\ntopic segmentation, but leaving the deeper relationship between coherence and\ntopic segmentation underexplored. Therefore, this paper enhances the ability of\nsupervised models to capture coherence from both logical structure and semantic\nsimilarity perspectives to further improve the topic segmentation performance,\nproposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive\nSemantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to\nforce the model to comprehend structural information by learning the original\nrelations between adjacent sentences in a disarrayed document, which is\nconstructed by jointly disrupting the original document at topic and sentence\nlevels. Moreover, we utilize inter- and intra-topic information to construct\ncontrastive samples and design the CSSL objective to ensure that the sentences\nrepresentations in the same topic have higher similarity, while those in\ndifferent topics are less similar. Extensive experiments show that the\nLongformer with our approach significantly outperforms old state-of-the-art\n(SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 -> 77.16)\nand reduces $P_k$ by 1.11 points (15.0 -> 13.89) on WIKI-727K and achieves an\naverage relative reduction of 4.3% on $P_k$ on WikiSection. The average\nrelative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates\nthe robustness of our approach.",
    "num_pages": 14
}