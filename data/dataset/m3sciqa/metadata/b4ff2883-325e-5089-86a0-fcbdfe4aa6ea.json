{
    "uuid": "b4ff2883-325e-5089-86a0-fcbdfe4aa6ea",
    "title": "Influence Scores at Scale for Efficient Language Data Sampling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Nikhil Anand",
        "Joshua Tan",
        "Maria Minakova"
    ],
    "pdf_url": "http://arxiv.org/pdf/2311.16298v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b4ff2883-325e-5089-86a0-fcbdfe4aa6ea.pdf",
    "bibtex": "@misc{anand2023influencescoresatscalefor,\n    title = {Influence Scores at Scale for Efficient Language Data Sampling},\n    author = {Nikhil Anand and Joshua Tan and Maria Minakova},\n    year = {2023},\n    eprint = {2311.16298},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2311.16298},\n}",
    "abstract": "Modern ML systems ingest data aggregated from diverse sources, such as\nsynthetic, human-annotated, and live customer traffic. Understanding\n\\textit{which} examples are important to the performance of a learning\nalgorithm is crucial for efficient model training. Recently, a growing body of\nliterature has given rise to various \"influence scores,\" which use training\nartifacts such as model confidence or checkpointed gradients to identify\nimportant subsets of data. However, these methods have primarily been developed\nin computer vision settings, and it remains unclear how well they generalize to\nlanguage-based tasks using pretrained models.\n  In this paper, we explore the applicability of influence scores in language\nclassification tasks. We evaluate a diverse subset of these scores on the SNLI\ndataset by quantifying accuracy changes in response to pruning training data\nthrough random and influence-score-based sampling. We then stress-test one of\nthe scores -- \"variance of gradients\" (VoG) from Agarwal et al. (2022) -- in an\nNLU model stack that was exposed to dynamic user speech patterns in a voice\nassistant type of setting. Our experiments demonstrate that in many cases,\nencoder-based language models can be finetuned on roughly 50% of the original\ndata without degradation in performance metrics. Along the way, we summarize\nlessons learned from applying out-of-the-box implementations of influence\nscores, quantify the effects of noisy and class-imbalanced data, and offer\nrecommendations on score-based sampling for better accuracy and training\nefficiency.",
    "num_pages": 26
}