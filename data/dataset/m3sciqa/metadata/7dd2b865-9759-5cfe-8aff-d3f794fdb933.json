{
    "uuid": "7dd2b865-9759-5cfe-8aff-d3f794fdb933",
    "title": "Rationale-Enhanced Language Models are Better Continual Relation Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Weimin Xiong",
        "Yifan Song",
        "Peiyi Wang",
        "Sujian Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.06547v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\7dd2b865-9759-5cfe-8aff-d3f794fdb933.pdf",
    "bibtex": "@misc{xiong2023rationaleenhancedlanguagemodelsarebetter,\n    title = {Rationale-Enhanced Language Models are Better Continual Relation Learners},\n    author = {Weimin Xiong and Yifan Song and Peiyi Wang and Sujian Li},\n    year = {2023},\n    eprint = {2310.06547},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.06547},\n}",
    "abstract": "Continual relation extraction (CRE) aims to solve the problem of catastrophic\nforgetting when learning a sequence of newly emerging relations. Recent CRE\nstudies have found that catastrophic forgetting arises from the model's lack of\nrobustness against future analogous relations. To address the issue, we\nintroduce rationale, i.e., the explanations of relation classification results\ngenerated by large language models (LLM), into CRE task. Specifically, we\ndesign the multi-task rationale tuning strategy to help the model learn current\nrelations robustly. We also conduct contrastive rationale replay to further\ndistinguish analogous relations. Experimental results on two standard\nbenchmarks demonstrate that our method outperforms the state-of-the-art CRE\nmodels.",
    "num_pages": 9
}