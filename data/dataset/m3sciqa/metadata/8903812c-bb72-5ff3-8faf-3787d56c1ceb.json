{
    "uuid": "8903812c-bb72-5ff3-8faf-3787d56c1ceb",
    "title": "Attentive Relational Networks for Mapping Images to Scene Graphs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Mengshi Qi",
        "Weijian Li",
        "Zhengyuan Yang",
        "Yunhong Wang",
        "Jiebo Luo"
    ],
    "pdf_url": "http://arxiv.org/pdf/1811.10696v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\8903812c-bb72-5ff3-8faf-3787d56c1ceb.pdf",
    "bibtex": "@misc{qi2019attentiverelationalnetworksformapping,\n    title = {Attentive Relational Networks for Mapping Images to Scene Graphs},\n    author = {Mengshi Qi and Weijian Li and Zhengyuan Yang and Yunhong Wang and Jiebo Luo},\n    year = {2019},\n    eprint = {1811.10696},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1811.10696},\n}",
    "abstract": "Scene graph generation refers to the task of automatically mapping an image\ninto a semantic structural graph, which requires correctly labeling each\nextracted object and their interaction relationships. Despite the recent\nsuccess in object detection using deep learning techniques, inferring complex\ncontextual relationships and structured graph representations from visual data\nremains a challenging topic. In this study, we propose a novel Attentive\nRelational Network that consists of two key modules with an object detection\nbackbone to approach this problem. The first module is a semantic\ntransformation module utilized to capture semantic embedded relation features,\nby translating visual features and linguistic features into a common semantic\nspace. The other module is a graph self-attention module introduced to embed a\njoint graph representation through assigning various importance weights to\nneighboring nodes. Finally, accurate scene graphs are produced by the relation\ninference module to recognize all entities and the corresponding relations. We\nevaluate our proposed method on the widely-adopted Visual Genome Dataset, and\nthe results demonstrate the effectiveness and superiority of our model.",
    "num_pages": 10
}