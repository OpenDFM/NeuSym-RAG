{
    "uuid": "2987b2aa-67ad-5312-91c3-067762156456",
    "title": "Benchmarking Large Language Models for News Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tianyi Zhang",
        "Faisal Ladhak",
        "Esin Durmus",
        "Percy Liang",
        "Kathleen McKeown",
        "Tatsunori B. Hashimoto"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.13848v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\2987b2aa-67ad-5312-91c3-067762156456.pdf",
    "bibtex": "@misc{zhang2023benchmarkinglargelanguagemodelsfor,\n    title = {Benchmarking Large Language Models for News Summarization},\n    author = {Tianyi Zhang and Faisal Ladhak and Esin Durmus and Percy Liang and Kathleen McKeown and Tatsunori B. Hashimoto},\n    year = {2023},\n    eprint = {2301.13848},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.13848},\n}",
    "abstract": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.",
    "num_pages": 14
}