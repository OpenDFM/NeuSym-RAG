{
    "uuid": "28ad8a82-46ea-55d5-9c2d-4faaf0e88ab0",
    "title": "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Zichao Yang",
        "Zhiting Hu",
        "Ruslan Salakhutdinov",
        "Taylor Berg-Kirkpatrick"
    ],
    "pdf_url": "http://arxiv.org/pdf/1702.08139v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\28ad8a82-46ea-55d5-9c2d-4faaf0e88ab0.pdf",
    "bibtex": "@misc{yang2017improvedvariationalautoencodersfortext,\n    title = {Improved Variational Autoencoders for Text Modeling using Dilated Convolutions},\n    author = {Zichao Yang and Zhiting Hu and Ruslan Salakhutdinov and Taylor Berg-Kirkpatrick},\n    year = {2017},\n    eprint = {1702.08139},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.NE},\n    url = {http://arxiv.org/abs/1702.08139},\n}",
    "abstract": "Recent work on generative modeling of text has found that variational\nauto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM\nlanguage models (Bowman et al., 2015). This negative result is so far poorly\nunderstood, but has been attributed to the propensity of LSTM decoders to\nignore conditioning information from the encoder. In this paper, we experiment\nwith a new type of decoder for VAE: a dilated CNN. By changing the decoder's\ndilation architecture, we control the effective context from previously\ngenerated words. In experiments, we find that there is a trade off between the\ncontextual capacity of the decoder and the amount of encoding information used.\nWe show that with the right decoder, VAE can outperform LSTM language models.\nWe demonstrate perplexity gains on two datasets, representing the first\npositive experimental result on the use VAE for generative modeling of text.\nFurther, we conduct an in-depth investigation of the use of VAE (with our new\ndecoding architecture) for semi-supervised and unsupervised labeling tasks,\ndemonstrating gains over several strong baselines.",
    "num_pages": 12
}