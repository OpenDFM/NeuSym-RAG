{
    "uuid": "b5412b02-ce00-5213-938c-47499b1a148a",
    "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Thomas Hartvigsen",
        "Swami Sankaranarayanan",
        "Hamid Palangi",
        "Yoon Kim",
        "Marzyeh Ghassemi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.11031v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b5412b02-ce00-5213-938c-47499b1a148a.pdf",
    "bibtex": "@misc{hartvigsen2023agingwithgracelifelongmodel,\n    title = {Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors},\n    author = {Thomas Hartvigsen and Swami Sankaranarayanan and Hamid Palangi and Yoon Kim and Marzyeh Ghassemi},\n    year = {2023},\n    eprint = {2211.11031},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2211.11031},\n}",
    "abstract": "Deployed language models decay over time due to shifting inputs, changing\nuser needs, or emergent world-knowledge gaps. When such problems are\nidentified, we want to make targeted edits while avoiding expensive retraining.\nHowever, current model editors, which modify such behaviors of pre-trained\nmodels, degrade model performance quickly across multiple, sequential edits. We\npropose GRACE, a lifelong model editing method, which implements spot-fixes on\nstreaming errors of a deployed model, ensuring minimal impact on unrelated\ninputs. GRACE writes new mappings into a pre-trained model's latent space,\ncreating a discrete, local codebook of edits without altering model weights.\nThis is the first method enabling thousands of sequential edits using only\nstreaming errors. Our experiments on T5, BERT, and GPT models show GRACE's\nstate-of-the-art performance in making and retaining edits, while generalizing\nto unseen inputs. Our code is available at\nhttps://www.github.com/thartvigsen/grace}.",
    "num_pages": 26
}