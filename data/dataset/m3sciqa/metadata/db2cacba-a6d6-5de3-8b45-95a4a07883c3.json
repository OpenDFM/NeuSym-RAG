{
    "uuid": "db2cacba-a6d6-5de3-8b45-95a4a07883c3",
    "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Huao Li",
        "Yu Quan Chong",
        "Simon Stepputtis",
        "Joseph Campbell",
        "Dana Hughes",
        "Michael Lewis",
        "Katia Sycara"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.10701v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\db2cacba-a6d6-5de3-8b45-95a4a07883c3.pdf",
    "bibtex": "@misc{li2024theoryofmindformultiagent,\n    title = {Theory of Mind for Multi-Agent Collaboration via Large Language Models},\n    author = {Huao Li and Yu Quan Chong and Simon Stepputtis and Joseph Campbell and Dana Hughes and Michael Lewis and Katia Sycara},\n    year = {2024},\n    eprint = {2310.10701},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.10701},\n}",
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.",
    "num_pages": 13
}