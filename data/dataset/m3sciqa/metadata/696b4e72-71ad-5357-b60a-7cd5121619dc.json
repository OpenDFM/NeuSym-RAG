{
    "uuid": "696b4e72-71ad-5357-b60a-7cd5121619dc",
    "title": "Learning compositional structures for semantic graph parsing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jonas Groschwitz",
        "Meaghan Fowlie",
        "Alexander Koller"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04398v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\696b4e72-71ad-5357-b60a-7cd5121619dc.pdf",
    "bibtex": "@misc{groschwitz2021learningcompositionalstructuresforsemantic,\n    title = {Learning compositional structures for semantic graph parsing},\n    author = {Jonas Groschwitz and Meaghan Fowlie and Alexander Koller},\n    year = {2021},\n    eprint = {2106.04398},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.04398},\n}",
    "abstract": "AM dependency parsing is a method for neural semantic graph parsing that\nexploits the principle of compositionality. While AM dependency parsers have\nbeen shown to be fast and accurate across several graphbanks, they require\nexplicit annotations of the compositional tree structures for training. In the\npast, these were obtained using complex graphbank-specific heuristics written\nby experts. Here we show how they can instead be trained directly on the graphs\nwith a neural latent-variable model, drastically reducing the amount and\ncomplexity of manual heuristics. We demonstrate that our model picks up on\nseveral linguistic phenomena on its own and achieves comparable accuracy to\nsupervised training, greatly facilitating the use of AM dependency parsing for\nnew sembanks.",
    "num_pages": 13
}