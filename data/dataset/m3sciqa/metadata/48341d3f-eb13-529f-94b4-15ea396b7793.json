{
    "uuid": "48341d3f-eb13-529f-94b4-15ea396b7793",
    "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Artidoro Pagnoni",
        "Vidhisha Balachandran",
        "Yulia Tsvetkov"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.13346v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\48341d3f-eb13-529f-94b4-15ea396b7793.pdf",
    "bibtex": "@misc{pagnoni2021understandingfactualityinabstractivesummarization,\n    title = {Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics},\n    author = {Artidoro Pagnoni and Vidhisha Balachandran and Yulia Tsvetkov},\n    year = {2021},\n    eprint = {2104.13346},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.13346},\n}",
    "abstract": "Modern summarization models generate highly fluent but often factually\nunreliable outputs. This motivated a surge of metrics attempting to measure the\nfactuality of automatically generated summaries. Due to the lack of common\nbenchmarks, these metrics cannot be compared. Moreover, all these methods treat\nfactuality as a binary concept and fail to provide deeper insights into the\nkinds of inconsistencies made by different systems. To address these\nlimitations, we devise a typology of factual errors and use it to collect human\nannotations of generated summaries from state-of-the-art summarization systems\nfor the CNN/DM and XSum datasets. Through these annotations, we identify the\nproportion of different categories of factual errors in various summarization\nmodels and benchmark factuality metrics, showing their correlation with human\njudgment as well as their specific strengths and weaknesses.",
    "num_pages": 18
}