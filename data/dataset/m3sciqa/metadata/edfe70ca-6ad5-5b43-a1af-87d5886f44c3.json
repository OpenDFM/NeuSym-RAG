{
    "uuid": "edfe70ca-6ad5-5b43-a1af-87d5886f44c3",
    "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Dmitry Lepikhin",
        "HyoukJoong Lee",
        "Yuanzhong Xu",
        "Dehao Chen",
        "Orhan Firat",
        "Yanping Huang",
        "Maxim Krikun",
        "Noam Shazeer",
        "Zhifeng Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.16668v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\edfe70ca-6ad5-5b43-a1af-87d5886f44c3.pdf",
    "bibtex": "@misc{lepikhin2020gshardscalinggiantmodelswith,\n    title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},\n    author = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},\n    year = {2020},\n    eprint = {2006.16668},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2006.16668},\n}",
    "abstract": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.",
    "num_pages": 35
}