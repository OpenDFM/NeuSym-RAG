{
    "uuid": "39651ad0-168e-5246-98c3-2f973f1202c4",
    "title": "Integrating Multimodal Information in Large Pretrained Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Wasifur Rahman",
        "Md. Kamrul Hasan",
        "Sangwu Lee",
        "Amir Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.05787v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\39651ad0-168e-5246-98c3-2f973f1202c4.pdf",
    "bibtex": "@misc{rahman2020integratingmultimodalinformationinlarge,\n    title = {Integrating Multimodal Information in Large Pretrained Transformers},\n    author = {Wasifur Rahman and Md. Kamrul Hasan and Sangwu Lee and Amir Zadeh and Chengfeng Mao and Louis-Philippe Morency and Ehsan Hoque},\n    year = {2020},\n    eprint = {1908.05787},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1908.05787},\n}",
    "abstract": "Recent Transformer-based contextual word representations, including BERT and\nXLNet, have shown state-of-the-art performance in multiple disciplines within\nNLP. Fine-tuning the trained contextual models on task-specific datasets has\nbeen the key to achieving superior performance downstream. While fine-tuning\nthese pre-trained models is straightforward for lexical applications\n(applications with only language modality), it is not trivial for multimodal\nlanguage (a growing area in NLP focused on modeling face-to-face\ncommunication). Pre-trained models don't have the necessary components to\naccept two extra modalities of vision and acoustic. In this paper, we proposed\nan attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG\nallows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.\nIt does so by generating a shift to internal representation of BERT and XLNet;\na shift that is conditioned on the visual and acoustic modalities. In our\nexperiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for\nmultimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly\nboosts the sentiment analysis performance over previous baselines as well as\nlanguage-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet\nachieves human-level multimodal sentiment analysis performance for the first\ntime in the NLP community.",
    "num_pages": 11
}