{
    "uuid": "bcf8872a-b92d-5cbe-8be5-350bcf7d5e15",
    "title": "CLEAR: Contrastive Learning for Sentence Representation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Zhuofeng Wu",
        "Sinong Wang",
        "Jiatao Gu",
        "Madian Khabsa",
        "Fei Sun",
        "Hao Ma"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15466v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\bcf8872a-b92d-5cbe-8be5-350bcf7d5e15.pdf",
    "bibtex": "@misc{wu2020clearcontrastivelearningforsentence,\n    title = {CLEAR: Contrastive Learning for Sentence Representation},\n    author = {Zhuofeng Wu and Sinong Wang and Jiatao Gu and Madian Khabsa and Fei Sun and Hao Ma},\n    year = {2020},\n    eprint = {2012.15466},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.15466},\n}",
    "abstract": "Pre-trained language models have proven their unique powers in capturing\nimplicit language features. However, most pre-training approaches focus on the\nword-level training objective, while sentence-level objectives are rarely\nstudied. In this paper, we propose Contrastive LEArning for sentence\nRepresentation (CLEAR), which employs multiple sentence-level augmentation\nstrategies in order to learn a noise-invariant sentence representation. These\naugmentations include word and span deletion, reordering, and substitution.\nFurthermore, we investigate the key reasons that make contrastive learning\neffective through numerous experiments. We observe that different sentence\naugmentations during pre-training lead to different performance improvements on\nvarious downstream tasks. Our approach is shown to outperform multiple existing\nmethods on both SentEval and GLUE benchmarks.",
    "num_pages": 10
}