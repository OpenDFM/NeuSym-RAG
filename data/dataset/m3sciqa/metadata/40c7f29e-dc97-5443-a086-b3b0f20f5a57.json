{
    "uuid": "40c7f29e-dc97-5443-a086-b3b0f20f5a57",
    "title": "Crosslingual Generalization through Multitask Finetuning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful Bari",
        "Sheng Shen",
        "Zheng-Xin Yong",
        "Hailey Schoelkopf",
        "Xiangru Tang",
        "Dragomir Radev",
        "Alham Fikri Aji",
        "Khalid Almubarak",
        "Samuel Albanie",
        "Zaid Alyafeai",
        "Albert Webson",
        "Edward Raff",
        "Colin Raffel"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.01786v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\40c7f29e-dc97-5443-a086-b3b0f20f5a57.pdf",
    "bibtex": "@misc{muennighoff2023crosslingualgeneralizationthroughmultitaskfinetuning,\n    title = {Crosslingual Generalization through Multitask Finetuning},\n    author = {Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},\n    year = {2023},\n    eprint = {2211.01786},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.01786},\n}",
    "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\nof MTF have focused on English data and models. We apply MTF to the pretrained\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\nBLOOMZ and mT0. We find finetuning large multilingual language models on\nEnglish tasks with English prompts allows for task generalization to\nnon-English languages that appear only in the pretraining corpus. Finetuning on\nmultilingual tasks with English prompts further improves performance on English\nand non-English tasks leading to various state-of-the-art zero-shot results. We\nalso investigate finetuning on multilingual tasks with prompts that have been\nmachine-translated from English to match the language of each dataset. We find\ntraining on these machine-translated prompts leads to better performance on\nhuman-written prompts in the respective languages. Surprisingly, we find models\nare capable of zero-shot generalization to tasks in languages they have never\nintentionally seen. We conjecture that the models are learning higher-level\ncapabilities that are both task- and language-agnostic. In addition, we\nintroduce xP3, a composite of supervised datasets in 46 languages with English\nand machine-translated prompts. Our code, datasets and models are freely\navailable at https://github.com/bigscience-workshop/xmtf.",
    "num_pages": 119
}