{
    "uuid": "a06173ea-6789-5ed2-b47b-5048b973ff05",
    "title": "Shades of BLEU, Flavours of Success: The Case of MultiWOZ",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Tomáš Nekvinda",
        "Ondřej Dušek"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05555v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\a06173ea-6789-5ed2-b47b-5048b973ff05.pdf",
    "bibtex": "@misc{nekvinda2021shadesofbleuflavoursof,\n    title = {Shades of BLEU, Flavours of Success: The Case of MultiWOZ},\n    author = {Tomáš Nekvinda and Ondřej Dušek},\n    year = {2021},\n    eprint = {2106.05555},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.05555},\n}",
    "abstract": "The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for\nbenchmarking context-to-response abilities of task-oriented dialogue systems.\nIn this work, we identify inconsistencies in data preprocessing and reporting\nof three corpus-based metrics used on this dataset, i.e., BLEU score and Inform\n& Success rates. We point out a few problems of the MultiWOZ benchmark such as\nunsatisfactory preprocessing, insufficient or under-specified evaluation\nmetrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy\noptimization models in as-fair-as-possible setups, and we show that their\nreported scores cannot be directly compared. To facilitate comparison of future\nsystems, we release our stand-alone standardized evaluation scripts. We also\ngive basic recommendations for corpus-based benchmarking in future works.",
    "num_pages": 13
}