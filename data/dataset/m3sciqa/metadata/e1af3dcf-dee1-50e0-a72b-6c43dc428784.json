{
    "uuid": "e1af3dcf-dee1-50e0-a72b-6c43dc428784",
    "title": "Better Rewards Yield Better Summaries: Learning to Summarise Without References",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Florian Böhm",
        "Yang Gao",
        "Christian M. Meyer",
        "Ori Shapira",
        "Ido Dagan",
        "Iryna Gurevych"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01214v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\e1af3dcf-dee1-50e0-a72b-6c43dc428784.pdf",
    "bibtex": "@misc{bhm2019betterrewardsyieldbettersummaries,\n    title = {Better Rewards Yield Better Summaries: Learning to Summarise Without References},\n    author = {Florian Böhm and Yang Gao and Christian M. Meyer and Ori Shapira and Ido Dagan and Iryna Gurevych},\n    year = {2019},\n    eprint = {1909.01214},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.01214},\n}",
    "abstract": "Reinforcement Learning (RL) based document summarisation systems yield\nstate-of-the-art performance in terms of ROUGE scores, because they directly\nuse ROUGE as the rewards during training. However, summaries with high ROUGE\nscores often receive low human judgement. To find a better reward function that\ncan guide RL to generate human-appealing summaries, we learn a reward function\nfrom human ratings on 2,500 summaries. Our reward function only takes the\ndocument and system summary as input. Hence, once trained, it can be used to\ntrain RL-based summarisation systems without using any reference summaries. We\nshow that our learned rewards have significantly higher correlation with human\nratings than previous approaches. Human evaluation experiments show that,\ncompared to the state-of-the-art supervised-learning systems and\nROUGE-as-rewards RL summarisation systems, the RL systems using our learned\nrewards during training generate summarieswith higher human ratings. The\nlearned reward function and our source code are available at\nhttps://github.com/yg211/summary-reward-no-reference.",
    "num_pages": 11
}