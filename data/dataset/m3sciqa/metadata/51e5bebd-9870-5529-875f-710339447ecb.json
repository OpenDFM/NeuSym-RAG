{
    "uuid": "51e5bebd-9870-5529-875f-710339447ecb",
    "title": "CM3: A Causal Masked Multimodal Model of the Internet",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Armen Aghajanyan",
        "Bernie Huang",
        "Candace Ross",
        "Vladimir Karpukhin",
        "Hu Xu",
        "Naman Goyal",
        "Dmytro Okhonko",
        "Mandar Joshi",
        "Gargi Ghosh",
        "Mike Lewis",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.07520v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\51e5bebd-9870-5529-875f-710339447ecb.pdf",
    "bibtex": "@misc{aghajanyan2022cm3acausalmaskedmultimodal,\n    title = {CM3: A Causal Masked Multimodal Model of the Internet},\n    author = {Armen Aghajanyan and Bernie Huang and Candace Ross and Vladimir Karpukhin and Hu Xu and Naman Goyal and Dmytro Okhonko and Mandar Joshi and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer},\n    year = {2022},\n    eprint = {2201.07520},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.07520},\n}",
    "abstract": "We introduce CM3, a family of causally masked generative models trained over\na large corpus of structured multi-modal documents that can contain both text\nand image tokens. Our new causally masked approach generates tokens left to\nright while also masking out a small number of long token spans that are\ngenerated at the end of the string, instead of their original positions. The\ncasual masking object provides a type of hybrid of the more common causal and\nmasked language models, by enabling full generative modeling while also\nproviding bidirectional context when generating the masked spans. We train\ncausally masked language-image models on large-scale web and Wikipedia\narticles, where each document contains all of the text, hypertext markup,\nhyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they\nappear in the original HTML source (before masking). The resulting CM3 models\ncan generate rich structured, multi-modal outputs while conditioning on\narbitrary masked document contexts, and thereby implicitly learn a wide range\nof text, image, and cross modal tasks. They can be prompted to recover, in a\nzero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.\nWe set the new state-of-the-art in zero-shot summarization, entity linking, and\nentity disambiguation while maintaining competitive performance in the\nfine-tuning setting. We can generate images unconditionally, conditioned on\ntext (like DALL-E) and do captioning all in a zero-shot setting with a single\nmodel.",
    "num_pages": 20
}