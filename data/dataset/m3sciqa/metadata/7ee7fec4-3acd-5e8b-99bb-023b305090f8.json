{
    "uuid": "7ee7fec4-3acd-5e8b-99bb-023b305090f8",
    "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yao Lu",
        "Max Bartolo",
        "Alastair Moore",
        "Sebastian Riedel",
        "Pontus Stenetorp"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08786v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\7ee7fec4-3acd-5e8b-99bb-023b305090f8.pdf",
    "bibtex": "@misc{lu2022fantasticallyorderedpromptsandwhere,\n    title = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},\n    author = {Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},\n    year = {2022},\n    eprint = {2104.08786},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08786},\n}",
    "abstract": "When primed with only a handful of training samples, very large, pretrained\nlanguage models such as GPT-3 have shown competitive results when compared to\nfully-supervised, fine-tuned, large, pretrained language models. We demonstrate\nthat the order in which the samples are provided can make the difference\nbetween near state-of-the-art and random guess performance: essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the true few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of language\nmodels to construct an artificial development set and based on entropy\nstatistics of the candidate permutations on this set, we identify performant\nprompts. Our method yields a 13% relative improvement for GPT-family models\nacross eleven different established text classification tasks.",
    "num_pages": 13
}