{
    "uuid": "1d6fa876-022c-518c-89fc-73b4ddb349d1",
    "title": "CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Pei Ke",
        "Hao Zhou",
        "Yankai Lin",
        "Peng Li",
        "Jie Zhou",
        "Xiaoyan Zhu",
        "Minlie Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.00862v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\1d6fa876-022c-518c-89fc-73b4ddb349d1.pdf",
    "bibtex": "@misc{ke2022ctrlevalanunsupervisedreferencefreemetric,\n    title = {CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation},\n    author = {Pei Ke and Hao Zhou and Yankai Lin and Peng Li and Jie Zhou and Xiaoyan Zhu and Minlie Huang},\n    year = {2022},\n    eprint = {2204.00862},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.00862},\n}",
    "abstract": "Existing reference-free metrics have obvious limitations for evaluating\ncontrolled text generation models. Unsupervised metrics can only provide a\ntask-agnostic evaluation result which correlates weakly with human judgments,\nwhereas supervised ones may overfit task-specific data with poor generalization\nability to other datasets. In this paper, we propose an unsupervised\nreference-free metric called CTRLEval, which evaluates controlled text\ngeneration from different aspects by formulating each aspect into multiple text\ninfilling tasks. On top of these tasks, the metric assembles the generation\nprobabilities from a pre-trained language model without any model training.\nExperimental results show that our metric has higher correlations with human\njudgments than other baselines, while obtaining better generalization of\nevaluating generated texts from different models and with different qualities.",
    "num_pages": 14
}