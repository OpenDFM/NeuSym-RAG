{
    "uuid": "03e70d5c-ae9c-50f3-8d54-6e6fe067dd8d",
    "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Taku Kudo",
        "John Richardson"
    ],
    "pdf_url": "http://arxiv.org/pdf/1808.06226v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\03e70d5c-ae9c-50f3-8d54-6e6fe067dd8d.pdf",
    "bibtex": "@misc{kudo2018sentencepieceasimpleandlanguage,\n    title = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},\n    author = {Taku Kudo and John Richardson},\n    year = {2018},\n    eprint = {1808.06226},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1808.06226},\n}",
    "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer\nand detokenizer designed for Neural-based text processing, including Neural\nMachine Translation. It provides open-source C++ and Python implementations for\nsubword units. While existing subword segmentation tools assume that the input\nis pre-tokenized into word sequences, SentencePiece can train subword models\ndirectly from raw sentences, which allows us to make a purely end-to-end and\nlanguage independent system. We perform a validation experiment of NMT on\nEnglish-Japanese machine translation, and find that it is possible to achieve\ncomparable accuracy to direct subword training from raw sentences. We also\ncompare the performance of subword training and segmentation with various\nconfigurations. SentencePiece is available under the Apache 2 license at\nhttps://github.com/google/sentencepiece.",
    "num_pages": 6
}