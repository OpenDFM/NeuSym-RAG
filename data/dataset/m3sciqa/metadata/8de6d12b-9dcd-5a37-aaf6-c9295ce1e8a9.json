{
    "uuid": "8de6d12b-9dcd-5a37-aaf6-c9295ce1e8a9",
    "title": "Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ini Oguntola",
        "Joseph Campbell",
        "Simon Stepputtis",
        "Katia Sycara"
    ],
    "pdf_url": "http://arxiv.org/pdf/2307.01158v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\8de6d12b-9dcd-5a37-aaf6-c9295ce1e8a9.pdf",
    "bibtex": "@misc{oguntola2023theoryofmindasintrinsic,\n    title = {Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning},\n    author = {Ini Oguntola and Joseph Campbell and Simon Stepputtis and Katia Sycara},\n    year = {2023},\n    eprint = {2307.01158},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2307.01158},\n}",
    "abstract": "The ability to model the mental states of others is crucial to human social\nintelligence, and can offer similar benefits to artificial agents with respect\nto the social dynamics induced in multi-agent settings. We present a method of\ngrounding semantically meaningful, human-interpretable beliefs within policies\nmodeled by deep networks. We then consider the task of 2nd-order belief\nprediction. We propose that ability of each agent to predict the beliefs of the\nother agents can be used as an intrinsic reward signal for multi-agent\nreinforcement learning. Finally, we present preliminary empirical results in a\nmixed cooperative-competitive environment.",
    "num_pages": 8
}