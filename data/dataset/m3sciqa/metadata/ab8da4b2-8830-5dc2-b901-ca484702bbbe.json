{
    "uuid": "ab8da4b2-8830-5dc2-b901-ca484702bbbe",
    "title": "Graph Attention Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1710.10903v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\ab8da4b2-8830-5dc2-b901-ca484702bbbe.pdf",
    "bibtex": "@misc{velikovi2018graphattentionnetworks,\n    title = {Graph Attention Networks},\n    author = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},\n    year = {2018},\n    eprint = {1710.10903},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1710.10903},\n}",
    "abstract": "We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training).",
    "num_pages": 12
}