{
    "uuid": "db557612-48f9-507a-bbae-d8f4f17bc192",
    "title": "A Mutual Information Maximization Perspective of Language Representation Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Lingpeng Kong",
        "Cyprien de Masson d'Autume",
        "Wang Ling",
        "Lei Yu",
        "Zihang Dai",
        "Dani Yogatama"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.08350v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\db557612-48f9-507a-bbae-d8f4f17bc192.pdf",
    "bibtex": "@misc{kong2019amutualinformationmaximizationperspective,\n    title = {A Mutual Information Maximization Perspective of Language Representation Learning},\n    author = {Lingpeng Kong and Cyprien de Masson d'Autume and Wang Ling and Lei Yu and Zihang Dai and Dani Yogatama},\n    year = {2019},\n    eprint = {1910.08350},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1910.08350},\n}",
    "abstract": "We show state-of-the-art word representation learning methods maximize an\nobjective function that is a lower bound on the mutual information between\ndifferent parts of a word sequence (i.e., a sentence). Our formulation provides\nan alternative perspective that unifies classical word embedding models (e.g.,\nSkip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to\nenhancing our theoretical understanding of these methods, our derivation leads\nto a principled framework that can be used to construct new self-supervised\ntasks. We provide an example by drawing inspirations from related methods based\non mutual information maximization that have been successful in computer\nvision, and introduce a simple self-supervised objective that maximizes the\nmutual information between a global sentence representation and n-grams in the\nsentence. Our analysis offers a holistic view of representation learning\nmethods to transfer knowledge and translate progress across multiple domains\n(e.g., natural language processing, computer vision, audio processing).",
    "num_pages": 12
}