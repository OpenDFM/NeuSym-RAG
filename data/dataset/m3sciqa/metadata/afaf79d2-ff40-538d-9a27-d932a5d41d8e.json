{
    "uuid": "afaf79d2-ff40-538d-9a27-d932a5d41d8e",
    "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Abhishek Das",
        "Harsh Agrawal",
        "C. Lawrence Zitnick",
        "Devi Parikh",
        "Dhruv Batra"
    ],
    "pdf_url": "http://arxiv.org/pdf/1606.05589v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\afaf79d2-ff40-538d-9a27-d932a5d41d8e.pdf",
    "bibtex": "@misc{das2016humanattentioninvisualquestion,\n    title = {Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?},\n    author = {Abhishek Das and Harsh Agrawal and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},\n    year = {2016},\n    eprint = {1606.05589},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1606.05589},\n}",
    "abstract": "We conduct large-scale studies on `human attention' in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.",
    "num_pages": 5
}