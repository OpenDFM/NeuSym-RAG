{
    "uuid": "058d0055-8d50-5b52-ac1a-8c36d074e246",
    "title": "Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yu Gu",
        "Sue Kase",
        "Michelle Vanni",
        "Brian Sadler",
        "Percy Liang",
        "Xifeng Yan",
        "Yu Su"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.07743v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\058d0055-8d50-5b52-ac1a-8c36d074e246.pdf",
    "bibtex": "@misc{gu2021beyondiidthreelevelsof,\n    title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},\n    author = {Yu Gu and Sue Kase and Michelle Vanni and Brian Sadler and Percy Liang and Xifeng Yan and Yu Su},\n    year = {2021},\n    eprint = {2011.07743},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2011.07743},\n}",
    "abstract": "Existing studies on question answering on knowledge bases (KBQA) mainly\noperate with the standard i.i.d assumption, i.e., training distribution over\nquestions is the same as the test distribution. However, i.i.d may be neither\nreasonably achievable nor desirable on large-scale KBs because 1) true user\ndistribution is hard to capture and 2) randomly sample training examples from\nthe enormous space would be highly data-inefficient. Instead, we suggest that\nKBQA models should have three levels of built-in generalization: i.i.d,\ncompositional, and zero-shot. To facilitate the development of KBQA models with\nstronger generalization, we construct and release a new large-scale,\nhigh-quality dataset with 64,331 questions, GrailQA, and provide evaluation\nsettings for all three levels of generalization. In addition, we propose a\nnovel BERT-based KBQA model. The combination of our dataset and model enables\nus to thoroughly examine and demonstrate, for the first time, the key role of\npre-trained contextual embeddings like BERT in the generalization of KBQA.",
    "num_pages": 12
}