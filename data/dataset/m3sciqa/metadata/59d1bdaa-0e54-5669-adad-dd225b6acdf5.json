{
    "uuid": "59d1bdaa-0e54-5669-adad-dd225b6acdf5",
    "title": "Pointer Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Oriol Vinyals",
        "Meire Fortunato",
        "Navdeep Jaitly"
    ],
    "pdf_url": "http://arxiv.org/pdf/1506.03134v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\59d1bdaa-0e54-5669-adad-dd225b6acdf5.pdf",
    "bibtex": "@misc{vinyals2017pointernetworks,\n    title = {Pointer Networks},\n    author = {Oriol Vinyals and Meire Fortunato and Navdeep Jaitly},\n    year = {2017},\n    eprint = {1506.03134},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1506.03134},\n}",
    "abstract": "We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems.",
    "num_pages": 9
}