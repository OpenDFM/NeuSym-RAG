{
    "uuid": "9b154fc7-cac7-5a8f-89ac-1ea50638b087",
    "title": "The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Vipula Rawte",
        "Swagata Chakraborty",
        "Agnibh Pathak",
        "Anubhav Sarkar",
        "S. M Towhidul Islam Tonmoy",
        "Aman Chadha",
        "Amit P. Sheth",
        "Amitava Das"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.04988v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\9b154fc7-cac7-5a8f-89ac-1ea50638b087.pdf",
    "bibtex": "@misc{rawte2023thetroublingemergenceofhallucination,\n    title = {The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations},\n    author = {Vipula Rawte and Swagata Chakraborty and Agnibh Pathak and Anubhav Sarkar and S. M Towhidul Islam Tonmoy and Aman Chadha and Amit P. Sheth and Amitava Das},\n    year = {2023},\n    eprint = {2310.04988},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2310.04988},\n}",
    "abstract": "The recent advancements in Large Language Models (LLMs) have garnered\nwidespread acclaim for their remarkable emerging capabilities. However, the\nissue of hallucination has parallelly emerged as a by-product, posing\nsignificant concerns. While some recent endeavors have been made to identify\nand mitigate different types of hallucination, there has been a limited\nemphasis on the nuanced categorization of hallucination and associated\nmitigation methods. To address this gap, we offer a fine-grained discourse on\nprofiling hallucination based on its degree, orientation, and category, along\nwith offering strategies for alleviation. As such, we define two overarching\norientations of hallucination: (i) factual mirage (FM) and (ii) silver lining\n(SL). To provide a more comprehensive understanding, both orientations are\nfurther sub-categorized into intrinsic and extrinsic, with three degrees of\nseverity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously\ncategorize hallucination into six types: (i) acronym ambiguity, (ii) numeric\nnuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum,\nand (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a\npublicly available dataset comprising of 75,000 samples generated using 15\ncontemporary LLMs along with human annotations for the aforementioned\ncategories. Finally, to establish a method for quantifying and to offer a\ncomparative spectrum that allows us to evaluate and rank LLMs based on their\nvulnerability to producing hallucinations, we propose Hallucination\nVulnerability Index (HVI). We firmly believe that HVI holds significant value\nas a tool for the wider NLP community, with the potential to serve as a rubric\nin AI-related policy-making. In conclusion, we propose two solution strategies\nfor mitigating hallucinations.",
    "num_pages": 33
}