{
    "uuid": "5bf1e056-9343-55e5-967a-c63726575c9e",
    "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zhebin Zhang",
        "Xinyu Zhang",
        "Yuanhang Ren",
        "Saijiang Shi",
        "Meng Han",
        "Yongkang Wu",
        "Ruofei Lai",
        "Zhao Cao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2311.18397v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5bf1e056-9343-55e5-967a-c63726575c9e.pdf",
    "bibtex": "@misc{zhang2023iaginductionaugmentedgenerationframeworkfor,\n    title = {IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions},\n    author = {Zhebin Zhang and Xinyu Zhang and Yuanhang Ren and Saijiang Shi and Meng Han and Yongkang Wu and Ruofei Lai and Zhao Cao},\n    year = {2023},\n    eprint = {2311.18397},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2311.18397},\n}",
    "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge\nwith parametric memory of language models, has become the state-of-the-art\narchitecture for open-domain QA tasks. However, common knowledge bases are\ninherently constrained by limited coverage and noisy information, making\nretrieval-based approaches inadequate to answer implicit reasoning questions.\nIn this paper, we propose an Induction-Augmented Generation (IAG) framework\nthat utilizes inductive knowledge along with the retrieved documents for\nimplicit reasoning. We leverage large language models (LLMs) for deriving such\nknowledge via a novel prompting method based on inductive reasoning patterns.\nOn top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,\nrespectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for\nanswer prediction, while IAG-Student gets rid of dependencies on GPT service at\ninference time by incorporating a student inductor model. The inductor is\nfirstly trained via knowledge distillation and further optimized by\nback-propagating the generator feedback via differentiable beam scores.\nExperimental results show that IAG outperforms RAG baselines as well as ChatGPT\non two Open-Domain QA tasks. Notably, our best models have won the first place\nin the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA\n(since Jan 8, 2023).",
    "num_pages": 14
}