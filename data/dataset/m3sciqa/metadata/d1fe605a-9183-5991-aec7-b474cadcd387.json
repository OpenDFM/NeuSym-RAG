{
    "uuid": "d1fe605a-9183-5991-aec7-b474cadcd387",
    "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2014,
    "authors": [
        "Ciprian Chelba",
        "Tomas Mikolov",
        "Mike Schuster",
        "Qi Ge",
        "Thorsten Brants",
        "Phillipp Koehn",
        "Tony Robinson"
    ],
    "pdf_url": "http://arxiv.org/pdf/1312.3005v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2014\\d1fe605a-9183-5991-aec7-b474cadcd387.pdf",
    "bibtex": "@misc{chelba2014onebillionwordbenchmarkfor,\n    title = {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},\n    author = {Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and Thorsten Brants and Phillipp Koehn and Tony Robinson},\n    year = {2014},\n    eprint = {1312.3005},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1312.3005},\n}",
    "abstract": "We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models.",
    "num_pages": 6
}