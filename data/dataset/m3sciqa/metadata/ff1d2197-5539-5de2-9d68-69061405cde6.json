{
    "uuid": "ff1d2197-5539-5de2-9d68-69061405cde6",
    "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yikang Shen",
        "Zheyu Zhang",
        "Tianyou Cao",
        "Shawn Tan",
        "Zhenfang Chen",
        "Chuang Gan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.04640v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ff1d2197-5539-5de2-9d68-69061405cde6.pdf",
    "bibtex": "@misc{shen2023moduleformermodularityemergesfrommixtureofexperts,\n    title = {ModuleFormer: Modularity Emerges from Mixture-of-Experts},\n    author = {Yikang Shen and Zheyu Zhang and Tianyou Cao and Shawn Tan and Zhenfang Chen and Chuang Gan},\n    year = {2023},\n    eprint = {2306.04640},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.04640},\n}",
    "abstract": "Large Language Models (LLMs) have achieved remarkable results. However,\nexisting models are expensive to train and deploy, and it is also difficult to\nexpand their knowledge beyond pre-training data without forgetting previous\nknowledge. This paper proposes a new neural network architecture, ModuleFormer,\nthat leverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model, which requires\ndomain-labeled data to learn domain-specific experts, ModuleFormer can induce\nmodularity from uncurated data with its new load balancing and concentration\nlosses. ModuleFormer is a modular architecture that includes two different\ntypes of modules: new stick-breaking attention heads and feedforward experts.\nDifferent modules are sparsely activated conditions on the input token during\ntraining and inference. In our experiment, we found that the modular\narchitecture enables three important abilities for large pre-trained language\nmodels: 1) Efficiency, since ModuleFormer only activates a subset of its\nmodules for each input token, thus it could achieve the same performance as\ndense LLMs with more than two times throughput; 2) Extendability, ModuleFormer\nis more immune to catastrophic forgetting than dense LLMs and can be easily\nextended with new modules to learn new knowledge that is not included in the\ntraining data; 3) Specialisation, finetuning ModuleFormer could specialize a\nsubset of modules to the finetuning task and the task-unrelated modules could\nbe easily pruned for a lightweight deployment.",
    "num_pages": 17
}