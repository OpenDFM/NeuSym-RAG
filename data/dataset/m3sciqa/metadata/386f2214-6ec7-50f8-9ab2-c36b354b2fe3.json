{
    "uuid": "386f2214-6ec7-50f8-9ab2-c36b354b2fe3",
    "title": "Mass-Editing Memory in a Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Kevin Meng",
        "Arnab Sen Sharma",
        "Alex Andonian",
        "Yonatan Belinkov",
        "David Bau"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.07229v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\386f2214-6ec7-50f8-9ab2-c36b354b2fe3.pdf",
    "bibtex": "@misc{meng2023masseditingmemoryinatransformer,\n    title = {Mass-Editing Memory in a Transformer},\n    author = {Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},\n    year = {2023},\n    eprint = {2210.07229},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.07229},\n}",
    "abstract": "Recent work has shown exciting promise in updating large language models with\nnew memories, so as to replace obsolete information or add specialized\nknowledge. However, this line of work is predominantly limited to updating\nsingle associations. We develop MEMIT, a method for directly updating a\nlanguage model with many memories, demonstrating experimentally that it can\nscale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),\nexceeding prior work by orders of magnitude. Our code and data are at\nhttps://memit.baulab.info.",
    "num_pages": 21
}