{
    "uuid": "8511a75d-b196-502c-87f3-3b8a5cdea12a",
    "title": "A Generalization of Transformer Networks to Graphs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Vijay Prakash Dwivedi",
        "Xavier Bresson"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.09699v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\8511a75d-b196-502c-87f3-3b8a5cdea12a.pdf",
    "bibtex": "@misc{dwivedi2021ageneralizationoftransformernetworks,\n    title = {A Generalization of Transformer Networks to Graphs},\n    author = {Vijay Prakash Dwivedi and Xavier Bresson},\n    year = {2021},\n    eprint = {2012.09699},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2012.09699},\n}",
    "abstract": "We propose a generalization of transformer neural network architecture for\narbitrary graphs. The original transformer was designed for Natural Language\nProcessing (NLP), which operates on fully connected graphs representing all\nconnections between the words in a sequence. Such architecture does not\nleverage the graph connectivity inductive bias, and can perform poorly when the\ngraph topology is important and has not been encoded into the node features. We\nintroduce a graph transformer with four new properties compared to the standard\nmodel. First, the attention mechanism is a function of the neighborhood\nconnectivity for each node in the graph. Second, the positional encoding is\nrepresented by the Laplacian eigenvectors, which naturally generalize the\nsinusoidal positional encodings often used in NLP. Third, the layer\nnormalization is replaced by a batch normalization layer, which provides faster\ntraining and better generalization performance. Finally, the architecture is\nextended to edge feature representation, which can be critical to tasks s.a.\nchemistry (bond type) or link prediction (entity relationship in knowledge\ngraphs). Numerical experiments on a graph benchmark demonstrate the performance\nof the proposed graph transformer architecture. This work closes the gap\nbetween the original transformer, which was designed for the limited case of\nline graphs, and graph neural networks, that can work with arbitrary graphs. As\nour architecture is simple and generic, we believe it can be used as a black\nbox for future applications that wish to consider transformer and graphs.",
    "num_pages": 8
}