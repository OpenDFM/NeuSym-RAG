{
    "uuid": "6666b1b6-588c-56c5-9199-cd16d1db2a49",
    "title": "Knowledge Distillation: A Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jianping Gou",
        "Baosheng Yu",
        "Stephen John Maybank",
        "Dacheng Tao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.05525v7",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\6666b1b6-588c-56c5-9199-cd16d1db2a49.pdf",
    "bibtex": "@misc{gou2021knowledgedistillationasurvey,\n    title = {Knowledge Distillation: A Survey},\n    author = {Jianping Gou and Baosheng Yu and Stephen John Maybank and Dacheng Tao},\n    year = {2021},\n    eprint = {2006.05525},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2006.05525},\n}",
    "abstract": "In recent years, deep neural networks have been successful in both industry\nand academia, especially for computer vision tasks. The great success of deep\nlearning is mainly due to its scalability to encode large-scale data and to\nmaneuver billions of model parameters. However, it is a challenge to deploy\nthese cumbersome deep models on devices with limited resources, e.g., mobile\nphones and embedded devices, not only because of the high computational\ncomplexity but also the large storage requirements. To this end, a variety of\nmodel compression and acceleration techniques have been developed. As a\nrepresentative type of model compression and acceleration, knowledge\ndistillation effectively learns a small student model from a large teacher\nmodel. It has received rapid increasing attention from the community. This\npaper provides a comprehensive survey of knowledge distillation from the\nperspectives of knowledge categories, training schemes, teacher-student\narchitecture, distillation algorithms, performance comparison and applications.\nFurthermore, challenges in knowledge distillation are briefly reviewed and\ncomments on future research are discussed and forwarded.",
    "num_pages": 36
}