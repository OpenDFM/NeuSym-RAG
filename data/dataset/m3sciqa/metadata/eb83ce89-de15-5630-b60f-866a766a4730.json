{
    "uuid": "eb83ce89-de15-5630-b60f-866a766a4730",
    "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Pei Ke",
        "Haozhe Ji",
        "Yu Ran",
        "Xin Cui",
        "Liwei Wang",
        "Linfeng Song",
        "Xiaoyan Zhu",
        "Minlie Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.10502v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\eb83ce89-de15-5630-b60f-866a766a4730.pdf",
    "bibtex": "@misc{ke2021jointgtgraphtextjointrepresentationlearning,\n    title = {JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs},\n    author = {Pei Ke and Haozhe Ji and Yu Ran and Xin Cui and Liwei Wang and Linfeng Song and Xiaoyan Zhu and Minlie Huang},\n    year = {2021},\n    eprint = {2106.10502},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.10502},\n}",
    "abstract": "Existing pre-trained models for knowledge-graph-to-text (KG-to-text)\ngeneration simply fine-tune text-to-text pre-trained models such as BART or T5\non KG-to-text datasets, which largely ignore the graph structure during\nencoding and lack elaborate pre-training tasks to explicitly model graph-text\nalignments. To tackle these problems, we propose a graph-text joint\nrepresentation learning model called JointGT. During encoding, we devise a\nstructure-aware semantic aggregation module which is plugged into each\nTransformer layer to preserve the graph structure. Furthermore, we propose\nthree new pre-training tasks to explicitly enhance the graph-text alignment\nincluding respective text / graph reconstruction, and graph-text alignment in\nthe embedding space via Optimal Transport. Experiments show that JointGT\nobtains new state-of-the-art performance on various KG-to-text datasets.",
    "num_pages": 13
}