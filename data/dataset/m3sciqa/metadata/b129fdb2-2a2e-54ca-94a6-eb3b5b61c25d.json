{
    "uuid": "b129fdb2-2a2e-54ca-94a6-eb3b5b61c25d",
    "title": "Prototypical Verbalizer for Prompt-based Few-shot Tuning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ganqu Cui",
        "Shengding Hu",
        "Ning Ding",
        "Longtao Huang",
        "Zhiyuan Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.09770v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\b129fdb2-2a2e-54ca-94a6-eb3b5b61c25d.pdf",
    "bibtex": "@misc{cui2022prototypicalverbalizerforpromptbasedfewshot,\n    title = {Prototypical Verbalizer for Prompt-based Few-shot Tuning},\n    author = {Ganqu Cui and Shengding Hu and Ning Ding and Longtao Huang and Zhiyuan Liu},\n    year = {2022},\n    eprint = {2203.09770},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.09770},\n}",
    "abstract": "Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.",
    "num_pages": 11
}