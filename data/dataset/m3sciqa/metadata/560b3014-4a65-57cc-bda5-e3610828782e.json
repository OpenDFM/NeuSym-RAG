{
    "uuid": "560b3014-4a65-57cc-bda5-e3610828782e",
    "title": "Zero-Shot Entity Linking by Reading Entity Descriptions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Lajanugen Logeswaran",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Jacob Devlin",
        "Honglak Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.07348v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\560b3014-4a65-57cc-bda5-e3610828782e.pdf",
    "bibtex": "@misc{logeswaran2019zeroshotentitylinkingbyreading,\n    title = {Zero-Shot Entity Linking by Reading Entity Descriptions},\n    author = {Lajanugen Logeswaran and Ming-Wei Chang and Kenton Lee and Kristina Toutanova and Jacob Devlin and Honglak Lee},\n    year = {2019},\n    eprint = {1906.07348},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1906.07348},\n}",
    "abstract": "We present the zero-shot entity linking task, where mentions must be linked\nto unseen entities without in-domain labeled data. The goal is to enable robust\ntransfer to highly specialized domains, and so no metadata or alias tables are\nassumed. In this setting, entities are only identified by text descriptions,\nand models must rely strictly on language understanding to resolve the new\nentities. First, we show that strong reading comprehension models pre-trained\non large unlabeled data can be used to generalize to unseen entities. Second,\nwe propose a simple and effective adaptive pre-training strategy, which we term\ndomain-adaptive pre-training (DAP), to address the domain shift problem\nassociated with linking unseen entities in a new domain. We present experiments\non a new dataset that we construct for this task and show that DAP improves\nover strong pre-training baselines, including BERT. The data and code are\navailable at https://github.com/lajanugen/zeshel.",
    "num_pages": 12
}