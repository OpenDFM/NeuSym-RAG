{
    "uuid": "62e25caa-070c-56c0-a5d5-c200c1413cc8",
    "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Vamsi Aribandi",
        "Yi Tay",
        "Tal Schuster",
        "Jinfeng Rao",
        "Huaixiu Steven Zheng",
        "Sanket Vaibhav Mehta",
        "Honglei Zhuang",
        "Vinh Q. Tran",
        "Dara Bahri",
        "Jianmo Ni",
        "Jai Gupta",
        "Kai Hui",
        "Sebastian Ruder",
        "Donald Metzler"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.10952v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\62e25caa-070c-56c0-a5d5-c200c1413cc8.pdf",
    "bibtex": "@misc{aribandi2022ext5towardsextrememultitaskscaling,\n    title = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},\n    author = {Vamsi Aribandi and Yi Tay and Tal Schuster and Jinfeng Rao and Huaixiu Steven Zheng and Sanket Vaibhav Mehta and Honglei Zhuang and Vinh Q. Tran and Dara Bahri and Jianmo Ni and Jai Gupta and Kai Hui and Sebastian Ruder and Donald Metzler},\n    year = {2022},\n    eprint = {2111.10952},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2111.10952},\n}",
    "abstract": "Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.",
    "num_pages": 28
}