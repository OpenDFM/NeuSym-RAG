{
    "uuid": "62e2795c-64b9-54eb-8d86-ad458ab18b05",
    "title": "Adaptation Approaches for Nearest Neighbor Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Rishabh Bhardwaj",
        "George Polovets",
        "Monica Sunkara"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.07828v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\62e2795c-64b9-54eb-8d86-ad458ab18b05.pdf",
    "bibtex": "@misc{bhardwaj2023adaptationapproachesfornearestneighbor,\n    title = {Adaptation Approaches for Nearest Neighbor Language Models},\n    author = {Rishabh Bhardwaj and George Polovets and Monica Sunkara},\n    year = {2023},\n    eprint = {2211.07828},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.07828},\n}",
    "abstract": "Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced\nimpressive gains over purely parametric LMs, by leveraging large-scale\nneighborhood retrieval over external memory datastores. However, there has been\nlittle investigation into adapting such models for new domains. This work\nattempts to fill that gap and suggests the following approaches for adapting\n$k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding\nneighborhood retrieval over an additional adaptation datastore, and 3) adapting\nthe weights (scores) of retrieved neighbors using a learned Rescorer module. We\nstudy each adaptation strategy separately, as well as the combined performance\nimprovement through ablation experiments and an extensive set of evaluations\nrun over seven adaptation domains. Our combined adaptation approach\nconsistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM)\nbaselines that construct datastores from the adaptation data. On average, we\nsee perplexity improvements of 17.1% and 16% for these respective baselines,\nacross domains.",
    "num_pages": 10
}