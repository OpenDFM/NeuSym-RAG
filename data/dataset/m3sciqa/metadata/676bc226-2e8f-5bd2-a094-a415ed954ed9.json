{
    "uuid": "676bc226-2e8f-5bd2-a094-a415ed954ed9",
    "title": "Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jiaang Li",
        "Quan Wang",
        "Yi Liu",
        "Licheng Zhang",
        "Zhendong Mao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.15797v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\676bc226-2e8f-5bd2-a094-a415ed954ed9.pdf",
    "bibtex": "@misc{li2023randomentityquantizationforparameterefficient,\n    title = {Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation},\n    author = {Jiaang Li and Quan Wang and Yi Liu and Licheng Zhang and Zhendong Mao},\n    year = {2023},\n    eprint = {2310.15797},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2310.15797},\n}",
    "abstract": "Representation Learning on Knowledge Graphs (KGs) is essential for downstream\ntasks. The dominant approach, KG Embedding (KGE), represents entities with\nindependent vectors and faces the scalability challenge. Recent studies propose\nan alternative way for parameter efficiency, which represents entities by\ncomposing entity-corresponding codewords matched from predefined small-scale\ncodebooks. We refer to the process of obtaining corresponding codewords of each\nentity as entity quantization, for which previous works have designed\ncomplicated strategies. Surprisingly, this paper shows that simple random\nentity quantization can achieve similar results to current strategies. We\nanalyze this phenomenon and reveal that entity codes, the quantization outcomes\nfor expressing entities, have higher entropy at the code level and Jaccard\ndistance at the codeword level under random entity quantization. Therefore,\ndifferent entities become more easily distinguished, facilitating effective KG\nrepresentation. The above results show that current quantization strategies are\nnot critical for KG representation, and there is still room for improvement in\nentity distinguishability beyond current strategies. The code to reproduce our\nresults is available at https://github.com/JiaangL/RandomQuantization.",
    "num_pages": 12
}