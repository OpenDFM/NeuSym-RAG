{
    "uuid": "dc2db109-c43d-5dcf-ad8d-6949d17cc731",
    "title": "Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chenze Shao",
        "Zhengrui Ma",
        "Yang Feng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.05193v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\dc2db109-c43d-5dcf-ad8d-6949d17cc731.pdf",
    "bibtex": "@misc{shao2023viterbidecodingofdirectedacyclic,\n    title = {Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation},\n    author = {Chenze Shao and Zhengrui Ma and Yang Feng},\n    year = {2023},\n    eprint = {2210.05193},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.05193},\n}",
    "abstract": "Non-autoregressive models achieve significant decoding speedup in neural\nmachine translation but lack the ability to capture sequential dependency.\nDirected Acyclic Transformer (DA-Transformer) was recently proposed to model\nsequential dependency with a directed acyclic graph. Consequently, it has to\napply a sequential decision process at inference time, which harms the global\ntranslation accuracy. In this paper, we present a Viterbi decoding framework\nfor DA-Transformer, which guarantees to find the joint optimal solution for the\ntranslation and decoding path under any length constraint. Experimental results\ndemonstrate that our approach consistently improves the performance of\nDA-Transformer while maintaining a similar decoding speedup.",
    "num_pages": 8
}