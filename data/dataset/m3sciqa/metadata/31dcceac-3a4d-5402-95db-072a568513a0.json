{
    "uuid": "31dcceac-3a4d-5402-95db-072a568513a0",
    "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Kushal Arora",
        "Layla El Asri",
        "Hareesh Bahuleyan",
        "Jackie Chi Kit Cheung"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.01171v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\31dcceac-3a4d-5402-95db-072a568513a0.pdf",
    "bibtex": "@misc{arora2023whyexposurebiasmattersan,\n    title = {Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation},\n    author = {Kushal Arora and Layla El Asri and Hareesh Bahuleyan and Jackie Chi Kit Cheung},\n    year = {2023},\n    eprint = {2204.01171},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.01171},\n}",
    "abstract": "Current language generation models suffer from issues such as repetition,\nincoherence, and hallucinations. An often-repeated hypothesis is that this\nbrittleness of generation models is caused by the training and the generation\nprocedure mismatch, also referred to as exposure bias. In this paper, we verify\nthis hypothesis by analyzing exposure bias from an imitation learning\nperspective. We show that exposure bias leads to an accumulation of errors,\nanalyze why perplexity fails to capture this accumulation, and empirically show\nthat this accumulation results in poor generation quality. Source code to\nreproduce these experiments is available at\nhttps://github.com/kushalarora/quantifying_exposure_bias",
    "num_pages": 11
}