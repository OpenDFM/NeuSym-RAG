{
    "uuid": "46351f44-6981-5f11-8936-24b1d926681f",
    "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xinyang Zhang",
        "Yury Malkov",
        "Omar Florez",
        "Serim Park",
        "Brian McWilliams",
        "Jiawei Han",
        "Ahmed El-Kishky"
    ],
    "pdf_url": "http://arxiv.org/pdf/2209.07562v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\46351f44-6981-5f11-8936-24b1d926681f.pdf",
    "bibtex": "@misc{zhang2023twhinbertasociallyenrichedpretrainedlanguage,\n    title = {TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter},\n    author = {Xinyang Zhang and Yury Malkov and Omar Florez and Serim Park and Brian McWilliams and Jiawei Han and Ahmed El-Kishky},\n    year = {2023},\n    eprint = {2209.07562},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2209.07562},\n}",
    "abstract": "Pre-trained language models (PLMs) are fundamental for natural language\nprocessing applications. Most existing PLMs are not tailored to the noisy\nuser-generated text on social media, and the pre-training does not factor in\nthe valuable social engagement logs available in a social network. We present\nTwHIN-BERT, a multilingual language model productionized at Twitter, trained on\nin-domain data from the popular social network. TwHIN-BERT differs from prior\npre-trained language models as it is trained with not only text-based\nself-supervision, but also with a social objective based on the rich social\nengagements within a Twitter heterogeneous information network (TwHIN). Our\nmodel is trained on 7 billion tweets covering over 100 distinct languages,\nproviding a valuable representation to model short, noisy, user-generated text.\nWe evaluate our model on various multilingual social recommendation and\nsemantic understanding tasks and demonstrate significant metric improvement\nover established pre-trained language models. We open-source TwHIN-BERT and our\ncurated hashtag prediction and social engagement benchmark datasets to the\nresearch community.",
    "num_pages": 11
}