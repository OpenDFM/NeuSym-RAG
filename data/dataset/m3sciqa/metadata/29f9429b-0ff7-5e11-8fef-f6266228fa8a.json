{
    "uuid": "29f9429b-0ff7-5e11-8fef-f6266228fa8a",
    "title": "BERTweet: A pre-trained language model for English Tweets",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Dat Quoc Nguyen",
        "Thanh Vu",
        "Anh Tuan Nguyen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.10200v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\29f9429b-0ff7-5e11-8fef-f6266228fa8a.pdf",
    "bibtex": "@misc{nguyen2020bertweetapretrainedlanguagemodel,\n    title = {BERTweet: A pre-trained language model for English Tweets},\n    author = {Dat Quoc Nguyen and Thanh Vu and Anh Tuan Nguyen},\n    year = {2020},\n    eprint = {2005.10200},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.10200},\n}",
    "abstract": "We present BERTweet, the first public large-scale pre-trained language model\nfor English Tweets. Our BERTweet, having the same architecture as BERT-base\n(Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu\net al., 2019). Experiments show that BERTweet outperforms strong baselines\nRoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better\nperformance results than the previous state-of-the-art models on three Tweet\nNLP tasks: Part-of-speech tagging, Named-entity recognition and text\nclassification. We release BERTweet under the MIT License to facilitate future\nresearch and applications on Tweet data. Our BERTweet is available at\nhttps://github.com/VinAIResearch/BERTweet",
    "num_pages": 6
}