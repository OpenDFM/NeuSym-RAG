{
    "uuid": "df911696-4c59-5e6d-95e4-3a2b0dbf9d76",
    "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Amro Abbas",
        "Kushal Tirumala",
        "Dániel Simig",
        "Surya Ganguli",
        "Ari S. Morcos"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.09540v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\df911696-4c59-5e6d-95e4-3a2b0dbf9d76.pdf",
    "bibtex": "@misc{abbas2023semdedupdataefficientlearningatwebscale,\n    title = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication},\n    author = {Amro Abbas and Kushal Tirumala and Dániel Simig and Surya Ganguli and Ari S. Morcos},\n    year = {2023},\n    eprint = {2303.09540},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2303.09540},\n}",
    "abstract": "Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.",
    "num_pages": 34
}