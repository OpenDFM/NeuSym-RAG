{
    "uuid": "4eb654a7-54b7-5270-8a8b-e0ba00d789fe",
    "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zhihong Shao",
        "Yeyun Gong",
        "Yelong Shen",
        "Minlie Huang",
        "Nan Duan",
        "Weizhu Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.15294v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4eb654a7-54b7-5270-8a8b-e0ba00d789fe.pdf",
    "bibtex": "@misc{shao2023enhancingretrievalaugmentedlargelanguagemodels,\n    title = {Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy},\n    author = {Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},\n    year = {2023},\n    eprint = {2305.15294},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.15294},\n}",
    "abstract": "Large language models are powerful text processors and reasoners, but are\nstill subject to limitations including outdated knowledge and hallucinations,\nwhich necessitates connecting them to the world. Retrieval-augmented large\nlanguage models have raised extensive attention for grounding model generation\non external knowledge. However, retrievers struggle to capture relevance,\nespecially for queries with complex information needs. Recent work has proposed\nto improve relevance modeling by having large language models actively involved\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\nshow that strong performance can be achieved by a method we call Iter-RetGen,\nwhich synergizes retrieval and generation in an iterative manner. A model\noutput shows what might be needed to finish a task, and thus provides an\ninformative context for retrieving more relevant knowledge which in turn helps\ngenerate a better output in the next iteration. Compared with recent work which\ninterleaves retrieval with generation when producing an output, Iter-RetGen\nprocesses all retrieved knowledge as a whole and largely preserves the\nflexibility in generation without structural constraints. We evaluate\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\nreasoning, and show that it can flexibly leverage parametric knowledge and\nnon-parametric knowledge, and is superior to or competitive with\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\nretrieval and generation. We can further improve performance via\ngeneration-augmented retrieval adaptation.",
    "num_pages": 27
}