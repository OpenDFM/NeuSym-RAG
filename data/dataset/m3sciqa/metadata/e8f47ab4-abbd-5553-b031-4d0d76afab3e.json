{
    "uuid": "e8f47ab4-abbd-5553-b031-4d0d76afab3e",
    "title": "Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Xinyin Ma",
        "Xinchao Wang",
        "Gongfan Fang",
        "Yongliang Shen",
        "Weiming Lu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.07523v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e8f47ab4-abbd-5553-b031-4d0d76afab3e.pdf",
    "bibtex": "@misc{ma2022promptingtodistillboostingdatafree,\n    title = {Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt},\n    author = {Xinyin Ma and Xinchao Wang and Gongfan Fang and Yongliang Shen and Weiming Lu},\n    year = {2022},\n    eprint = {2205.07523},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.07523},\n}",
    "abstract": "Data-free knowledge distillation (DFKD) conducts knowledge distillation via\neliminating the dependence of original training data, and has recently achieved\nimpressive results in accelerating pre-trained language models. At the heart of\nDFKD is to reconstruct a synthetic dataset by inverting the parameters of the\nuncompressed model. Prior DFKD approaches, however, have largely relied on\nhand-crafted priors of the target data distribution for the reconstruction,\nwhich can be inevitably biased and often incompetent to capture the intrinsic\ndistributions. To address this problem, we propose a prompt-based method,\ntermed as PromptDFD, that allows us to take advantage of learned language\npriors, which effectively harmonizes the synthetic sentences to be semantically\nand grammatically correct. Specifically, PromptDFD leverages a pre-trained\ngenerative model to provide language priors and introduces a reinforced topic\nprompter to control data synthesis, making the generated samples thematically\nrelevant and semantically plausible, and thus friendly to downstream tasks. As\nshown in our experiments, the proposed method substantially improves the\nsynthesis quality and achieves considerable improvements on distillation\nperformance. In some cases, PromptDFD even gives rise to results on par with\nthose from the data-driven knowledge distillation with access to the original\ntraining data.",
    "num_pages": 9
}