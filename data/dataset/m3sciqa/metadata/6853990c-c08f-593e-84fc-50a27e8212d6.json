{
    "uuid": "6853990c-c08f-593e-84fc-50a27e8212d6",
    "title": "Think Before You Act: Decision Transformers with Working Memory",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Jikun Kang",
        "Romain Laroche",
        "Xingdi Yuan",
        "Adam Trischler",
        "Xue Liu",
        "Jie Fu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.16338v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\6853990c-c08f-593e-84fc-50a27e8212d6.pdf",
    "bibtex": "@misc{kang2024thinkbeforeyouactdecision,\n    title = {Think Before You Act: Decision Transformers with Working Memory},\n    author = {Jikun Kang and Romain Laroche and Xingdi Yuan and Adam Trischler and Xue Liu and Jie Fu},\n    year = {2024},\n    eprint = {2305.16338},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2305.16338},\n}",
    "abstract": "Decision Transformer-based decision-making agents have shown the ability to\ngeneralize across multiple tasks. However, their performance relies on massive\ndata and computation. We argue that this inefficiency stems from the forgetting\nphenomenon, in which a model memorizes its behaviors in parameters throughout\ntraining. As a result, training on a new task may deteriorate the model's\nperformance on previous tasks. In contrast to LLMs' implicit memory mechanism,\nthe human brain utilizes distributed memory storage, which helps manage and\norganize multiple skills efficiently, mitigating the forgetting phenomenon.\nInspired by this, we propose a working memory module to store, blend, and\nretrieve information for different downstream tasks. Evaluation results show\nthat the proposed method improves training efficiency and generalization in\nAtari games and Meta-World object manipulation tasks. Moreover, we demonstrate\nthat memory fine-tuning further enhances the adaptability of the proposed\narchitecture.",
    "num_pages": 21
}