{
    "uuid": "a2677b99-9e0d-5c14-8e19-f1c7d5cfa93f",
    "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yu Rong",
        "Yatao Bian",
        "Tingyang Xu",
        "Weiyang Xie",
        "Ying Wei",
        "Wenbing Huang",
        "Junzhou Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.02835v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\a2677b99-9e0d-5c14-8e19-f1c7d5cfa93f.pdf",
    "bibtex": "@misc{rong2020selfsupervisedgraphtransformeronlargescale,\n    title = {Self-Supervised Graph Transformer on Large-Scale Molecular Data},\n    author = {Yu Rong and Yatao Bian and Tingyang Xu and Weiyang Xie and Ying Wei and Wenbing Huang and Junzhou Huang},\n    year = {2020},\n    eprint = {2007.02835},\n    archivePrefix = {arXiv},\n    primaryClass = {q-bio.BM},\n    url = {http://arxiv.org/abs/2007.02835},\n}",
    "abstract": "How to obtain informative representations of molecules is a crucial\nprerequisite in AI-driven drug design and discovery. Recent researches abstract\nmolecules as graphs and employ Graph Neural Networks (GNNs) for molecular\nrepresentation learning. Nevertheless, two issues impede the usage of GNNs in\nreal scenarios: (1) insufficient labeled molecules for supervised training; (2)\npoor generalization capability to new-synthesized molecules. To address them\nboth, we propose a novel framework, GROVER, which stands for Graph\nRepresentation frOm self-superVised mEssage passing tRansformer. With carefully\ndesigned self-supervised tasks in node-, edge- and graph-level, GROVER can\nlearn rich structural and semantic information of molecules from enormous\nunlabelled molecular data. Rather, to encode such complex information, GROVER\nintegrates Message Passing Networks into the Transformer-style architecture to\ndeliver a class of more expressive encoders of molecules. The flexibility of\nGROVER allows it to be trained efficiently on large-scale molecular dataset\nwithout requiring any supervision, thus being immunized to the two issues\nmentioned above. We pre-train GROVER with 100 million parameters on 10 million\nunlabelled molecules -- the biggest GNN and the largest training dataset in\nmolecular representation learning. We then leverage the pre-trained GROVER for\nmolecular property prediction followed by task-specific fine-tuning, where we\nobserve a huge improvement (more than 6% on average) from current\nstate-of-the-art methods on 11 challenging benchmarks. The insights we gained\nare that well-designed self-supervision losses and largely-expressive\npre-trained models enjoy the significant potential on performance boosting.",
    "num_pages": 18
}