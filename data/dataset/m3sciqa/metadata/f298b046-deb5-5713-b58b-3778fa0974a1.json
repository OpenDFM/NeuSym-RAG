{
    "uuid": "f298b046-deb5-5713-b58b-3778fa0974a1",
    "title": "DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Qingkai Fang",
        "Yan Zhou",
        "Yang Feng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.07403v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f298b046-deb5-5713-b58b-3778fa0974a1.pdf",
    "bibtex": "@misc{fang2023daspeechdirectedacyclictransformerfor,\n    title = {DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation},\n    author = {Qingkai Fang and Yan Zhou and Yang Feng},\n    year = {2023},\n    eprint = {2310.07403},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.07403},\n}",
    "abstract": "Direct speech-to-speech translation (S2ST) translates speech from one\nlanguage into another using a single model. However, due to the presence of\nlinguistic and acoustic diversity, the target speech follows a complex\nmultimodal distribution, posing challenges to achieving both high-quality\ntranslations and fast decoding speeds for S2ST models. In this paper, we\npropose DASpeech, a non-autoregressive direct S2ST model which realizes both\nfast and high-quality S2ST. To better capture the complex distribution of the\ntarget speech, DASpeech adopts the two-pass architecture to decompose the\ngeneration process into two steps, where a linguistic decoder first generates\nthe target text, and an acoustic decoder then generates the target speech based\non the hidden states of the linguistic decoder. Specifically, we use the\ndecoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as\nthe acoustic decoder. DA-Transformer models translations with a directed\nacyclic graph (DAG). To consider all potential paths in the DAG during\ntraining, we calculate the expected hidden states for each target token via\ndynamic programming, and feed them into the acoustic decoder to predict the\ntarget mel-spectrogram. During inference, we select the most probable path and\ntake hidden states on that path as input to the acoustic decoder. Experiments\non the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or\neven better performance than the state-of-the-art S2ST model Translatotron 2,\nwhile preserving up to 18.53x speedup compared to the autoregressive baseline.\nCompared with the previous non-autoregressive S2ST model, DASpeech does not\nrely on knowledge distillation and iterative decoding, achieving significant\nimprovements in both translation quality and decoding speed. Furthermore,\nDASpeech shows the ability to preserve the speaker's voice of the source speech\nduring translation.",
    "num_pages": 20
}