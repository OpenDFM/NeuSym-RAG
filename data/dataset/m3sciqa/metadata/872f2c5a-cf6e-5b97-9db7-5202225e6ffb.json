{
    "uuid": "872f2c5a-cf6e-5b97-9db7-5202225e6ffb",
    "title": "A Theoretical Analysis of the Repetition Problem in Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Zihao Fu",
        "Wai Lam",
        "Anthony Man-Cho So",
        "Bei Shi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14660v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\872f2c5a-cf6e-5b97-9db7-5202225e6ffb.pdf",
    "bibtex": "@misc{fu2021atheoreticalanalysisofthe,\n    title = {A Theoretical Analysis of the Repetition Problem in Text Generation},\n    author = {Zihao Fu and Wai Lam and Anthony Man-Cho So and Bei Shi},\n    year = {2021},\n    eprint = {2012.14660},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.14660},\n}",
    "abstract": "Text generation tasks, including translation, summarization, language models,\nand etc. see rapid growth during recent years. Despite the remarkable\nachievements, the repetition problem has been observed in nearly all text\ngeneration models undermining the generation performance extensively. To solve\nthe repetition problem, many methods have been proposed, but there is no\nexisting theoretical analysis to show why this problem happens and how it is\nresolved. In this paper, we propose a new framework for theoretical analysis\nfor the repetition problem. We first define the Average Repetition Probability\n(ARP) to characterize the repetition problem quantitatively. Then, we conduct\nan extensive analysis of the Markov generation model and derive several upper\nbounds of the average repetition probability with intuitive understanding. We\nshow that most of the existing methods are essentially minimizing the upper\nbounds explicitly or implicitly. Grounded on our theory, we show that the\nrepetition problem is, unfortunately, caused by the traits of our language\nitself. One major reason is attributed to the fact that there exist too many\nwords predicting the same word as the subsequent word with high probability.\nConsequently, it is easy to go back to that word and form repetitions and we\ndub it as the high inflow problem. Furthermore, we derive a concentration bound\nof the average repetition probability for a general generation model. Finally,\nbased on the theoretical upper bounds, we propose a novel rebalanced encoding\napproach to alleviate the high inflow problem. The experimental results show\nthat our theoretical framework is applicable in general generation models and\nour proposed rebalanced encoding approach alleviates the repetition problem\nsignificantly. The source code of this paper can be obtained from\nhttps://github.com/fuzihaofzh/repetition-problem-nlg.",
    "num_pages": 12
}