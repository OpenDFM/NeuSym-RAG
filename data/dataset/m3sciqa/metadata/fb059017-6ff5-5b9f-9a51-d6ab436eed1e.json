{
    "uuid": "fb059017-6ff5-5b9f-9a51-d6ab436eed1e",
    "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Alisa Liu",
        "Swabha Swayamdipta",
        "Noah A. Smith",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.05955v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\fb059017-6ff5-5b9f-9a51-d6ab436eed1e.pdf",
    "bibtex": "@misc{liu2022wanliworkerandaicollaboration,\n    title = {WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},\n    author = {Alisa Liu and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},\n    year = {2022},\n    eprint = {2201.05955},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.05955},\n}",
    "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI improves performance on eight out-of-domain test sets we\nconsider, including by 11% on HANS and 9% on Adversarial NLI, compared to\ntraining on the 4x larger MultiNLI. Moreover, it continues to be more effective\nthan MultiNLI augmented with other NLI datasets. Our results demonstrate the\npromise of leveraging natural language generation techniques and re-imagining\nthe role of humans in the dataset creation process.",
    "num_pages": 22
}