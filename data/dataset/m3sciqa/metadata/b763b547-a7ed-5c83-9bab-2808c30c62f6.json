{
    "uuid": "b763b547-a7ed-5c83-9bab-2808c30c62f6",
    "title": "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Peter Anderson",
        "Qi Wu",
        "Damien Teney",
        "Jake Bruce",
        "Mark Johnson",
        "Niko Sünderhauf",
        "Ian Reid",
        "Stephen Gould",
        "Anton van den Hengel"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.07280v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\b763b547-a7ed-5c83-9bab-2808c30c62f6.pdf",
    "bibtex": "@misc{anderson2018visionandlanguagenavigationinterpretingvisuallygroundednavigation,\n    title = {Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments},\n    author = {Peter Anderson and Qi Wu and Damien Teney and Jake Bruce and Mark Johnson and Niko Sünderhauf and Ian Reid and Stephen Gould and Anton van den Hengel},\n    year = {2018},\n    eprint = {1711.07280},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1711.07280},\n}",
    "abstract": "A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator -- a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.",
    "num_pages": 13
}