{
    "uuid": "0c2deafb-5ba6-5f61-93bf-cba4a976a858",
    "title": "SciBERT: A Pretrained Language Model for Scientific Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Iz Beltagy",
        "Kyle Lo",
        "Arman Cohan"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.10676v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\0c2deafb-5ba6-5f61-93bf-cba4a976a858.pdf",
    "bibtex": "@misc{beltagy2019scibertapretrainedlanguagemodel,\n    title = {SciBERT: A Pretrained Language Model for Scientific Text},\n    author = {Iz Beltagy and Kyle Lo and Arman Cohan},\n    year = {2019},\n    eprint = {1903.10676},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1903.10676},\n}",
    "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain\nis challenging and expensive. We release SciBERT, a pretrained language model\nbased on BERT (Devlin et al., 2018) to address the lack of high-quality,\nlarge-scale labeled scientific data. SciBERT leverages unsupervised pretraining\non a large multi-domain corpus of scientific publications to improve\nperformance on downstream scientific NLP tasks. We evaluate on a suite of tasks\nincluding sequence tagging, sentence classification and dependency parsing,\nwith datasets from a variety of scientific domains. We demonstrate\nstatistically significant improvements over BERT and achieve new\nstate-of-the-art results on several of these tasks. The code and pretrained\nmodels are available at https://github.com/allenai/scibert/.",
    "num_pages": 6
}