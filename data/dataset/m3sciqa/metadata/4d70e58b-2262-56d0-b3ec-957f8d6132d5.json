{
    "uuid": "4d70e58b-2262-56d0-b3ec-957f8d6132d5",
    "title": "Lessons on Parameter Sharing across Layers in Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Sho Takase",
        "Shun Kiyono"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06022v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4d70e58b-2262-56d0-b3ec-957f8d6132d5.pdf",
    "bibtex": "@misc{takase2023lessonsonparametersharingacross,\n    title = {Lessons on Parameter Sharing across Layers in Transformers},\n    author = {Sho Takase and Shun Kiyono},\n    year = {2023},\n    eprint = {2104.06022},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.06022},\n}",
    "abstract": "We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.",
    "num_pages": 13
}