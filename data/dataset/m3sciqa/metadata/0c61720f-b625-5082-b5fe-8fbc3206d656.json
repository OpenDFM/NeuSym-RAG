{
    "uuid": "0c61720f-b625-5082-b5fe-8fbc3206d656",
    "title": "JuriBERT: A Masked-Language Model Adaptation for French Legal Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Stella Douka",
        "Hadi Abdine",
        "Michalis Vazirgiannis",
        "Rajaa El Hamdani",
        "David Restrepo Amariles"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.01485v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\0c61720f-b625-5082-b5fe-8fbc3206d656.pdf",
    "bibtex": "@misc{douka2022juribertamaskedlanguagemodeladaptation,\n    title = {JuriBERT: A Masked-Language Model Adaptation for French Legal Text},\n    author = {Stella Douka and Hadi Abdine and Michalis Vazirgiannis and Rajaa El Hamdani and David Restrepo Amariles},\n    year = {2022},\n    eprint = {2110.01485},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.01485},\n}",
    "abstract": "Language models have proven to be very useful when adapted to specific\ndomains. Nonetheless, little research has been done on the adaptation of\ndomain-specific BERT models in the French language. In this paper, we focus on\ncreating a language model adapted to French legal text with the goal of helping\nlaw professionals. We conclude that some specific tasks do not benefit from\ngeneric language models pre-trained on large amounts of data. We explore the\nuse of smaller architectures in domain-specific sub-languages and their\nbenefits for French legal text. We prove that domain-specific pre-trained\nmodels can perform better than their equivalent generalised ones in the legal\ndomain. Finally, we release JuriBERT, a new set of BERT models adapted to the\nFrench legal domain.",
    "num_pages": 7
}