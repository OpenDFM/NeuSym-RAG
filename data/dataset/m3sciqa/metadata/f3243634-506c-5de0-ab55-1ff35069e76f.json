{
    "uuid": "f3243634-506c-5de0-ab55-1ff35069e76f",
    "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yujia Qin",
        "Jiajie Zhang",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Peng Li",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.06311v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\f3243634-506c-5de0-ab55-1ff35069e76f.pdf",
    "bibtex": "@misc{qin2022elleefficientlifelongpretrainingfor,\n    title = {ELLE: Efficient Lifelong Pre-training for Emerging Data},\n    author = {Yujia Qin and Jiajie Zhang and Yankai Lin and Zhiyuan Liu and Peng Li and Maosong Sun and Jie Zhou},\n    year = {2022},\n    eprint = {2203.06311},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.06311},\n}",
    "abstract": "Current pre-trained language models (PLM) are typically trained with static\ndata, ignoring that in real-world scenarios, streaming data of various sources\nmay continuously grow. This requires PLMs to integrate the information from all\nthe sources in a lifelong manner. Although this goal could be achieved by\nexhaustive pre-training on all the existing data, such a process is known to be\ncomputationally expensive. To this end, we propose ELLE, aiming at efficient\nlifelong pre-training for emerging data. Specifically, ELLE consists of (1)\nfunction preserved model expansion, which flexibly expands an existing PLM's\nwidth and depth to improve the efficiency of knowledge acquisition; and (2)\npre-trained domain prompts, which disentangle the versatile knowledge learned\nduring pre-training and stimulate the proper knowledge for downstream tasks. We\nexperiment ELLE with streaming data from 5 domains on BERT and GPT. The results\nshow the superiority of ELLE over various lifelong learning baselines in both\npre-training efficiency and downstream performances. The codes are publicly\navailable at https://github.com/thunlp/ELLE.",
    "num_pages": 22
}