{
    "uuid": "46202f6e-dfd7-5efd-816d-f285579141d0",
    "title": "Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yu Chen",
        "Lingfei Wu",
        "Mohammed J. Zaki"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.02188v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\46202f6e-dfd7-5efd-816d-f285579141d0.pdf",
    "bibtex": "@misc{chen2019bidirectionalattentivememorynetworksfor,\n    title = {Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases},\n    author = {Yu Chen and Lingfei Wu and Mohammed J. Zaki},\n    year = {2019},\n    eprint = {1903.02188},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1903.02188},\n}",
    "abstract": "When answering natural language questions over knowledge bases (KBs),\ndifferent question components and KB aspects play different roles. However,\nmost existing embedding-based methods for knowledge base question answering\n(KBQA) ignore the subtle inter-relationships between the question and the KB\n(e.g., entity types, relation paths and context). In this work, we propose to\ndirectly model the two-way flow of interactions between the questions and the\nKB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring\nno external resources and only very few hand-crafted features, on the\nWebQuestions benchmark, our method significantly outperforms existing\ninformation-retrieval based methods, and remains competitive with\n(hand-crafted) semantic parsing based methods. Also, since we use attention\nmechanisms, our method offers better interpretability compared to other\nbaselines.",
    "num_pages": 11
}