{
    "uuid": "5c7fb1dc-efe9-585f-a52c-656bed6b1c72",
    "title": "Sparse Universal Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shawn Tan",
        "Yikang Shen",
        "Zhenfang Chen",
        "Aaron Courville",
        "Chuang Gan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.07096v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5c7fb1dc-efe9-585f-a52c-656bed6b1c72.pdf",
    "bibtex": "@misc{tan2023sparseuniversaltransformer,\n    title = {Sparse Universal Transformer},\n    author = {Shawn Tan and Yikang Shen and Zhenfang Chen and Aaron Courville and Chuang Gan},\n    year = {2023},\n    eprint = {2310.07096},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.07096},\n}",
    "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares\nparameters across its layers. Empirical evidence shows that UTs have better\ncompositional generalization than Vanilla Transformers (VTs) in formal language\ntasks. The parameter-sharing also affords it better parameter efficiency than\nVTs. Despite its many advantages, scaling UT parameters is much more compute\nand memory intensive than scaling up a VT. This paper proposes the Sparse\nUniversal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE)\nand a new stick-breaking-based dynamic halting mechanism to reduce UT's\ncomputation complexity while retaining its parameter efficiency and\ngeneralization ability. Experiments show that SUT achieves the same performance\nas strong baseline models while only using half computation and parameters on\nWMT'14 and strong generalization results on formal language tasks (Logical\ninference and CFQ). The new halting mechanism also enables around 50\\%\nreduction in computation during inference with very little performance decrease\non formal language tasks.",
    "num_pages": 13
}