{
    "uuid": "01d0de9d-f910-5674-841b-39ed4688eed7",
    "title": "Exploring and Distilling Cross-Modal Information for Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Fenglin Liu",
        "Xuancheng Ren",
        "Yuanxin Liu",
        "Kai Lei",
        "Xu Sun"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.12585v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\01d0de9d-f910-5674-841b-39ed4688eed7.pdf",
    "bibtex": "@misc{liu2020exploringanddistillingcrossmodalinformation,\n    title = {Exploring and Distilling Cross-Modal Information for Image Captioning},\n    author = {Fenglin Liu and Xuancheng Ren and Yuanxin Liu and Kai Lei and Xu Sun},\n    year = {2020},\n    eprint = {2002.12585},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2002.12585},\n}",
    "abstract": "Recently, attention-based encoder-decoder models have been used extensively\nin image captioning. Yet there is still great difficulty for the current\nmethods to achieve deep image understanding. In this work, we argue that such\nunderstanding requires visual attention to correlated image regions and\nsemantic attention to coherent attributes of interest. Based on the\nTransformer, to perform effective attention, we explore image captioning from a\ncross-modal perspective and propose the Global-and-Local Information\nExploring-and-Distilling approach that explores and distills the source\ninformation in vision and language. It globally provides the aspect vector, a\nspatial and relational representation of images based on caption contexts,\nthrough the extraction of salient region groupings and attribute collocations,\nand locally extracts the fine-grained regions and attributes in reference to\nthe aspect vector for word selection. Our Transformer-based model achieves a\nCIDEr score of 129.3 in offline COCO evaluation on the COCO testing set with\nremarkable efficiency in terms of accuracy, speed, and parameter budget.",
    "num_pages": 7
}