{
    "uuid": "04239ec0-eb90-5280-a424-4bf097a3b85b",
    "title": "Habitat: A Platform for Embodied AI Research",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Manolis Savva",
        "Abhishek Kadian",
        "Oleksandr Maksymets",
        "Yili Zhao",
        "Erik Wijmans",
        "Bhavana Jain",
        "Julian Straub",
        "Jia Liu",
        "Vladlen Koltun",
        "Jitendra Malik",
        "Devi Parikh",
        "Dhruv Batra"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.01201v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\04239ec0-eb90-5280-a424-4bf097a3b85b.pdf",
    "bibtex": "@misc{savva2019habitataplatformforembodied,\n    title = {Habitat: A Platform for Embodied AI Research},\n    author = {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},\n    year = {2019},\n    eprint = {1904.01201},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1904.01201},\n}",
    "abstract": "We present Habitat, a platform for research in embodied artificial\nintelligence (AI). Habitat enables training embodied agents (virtual robots) in\nhighly efficient photorealistic 3D simulation. Specifically, Habitat consists\nof: (i) Habitat-Sim: a flexible, high-performance 3D simulator with\nconfigurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is\nfast -- when rendering a scene from Matterport3D, it achieves several thousand\nframes per second (fps) running single-threaded, and can reach over 10,000 fps\nmulti-process on a single GPU. (ii) Habitat-API: a modular high-level library\nfor end-to-end development of embodied AI algorithms -- defining tasks (e.g.,\nnavigation, instruction following, question answering), configuring, training,\nand benchmarking embodied agents.\n  These large-scale engineering contributions enable us to answer scientific\nquestions requiring experiments that were till now impracticable or 'merely'\nimpractical. Specifically, in the context of point-goal navigation: (1) we\nrevisit the comparison between learning and SLAM approaches from two recent\nworks and find evidence for the opposite conclusion -- that learning\noutperforms SLAM if scaled to an order of magnitude more experience than\nprevious investigations, and (2) we conduct the first cross-dataset\ngeneralization experiments {train, test} x {Matterport3D, Gibson} for multiple\nsensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors\ngeneralize across datasets. We hope that our open-source platform and these\nfindings will advance research in embodied AI.",
    "num_pages": 17
}