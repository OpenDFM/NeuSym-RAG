{
    "uuid": "10eb6999-ec5e-5eac-91f7-359a777a828e",
    "title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Dipendra Misra",
        "Andrew Bennett",
        "Valts Blukis",
        "Eyvind Niklasson",
        "Max Shatkhin",
        "Yoav Artzi"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.00786v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\10eb6999-ec5e-5eac-91f7-359a777a828e.pdf",
    "bibtex": "@misc{misra2019mappinginstructionstoactionsin,\n    title = {Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction},\n    author = {Dipendra Misra and Andrew Bennett and Valts Blukis and Eyvind Niklasson and Max Shatkhin and Yoav Artzi},\n    year = {2019},\n    eprint = {1809.00786},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1809.00786},\n}",
    "abstract": "We propose to decompose instruction execution to goal prediction and action\ngeneration. We design a model that maps raw visual observations to goals using\nLINGUNET, a language-conditioned image generation network, and then generates\nthe actions required to complete them. Our model is trained from demonstration\nonly without external resources. To evaluate our approach, we introduce two\nbenchmarks for instruction following: LANI, a navigation task; and CHAI, where\nan agent executes household instructions. Our evaluation demonstrates the\nadvantages of our model decomposition, and illustrates the challenges posed by\nour new benchmarks.",
    "num_pages": 15
}