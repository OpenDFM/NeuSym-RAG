{
    "uuid": "3b8af772-96a1-578e-a3de-272d2cb9ad7f",
    "title": "STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Qingkai Fang",
        "Rong Ye",
        "Lei Li",
        "Yang Feng",
        "Mingxuan Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.10426v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\3b8af772-96a1-578e-a3de-272d2cb9ad7f.pdf",
    "bibtex": "@misc{fang2022stemmselflearningwithspeechtextmanifold,\n    title = {STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation},\n    author = {Qingkai Fang and Rong Ye and Lei Li and Yang Feng and Mingxuan Wang},\n    year = {2022},\n    eprint = {2203.10426},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.10426},\n}",
    "abstract": "How to learn a better speech representation for end-to-end speech-to-text\ntranslation (ST) with limited labeled data? Existing techniques often attempt\nto transfer powerful machine translation (MT) capabilities to ST, but neglect\nthe representation discrepancy across modalities. In this paper, we propose the\nSpeech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.\nSpecifically, we mix up the representation sequences of different modalities,\nand take both unimodal speech sequences and multimodal mixed sequences as input\nto the translation model in parallel, and regularize their output predictions\nwith a self-learning framework. Experiments on MuST-C speech translation\nbenchmark and further analysis show that our method effectively alleviates the\ncross-modal representation discrepancy, and achieves significant improvements\nover a strong baseline on eight translation directions.",
    "num_pages": 13
}