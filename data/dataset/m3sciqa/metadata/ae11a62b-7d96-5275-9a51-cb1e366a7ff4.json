{
    "uuid": "ae11a62b-7d96-5275-9a51-cb1e366a7ff4",
    "title": "Prototype selection for interpretable classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2012,
    "authors": [
        "Jacob Bien",
        "Robert Tibshirani"
    ],
    "pdf_url": "http://arxiv.org/pdf/1202.5933v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2012\\ae11a62b-7d96-5275-9a51-cb1e366a7ff4.pdf",
    "bibtex": "@misc{bien2012prototypeselectionforinterpretableclassification,\n    title = {Prototype selection for interpretable classification},\n    author = {Jacob Bien and Robert Tibshirani},\n    year = {2012},\n    eprint = {1202.5933},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.AP},\n    url = {http://arxiv.org/abs/1202.5933},\n}",
    "abstract": "Prototype methods seek a minimal subset of samples that can serve as a\ndistillation or condensed view of a data set. As the size of modern data sets\ngrows, being able to present a domain specialist with a short list of\n\"representative\" samples chosen from the data set is of increasing\ninterpretative value. While much recent statistical research has been focused\non producing sparse-in-the-variables methods, this paper aims at achieving\nsparsity in the samples. We discuss a method for selecting prototypes in the\nclassification setting (in which the samples fall into known discrete\ncategories). Our method of focus is derived from three basic properties that we\nbelieve a good prototype set should satisfy. This intuition is translated into\na set cover optimization problem, which we solve approximately using standard\napproaches. While prototype selection is usually viewed as purely a means\ntoward building an efficient classifier, in this paper we emphasize the\ninherent value of having a set of prototypical elements. That said, by using\nthe nearest-neighbor rule on the set of prototypes, we can of course discuss\nour method as a classifier as well.",
    "num_pages": 23
}