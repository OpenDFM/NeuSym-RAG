{
    "uuid": "6afec1dc-863f-5f4d-b07b-42568d79c0a2",
    "title": "Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Laila Rasmy",
        "Yang Xiang",
        "Ziqian Xie",
        "Cui Tao",
        "Degui Zhi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.12833v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\6afec1dc-863f-5f4d-b07b-42568d79c0a2.pdf",
    "bibtex": "@misc{rasmy2020medbertpretrainedcontextualizedembeddingson,\n    title = {Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction},\n    author = {Laila Rasmy and Yang Xiang and Ziqian Xie and Cui Tao and Degui Zhi},\n    year = {2020},\n    eprint = {2005.12833},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.12833},\n}",
    "abstract": "Deep learning (DL) based predictive models from electronic health records\n(EHR) deliver impressive performance in many clinical tasks. Large training\ncohorts, however, are often required to achieve high accuracy, hindering the\nadoption of DL-based models in scenarios with limited training data size.\nRecently, bidirectional encoder representations from transformers (BERT) and\nrelated models have achieved tremendous successes in the natural language\nprocessing domain. The pre-training of BERT on a very large training corpus\ngenerates contextualized embeddings that can boost the performance of models\ntrained on smaller datasets. We propose Med-BERT, which adapts the BERT\nframework for pre-training contextualized embedding models on structured\ndiagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments\nare conducted on two disease-prediction tasks: (1) prediction of heart failure\nin patients with diabetes and (2) prediction of pancreatic cancer from two\nclinical databases. Med-BERT substantially improves prediction accuracy,\nboosting the area under receiver operating characteristics curve (AUC) by\n2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the\nperformance of tasks with very small fine-tuning training sets (300-500\nsamples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times\nlarger training set. We believe that Med-BERT will benefit disease-prediction\nstudies with small local training datasets, reduce data collection expenses,\nand accelerate the pace of artificial intelligence aided healthcare.",
    "num_pages": 23
}