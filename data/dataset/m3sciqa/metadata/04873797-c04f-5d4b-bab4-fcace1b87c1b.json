{
    "uuid": "04873797-c04f-5d4b-bab4-fcace1b87c1b",
    "title": "CogView: Mastering Text-to-Image Generation via Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Ming Ding",
        "Zhuoyi Yang",
        "Wenyi Hong",
        "Wendi Zheng",
        "Chang Zhou",
        "Da Yin",
        "Junyang Lin",
        "Xu Zou",
        "Zhou Shao",
        "Hongxia Yang",
        "Jie Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.13290v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\04873797-c04f-5d4b-bab4-fcace1b87c1b.pdf",
    "bibtex": "@misc{ding2021cogviewmasteringtexttoimagegenerationvia,\n    title = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year = {2021},\n    eprint = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2105.13290},\n}",
    "abstract": "Text-to-Image generation in the general domain has long been an open problem,\nwhich requires both a powerful generative model and cross-modal understanding.\nWe propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to\nadvance this problem. We also demonstrate the finetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN\nlosses. CogView achieves the state-of-the-art FID on the blurred MS COCO\ndataset, outperforming previous GAN-based models and a recent similar work\nDALL-E.",
    "num_pages": 21
}