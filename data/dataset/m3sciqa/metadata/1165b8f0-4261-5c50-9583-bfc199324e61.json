{
    "uuid": "1165b8f0-4261-5c50-9583-bfc199324e61",
    "title": "LIMA: Less Is More for Alignment",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chunting Zhou",
        "Pengfei Liu",
        "Puxin Xu",
        "Srini Iyer",
        "Jiao Sun",
        "Yuning Mao",
        "Xuezhe Ma",
        "Avia Efrat",
        "Ping Yu",
        "Lili Yu",
        "Susan Zhang",
        "Gargi Ghosh",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Omer Levy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.11206v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\1165b8f0-4261-5c50-9583-bfc199324e61.pdf",
    "bibtex": "@misc{zhou2023limalessismorefor,\n    title = {LIMA: Less Is More for Alignment},\n    author = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},\n    year = {2023},\n    eprint = {2305.11206},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.11206},\n}",
    "abstract": "Large language models are trained in two stages: (1) unsupervised pretraining\nfrom raw text, to learn general-purpose representations, and (2) large scale\ninstruction tuning and reinforcement learning, to better align to end tasks and\nuser preferences. We measure the relative importance of these two stages by\ntraining LIMA, a 65B parameter LLaMa language model fine-tuned with the\nstandard supervised loss on only 1,000 carefully curated prompts and responses,\nwithout any reinforcement learning or human preference modeling. LIMA\ndemonstrates remarkably strong performance, learning to follow specific\nresponse formats from only a handful of examples in the training data,\nincluding complex queries that range from planning trip itineraries to\nspeculating about alternate history. Moreover, the model tends to generalize\nwell to unseen tasks that did not appear in the training data. In a controlled\nhuman study, responses from LIMA are either equivalent or strictly preferred to\nGPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard\nand 65% versus DaVinci003, which was trained with human feedback. Taken\ntogether, these results strongly suggest that almost all knowledge in large\nlanguage models is learned during pretraining, and only limited instruction\ntuning data is necessary to teach models to produce high quality output.",
    "num_pages": 15
}