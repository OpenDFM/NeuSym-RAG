{
    "uuid": "3ea1a5e9-0057-54a5-b359-7d99fc882842",
    "title": "Automatic Model Selection with Large Language Models for Reasoning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "James Xu Zhao",
        "Yuxi Xie",
        "Kenji Kawaguchi",
        "Junxian He",
        "Michael Qizhe Xie"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.14333v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\3ea1a5e9-0057-54a5-b359-7d99fc882842.pdf",
    "bibtex": "@misc{zhao2023automaticmodelselectionwithlarge,\n    title = {Automatic Model Selection with Large Language Models for Reasoning},\n    author = {James Xu Zhao and Yuxi Xie and Kenji Kawaguchi and Junxian He and Michael Qizhe Xie},\n    year = {2023},\n    eprint = {2305.14333},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.14333},\n}",
    "abstract": "Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two\ndistinct reasoning methods, each with its own strengths. CoT employs natural\nlanguage, offering flexibility and interpretability, while PAL utilizes\nprogramming language, yielding more structured and rigorous logic. We introduce\na model selection method to combine the best of both worlds by employing a\nlarge language model (LLM) to dynamically select between them. Our theoretical\nanalysis underscores the feasibility of this method, which is further\ncorroborated by empirical results. Our proposed method demonstrates significant\nperformance improvements across eight reasoning datasets with Codex, ChatGPT,\nand GPT-4. Additionally, our method is complementary to self-consistency; when\nintegrated, it can further enhance performance while significantly reducing\ncomputation costs. Moreover, we achieve new state-of-the-art results on GSM8K\nand SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and\nprompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning",
    "num_pages": 26
}