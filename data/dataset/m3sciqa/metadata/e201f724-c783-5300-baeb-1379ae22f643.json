{
    "uuid": "e201f724-c783-5300-baeb-1379ae22f643",
    "title": "Towards Understanding and Mitigating Social Biases in Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Paul Pu Liang",
        "Chiyu Wu",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.13219v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\e201f724-c783-5300-baeb-1379ae22f643.pdf",
    "bibtex": "@misc{liang2021towardsunderstandingandmitigatingsocial,\n    title = {Towards Understanding and Mitigating Social Biases in Language Models},\n    author = {Paul Pu Liang and Chiyu Wu and Louis-Philippe Morency and Ruslan Salakhutdinov},\n    year = {2021},\n    eprint = {2106.13219},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.13219},\n}",
    "abstract": "As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.",
    "num_pages": 22
}