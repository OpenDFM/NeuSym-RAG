{
    "uuid": "b85188c4-5fd6-5ce7-91e4-78dc80eaf986",
    "title": "Online and Linear-Time Attention by Enforcing Monotonic Alignments",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Colin Raffel",
        "Minh-Thang Luong",
        "Peter J. Liu",
        "Ron J. Weiss",
        "Douglas Eck"
    ],
    "pdf_url": "http://arxiv.org/pdf/1704.00784v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\b85188c4-5fd6-5ce7-91e4-78dc80eaf986.pdf",
    "bibtex": "@misc{raffel2017onlineandlineartimeattentionby,\n    title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},\n    author = {Colin Raffel and Minh-Thang Luong and Peter J. Liu and Ron J. Weiss and Douglas Eck},\n    year = {2017},\n    eprint = {1704.00784},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1704.00784},\n}",
    "abstract": "Recurrent neural network models with an attention mechanism have proven to be\nextremely effective on a wide variety of sequence-to-sequence problems.\nHowever, the fact that soft attention mechanisms perform a pass over the entire\ninput sequence when producing each element in the output sequence precludes\ntheir use in online settings and results in a quadratic time complexity. Based\non the insight that the alignment between input and output sequence elements is\nmonotonic in many problems of interest, we propose an end-to-end differentiable\nmethod for learning monotonic alignments which, at test time, enables computing\nattention online and in linear time. We validate our approach on sentence\nsummarization, machine translation, and online speech recognition problems and\nachieve results competitive with existing sequence-to-sequence models.",
    "num_pages": 19
}