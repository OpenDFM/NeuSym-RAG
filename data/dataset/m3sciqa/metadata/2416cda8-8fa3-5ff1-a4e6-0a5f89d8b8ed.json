{
    "uuid": "2416cda8-8fa3-5ff1-a4e6-0a5f89d8b8ed",
    "title": "Variational Neural Machine Translation with Normalizing Flows",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Hendra Setiawan",
        "Matthias Sperber",
        "Udhay Nallasamy",
        "Matthias Paulik"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.13978v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\2416cda8-8fa3-5ff1-a4e6-0a5f89d8b8ed.pdf",
    "bibtex": "@misc{setiawan2020variationalneuralmachinetranslationwith,\n    title = {Variational Neural Machine Translation with Normalizing Flows},\n    author = {Hendra Setiawan and Matthias Sperber and Udhay Nallasamy and Matthias Paulik},\n    year = {2020},\n    eprint = {2005.13978},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.13978},\n}",
    "abstract": "Variational Neural Machine Translation (VNMT) is an attractive framework for\nmodeling the generation of target translations, conditioned not only on the\nsource sentence but also on some latent random variables. The latent variable\nmodeling may introduce useful statistical dependencies that can improve\ntranslation accuracy. Unfortunately, learning informative latent variables is\nnon-trivial, as the latent space can be prohibitively large, and the latent\ncodes are prone to be ignored by many translation models at training time.\nPrevious works impose strong assumptions on the distribution of the latent code\nand limit the choice of the NMT architecture. In this paper, we propose to\napply the VNMT framework to the state-of-the-art Transformer and introduce a\nmore flexible approximate posterior based on normalizing flows. We demonstrate\nthe efficacy of our proposal under both in-domain and out-of-domain conditions,\nsignificantly outperforming strong baselines.",
    "num_pages": 8
}