{
    "uuid": "553f9789-8b5c-5e02-bf13-5458dc6c31ef",
    "title": "Editable Neural Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Anton Sinitsin",
        "Vsevolod Plokhotnyuk",
        "Dmitriy Pyrkin",
        "Sergei Popov",
        "Artem Babenko"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.00345v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\553f9789-8b5c-5e02-bf13-5458dc6c31ef.pdf",
    "bibtex": "@misc{sinitsin2020editableneuralnetworks,\n    title = {Editable Neural Networks},\n    author = {Anton Sinitsin and Vsevolod Plokhotnyuk and Dmitriy Pyrkin and Sergei Popov and Artem Babenko},\n    year = {2020},\n    eprint = {2004.00345},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2004.00345},\n}",
    "abstract": "These days deep neural networks are ubiquitously used in a wide range of\ntasks, from image classification and machine translation to face identification\nand self-driving cars. In many applications, a single model error can lead to\ndevastating financial, reputational and even life-threatening consequences.\nTherefore, it is crucially important to correct model mistakes quickly as they\nappear. In this work, we investigate the problem of neural network editing $-$\nhow one can efficiently patch a mistake of the model on a particular sample,\nwithout influencing the model behavior on other samples. Namely, we propose\nEditable Training, a model-agnostic training technique that encourages fast\nediting of the trained model. We empirically demonstrate the effectiveness of\nthis method on large-scale image classification and machine translation tasks.",
    "num_pages": 12
}