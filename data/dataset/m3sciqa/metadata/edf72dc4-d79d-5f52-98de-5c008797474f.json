{
    "uuid": "edf72dc4-d79d-5f52-98de-5c008797474f",
    "title": "Distilling Knowledge from Reader to Retriever for Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Gautier Izacard",
        "Edouard Grave"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04584v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\edf72dc4-d79d-5f52-98de-5c008797474f.pdf",
    "bibtex": "@misc{izacard2022distillingknowledgefromreaderto,\n    title = {Distilling Knowledge from Reader to Retriever for Question Answering},\n    author = {Gautier Izacard and Edouard Grave},\n    year = {2022},\n    eprint = {2012.04584},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.04584},\n}",
    "abstract": "The task of information retrieval is an important component of many natural\nlanguage processing systems, such as open domain question answering. While\ntraditional methods were based on hand-crafted features, continuous\nrepresentations based on neural networks recently obtained competitive results.\nA challenge of using such methods is to obtain supervised data to train the\nretriever model, corresponding to pairs of query and support documents. In this\npaper, we propose a technique to learn retriever models for downstream tasks,\ninspired by knowledge distillation, and which does not require annotated pairs\nof query and documents. Our approach leverages attention scores of a reader\nmodel, used to solve the task based on retrieved documents, to obtain synthetic\nlabels for the retriever. We evaluate our method on question answering,\nobtaining state-of-the-art results.",
    "num_pages": 13
}