{
    "uuid": "d8d3751a-674d-5ca0-90ef-f16fff2a3e15",
    "title": "Condenser: a Pre-training Architecture for Dense Retrieval",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Luyu Gao",
        "Jamie Callan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08253v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\d8d3751a-674d-5ca0-90ef-f16fff2a3e15.pdf",
    "bibtex": "@misc{gao2021condenserapretrainingarchitecturefor,\n    title = {Condenser: a Pre-training Architecture for Dense Retrieval},\n    author = {Luyu Gao and Jamie Callan},\n    year = {2021},\n    eprint = {2104.08253},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08253},\n}",
    "abstract": "Pre-trained Transformer language models (LM) have become go-to text\nrepresentation encoders. Prior research fine-tunes deep LMs to encode text\nsequences such as sentences and passages into single dense vector\nrepresentations for efficient text comparison and retrieval. However, dense\nencoders require a lot of data and sophisticated techniques to effectively\ntrain and suffer in low data situations. This paper finds a key reason is that\nstandard LMs' internal attention structure is not ready-to-use for dense\nencoders, which needs to aggregate text information into the dense\nrepresentation. We propose to pre-train towards dense encoder with a novel\nTransformer architecture, Condenser, where LM prediction CONditions on DENSE\nRepresentation. Our experiments show Condenser improves over standard LM by\nlarge margins on various text retrieval and similarity tasks.",
    "num_pages": 13
}