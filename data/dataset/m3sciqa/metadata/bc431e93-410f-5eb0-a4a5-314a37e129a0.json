{
    "uuid": "bc431e93-410f-5eb0-a4a5-314a37e129a0",
    "title": "Q-HyViT: Post-Training Quantization of Hybrid Vision Transformers with Bridge Block Reconstruction for IoT Systems",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Jemin Lee",
        "Yongin Kwon",
        "Sihyeong Park",
        "Misun Yu",
        "Jeman Park",
        "Hwanjun Song"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.12557v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\bc431e93-410f-5eb0-a4a5-314a37e129a0.pdf",
    "bibtex": "@misc{lee2024qhyvitposttrainingquantizationofhybrid,\n    title = {Q-HyViT: Post-Training Quantization of Hybrid Vision Transformers with Bridge Block Reconstruction for IoT Systems},\n    author = {Jemin Lee and Yongin Kwon and Sihyeong Park and Misun Yu and Jeman Park and Hwanjun Song},\n    year = {2024},\n    eprint = {2303.12557},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2303.12557},\n}",
    "abstract": "Recently, vision transformers (ViTs) have superseded convolutional neural\nnetworks in numerous applications, including classification, detection, and\nsegmentation. However, the high computational requirements of ViTs hinder their\nwidespread implementation. To address this issue, researchers have proposed\nefficient hybrid transformer architectures that combine convolutional and\ntransformer layers with optimized attention computation of linear complexity.\nAdditionally, post-training quantization has been proposed as a means of\nmitigating computational demands. For mobile devices, achieving optimal\nacceleration for ViTs necessitates the strategic integration of quantization\ntechniques and efficient hybrid transformer structures. However, no prior\ninvestigation has applied quantization to efficient hybrid transformers. In\nthis paper, we discover that applying existing post-training quantization (PTQ)\nmethods for ViTs to efficient hybrid transformers leads to a drastic accuracy\ndrop, attributed to the four following challenges: (i) highly dynamic ranges,\n(ii) zero-point overflow, (iii) diverse normalization, and (iv) limited model\nparameters ($<$5M). To overcome these challenges, we propose a new\npost-training quantization method, which is the first to quantize efficient\nhybrid ViTs (MobileViTv1, MobileViTv2, Mobile-Former, EfficientFormerV1,\nEfficientFormerV2). We achieve a significant improvement of 17.73% for 8-bit\nand 29.75% for 6-bit on average, respectively, compared with existing PTQ\nmethods (EasyQuant, FQ-ViT, PTQ4ViT, and RepQ-ViT)}. We plan to release our\ncode at https://gitlab.com/ones-ai/q-hyvit.",
    "num_pages": 14
}