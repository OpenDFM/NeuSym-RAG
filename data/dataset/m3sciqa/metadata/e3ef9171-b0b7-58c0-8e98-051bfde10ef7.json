{
    "uuid": "e3ef9171-b0b7-58c0-8e98-051bfde10ef7",
    "title": "End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Armin Oliya",
        "Amir Saffari",
        "Priyanka Sen",
        "Tom Ayoola"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05817v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\e3ef9171-b0b7-58c0-8e98-051bfde10ef7.pdf",
    "bibtex": "@misc{oliya2021endtoendentityresolutionandquestion,\n    title = {End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs},\n    author = {Armin Oliya and Amir Saffari and Priyanka Sen and Tom Ayoola},\n    year = {2021},\n    eprint = {2109.05817},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.05817},\n}",
    "abstract": "Recently, end-to-end (E2E) trained models for question answering over\nknowledge graphs (KGQA) have delivered promising results using only a weakly\nsupervised dataset. However, these models are trained and evaluated in a\nsetting where hand-annotated question entities are supplied to the model,\nleaving the important and non-trivial task of entity resolution (ER) outside\nthe scope of E2E learning. In this work, we extend the boundaries of E2E\nlearning for KGQA to include the training of an ER component. Our model only\nneeds the question text and the answer entities to train, and delivers a\nstand-alone QA model that does not require an additional ER component to be\nsupplied during runtime. Our approach is fully differentiable, thanks to its\nreliance on a recent method for building differentiable KGs (Cohen et al.,\n2020). We evaluate our E2E trained model on two public datasets and show that\nit comes close to baseline models that use hand-annotated entities.",
    "num_pages": 8
}