{
    "uuid": "d75af728-525e-5784-a8c9-abc5cc9d3efb",
    "title": "In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Sang Michael Xie",
        "Ananya Kumar",
        "Robbie Jones",
        "Fereshte Khani",
        "Tengyu Ma",
        "Percy Liang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04550v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\d75af728-525e-5784-a8c9-abc5cc9d3efb.pdf",
    "bibtex": "@misc{xie2021innoutpretrainingandselftrainingusing,\n    title = {In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness},\n    author = {Sang Michael Xie and Ananya Kumar and Robbie Jones and Fereshte Khani and Tengyu Ma and Percy Liang},\n    year = {2021},\n    eprint = {2012.04550},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2012.04550},\n}",
    "abstract": "Consider a prediction setting with few in-distribution labeled examples and\nmany unlabeled examples both in- and out-of-distribution (OOD). The goal is to\nlearn a model which performs well both in-distribution and OOD. In these\nsettings, auxiliary information is often cheaply available for every input. How\nshould we best leverage this auxiliary information for the prediction task?\nEmpirically across three image and time-series datasets, and theoretically in a\nmulti-task linear regression setting, we show that (i) using auxiliary\ninformation as input features improves in-distribution error but can hurt OOD\nerror; but (ii) using auxiliary information as outputs of auxiliary\npre-training tasks improves OOD error. To get the best of both worlds, we\nintroduce In-N-Out, which first trains a model with auxiliary inputs and uses\nit to pseudolabel all the in-distribution inputs, then pre-trains a model on\nOOD auxiliary outputs and fine-tunes this model with the pseudolabels\n(self-training). We show both theoretically and empirically that In-N-Out\noutperforms auxiliary inputs or outputs alone on both in-distribution and OOD\nerror.",
    "num_pages": 27
}