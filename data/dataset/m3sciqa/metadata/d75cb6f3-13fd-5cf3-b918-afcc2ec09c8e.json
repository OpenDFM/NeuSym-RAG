{
    "uuid": "d75cb6f3-13fd-5cf3-b918-afcc2ec09c8e",
    "title": "Learning Dialog Policies from Weak Demonstrations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Gabriel Gordon-Hall",
        "Philip John Gorinski",
        "Shay B. Cohen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.11054v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\d75cb6f3-13fd-5cf3-b918-afcc2ec09c8e.pdf",
    "bibtex": "@misc{gordonhall2020learningdialogpoliciesfromweak,\n    title = {Learning Dialog Policies from Weak Demonstrations},\n    author = {Gabriel Gordon-Hall and Philip John Gorinski and Shay B. Cohen},\n    year = {2020},\n    eprint = {2004.11054},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.11054},\n}",
    "abstract": "Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data.",
    "num_pages": 12
}