{
    "uuid": "cd71e042-6d38-56be-b014-7b9e02e8733e",
    "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Peter Anderson",
        "Xiaodong He",
        "Chris Buehler",
        "Damien Teney",
        "Mark Johnson",
        "Stephen Gould",
        "Lei Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1707.07998v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\cd71e042-6d38-56be-b014-7b9e02e8733e.pdf",
    "bibtex": "@misc{anderson2018bottomupandtopdownattentionfor,\n    title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},\n    author = {Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},\n    year = {2018},\n    eprint = {1707.07998},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1707.07998},\n}",
    "abstract": "Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of\n117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of\nthe method, applying the same approach to VQA we obtain first place in the 2017\nVQA Challenge.",
    "num_pages": 15
}