{
    "uuid": "b01da5d8-2db0-5d8c-b2f3-37ed254729fd",
    "title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yun He",
        "Huaixiu Steven Zheng",
        "Yi Tay",
        "Jai Gupta",
        "Yu Du",
        "Vamsi Aribandi",
        "Zhe Zhao",
        "YaGuang Li",
        "Zhao Chen",
        "Donald Metzler",
        "Heng-Tze Cheng",
        "Ed H. Chi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.00759v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\b01da5d8-2db0-5d8c-b2f3-37ed254729fd.pdf",
    "bibtex": "@misc{he2022hyperpromptpromptbasedtaskconditioningoftransformers,\n    title = {HyperPrompt: Prompt-based Task-Conditioning of Transformers},\n    author = {Yun He and Huaixiu Steven Zheng and Yi Tay and Jai Gupta and Yu Du and Vamsi Aribandi and Zhe Zhao and YaGuang Li and Zhao Chen and Donald Metzler and Heng-Tze Cheng and Ed H. Chi},\n    year = {2022},\n    eprint = {2203.00759},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.00759},\n}",
    "abstract": "Prompt-Tuning is a new paradigm for finetuning pre-trained language models in\na parameter-efficient way. Here, we explore the use of HyperNetworks to\ngenerate hyper-prompts: we propose HyperPrompt, a novel architecture for\nprompt-based task-conditioning of self-attention in Transformers. The\nhyper-prompts are end-to-end learnable via generation by a HyperNetwork.\nHyperPrompt allows the network to learn task-specific feature maps where the\nhyper-prompts serve as task global memories for the queries to attend to, at\nthe same time enabling flexible information sharing among tasks. We show that\nHyperPrompt is competitive against strong multi-task learning baselines with as\nfew as $0.14\\%$ of additional task-conditioning parameters, achieving great\nparameter and computational efficiency. Through extensive empirical\nexperiments, we demonstrate that HyperPrompt can achieve superior performances\nover strong T5 multi-task learning baselines and parameter-efficient adapter\nvariants including Prompt-Tuning and HyperFormer++ on Natural Language\nUnderstanding benchmarks of GLUE and SuperGLUE across many model sizes.",
    "num_pages": 14
}