{
    "uuid": "4aee357f-d44d-5032-8e07-cd6e0fb7f0f8",
    "title": "Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Cunxiao Du",
        "Zhaopeng Tu",
        "Jing Jiang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.05093v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\4aee357f-d44d-5032-8e07-cd6e0fb7f0f8.pdf",
    "bibtex": "@misc{du2021orderagnosticcrossentropyfornonautoregressive,\n    title = {Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation},\n    author = {Cunxiao Du and Zhaopeng Tu and Jing Jiang},\n    year = {2021},\n    eprint = {2106.05093},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.05093},\n}",
    "abstract": "We propose a new training objective named order-agnostic cross entropy (OaXE)\nfor fully non-autoregressive translation (NAT) models. OaXE improves the\nstandard cross-entropy loss to ameliorate the effect of word reordering, which\nis a common source of the critical multimodality problem in NAT. Concretely,\nOaXE removes the penalty for word order errors, and computes the cross entropy\nloss based on the best possible alignment between model predictions and target\ntokens. Since the log loss is very sensitive to invalid references, we leverage\ncross entropy initialization and loss truncation to ensure the model focuses on\na good part of the search space. Extensive experiments on major WMT benchmarks\nshow that OaXE substantially improves translation performance, setting new\nstate of the art for fully NAT models. Further analyses show that OaXE\nalleviates the multimodality problem by reducing token repetitions and\nincreasing prediction confidence. Our code, data, and trained models are\navailable at https://github.com/tencent-ailab/ICML21_OAXE.",
    "num_pages": 12
}