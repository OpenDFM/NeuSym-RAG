{
    "uuid": "d27ee810-3a49-5970-b14a-c71604d54388",
    "title": "iCaRL: Incremental Classifier and Representation Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Sylvestre-Alvise Rebuffi",
        "Alexander Kolesnikov",
        "Georg Sperl",
        "Christoph H. Lampert"
    ],
    "pdf_url": "http://arxiv.org/pdf/1611.07725v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\d27ee810-3a49-5970-b14a-c71604d54388.pdf",
    "bibtex": "@misc{rebuffi2017icarlincrementalclassifierandrepresentation,\n    title = {iCaRL: Incremental Classifier and Representation Learning},\n    author = {Sylvestre-Alvise Rebuffi and Alexander Kolesnikov and Georg Sperl and Christoph H. Lampert},\n    year = {2017},\n    eprint = {1611.07725},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1611.07725},\n}",
    "abstract": "A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.",
    "num_pages": 15
}