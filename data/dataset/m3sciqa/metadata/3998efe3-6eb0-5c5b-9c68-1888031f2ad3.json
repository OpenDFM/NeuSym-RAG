{
    "uuid": "3998efe3-6eb0-5c5b-9c68-1888031f2ad3",
    "title": "Scaling Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Myle Ott",
        "Sergey Edunov",
        "David Grangier",
        "Michael Auli"
    ],
    "pdf_url": "http://arxiv.org/pdf/1806.00187v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\3998efe3-6eb0-5c5b-9c68-1888031f2ad3.pdf",
    "bibtex": "@misc{ott2018scalingneuralmachinetranslation,\n    title = {Scaling Neural Machine Translation},\n    author = {Myle Ott and Sergey Edunov and David Grangier and Michael Auli},\n    year = {2018},\n    eprint = {1806.00187},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1806.00187},\n}",
    "abstract": "Sequence to sequence learning models still require several days to reach\nstate of the art performance on large benchmark datasets using a single\nmachine. This paper shows that reduced precision and large batch training can\nspeedup training by nearly 5x on a single 8-GPU machine with careful tuning and\nimplementation. On WMT'14 English-German translation, we match the accuracy of\nVaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a\nnew state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We\nfurther improve these results to 29.8 BLEU by training on the much larger\nParacrawl dataset. On the WMT'14 English-French task, we obtain a\nstate-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
    "num_pages": 9
}