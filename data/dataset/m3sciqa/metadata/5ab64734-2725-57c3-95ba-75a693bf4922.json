{
    "uuid": "5ab64734-2725-57c3-95ba-75a693bf4922",
    "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Leonardo Ranaldi",
        "Elena Sofia Ruzzetti",
        "Davide Venditti",
        "Dario Onorati",
        "Fabio Massimo Zanzotto"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.13862v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5ab64734-2725-57c3-95ba-75a693bf4922.pdf",
    "bibtex": "@misc{ranaldi2023atriptowardsfairnessbias,\n    title = {A Trip Towards Fairness: Bias and De-Biasing in Large Language Models},\n    author = {Leonardo Ranaldi and Elena Sofia Ruzzetti and Davide Venditti and Dario Onorati and Fabio Massimo Zanzotto},\n    year = {2023},\n    eprint = {2305.13862},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.13862},\n}",
    "abstract": "Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training\nare emerging as the next big revolution in natural language processing and\nunderstanding. These CtB-LLMs are democratizing access to trainable Very\nLarge-Language Models (VLLMs) and, thus, may represent the building blocks of\nmany NLP systems solving downstream tasks. Hence, a little or a large bias in\nCtB-LLMs may cause huge harm. In this paper, we performed a large investigation\nof the bias of three families of CtB-LLMs, and we showed that debiasing\ntechniques are effective and usable. Indeed, according to current tests, the\nLLaMA and the OPT families have an important bias in gender, race, religion,\nand profession. In contrast to the analysis for other LLMs, we discovered that\nbias depends not on the number of parameters but on the perplexity. Finally,\nthe debiasing of OPT using LoRA reduces bias up to 4.12 points in the\nnormalized stereotype score.",
    "num_pages": 12
}