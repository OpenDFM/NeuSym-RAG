{
    "uuid": "d0b35b26-2b4c-5209-bb22-d8a44032dd05",
    "title": "From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xin Xie",
        "Ningyu Zhang",
        "Zhoubo Li",
        "Shumin Deng",
        "Hui Chen",
        "Feiyu Xiong",
        "Mosha Chen",
        "Huajun Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.02113v7",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\d0b35b26-2b4c-5209-bb22-d8a44032dd05.pdf",
    "bibtex": "@misc{xie2023fromdiscriminationtogenerationknowledge,\n    title = {From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer},\n    author = {Xin Xie and Ningyu Zhang and Zhoubo Li and Shumin Deng and Hui Chen and Feiyu Xiong and Mosha Chen and Huajun Chen},\n    year = {2023},\n    eprint = {2202.02113},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2202.02113},\n}",
    "abstract": "Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.",
    "num_pages": 4
}