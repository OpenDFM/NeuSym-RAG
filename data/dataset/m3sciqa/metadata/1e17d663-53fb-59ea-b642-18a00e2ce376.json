{
    "uuid": "1e17d663-53fb-59ea-b642-18a00e2ce376",
    "title": "A Hybrid Convolutional Variational Autoencoder for Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Stanislau Semeniuta",
        "Aliaksei Severyn",
        "Erhardt Barth"
    ],
    "pdf_url": "http://arxiv.org/pdf/1702.02390v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\1e17d663-53fb-59ea-b642-18a00e2ce376.pdf",
    "bibtex": "@misc{semeniuta2017ahybridconvolutionalvariationalautoencoder,\n    title = {A Hybrid Convolutional Variational Autoencoder for Text Generation},\n    author = {Stanislau Semeniuta and Aliaksei Severyn and Erhardt Barth},\n    year = {2017},\n    eprint = {1702.02390},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1702.02390},\n}",
    "abstract": "In this paper we explore the effect of architectural choices on learning a\nVariational Autoencoder (VAE) for text generation. In contrast to the\npreviously introduced VAE model for text where both the encoder and decoder are\nRNNs, we propose a novel hybrid architecture that blends fully feed-forward\nconvolutional and deconvolutional components with a recurrent language model.\nOur architecture exhibits several attractive properties such as faster run time\nand convergence, ability to better handle long sequences and, more importantly,\nit helps to avoid some of the major difficulties posed by training VAE models\non textual data.",
    "num_pages": 12
}