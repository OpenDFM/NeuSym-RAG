{
    "uuid": "7aa9fab2-572b-5e8c-a92a-7b09e4f4c306",
    "title": "Injecting Semantic Concepts into End-to-End Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Zhiyuan Fang",
        "Jianfeng Wang",
        "Xiaowei Hu",
        "Lin Liang",
        "Zhe Gan",
        "Lijuan Wang",
        "Yezhou Yang",
        "Zicheng Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.05230v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\7aa9fab2-572b-5e8c-a92a-7b09e4f4c306.pdf",
    "bibtex": "@misc{fang2022injectingsemanticconceptsintoendtoend,\n    title = {Injecting Semantic Concepts into End-to-End Image Captioning},\n    author = {Zhiyuan Fang and Jianfeng Wang and Xiaowei Hu and Lin Liang and Zhe Gan and Lijuan Wang and Yezhou Yang and Zicheng Liu},\n    year = {2022},\n    eprint = {2112.05230},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2112.05230},\n}",
    "abstract": "Tremendous progress has been made in recent years in developing better image\ncaptioning models, yet most of them rely on a separate object detector to\nextract regional features. Recent vision-language studies are shifting towards\nthe detector-free trend by leveraging grid representations for more flexible\nmodel training and faster inference speed. However, such development is\nprimarily focused on image understanding tasks, and remains less investigated\nfor the caption generation task. In this paper, we are concerned with a\nbetter-performing detector-free image captioning model, and propose a pure\nvision transformer-based image captioning model, dubbed as ViTCAP, in which\ngrid representations are used without extracting the regional features. For\nimproved performance, we introduce a novel Concept Token Network (CTN) to\npredict the semantic concepts and then incorporate them into the end-to-end\ncaptioning. In particular, the CTN is built on the basis of a vision\ntransformer and is designed to predict the concept tokens through a\nclassification task, from which the rich semantic information contained greatly\nbenefits the captioning task. Compared with the previous detector-based models,\nViTCAP drastically simplifies the architectures and at the same time achieves\ncompetitive performance on various challenging image captioning datasets. In\nparticular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,\n93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,\nrespectively.",
    "num_pages": 18
}