{
    "uuid": "ea69aa8a-081a-5b13-bb55-df26650a47da",
    "title": "Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yingxiu Zhao",
        "Zhiliang Tian",
        "Huaxiu Yao",
        "Yinhe Zheng",
        "Dongkyu Lee",
        "Yiping Song",
        "Jian Sun",
        "Nevin L. Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.11670v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ea69aa8a-081a-5b13-bb55-df26650a47da.pdf",
    "bibtex": "@misc{zhao2022improvingmetalearningforlowresourcetext,\n    title = {Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation},\n    author = {Yingxiu Zhao and Zhiliang Tian and Huaxiu Yao and Yinhe Zheng and Dongkyu Lee and Yiping Song and Jian Sun and Nevin L. Zhang},\n    year = {2022},\n    eprint = {2203.11670},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.11670},\n}",
    "abstract": "Building models of natural language processing (NLP) is challenging in\nlow-resource scenarios where only limited data are available.\nOptimization-based meta-learning algorithms achieve promising results in\nlow-resource scenarios by adapting a well-generalized model initialization to\nhandle new tasks. Nonetheless, these approaches suffer from the memorization\noverfitting issue, where the model tends to memorize the meta-training tasks\nwhile ignoring support sets when adapting to new tasks. To address this issue,\nwe propose a memory imitation meta-learning (MemIML) method that enhances the\nmodel's reliance on support sets for task adaptation. Specifically, we\nintroduce a task-specific memory module to store support set information and\nconstruct an imitation module to force query sets to imitate the behaviors of\nsome representative support-set samples stored in the memory. A theoretical\nanalysis is provided to prove the effectiveness of our method, and empirical\nresults also demonstrate that our method outperforms competitive baselines on\nboth text classification and generation tasks.",
    "num_pages": 13
}