{
    "uuid": "c7cf8100-fcbd-575b-a980-7f6d3c4f6761",
    "title": "Teaching Large Language Models to Self-Debug",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xinyun Chen",
        "Maxwell Lin",
        "Nathanael Schärli",
        "Denny Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.05128v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\c7cf8100-fcbd-575b-a980-7f6d3c4f6761.pdf",
    "bibtex": "@misc{chen2023teachinglargelanguagemodelsto,\n    title = {Teaching Large Language Models to Self-Debug},\n    author = {Xinyun Chen and Maxwell Lin and Nathanael Schärli and Denny Zhou},\n    year = {2023},\n    eprint = {2304.05128},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.05128},\n}",
    "abstract": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.",
    "num_pages": 78
}