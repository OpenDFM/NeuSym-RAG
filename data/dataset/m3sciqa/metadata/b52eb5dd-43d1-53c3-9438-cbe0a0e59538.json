{
    "uuid": "b52eb5dd-43d1-53c3-9438-cbe0a0e59538",
    "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Sébastien Bubeck",
        "Varun Chandrasekaran",
        "Ronen Eldan",
        "Johannes Gehrke",
        "Eric Horvitz",
        "Ece Kamar",
        "Peter Lee",
        "Yin Tat Lee",
        "Yuanzhi Li",
        "Scott Lundberg",
        "Harsha Nori",
        "Hamid Palangi",
        "Marco Tulio Ribeiro",
        "Yi Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.12712v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b52eb5dd-43d1-53c3-9438-cbe0a0e59538.pdf",
    "bibtex": "@misc{bubeck2023sparksofartificialgeneralintelligence,\n    title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},\n    author = {Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},\n    year = {2023},\n    eprint = {2303.12712},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.12712},\n}",
    "abstract": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.",
    "num_pages": 155
}