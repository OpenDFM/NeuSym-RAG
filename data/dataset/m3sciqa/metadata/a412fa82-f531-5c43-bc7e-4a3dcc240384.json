{
    "uuid": "a412fa82-f531-5c43-bc7e-4a3dcc240384",
    "title": "Time Waits for No One! Analysis and Challenges of Temporal Misalignment",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Kelvin Luu",
        "Daniel Khashabi",
        "Suchin Gururangan",
        "Karishma Mandyam",
        "Noah A. Smith"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07408v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\a412fa82-f531-5c43-bc7e-4a3dcc240384.pdf",
    "bibtex": "@misc{luu2022timewaitsfornoone,\n    title = {Time Waits for No One! Analysis and Challenges of Temporal Misalignment},\n    author = {Kelvin Luu and Daniel Khashabi and Suchin Gururangan and Karishma Mandyam and Noah A. Smith},\n    year = {2022},\n    eprint = {2111.07408},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2111.07408},\n}",
    "abstract": "When an NLP model is trained on text data from one time period and tested or\ndeployed on data from another, the resulting temporal misalignment can degrade\nend-task performance. In this work, we establish a suite of eight diverse tasks\nacross different domains (social media, science papers, news, and reviews) and\nperiods of time (spanning five years or more) to quantify the effects of\ntemporal misalignment. Our study is focused on the ubiquitous setting where a\npretrained model is optionally adapted through continued domain-specific\npretraining, followed by task-specific finetuning. We establish a suite of\ntasks across multiple domains to study temporal misalignment in modern NLP\nsystems. We find stronger effects of temporal misalignment on task performance\nthan have been previously reported. We also find that, while temporal\nadaptation through continued pretraining can help, these gains are small\ncompared to task-specific finetuning on data from the target time period. Our\nfindings motivate continued research to improve temporal robustness of NLP\nmodels.",
    "num_pages": 15
}