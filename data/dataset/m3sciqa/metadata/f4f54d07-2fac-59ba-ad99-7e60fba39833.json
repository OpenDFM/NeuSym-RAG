{
    "uuid": "f4f54d07-2fac-59ba-ad99-7e60fba39833",
    "title": "Improving Image Captioning with Better Use of Captions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Zhan Shi",
        "Xu Zhou",
        "Xipeng Qiu",
        "Xiaodan Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.11807v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\f4f54d07-2fac-59ba-ad99-7e60fba39833.pdf",
    "bibtex": "@misc{shi2020improvingimagecaptioningwithbetter,\n    title = {Improving Image Captioning with Better Use of Captions},\n    author = {Zhan Shi and Xu Zhou and Xipeng Qiu and Xiaodan Zhu},\n    year = {2020},\n    eprint = {2006.11807},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2006.11807},\n}",
    "abstract": "Image captioning is a multimodal problem that has drawn extensive attention\nin both the natural language processing and computer vision community. In this\npaper, we present a novel image captioning architecture to better explore\nsemantics available in captions and leverage that to enhance both image\nrepresentation and caption generation. Our models first construct\ncaption-guided visual relationship graphs that introduce beneficial inductive\nbias using weakly supervised multi-instance learning. The representation is\nthen enhanced with neighbouring and contextual nodes with their textual and\nvisual features. During generation, the model further incorporates visual\nrelationships using multi-task learning for jointly predicting word and\nobject/predicate tag sequences. We perform extensive experiments on the MSCOCO\ndataset, showing that the proposed framework significantly outperforms the\nbaselines, resulting in the state-of-the-art performance under a wide range of\nevaluation metrics.",
    "num_pages": 11
}