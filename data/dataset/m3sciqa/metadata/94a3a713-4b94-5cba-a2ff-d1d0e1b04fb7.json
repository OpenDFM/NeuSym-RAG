{
    "uuid": "94a3a713-4b94-5cba-a2ff-d1d0e1b04fb7",
    "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.04830v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\94a3a713-4b94-5cba-a2ff-d1d0e1b04fb7.pdf",
    "bibtex": "@misc{yu2021learningmodalityspecificrepresentationswithselfsupervised,\n    title = {Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis},\n    author = {Wenmeng Yu and Hua Xu and Ziqi Yuan and Jiele Wu},\n    year = {2021},\n    eprint = {2102.04830},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2102.04830},\n}",
    "abstract": "Representation Learning is a significant and challenging task in multimodal\nlearning. Effective modality representations should contain two parts of\ncharacteristics: the consistency and the difference. Due to the unified\nmultimodal annotation, existing methods are restricted in capturing\ndifferentiated information. However, additional uni-modal annotations are high\ntime- and labor-cost. In this paper, we design a label generation module based\non the self-supervised learning strategy to acquire independent unimodal\nsupervisions. Then, joint training the multi-modal and uni-modal tasks to learn\nthe consistency and difference, respectively. Moreover, during the training\nstage, we design a weight-adjustment strategy to balance the learning progress\namong different subtasks. That is to guide the subtasks to focus on samples\nwith a larger difference between modality supervisions. Last, we conduct\nextensive experiments on three public multimodal baseline datasets. The\nexperimental results validate the reliability and stability of auto-generated\nunimodal supervisions. On MOSI and MOSEI datasets, our method surpasses the\ncurrent state-of-the-art methods. On the SIMS dataset, our method achieves\ncomparable performance than human-annotated unimodal labels. The full codes are\navailable at https://github.com/thuiar/Self-MM.",
    "num_pages": 8
}