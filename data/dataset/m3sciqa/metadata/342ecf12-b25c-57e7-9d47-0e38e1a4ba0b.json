{
    "uuid": "342ecf12-b25c-57e7-9d47-0e38e1a4ba0b",
    "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Mariya Toneva",
        "Alessandro Sordoni",
        "Remi Tachet des Combes",
        "Adam Trischler",
        "Yoshua Bengio",
        "Geoffrey J. Gordon"
    ],
    "pdf_url": "http://arxiv.org/pdf/1812.05159v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\342ecf12-b25c-57e7-9d47-0e38e1a4ba0b.pdf",
    "bibtex": "@misc{toneva2019anempiricalstudyofexample,\n    title = {An Empirical Study of Example Forgetting during Deep Neural Network Learning},\n    author = {Mariya Toneva and Alessandro Sordoni and Remi Tachet des Combes and Adam Trischler and Yoshua Bengio and Geoffrey J. Gordon},\n    year = {2019},\n    eprint = {1812.05159},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1812.05159},\n}",
    "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the\nlearning dynamics of neural networks as they train on single classification\ntasks. Our goal is to understand whether a related phenomenon occurs when data\ndoes not undergo a clear distributional shift. We define a `forgetting event'\nto have occurred when an individual training example transitions from being\nclassified correctly to incorrectly over the course of learning. Across several\nbenchmark data sets, we find that: (i) certain examples are forgotten with high\nfrequency, and some not at all; (ii) a data set's (un)forgettable examples\ngeneralize across neural architectures; and (iii) based on forgetting dynamics,\na significant fraction of examples can be omitted from the training data set\nwhile still maintaining state-of-the-art generalization performance.",
    "num_pages": 19
}