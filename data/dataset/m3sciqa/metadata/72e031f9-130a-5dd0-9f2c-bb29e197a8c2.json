{
    "uuid": "72e031f9-130a-5dd0-9f2c-bb29e197a8c2",
    "title": "FBERT: A Neural Transformer for Identifying Offensive Content",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Diptanu Sarkar",
        "Marcos Zampieri",
        "Tharindu Ranasinghe",
        "Alexander Ororbia"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05074v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\72e031f9-130a-5dd0-9f2c-bb29e197a8c2.pdf",
    "bibtex": "@misc{sarkar2021fbertaneuraltransformerfor,\n    title = {FBERT: A Neural Transformer for Identifying Offensive Content},\n    author = {Diptanu Sarkar and Marcos Zampieri and Tharindu Ranasinghe and Alexander Ororbia},\n    year = {2021},\n    eprint = {2109.05074},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.05074},\n}",
    "abstract": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.",
    "num_pages": 7
}