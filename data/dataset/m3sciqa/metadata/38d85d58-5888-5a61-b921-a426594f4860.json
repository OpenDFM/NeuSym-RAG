{
    "uuid": "38d85d58-5888-5a61-b921-a426594f4860",
    "title": "Learning deep representations by mutual information estimation and maximization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "R Devon Hjelm",
        "Alex Fedorov",
        "Samuel Lavoie-Marchildon",
        "Karan Grewal",
        "Phil Bachman",
        "Adam Trischler",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1808.06670v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\38d85d58-5888-5a61-b921-a426594f4860.pdf",
    "bibtex": "@misc{hjelm2019learningdeeprepresentationsbymutual,\n    title = {Learning deep representations by mutual information estimation and maximization},\n    author = {R Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Phil Bachman and Adam Trischler and Yoshua Bengio},\n    year = {2019},\n    eprint = {1808.06670},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1808.06670},\n}",
    "abstract": "In this work, we perform unsupervised learning of representations by\nmaximizing mutual information between an input and the output of a deep neural\nnetwork encoder. Importantly, we show that structure matters: incorporating\nknowledge about locality of the input to the objective can greatly influence a\nrepresentation's suitability for downstream tasks. We further control\ncharacteristics of the representation by matching to a prior distribution\nadversarially. Our method, which we call Deep InfoMax (DIM), outperforms a\nnumber of popular unsupervised learning methods and competes with\nfully-supervised learning on several classification tasks. DIM opens new\navenues for unsupervised learning of representations and is an important step\ntowards flexible formulations of representation-learning objectives for\nspecific end-goals.",
    "num_pages": 24
}