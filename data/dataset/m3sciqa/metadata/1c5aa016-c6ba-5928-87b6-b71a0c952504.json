{
    "uuid": "1c5aa016-c6ba-5928-87b6-b71a0c952504",
    "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jaemin Cho",
        "Abhay Zala",
        "Mohit Bansal"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.04053v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\1c5aa016-c6ba-5928-87b6-b71a0c952504.pdf",
    "bibtex": "@misc{cho2023dallevalprobingthereasoningskills,\n    title = {DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models},\n    author = {Jaemin Cho and Abhay Zala and Mohit Bansal},\n    year = {2023},\n    eprint = {2202.04053},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2202.04053},\n}",
    "abstract": "Recently, DALL-E, a multimodal transformer language model, and its variants,\nincluding diffusion models, have shown high-quality text-to-image generation\ncapabilities. However, despite the realistic image generation results, there\nhas not been a detailed analysis of how to evaluate such models. In this work,\nwe investigate the visual reasoning capabilities and social biases of different\ntext-to-image models, covering both multimodal transformer language models and\ndiffusion models. First, we measure three visual reasoning skills: object\nrecognition, object counting, and spatial relation understanding. For this, we\npropose PaintSkills, a compositional diagnostic evaluation dataset that\nmeasures these skills. Despite the high-fidelity image generation capability, a\nlarge gap exists between the performance of recent models and the upper bound\naccuracy in object counting and spatial relation understanding skills. Second,\nwe assess the gender and skin tone biases by measuring the gender/skin tone\ndistribution of generated images across various professions and attributes. We\ndemonstrate that recent text-to-image generation models learn specific biases\nabout gender and skin tone from web image-text pairs. We hope our work will\nhelp guide future progress in improving text-to-image generation models on\nvisual reasoning skills and learning socially unbiased representations. Code\nand data: https://github.com/j-min/DallEval",
    "num_pages": 34
}