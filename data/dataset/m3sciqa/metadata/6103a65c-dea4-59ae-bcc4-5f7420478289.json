{
    "uuid": "6103a65c-dea4-59ae-bcc4-5f7420478289",
    "title": "Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yujian Gan",
        "Xinyun Chen",
        "Matthew Purver"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05157v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\6103a65c-dea4-59ae-bcc4-5f7420478289.pdf",
    "bibtex": "@misc{gan2021exploringunderexploredlimitationsofcrossdomain,\n    title = {Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization},\n    author = {Yujian Gan and Xinyun Chen and Matthew Purver},\n    year = {2021},\n    eprint = {2109.05157},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.05157},\n}",
    "abstract": "Recently, there has been significant progress in studying neural networks for\ntranslating text descriptions into SQL queries under the zero-shot cross-domain\nsetting. Despite achieving good performance on some public benchmarks, we\nobserve that existing text-to-SQL models do not generalize when facing domain\nknowledge that does not frequently appear in the training data, which may\nrender the worse prediction performance for unseen domains. In this work, we\ninvestigate the robustness of text-to-SQL models when the questions require\nrarely observed domain knowledge. In particular, we define five types of domain\nknowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),\na human-curated dataset based on the Spider benchmark for text-to-SQL\ntranslation. NL questions in Spider-DK are selected from Spider, and we modify\nsome samples by adding domain knowledge that reflects real-world question\nparaphrases. We demonstrate that the prediction accuracy dramatically drops on\nsamples that require such domain knowledge, even if the domain knowledge\nappears in the training set, and the model provides the correct predictions for\nrelated training samples.",
    "num_pages": 6
}