{
    "uuid": "15271c1e-5d84-5507-bf2d-1dc547a7a2c0",
    "title": "Ultra-Fine Entity Typing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Eunsol Choi",
        "Omer Levy",
        "Yejin Choi",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/1807.04905v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\15271c1e-5d84-5507-bf2d-1dc547a7a2c0.pdf",
    "bibtex": "@misc{choi2018ultrafineentitytyping,\n    title = {Ultra-Fine Entity Typing},\n    author = {Eunsol Choi and Omer Levy and Yejin Choi and Luke Zettlemoyer},\n    year = {2018},\n    eprint = {1807.04905},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1807.04905},\n}",
    "abstract": "We introduce a new entity typing task: given a sentence with an entity\nmention, the goal is to predict a set of free-form phrases (e.g. skyscraper,\nsongwriter, or criminal) that describe appropriate types for the target entity.\nThis formulation allows us to use a new type of distant supervision at large\nscale: head words, which indicate the type of the noun phrases they appear in.\nWe show that these ultra-fine types can be crowd-sourced, and introduce new\nevaluation sets that are much more diverse and fine-grained than existing\nbenchmarks. We present a model that can predict open types, and is trained\nusing a multitask objective that pools our new head-word supervision with prior\nsupervision from entity linking. Experimental results demonstrate that our\nmodel is effective in predicting entity types at varying granularity; it\nachieves state of the art performance on an existing fine-grained entity typing\nbenchmark, and sets baselines for our newly-introduced datasets. Our data and\nmodel can be downloaded from: http://nlp.cs.washington.edu/entity_type",
    "num_pages": 10
}