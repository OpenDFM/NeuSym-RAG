{
    "uuid": "941ac574-4c1e-5b87-9a8d-b5fd7db093fc",
    "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Muhammad Abdul-Mageed",
        "AbdelRahim Elmadany",
        "El Moatez Billah Nagoudi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.01785v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\941ac574-4c1e-5b87-9a8d-b5fd7db093fc.pdf",
    "bibtex": "@misc{abdulmageed2021arbertmarbertdeepbidirectional,\n    title = {ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic},\n    author = {Muhammad Abdul-Mageed and AbdelRahim Elmadany and El Moatez Billah Nagoudi},\n    year = {2021},\n    eprint = {2101.01785},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2101.01785},\n}",
    "abstract": "Pre-trained language models (LMs) are currently integral to many natural\nlanguage processing systems. Although multilingual LMs were also introduced to\nserve many languages, these have limitations such as being costly at inference\ntime and the size and diversity of non-English data involved in their\npre-training. We remedy these issues for a collection of diverse Arabic\nvarieties by introducing two powerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a\nnew benchmark for multi-dialectal Arabic language understanding evaluation.\nARLUE is built using 42 datasets targeting six different task clusters,\nallowing us to offer a series of standardized experiments under rich\nconditions. When fine-tuned on ARLUE, our models collectively achieve new\nstate-of-the-art results across the majority of tasks (37 out of 48\nclassification tasks, on the 42 datasets). Our best model acquires the highest\nARLUE score (77.40) across all six task clusters, outperforming all other\nmodels including XLM-R Large (~ 3.4 x larger size). Our models are publicly\navailable at https://github.com/UBC-NLP/marbert and ARLUE will be released\nthrough the same repository.",
    "num_pages": 18
}