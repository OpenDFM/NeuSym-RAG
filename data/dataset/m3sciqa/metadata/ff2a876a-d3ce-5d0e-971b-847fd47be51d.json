{
    "uuid": "ff2a876a-d3ce-5d0e-971b-847fd47be51d",
    "title": "One pixel attack for fooling deep neural networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Jiawei Su",
        "Danilo Vasconcellos Vargas",
        "Sakurai Kouichi"
    ],
    "pdf_url": "http://arxiv.org/pdf/1710.08864v7",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\ff2a876a-d3ce-5d0e-971b-847fd47be51d.pdf",
    "bibtex": "@misc{su2019onepixelattackforfooling,\n    title = {One pixel attack for fooling deep neural networks},\n    author = {Jiawei Su and Danilo Vasconcellos Vargas and Sakurai Kouichi},\n    year = {2019},\n    eprint = {1710.08864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1710.08864},\n}",
    "abstract": "Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness.",
    "num_pages": 15
}