{
    "uuid": "200b3e0c-5509-5d2d-933d-f59e25c57c43",
    "title": "GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Van-Quang Nguyen",
        "Masanori Suganuma",
        "Takayuki Okatani"
    ],
    "pdf_url": "http://arxiv.org/pdf/2207.09666v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\200b3e0c-5509-5d2d-933d-f59e25c57c43.pdf",
    "bibtex": "@misc{nguyen2022gritfasterandbetterimage,\n    title = {GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features},\n    author = {Van-Quang Nguyen and Masanori Suganuma and Takayuki Okatani},\n    year = {2022},\n    eprint = {2207.09666},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2207.09666},\n}",
    "abstract": "Current state-of-the-art methods for image captioning employ region-based\nfeatures, as they provide object-level information that is essential to\ndescribe the content of images; they are usually extracted by an object\ndetector such as Faster R-CNN. However, they have several issues, such as lack\nof contextual information, the risk of inaccurate detection, and the high\ncomputational cost. The first two could be resolved by additionally using\ngrid-based features. However, how to extract and fuse these two types of\nfeatures is uncharted. This paper proposes a Transformer-only neural\narchitecture, dubbed GRIT (Grid- and Region-based Image captioning\nTransformer), that effectively utilizes the two visual features to generate\nbetter captions. GRIT replaces the CNN-based detector employed in previous\nmethods with a DETR-based one, making it computationally faster. Moreover, its\nmonolithic design consisting only of Transformers enables end-to-end training\nof the model. This innovative design and the integration of the dual visual\nfeatures bring about significant performance improvement. The experimental\nresults on several image captioning benchmarks show that GRIT outperforms\nprevious methods in inference accuracy and speed.",
    "num_pages": 27
}