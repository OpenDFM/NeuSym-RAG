{
    "uuid": "51b6e073-e1a7-51c0-8a23-314d84c6d9cd",
    "title": "BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yanzhu Guo",
        "Virgile Rennard",
        "Christos Xypolopoulos",
        "Michalis Vazirgiannis"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10234v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\51b6e073-e1a7-51c0-8a23-314d84c6d9cd.pdf",
    "bibtex": "@misc{guo2021bertweetfrdomainadaptationof,\n    title = {BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets},\n    author = {Yanzhu Guo and Virgile Rennard and Christos Xypolopoulos and Michalis Vazirgiannis},\n    year = {2021},\n    eprint = {2109.10234},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.10234},\n}",
    "abstract": "We introduce BERTweetFR, the first large-scale pre-trained language model for\nFrench tweets. Our model is initialized using the general-domain French\nlanguage model CamemBERT which follows the base architecture of RoBERTa.\nExperiments show that BERTweetFR outperforms all previous general-domain French\nlanguage models on two downstream Twitter NLP tasks of offensiveness\nidentification and named entity recognition. The dataset used in the\noffensiveness detection task is first created and annotated by our team,\nfilling in the gap of such analytic datasets in French. We make our model\npublicly available in the transformers library with the aim of promoting future\nresearch in analytic tasks for French tweets.",
    "num_pages": 6
}