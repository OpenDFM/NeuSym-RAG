{
    "uuid": "41982a4d-b3c2-5e5f-bb78-4c0a480299c1",
    "title": "Efficiently Scaling Transformer Inference",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Reiner Pope",
        "Sholto Douglas",
        "Aakanksha Chowdhery",
        "Jacob Devlin",
        "James Bradbury",
        "Anselm Levskaya",
        "Jonathan Heek",
        "Kefan Xiao",
        "Shivani Agrawal",
        "Jeff Dean"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.05102v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\41982a4d-b3c2-5e5f-bb78-4c0a480299c1.pdf",
    "bibtex": "@misc{pope2022efficientlyscalingtransformerinference,\n    title = {Efficiently Scaling Transformer Inference},\n    author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},\n    year = {2022},\n    eprint = {2211.05102},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2211.05102},\n}",
    "abstract": "We study the problem of efficient generative inference for Transformer\nmodels, in one of its most challenging settings: large deep models, with tight\nlatency targets and long sequence lengths. Better understanding of the\nengineering tradeoffs for inference for large Transformer-based models is\nimportant as use cases of these models are growing rapidly throughout\napplication areas. We develop a simple analytical model for inference\nefficiency to select the best multi-dimensional partitioning techniques\noptimized for TPU v4 slices based on the application requirements. We combine\nthese with a suite of low-level optimizations to achieve a new Pareto frontier\non the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter\nmodels that outperforms the FasterTransformer suite of benchmarks. We further\nshow that with appropriate partitioning, the lower memory requirements of\nmultiquery attention (i.e. multiple query heads share single key/value head)\nenables scaling up to 32x larger context lengths. Finally, we achieve a\nlow-batch-size latency of 29ms per token during generation (using int8 weight\nquantization) and a 76% MFU during large-batch-size processing of input tokens,\nwhile supporting a long 2048-token context length on the PaLM 540B parameter\nmodel.",
    "num_pages": 18
}