{
    "uuid": "cfd9bcfc-924d-5175-96d2-c111a44925e3",
    "title": "ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Long Phan",
        "Hieu Tran",
        "Hieu Nguyen",
        "Trieu H. Trinh"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.06457v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\cfd9bcfc-924d-5175-96d2-c111a44925e3.pdf",
    "bibtex": "@misc{phan2022vit5pretrainedtexttotexttransformerfor,\n    title = {ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation},\n    author = {Long Phan and Hieu Tran and Hieu Nguyen and Trieu H. Trinh},\n    year = {2022},\n    eprint = {2205.06457},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.06457},\n}",
    "abstract": "We present ViT5, a pretrained Transformer-based encoder-decoder model for the\nVietnamese language. With T5-style self-supervised pretraining, ViT5 is trained\non a large corpus of high-quality and diverse Vietnamese texts. We benchmark\nViT5 on two downstream text generation tasks, Abstractive Text Summarization\nand Named Entity Recognition. Although Abstractive Text Summarization has been\nwidely studied for the English language thanks to its rich and large source of\ndata, there has been minimal research into the same task in Vietnamese, a much\nlower resource language. In this work, we perform exhaustive experiments on\nboth Vietnamese Abstractive Summarization and Named Entity Recognition,\nvalidating the performance of ViT5 against many other pretrained\nTransformer-based encoder-decoder models. Our experiments show that ViT5\nsignificantly outperforms existing models and achieves state-of-the-art results\non Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5\nis competitive against previous best results from pretrained encoder-based\nTransformer models. Further analysis shows the importance of context length\nduring the self-supervised pretraining on downstream performance across\ndifferent settings.",
    "num_pages": 7
}