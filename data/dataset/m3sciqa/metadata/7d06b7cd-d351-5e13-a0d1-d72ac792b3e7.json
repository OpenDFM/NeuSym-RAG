{
    "uuid": "7d06b7cd-d351-5e13-a0d1-d72ac792b3e7",
    "title": "GPT4All: An Ecosystem of Open Source Compressed Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yuvanesh Anand",
        "Zach Nussbaum",
        "Adam Treat",
        "Aaron Miller",
        "Richard Guo",
        "Ben Schmidt",
        "GPT4All Community",
        "Brandon Duderstadt",
        "Andriy Mulyar"
    ],
    "pdf_url": "http://arxiv.org/pdf/2311.04931v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\7d06b7cd-d351-5e13-a0d1-d72ac792b3e7.pdf",
    "bibtex": "@misc{anand2023gpt4allanecosystemofopen,\n    title = {GPT4All: An Ecosystem of Open Source Compressed Language Models},\n    author = {Yuvanesh Anand and Zach Nussbaum and Adam Treat and Aaron Miller and Richard Guo and Ben Schmidt and GPT4All Community and Brandon Duderstadt and Andriy Mulyar},\n    year = {2023},\n    eprint = {2311.04931},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2311.04931},\n}",
    "abstract": "Large language models (LLMs) have recently achieved human-level performance\non a range of professional and academic benchmarks. The accessibility of these\nmodels has lagged behind their performance. State-of-the-art LLMs require\ncostly infrastructure; are only accessible via rate-limited, geo-locked, and\ncensored web interfaces; and lack publicly available code and technical\nreports. In this paper, we tell the story of GPT4All, a popular open source\nrepository that aims to democratize access to LLMs. We outline the technical\ndetails of the original GPT4All model family, as well as the evolution of the\nGPT4All project from a single model into a fully fledged open source ecosystem.\nIt is our hope that this paper acts as both a technical overview of the\noriginal GPT4All models as well as a case study on the subsequent growth of the\nGPT4All open source ecosystem.",
    "num_pages": 6
}