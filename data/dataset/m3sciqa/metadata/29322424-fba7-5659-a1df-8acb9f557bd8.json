{
    "uuid": "29322424-fba7-5659-a1df-8acb9f557bd8",
    "title": "Improving Constituency Parsing with Span Attention",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yuanhe Tian",
        "Yan Song",
        "Fei Xia",
        "Tong Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.07543v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\29322424-fba7-5659-a1df-8acb9f557bd8.pdf",
    "bibtex": "@misc{tian2020improvingconstituencyparsingwithspan,\n    title = {Improving Constituency Parsing with Span Attention},\n    author = {Yuanhe Tian and Yan Song and Fei Xia and Tong Zhang},\n    year = {2020},\n    eprint = {2010.07543},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.07543},\n}",
    "abstract": "Constituency parsing is a fundamental and important task for natural language\nunderstanding, where a good representation of contextual information can help\nthis task. N-grams, which is a conventional type of feature for contextual\ninformation, have been demonstrated to be useful in many tasks, and thus could\nalso be beneficial for constituency parsing if they are appropriately modeled.\nIn this paper, we propose span attention for neural chart-based constituency\nparsing to leverage n-gram information. Considering that current chart-based\nparsers with Transformer-based encoder represent spans by subtraction of the\nhidden states at the span boundaries, which may cause information loss\nespecially for long spans, we incorporate n-grams into span representations by\nweighting them according to their contributions to the parsing process.\nMoreover, we propose categorical span attention to further enhance the model by\nweighting n-grams within different length categories, and thus benefit\nlong-sentence parsing. Experimental results on three widely used benchmark\ndatasets demonstrate the effectiveness of our approach in parsing Arabic,\nChinese, and English, where state-of-the-art performance is obtained by our\napproach on all of them.",
    "num_pages": 13
}