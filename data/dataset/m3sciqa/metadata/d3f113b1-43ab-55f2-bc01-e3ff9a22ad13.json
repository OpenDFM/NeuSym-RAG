{
    "uuid": "d3f113b1-43ab-55f2-bc01-e3ff9a22ad13",
    "title": "LEGAL-BERT: The Muppets straight out of Law School",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Ilias Chalkidis",
        "Manos Fergadiotis",
        "Prodromos Malakasiotis",
        "Nikolaos Aletras",
        "Ion Androutsopoulos"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.02559v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\d3f113b1-43ab-55f2-bc01-e3ff9a22ad13.pdf",
    "bibtex": "@misc{chalkidis2020legalbertthemuppetsstraightout,\n    title = {LEGAL-BERT: The Muppets straight out of Law School},\n    author = {Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},\n    year = {2020},\n    eprint = {2010.02559},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.02559},\n}",
    "abstract": "BERT has achieved impressive performance in several NLP tasks. However, there\nhas been limited investigation on its adaptation guidelines in specialised\ndomains. Here we focus on the legal domain, where we explore several approaches\nfor applying BERT models to downstream legal tasks, evaluating on multiple\ndatasets. Our findings indicate that the previous guidelines for pre-training\nand fine-tuning, often blindly followed, do not always generalize well in the\nlegal domain. Thus we propose a systematic investigation of the available\nstrategies when applying BERT in specialised domains. These are: (a) use the\noriginal BERT out of the box, (b) adapt BERT by additional pre-training on\ndomain-specific corpora, and (c) pre-train BERT from scratch on domain-specific\ncorpora. We also propose a broader hyper-parameter search space when\nfine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT\nmodels intended to assist legal NLP research, computational law, and legal\ntechnology applications.",
    "num_pages": 7
}