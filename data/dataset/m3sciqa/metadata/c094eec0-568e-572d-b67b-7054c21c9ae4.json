{
    "uuid": "c094eec0-568e-572d-b67b-7054c21c9ae4",
    "title": "Czech Dataset for Cross-lingual Subjectivity Classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Pavel Přibáň",
        "Josef Steinberger"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.13915v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c094eec0-568e-572d-b67b-7054c21c9ae4.pdf",
    "bibtex": "@misc{pib2022czechdatasetforcrosslingualsubjectivity,\n    title = {Czech Dataset for Cross-lingual Subjectivity Classification},\n    author = {Pavel Přibáň and Josef Steinberger},\n    year = {2022},\n    eprint = {2204.13915},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2204.13915},\n}",
    "abstract": "In this paper, we introduce a new Czech subjectivity dataset of 10k manually\nannotated subjective and objective sentences from movie reviews and\ndescriptions. Our prime motivation is to provide a reliable dataset that can be\nused with the existing English dataset as a benchmark to test the ability of\npre-trained multilingual models to transfer knowledge between Czech and English\nand vice versa. Two annotators annotated the dataset reaching 0.83 of the\nCohen's \\k{appa} inter-annotator agreement. To the best of our knowledge, this\nis the first subjectivity dataset for the Czech language. We also created an\nadditional dataset that consists of 200k automatically labeled sentences. Both\ndatasets are freely available for research purposes. Furthermore, we fine-tune\nfive pre-trained BERT-like models to set a monolingual baseline for the new\ndataset and we achieve 93.56% of accuracy. We fine-tune models on the existing\nEnglish dataset for which we obtained results that are on par with the current\nstate-of-the-art results. Finally, we perform zero-shot cross-lingual\nsubjectivity classification between Czech and English to verify the usability\nof our dataset as the cross-lingual benchmark. We compare and discuss the\ncross-lingual and monolingual results and the ability of multilingual models to\ntransfer knowledge between languages.",
    "num_pages": 11
}