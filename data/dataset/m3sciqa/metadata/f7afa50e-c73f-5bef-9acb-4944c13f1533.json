{
    "uuid": "f7afa50e-c73f-5bef-9acb-4944c13f1533",
    "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tianlong Chen",
        "Zhenyu Zhang",
        "Ajay Jaiswal",
        "Shiwei Liu",
        "Zhangyang Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.01610v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f7afa50e-c73f-5bef-9acb-4944c13f1533.pdf",
    "bibtex": "@misc{chen2023sparsemoeasthenew,\n    title = {Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers},\n    author = {Tianlong Chen and Zhenyu Zhang and Ajay Jaiswal and Shiwei Liu and Zhangyang Wang},\n    year = {2023},\n    eprint = {2303.01610},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2303.01610},\n}",
    "abstract": "Despite their remarkable achievement, gigantic transformers encounter\nsignificant drawbacks, including exorbitant computational and memory footprints\nduring training, as well as severe collapse evidenced by a high degree of\nparameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown\npromise to mitigate the issue of training efficiency, yet they are prone to (1)\nredundant experts due to representational collapse; and (2) poor expert\nscalability for inference and downstream fine-tuning, primarily due to\noverfitting of the learned routing policy to the number of activated experts\nduring training. As recent research efforts are predominantly focused on\nimproving routing policies to encourage expert specializations, this work\nfocuses on exploring the overlooked scalability bottleneck of SMoEs and\nleveraging it to effectively scale dense transformers. To this end, we propose\na new plug-and-play training framework, SMoE-Dropout, to enable scaling\ntransformers to better accuracy in their full capacity without collapse.\nSpecifically, SMoE-Dropout consists of a randomly initialized and fixed router\nnetwork to activate experts and gradually increases the activated expert number\nas training progresses over time. Transformers trained by SMoE-Dropout\nnaturally exhibit a self-slimmable property subject to resource availability,\noffering smooth and consistent performance boosts with an increase in activated\nexperts during inference or fine-tuning. Our extensive experiments demonstrate\nthe superior performance and substantial computation savings of SMoE-Dropout,\ncompared to dense training baselines with equivalent parameter counts. In\nparticular, our trained BERT outperforms its densely trained counterpart with\nconsistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks\n{ASDiv-A, MAWPS, SVAMP}, respectively.",
    "num_pages": 17
}