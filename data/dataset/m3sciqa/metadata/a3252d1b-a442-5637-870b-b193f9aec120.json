{
    "uuid": "a3252d1b-a442-5637-870b-b193f9aec120",
    "title": "Embodied Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Abhishek Das",
        "Samyak Datta",
        "Georgia Gkioxari",
        "Stefan Lee",
        "Devi Parikh",
        "Dhruv Batra"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.11543v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\a3252d1b-a442-5637-870b-b193f9aec120.pdf",
    "bibtex": "@misc{das2017embodiedquestionanswering,\n    title = {Embodied Question Answering},\n    author = {Abhishek Das and Samyak Datta and Georgia Gkioxari and Stefan Lee and Devi Parikh and Dhruv Batra},\n    year = {2017},\n    eprint = {1711.11543},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1711.11543},\n}",
    "abstract": "We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (\"What color is the car?\"). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\n  This challenging task requires a range of AI skills -- active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA.",
    "num_pages": 20
}