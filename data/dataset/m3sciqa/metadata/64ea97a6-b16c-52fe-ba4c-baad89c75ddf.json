{
    "uuid": "64ea97a6-b16c-52fe-ba4c-baad89c75ddf",
    "title": "Cross-lingual Language Model Pretraining",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Guillaume Lample",
        "Alexis Conneau"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.07291v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\64ea97a6-b16c-52fe-ba4c-baad89c75ddf.pdf",
    "bibtex": "@misc{lample2019crosslinguallanguagemodelpretraining,\n    title = {Cross-lingual Language Model Pretraining},\n    author = {Guillaume Lample and Alexis Conneau},\n    year = {2019},\n    eprint = {1901.07291},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1901.07291},\n}",
    "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for\nEnglish natural language understanding. In this work, we extend this approach\nto multiple languages and show the effectiveness of cross-lingual pretraining.\nWe propose two methods to learn cross-lingual language models (XLMs): one\nunsupervised that only relies on monolingual data, and one supervised that\nleverages parallel data with a new cross-lingual language model objective. We\nobtain state-of-the-art results on cross-lingual classification, unsupervised\nand supervised machine translation. On XNLI, our approach pushes the state of\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\ntranslation, we obtain 34.3 BLEU on WMT'16 German-English, improving the\nprevious state of the art by more than 9 BLEU. On supervised machine\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT'16\nRomanian-English, outperforming the previous best approach by more than 4 BLEU.\nOur code and pretrained models will be made publicly available.",
    "num_pages": 10
}