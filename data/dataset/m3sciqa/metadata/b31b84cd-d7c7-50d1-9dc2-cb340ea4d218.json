{
    "uuid": "b31b84cd-d7c7-50d1-9dc2-cb340ea4d218",
    "title": "Optimizing Prompts for Text-to-Image Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yaru Hao",
        "Zewen Chi",
        "Li Dong",
        "Furu Wei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.09611v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b31b84cd-d7c7-50d1-9dc2-cb340ea4d218.pdf",
    "bibtex": "@misc{hao2023optimizingpromptsfortexttoimagegeneration,\n    title = {Optimizing Prompts for Text-to-Image Generation},\n    author = {Yaru Hao and Zewen Chi and Li Dong and Furu Wei},\n    year = {2023},\n    eprint = {2212.09611},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.09611},\n}",
    "abstract": "Well-designed prompts can guide text-to-image models to generate amazing\nimages. However, the performant prompts are often model-specific and misaligned\nwith user input. Instead of laborious human engineering, we propose prompt\nadaptation, a general framework that automatically adapts original user input\nto model-preferred prompts. Specifically, we first perform supervised\nfine-tuning with a pretrained language model on a small collection of manually\nengineered prompts. Then we use reinforcement learning to explore better\nprompts. We define a reward function that encourages the policy to generate\nmore aesthetically pleasing images while preserving the original user\nintentions. Experimental results on Stable Diffusion show that our method\noutperforms manual prompt engineering in terms of both automatic metrics and\nhuman preference ratings. Moreover, reinforcement learning further boosts\nperformance, especially on out-of-domain prompts. The pretrained checkpoints\nare available at https://aka.ms/promptist. The demo can be found at\nhttps://aka.ms/promptist-demo.",
    "num_pages": 16
}