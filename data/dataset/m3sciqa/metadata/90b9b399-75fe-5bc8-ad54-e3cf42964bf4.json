{
    "uuid": "90b9b399-75fe-5bc8-ad54-e3cf42964bf4",
    "title": "Controllable Natural Language Generation with Contrastive Prefixes",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jing Qian",
        "Li Dong",
        "Yelong Shen",
        "Furu Wei",
        "Weizhu Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.13257v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\90b9b399-75fe-5bc8-ad54-e3cf42964bf4.pdf",
    "bibtex": "@misc{qian2022controllablenaturallanguagegenerationwith,\n    title = {Controllable Natural Language Generation with Contrastive Prefixes},\n    author = {Jing Qian and Li Dong and Yelong Shen and Furu Wei and Weizhu Chen},\n    year = {2022},\n    eprint = {2202.13257},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2202.13257},\n}",
    "abstract": "To guide the generation of large pretrained language models (LM), previous\nwork has focused on directly fine-tuning the language model or utilizing an\nattribute discriminator. In this work, we propose a novel lightweight framework\nfor controllable GPT2 generation, which utilizes a set of small\nattribute-specific vectors, called prefixes, to steer natural language\ngeneration. Different from prefix-tuning, where each prefix is trained\nindependently, we take the relationship among prefixes into consideration and\ntrain multiple prefixes simultaneously. We propose a novel supervised method\nand also an unsupervised method to train the prefixes for single-aspect control\nwhile the combination of these two methods can achieve multi-aspect control.\nExperimental results on both single-aspect and multi-aspect control show that\nour methods can guide generation towards the desired attributes while keeping\nhigh linguistic quality.",
    "num_pages": 13
}