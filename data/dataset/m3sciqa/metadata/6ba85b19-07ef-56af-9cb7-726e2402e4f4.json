{
    "uuid": "6ba85b19-07ef-56af-9cb7-726e2402e4f4",
    "title": "Deep Variational Information Bottleneck",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Alexander A. Alemi",
        "Ian Fischer",
        "Joshua V. Dillon",
        "Kevin Murphy"
    ],
    "pdf_url": "http://arxiv.org/pdf/1612.00410v7",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\6ba85b19-07ef-56af-9cb7-726e2402e4f4.pdf",
    "bibtex": "@misc{alemi2019deepvariationalinformationbottleneck,\n    title = {Deep Variational Information Bottleneck},\n    author = {Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},\n    year = {2019},\n    eprint = {1612.00410},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1612.00410},\n}",
    "abstract": "We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.",
    "num_pages": 19
}