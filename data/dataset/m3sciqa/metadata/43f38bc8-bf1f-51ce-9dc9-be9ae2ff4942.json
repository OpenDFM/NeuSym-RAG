{
    "uuid": "43f38bc8-bf1f-51ce-9dc9-be9ae2ff4942",
    "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Hongjin Su",
        "Jungo Kasai",
        "Chen Henry Wu",
        "Weijia Shi",
        "Tianlu Wang",
        "Jiayi Xin",
        "Rui Zhang",
        "Mari Ostendorf",
        "Luke Zettlemoyer",
        "Noah A. Smith",
        "Tao Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2209.01975v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\43f38bc8-bf1f-51ce-9dc9-be9ae2ff4942.pdf",
    "bibtex": "@misc{su2022selectiveannotationmakeslanguagemodels,\n    title = {Selective Annotation Makes Language Models Better Few-Shot Learners},\n    author = {Hongjin Su and Jungo Kasai and Chen Henry Wu and Weijia Shi and Tianlu Wang and Jiayi Xin and Rui Zhang and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},\n    year = {2022},\n    eprint = {2209.01975},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2209.01975},\n}",
    "abstract": "Many recent approaches to natural language tasks are built on the remarkable\nabilities of large language models. Large language models can perform\nin-context learning, where they learn a new task from a few task\ndemonstrations, without any parameter updates. This work examines the\nimplications of in-context learning for the creation of datasets for new\nnatural language tasks. Departing from recent in-context learning methods, we\nformulate an annotation-efficient, two-step framework: selective annotation\nthat chooses a pool of examples to annotate from unlabeled data in advance,\nfollowed by prompt retrieval that retrieves task examples from the annotated\npool at test time. Based on this framework, we propose an unsupervised,\ngraph-based selective annotation method, voke-k, to select diverse,\nrepresentative examples to annotate. Extensive experiments on 10 datasets\n(covering classification, commonsense reasoning, dialogue, and text/code\ngeneration) demonstrate that our selective annotation method improves the task\nperformance by a large margin. On average, vote-k achieves a 12.9%/11.4%\nrelative gain under an annotation budget of 18/100, as compared to randomly\nselecting examples to annotate. Compared to state-of-the-art supervised\nfinetuning approaches, it yields similar performance with 10-100x less\nannotation cost across 10 tasks. We further analyze the effectiveness of our\nframework in various scenarios: language models with varying sizes, alternative\nselective annotation methods, and cases where there is a test data domain\nshift. We hope that our studies will serve as a basis for data annotations as\nlarge language models are increasingly applied to new tasks. Our code is\navailable at https://github.com/HKUNLP/icl-selective-annotation.",
    "num_pages": 21
}