{
    "uuid": "591f54ea-2f59-5cdb-8117-74bd1028e5d1",
    "title": "Self-Refine: Iterative Refinement with Self-Feedback",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Prakhar Gupta",
        "Skyler Hallinan",
        "Luyu Gao",
        "Sarah Wiegreffe",
        "Uri Alon",
        "Nouha Dziri",
        "Shrimai Prabhumoye",
        "Yiming Yang",
        "Shashank Gupta",
        "Bodhisattwa Prasad Majumder",
        "Katherine Hermann",
        "Sean Welleck",
        "Amir Yazdanbakhsh",
        "Peter Clark"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.17651v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\591f54ea-2f59-5cdb-8117-74bd1028e5d1.pdf",
    "bibtex": "@misc{madaan2023selfrefineiterativerefinementwithselffeedback,\n    title = {Self-Refine: Iterative Refinement with Self-Feedback},\n    author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},\n    year = {2023},\n    eprint = {2303.17651},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.17651},\n}",
    "abstract": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.",
    "num_pages": 54
}