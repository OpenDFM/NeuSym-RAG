{
    "uuid": "0d634a2b-3091-5566-b890-9e5e06d8d59e",
    "title": "Grounded Language Learning Fast and Slow",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Felix Hill",
        "Olivier Tieleman",
        "Tamara von Glehn",
        "Nathaniel Wong",
        "Hamza Merzic",
        "Stephen Clark"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.01719v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\0d634a2b-3091-5566-b890-9e5e06d8d59e.pdf",
    "bibtex": "@misc{hill2020groundedlanguagelearningfastand,\n    title = {Grounded Language Learning Fast and Slow},\n    author = {Felix Hill and Olivier Tieleman and Tamara von Glehn and Nathaniel Wong and Hamza Merzic and Stephen Clark},\n    year = {2020},\n    eprint = {2009.01719},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.01719},\n}",
    "abstract": "Recent work has shown that large text-based neural language models, trained\nwith conventional supervised learning objectives, acquire a surprising\npropensity for few- and one-shot learning. Here, we show that an embodied agent\nsituated in a simulated 3D world, and endowed with a novel dual-coding external\nmemory, can exhibit similar one-shot word learning when trained with\nconventional reinforcement learning algorithms. After a single introduction to\na novel object via continuous visual perception and a language prompt (\"This is\na dax\"), the agent can re-identify the object and manipulate it as instructed\n(\"Put the dax on the bed\"). In doing so, it seamlessly integrates short-term,\nwithin-episode knowledge of the appropriate referent for the word \"dax\" with\nlong-term lexical and motor knowledge acquired across episodes (i.e. \"bed\" and\n\"putting\"). We find that, under certain training conditions and with a\nparticular memory writing mechanism, the agent's one-shot word-object binding\ngeneralizes to novel exemplars within the same ShapeNet category, and is\neffective in settings with unfamiliar numbers of objects. We further show how\ndual-coding memory can be exploited as a signal for intrinsic motivation,\nstimulating the agent to seek names for objects that may be useful for later\nexecuting instructions. Together, the results demonstrate that deep neural\nnetworks can exploit meta-learning, episodic memory and an explicitly\nmulti-modal environment to account for 'fast-mapping', a fundamental pillar of\nhuman cognitive development and a potentially transformative capacity for\nagents that interact with human users.",
    "num_pages": 17
}