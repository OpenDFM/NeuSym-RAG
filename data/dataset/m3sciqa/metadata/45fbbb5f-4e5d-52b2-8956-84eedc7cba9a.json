{
    "uuid": "45fbbb5f-4e5d-52b2-8956-84eedc7cba9a",
    "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Victor Zhong",
        "Caiming Xiong",
        "Richard Socher"
    ],
    "pdf_url": "http://arxiv.org/pdf/1709.00103v7",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\45fbbb5f-4e5d-52b2-8956-84eedc7cba9a.pdf",
    "bibtex": "@misc{zhong2017seq2sqlgeneratingstructuredqueriesfrom,\n    title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},\n    author = {Victor Zhong and Caiming Xiong and Richard Socher},\n    year = {2017},\n    eprint = {1709.00103},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1709.00103},\n}",
    "abstract": "A significant amount of the world's knowledge is stored in relational\ndatabases. However, the ability for users to retrieve facts from a database is\nlimited due to a lack of understanding of query languages such as SQL. We\npropose Seq2SQL, a deep neural network for translating natural language\nquestions to corresponding SQL queries. Our model leverages the structure of\nSQL queries to significantly reduce the output space of generated queries.\nMoreover, we use rewards from in-the-loop query execution over the database to\nlearn a policy to generate unordered parts of the query, which we show are less\nsuitable for optimization via cross entropy loss. In addition, we will publish\nWikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL\nqueries distributed across 24241 tables from Wikipedia. This dataset is\nrequired to train our model and is an order of magnitude larger than comparable\ndatasets. By applying policy-based reinforcement learning with a query\nexecution environment to WikiSQL, our model Seq2SQL outperforms attentional\nsequence to sequence models, improving execution accuracy from 35.9% to 59.4%\nand logical form accuracy from 23.4% to 48.3%.",
    "num_pages": 12
}