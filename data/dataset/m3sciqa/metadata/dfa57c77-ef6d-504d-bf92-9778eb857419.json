{
    "uuid": "dfa57c77-ef6d-504d-bf92-9778eb857419",
    "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Liang Xu",
        "Hai Hu",
        "Xuanwei Zhang",
        "Lu Li",
        "Chenjie Cao",
        "Yudong Li",
        "Yechen Xu",
        "Kai Sun",
        "Dian Yu",
        "Cong Yu",
        "Yin Tian",
        "Qianqian Dong",
        "Weitang Liu",
        "Bo Shi",
        "Yiming Cui",
        "Junyi Li",
        "Jun Zeng",
        "Rongzhao Wang",
        "Weijian Xie",
        "Yanting Li",
        "Yina Patterson",
        "Zuoyu Tian",
        "Yiwen Zhang",
        "He Zhou",
        "Shaoweihua Liu",
        "Zhe Zhao",
        "Qipeng Zhao",
        "Cong Yue",
        "Xinrui Zhang",
        "Zhengliang Yang",
        "Kyle Richardson",
        "Zhenzhong Lan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05986v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\dfa57c77-ef6d-504d-bf92-9778eb857419.pdf",
    "bibtex": "@misc{xu2020clueachineselanguageunderstanding,\n    title = {CLUE: A Chinese Language Understanding Evaluation Benchmark},\n    author = {Liang Xu and Hai Hu and Xuanwei Zhang and Lu Li and Chenjie Cao and Yudong Li and Yechen Xu and Kai Sun and Dian Yu and Cong Yu and Yin Tian and Qianqian Dong and Weitang Liu and Bo Shi and Yiming Cui and Junyi Li and Jun Zeng and Rongzhao Wang and Weijian Xie and Yanting Li and Yina Patterson and Zuoyu Tian and Yiwen Zhang and He Zhou and Shaoweihua Liu and Zhe Zhao and Qipeng Zhao and Cong Yue and Xinrui Zhang and Zhengliang Yang and Kyle Richardson and Zhenzhong Lan},\n    year = {2020},\n    eprint = {2004.05986},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.05986},\n}",
    "abstract": "The advent of natural language understanding (NLU) benchmarks for English,\nsuch as GLUE and SuperGLUE allows new NLU models to be evaluated across a\ndiverse set of tasks. These comprehensive benchmarks have facilitated a broad\nrange of research and applications in natural language processing (NLP). The\nproblem, however, is that most such benchmarks are limited to English, which\nhas made it difficult to replicate many of the successes in English NLU for\nother languages. To help remedy this issue, we introduce the first large-scale\nChinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an\nopen-ended, community-driven project that brings together 9 tasks spanning\nseveral well-established single-sentence/sentence-pair classification tasks, as\nwell as machine reading comprehension, all on original Chinese text. To\nestablish results on these tasks, we report scores using an exhaustive set of\ncurrent state-of-the-art pre-trained Chinese models (9 in total). We also\nintroduce a number of supplementary datasets and additional tools to help\nfacilitate further progress on Chinese NLU. Our benchmark is released at\nhttps://www.CLUEbenchmarks.com",
    "num_pages": 15
}