{
    "uuid": "452d4e28-7f06-559e-acaa-fd7cede347f4",
    "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Rabeeh Karimi Mahabadi",
        "James Henderson",
        "Sebastian Ruder"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04647v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\452d4e28-7f06-559e-acaa-fd7cede347f4.pdf",
    "bibtex": "@misc{mahabadi2021compacterefficientlowrankhypercomplexadapter,\n    title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},\n    author = {Rabeeh Karimi Mahabadi and James Henderson and Sebastian Ruder},\n    year = {2021},\n    eprint = {2106.04647},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.04647},\n}",
    "abstract": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers. Specifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared \"slow\" weights and \"fast\" rank-one matrices\ndefined per Compacter layer. By only training 0.047% of a pretrained model's\nparameters, Compacter performs on par with standard fine-tuning on GLUE and\noutperforms standard fine-tuning on SuperGLUE and low-resource settings. Our\ncode is publicly available at~\\url{https://github.com/rabeehk/compacter}.",
    "num_pages": 18
}