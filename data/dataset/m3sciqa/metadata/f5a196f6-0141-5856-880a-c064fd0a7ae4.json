{
    "uuid": "f5a196f6-0141-5856-880a-c064fd0a7ae4",
    "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Sascha Rothe",
        "Shashi Narayan",
        "Aliaksei Severyn"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.12461v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\f5a196f6-0141-5856-880a-c064fd0a7ae4.pdf",
    "bibtex": "@misc{rothe2020leveragingpretrainedcheckpointsforsequence,\n    title = {Leveraging Pre-trained Checkpoints for Sequence Generation Tasks},\n    author = {Sascha Rothe and Shashi Narayan and Aliaksei Severyn},\n    year = {2020},\n    eprint = {1907.12461},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1907.12461},\n}",
    "abstract": "Unsupervised pre-training of large neural models has recently revolutionized\nNatural Language Processing. By warm-starting from the publicly released\ncheckpoints, NLP practitioners have pushed the state-of-the-art on multiple\nbenchmarks while saving significant amounts of compute time. So far the focus\nhas been mainly on the Natural Language Understanding tasks. In this paper, we\ndemonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\ndeveloped a Transformer-based sequence-to-sequence model that is compatible\nwith publicly available pre-trained BERT, GPT-2 and RoBERTa checkpoints and\nconducted an extensive empirical study on the utility of initializing our\nmodel, both encoder and decoder, with these checkpoints. Our models result in\nnew state-of-the-art results on Machine Translation, Text Summarization,\nSentence Splitting, and Sentence Fusion.",
    "num_pages": 17
}