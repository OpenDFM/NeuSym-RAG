{
    "uuid": "172b7dad-d436-5ced-9eb4-0f5702d227b2",
    "title": "The StarCraft Multi-Agent Challenge",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Mikayel Samvelyan",
        "Tabish Rashid",
        "Christian Schroeder de Witt",
        "Gregory Farquhar",
        "Nantas Nardelli",
        "Tim G. J. Rudner",
        "Chia-Man Hung",
        "Philip H. S. Torr",
        "Jakob Foerster",
        "Shimon Whiteson"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.04043v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\172b7dad-d436-5ced-9eb4-0f5702d227b2.pdf",
    "bibtex": "@misc{samvelyan2019thestarcraftmultiagentchallenge,\n    title = {The StarCraft Multi-Agent Challenge},\n    author = {Mikayel Samvelyan and Tabish Rashid and Christian Schroeder de Witt and Gregory Farquhar and Nantas Nardelli and Tim G. J. Rudner and Chia-Man Hung and Philip H. S. Torr and Jakob Foerster and Shimon Whiteson},\n    year = {2019},\n    eprint = {1902.04043},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1902.04043},\n}",
    "abstract": "In the last few years, deep multi-agent reinforcement learning (RL) has\nbecome a highly active area of research. A particularly challenging class of\nproblems in this area is partially observable, cooperative, multi-agent\nlearning, in which teams of agents must learn to coordinate their behaviour\nwhile conditioning only on their private observations. This is an attractive\nresearch area since such problems are relevant to a large number of real-world\nsystems and are also more amenable to evaluation than general-sum problems.\nStandardised environments such as the ALE and MuJoCo have allowed single-agent\nRL to move beyond toy domains, such as grid worlds. However, there is no\ncomparable benchmark for cooperative multi-agent RL. As a result, most papers\nin this field use one-off toy problems, making it difficult to measure real\nprogress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC)\nas a benchmark problem to fill this gap. SMAC is based on the popular real-time\nstrategy game StarCraft II and focuses on micromanagement challenges where each\nunit is controlled by an independent agent that must act based on local\nobservations. We offer a diverse set of challenge maps and recommendations for\nbest practices in benchmarking and evaluations. We also open-source a deep\nmulti-agent RL learning framework including state-of-the-art algorithms. We\nbelieve that SMAC can provide a standard benchmark environment for years to\ncome. Videos of our best agents for several SMAC scenarios are available at:\nhttps://youtu.be/VZ7zmQ_obZ0.",
    "num_pages": 14
}