{
    "uuid": "c8717e4f-8ff0-5152-8082-266d0bb88071",
    "title": "Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Zitian Chen",
        "Yikang Shen",
        "Mingyu Ding",
        "Zhenfang Chen",
        "Hengshuang Zhao",
        "Erik Learned-Miller",
        "Chuang Gan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.08066v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c8717e4f-8ff0-5152-8082-266d0bb88071.pdf",
    "bibtex": "@misc{chen2022modsquaddesigningmixtureofexperts,\n    title = {Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners},\n    author = {Zitian Chen and Yikang Shen and Mingyu Ding and Zhenfang Chen and Hengshuang Zhao and Erik Learned-Miller and Chuang Gan},\n    year = {2022},\n    eprint = {2212.08066},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2212.08066},\n}",
    "abstract": "Optimization in multi-task learning (MTL) is more challenging than\nsingle-task learning (STL), as the gradient from different tasks can be\ncontradictory. When tasks are related, it can be beneficial to share some\nparameters among them (cooperation). However, some tasks require additional\nparameters with expertise in a specific type of data or discrimination\n(specialization). To address the MTL challenge, we propose Mod-Squad, a new\nmodel that is Modularized into groups of experts (a 'Squad'). This structure\nallows us to formalize cooperation and specialization as the process of\nmatching experts and tasks. We optimize this matching process during the\ntraining of a single model. Specifically, we incorporate mixture of experts\n(MoE) layers into a transformer model, with a new loss that incorporates the\nmutual dependence between tasks and experts. As a result, only a small set of\nexperts are activated for each task. This prevents the sharing of the entire\nbackbone model between all tasks, which strengthens the model, especially when\nthe training set size and the number of tasks scale up. More interestingly, for\neach task, we can extract the small set of experts as a standalone model that\nmaintains the same performance as the large model. Extensive experiments on the\nTaskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5\nvision tasks show the superiority of our approach.",
    "num_pages": 15
}