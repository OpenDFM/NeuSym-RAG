{
    "uuid": "01bc60a1-bcb2-58ea-a5e4-8585440a10da",
    "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ansong Ni",
        "Srini Iyer",
        "Dragomir Radev",
        "Ves Stoyanov",
        "Wen-tau Yih",
        "Sida I. Wang",
        "Xi Victoria Lin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2302.08468v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\01bc60a1-bcb2-58ea-a5e4-8585440a10da.pdf",
    "bibtex": "@misc{ni2023leverlearningtoverifylanguagetocode,\n    title = {LEVER: Learning to Verify Language-to-Code Generation with Execution},\n    author = {Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},\n    year = {2023},\n    eprint = {2302.08468},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2302.08468},\n}",
    "abstract": "The advent of large language models trained on code (code LLMs) has led to\nsignificant progress in language-to-code generation. State-of-the-art\napproaches in this area combine LLM decoding with sample pruning and reranking\nusing test cases or heuristics based on the execution results. However, it is\nchallenging to obtain test cases for many real-world language-to-code\napplications, and heuristics cannot well capture the semantic features of the\nexecution results, such as data type and value range, which often indicates the\ncorrectness of the program. In this work, we propose LEVER, a simple approach\nto improve language-to-code generation by learning to verify the generated\nprograms with their execution results. Specifically, we train verifiers to\ndetermine whether a program sampled from the LLMs is correct or not based on\nthe natural language input, the program itself and its execution results. The\nsampled programs are reranked by combining the verification score with the LLM\ngeneration probability, and marginalizing over programs with the same execution\nresults. On four datasets across the domains of table QA, math QA and basic\nPython programming, LEVER consistently improves over the base code LLMs(4.6% to\n10.9% with code-davinci-002) and achieves new state-of-the-art results on all\nof them.",
    "num_pages": 23
}