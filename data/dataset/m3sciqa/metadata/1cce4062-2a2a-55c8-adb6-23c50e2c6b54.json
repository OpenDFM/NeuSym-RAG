{
    "uuid": "1cce4062-2a2a-55c8-adb6-23c50e2c6b54",
    "title": "Prompting Large Language Model for Machine Translation: A Case Study",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Biao Zhang",
        "Barry Haddow",
        "Alexandra Birch"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.07069v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\1cce4062-2a2a-55c8-adb6-23c50e2c6b54.pdf",
    "bibtex": "@misc{zhang2023promptinglargelanguagemodelfor,\n    title = {Prompting Large Language Model for Machine Translation: A Case Study},\n    author = {Biao Zhang and Barry Haddow and Alexandra Birch},\n    year = {2023},\n    eprint = {2301.07069},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.07069},\n}",
    "abstract": "Research on prompting has shown excellent performance with little or even no\nsupervised training across many tasks. However, prompting for machine\ntranslation is still under-explored in the literature. We fill this gap by\noffering a systematic study on prompting strategies for translation, examining\nvarious factors for prompt template and demonstration example selection. We\nfurther explore the use of monolingual data and the feasibility of\ncross-lingual, cross-domain, and sentence-to-document transfer learning in\nprompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the\ntestbed show that 1) the number and the quality of prompt examples matter,\nwhere using suboptimal examples degenerates translation; 2) several features of\nprompt examples, such as semantic similarity, show significant Spearman\ncorrelation with their prompting performance; yet, none of the correlations are\nstrong enough; 3) using pseudo parallel prompt examples constructed from\nmonolingual data via zero-shot prompting could improve translation; and 4)\nimproved performance is achievable by transferring knowledge from prompt\nexamples selected in other settings. We finally provide an analysis on the\nmodel outputs and discuss several problems that prompting still suffers from.",
    "num_pages": 19
}