{
    "uuid": "411227b7-53f4-546d-9c14-70bf07f74473",
    "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shizhe Diao",
        "Wangchunshu Zhou",
        "Xinsong Zhang",
        "Jiawei Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.07699v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\411227b7-53f4-546d-9c14-70bf07f74473.pdf",
    "bibtex": "@misc{diao2023writeandpaintgenerativevisionlanguage,\n    title = {Write and Paint: Generative Vision-Language Models are Unified Modal Learners},\n    author = {Shizhe Diao and Wangchunshu Zhou and Xinsong Zhang and Jiawei Wang},\n    year = {2023},\n    eprint = {2206.07699},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2206.07699},\n}",
    "abstract": "Recent advances in vision-language pre-training have pushed the\nstate-of-the-art on various vision-language tasks, making machines more capable\nof multi-modal writing (image-to-text generation) and painting (text-to-image\ngeneration). However, few studies investigate if these two essential\ncapabilities can be learned together and boost each other, making a versatile\nand powerful multi-modal foundation model. In this work, we disclose the\npotential of symmetric generative vision-language pre-training in learning to\nwrite and paint concurrently, and propose a new unified modal model, named\nDaVinci, trained with prefix language modeling and prefix image modeling, a\nsimple generative self-supervised objective on image-text pairs. Thanks to the\nproposed prefix multi-modal modeling framework, DaVinci is simple to train,\nscalable to huge data, adaptable to both writing and painting tasks, and also\nstrong on other vision, text, and multi-modal understanding tasks. DaVinci\nachieves competitive performance on a wide range of 27 generation/understanding\ntasks and demonstrates the superiority of combining vision/language generative\npre-training. Furthermore, we carefully benchmark the performance of different\nvision-language pre-training objectives on different scales of pre-training\ndatasets on a heterogeneous and broad distribution coverage. Our results\ndemonstrate the potential of exploiting self-supervision in both language and\nvision inputs, and establish new, stronger baselines for future comparisons at\ndifferent data scales. The code and pre-trained models are available at\nhttps://github.com/shizhediao/DaVinci.",
    "num_pages": 25
}