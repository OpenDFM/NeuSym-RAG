{
    "uuid": "26cae332-3ea8-5f0f-854a-b8ed02739d31",
    "title": "Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Santhosh K. Ramakrishnan",
        "Aaron Gokaslan",
        "Erik Wijmans",
        "Oleksandr Maksymets",
        "Alex Clegg",
        "John Turner",
        "Eric Undersander",
        "Wojciech Galuba",
        "Andrew Westbury",
        "Angel X. Chang",
        "Manolis Savva",
        "Yili Zhao",
        "Dhruv Batra"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08238v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\26cae332-3ea8-5f0f-854a-b8ed02739d31.pdf",
    "bibtex": "@misc{ramakrishnan2021habitatmatterport3ddatasethm3d1000,\n    title = {Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI},\n    author = {Santhosh K. Ramakrishnan and Aaron Gokaslan and Erik Wijmans and Oleksandr Maksymets and Alex Clegg and John Turner and Eric Undersander and Wojciech Galuba and Andrew Westbury and Angel X. Chang and Manolis Savva and Yili Zhao and Dhruv Batra},\n    year = {2021},\n    eprint = {2109.08238},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2109.08238},\n}",
    "abstract": "We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale\ndataset of 1,000 building-scale 3D reconstructions from a diverse set of\nreal-world locations. Each scene in the dataset consists of a textured 3D mesh\nreconstruction of interiors such as multi-floor residences, stores, and other\nprivate indoor spaces.\n  HM3D surpasses existing datasets available for academic research in terms of\nphysical scale, completeness of the reconstruction, and visual fidelity. HM3D\ncontains 112.5k m^2 of navigable space, which is 1.4 - 3.7x larger than other\nbuilding-scale datasets such as MP3D and Gibson. When compared to existing\nphotorealistic 3D datasets such as Replica, MP3D, Gibson, and ScanNet, images\nrendered from HM3D have 20 - 85% higher visual fidelity w.r.t. counterpart\nimages captured with real cameras, and HM3D meshes have 34 - 91% fewer\nartifacts due to incomplete surface reconstruction.\n  The increased scale, fidelity, and diversity of HM3D directly impacts the\nperformance of embodied AI agents trained using it. In fact, we find that HM3D\nis `pareto optimal' in the following sense -- agents trained to perform\nPointGoal navigation on HM3D achieve the highest performance regardless of\nwhether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be\nmade about training on other datasets. HM3D-trained PointNav agents achieve\n100% performance on Gibson-test dataset, suggesting that it might be time to\nretire that episode dataset.",
    "num_pages": 21
}