{
    "uuid": "90f84f34-0fc3-5f12-80ee-eff109e02428",
    "title": "SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Luan Thanh Nguyen",
        "Kiet Van Nguyen",
        "Ngan Luu-Thuy Nguyen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2209.10482v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\90f84f34-0fc3-5f12-80ee-eff109e02428.pdf",
    "bibtex": "@misc{nguyen2022smtceasocialmediatext,\n    title = {SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese},\n    author = {Luan Thanh Nguyen and Kiet Van Nguyen and Ngan Luu-Thuy Nguyen},\n    year = {2022},\n    eprint = {2209.10482},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2209.10482},\n}",
    "abstract": "Text classification is a typical natural language processing or computational\nlinguistics task with various interesting applications. As the number of users\non social media platforms increases, data acceleration promotes emerging\nstudies on Social Media Text Classification (SMTC) or social media text mining\non these valuable resources. In contrast to English, Vietnamese, one of the\nlow-resource languages, is still not concentrated on and exploited thoroughly.\nInspired by the success of the GLUE, we introduce the Social Media Text\nClassification Evaluation (SMTCE) benchmark, as a collection of datasets and\nmodels across a diverse set of SMTC tasks. With the proposed benchmark, we\nimplement and analyze the effectiveness of a variety of multilingual BERT-based\nmodels (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models\n(PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark.\nMonolingual models outperform multilingual models and achieve state-of-the-art\nresults on all text classification tasks. It provides an objective assessment\nof multilingual and monolingual BERT-based models on the benchmark, which will\nbenefit future studies about BERTology in the Vietnamese language.",
    "num_pages": 10
}