{
    "uuid": "653e5d86-f991-5533-bd83-a5d95f760b18",
    "title": "Galactica: A Large Language Model for Science",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ross Taylor",
        "Marcin Kardas",
        "Guillem Cucurull",
        "Thomas Scialom",
        "Anthony Hartshorn",
        "Elvis Saravia",
        "Andrew Poulton",
        "Viktor Kerkez",
        "Robert Stojnic"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.09085v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\653e5d86-f991-5533-bd83-a5d95f760b18.pdf",
    "bibtex": "@misc{taylor2022galacticaalargelanguagemodel,\n    title = {Galactica: A Large Language Model for Science},\n    author = {Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},\n    year = {2022},\n    eprint = {2211.09085},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.09085},\n}",
    "abstract": "Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.",
    "num_pages": 58
}