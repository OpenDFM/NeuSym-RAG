{
    "uuid": "36a1c245-8803-586f-a39c-57a149b16892",
    "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yujia Qin",
        "Yankai Lin",
        "Ryuichi Takanobu",
        "Zhiyuan Liu",
        "Peng Li",
        "Heng Ji",
        "Minlie Huang",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15022v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\36a1c245-8803-586f-a39c-57a149b16892.pdf",
    "bibtex": "@misc{qin2021ericaimprovingentityandrelation,\n    title = {ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning},\n    author = {Yujia Qin and Yankai Lin and Ryuichi Takanobu and Zhiyuan Liu and Peng Li and Heng Ji and Minlie Huang and Maosong Sun and Jie Zhou},\n    year = {2021},\n    eprint = {2012.15022},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.15022},\n}",
    "abstract": "Pre-trained Language Models (PLMs) have shown superior performance on various\ndownstream Natural Language Processing (NLP) tasks. However, conventional\npre-training objectives do not explicitly model relational facts in text, which\nare crucial for textual understanding. To address this issue, we propose a\nnovel contrastive learning framework ERICA to obtain a deep understanding of\nthe entities and their relations in text. Specifically, we define two novel\npre-training tasks to better understand entities and relations: (1) the entity\ndiscrimination task to distinguish which tail entity can be inferred by the\ngiven head entity and relation; (2) the relation discrimination task to\ndistinguish whether two relations are close or not semantically, which involves\ncomplex relational reasoning. Experimental results demonstrate that ERICA can\nimprove typical PLMs (BERT and RoBERTa) on several language understanding\ntasks, including relation extraction, entity typing and question answering,\nespecially under low-resource settings.",
    "num_pages": 14
}