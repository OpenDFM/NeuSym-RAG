{
    "uuid": "e53e07a1-6a43-5b5f-908e-3c19e2375eac",
    "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Elias Frantar",
        "Dan Alistarh"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.00774v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\e53e07a1-6a43-5b5f-908e-3c19e2375eac.pdf",
    "bibtex": "@misc{frantar2023sparsegptmassivelanguagemodelscan,\n    title = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},\n    author = {Elias Frantar and Dan Alistarh},\n    year = {2023},\n    eprint = {2301.00774},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2301.00774},\n}",
    "abstract": "We show for the first time that large-scale generative pretrained transformer\n(GPT) family models can be pruned to at least 50% sparsity in one-shot, without\nany retraining, at minimal loss of accuracy. This is achieved via a new pruning\nmethod called SparseGPT, specifically designed to work efficiently and\naccurately on massive GPT-family models. We can execute SparseGPT on the\nlargest available open-source models, OPT-175B and BLOOM-176B, in under 4.5\nhours, and can reach 60% unstructured sparsity with negligible increase in\nperplexity: remarkably, more than 100 billion weights from these models can be\nignored at inference time. SparseGPT generalizes to semi-structured (2:4 and\n4:8) patterns, and is compatible with weight quantization approaches. The code\nis available at: https://github.com/IST-DASLab/sparsegpt.",
    "num_pages": 14
}