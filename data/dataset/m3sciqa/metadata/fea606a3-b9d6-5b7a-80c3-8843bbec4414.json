{
    "uuid": "fea606a3-b9d6-5b7a-80c3-8843bbec4414",
    "title": "Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zdeněk Kasner",
        "Ioannis Konstas",
        "Ondřej Dušek"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.07373v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\fea606a3-b9d6-5b7a-80c3-8843bbec4414.pdf",
    "bibtex": "@misc{kasner2023mindthelabelsdescribingrelations,\n    title = {Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models},\n    author = {Zdeněk Kasner and Ioannis Konstas and Ondřej Dušek},\n    year = {2023},\n    eprint = {2210.07373},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.07373},\n}",
    "abstract": "Pretrained language models (PLMs) for data-to-text (D2T) generation can use\nhuman-readable data labels such as column headings, keys, or relation names to\ngeneralize to out-of-domain examples. However, the models are well-known in\nproducing semantically inaccurate outputs if these labels are ambiguous or\nincomplete, which is often the case in D2T datasets. In this paper, we expose\nthis issue on the task of descibing a relation between two entities. For our\nexperiments, we collect a novel dataset for verbalizing a diverse set of 1,522\nunique relations from three large-scale knowledge graphs (Wikidata, DBPedia,\nYAGO). We find that although PLMs for D2T generation expectedly fail on unclear\ncases, models trained with a large variety of relation labels are surprisingly\nrobust in verbalizing novel, unseen relations. We argue that using data with a\ndiverse set of clear and meaningful labels is key to training D2T generation\nsystems capable of generalizing to novel domains.",
    "num_pages": 18
}