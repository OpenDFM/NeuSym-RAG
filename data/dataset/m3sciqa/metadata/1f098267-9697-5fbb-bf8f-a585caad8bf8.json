{
    "uuid": "1f098267-9697-5fbb-bf8f-a585caad8bf8",
    "title": "Compositional generalization in a deep seq2seq model by separating syntax and semantics",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Jake Russin",
        "Jason Jo",
        "Randall C. O'Reilly",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.09708v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\1f098267-9697-5fbb-bf8f-a585caad8bf8.pdf",
    "bibtex": "@misc{russin2019compositionalgeneralizationinadeep,\n    title = {Compositional generalization in a deep seq2seq model by separating syntax and semantics},\n    author = {Jake Russin and Jason Jo and Randall C. O'Reilly and Yoshua Bengio},\n    year = {2019},\n    eprint = {1904.09708},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1904.09708},\n}",
    "abstract": "Standard methods in deep learning for natural language processing fail to\ncapture the compositional structure of human language that allows for\nsystematic generalization outside of the training distribution. However, human\nlearners readily generalize in this way, e.g. by applying known grammatical\nrules to novel words. Inspired by work in neuroscience suggesting separate\nbrain systems for syntactic and semantic processing, we implement a\nmodification to standard approaches in neural machine translation, imposing an\nanalogous separation. The novel model, which we call Syntactic Attention,\nsubstantially outperforms standard methods in deep learning on the SCAN\ndataset, a compositional generalization task, without any hand-engineered\nfeatures or additional supervision. Our work suggests that separating syntactic\nfrom semantic learning may be a useful heuristic for capturing compositional\nstructure.",
    "num_pages": 18
}