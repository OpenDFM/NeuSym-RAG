{
    "uuid": "91b11d4c-44b0-54c6-83df-fa949b257373",
    "title": "Improving Sequence Tagging for Vietnamese Text Using Transformer-based Neural Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Viet Bui The",
        "Oanh Tran Thi",
        "Phuong Le-Hong"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.15994v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\91b11d4c-44b0-54c6-83df-fa949b257373.pdf",
    "bibtex": "@misc{the2020improvingsequencetaggingforvietnamese,\n    title = {Improving Sequence Tagging for Vietnamese Text Using Transformer-based Neural Models},\n    author = {Viet Bui The and Oanh Tran Thi and Phuong Le-Hong},\n    year = {2020},\n    eprint = {2006.15994},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2006.15994},\n}",
    "abstract": "This paper describes our study on using mutilingual BERT embeddings and some\nnew neural models for improving sequence tagging tasks for the Vietnamese\nlanguage. We propose new model architectures and evaluate them extensively on\ntwo named entity recognition datasets of VLSP 2016 and VLSP 2018, and on two\npart-of-speech tagging datasets of VLSP 2010 and VLSP 2013. Our proposed models\noutperform existing methods and achieve new state-of-the-art results. In\nparticular, we have pushed the accuracy of part-of-speech tagging to 95.40% on\nthe VLSP 2010 corpus, to 96.77% on the VLSP 2013 corpus; and the F1 score of\nnamed entity recognition to 94.07% on the VLSP 2016 corpus, to 90.31% on the\nVLSP 2018 corpus. Our code and pre-trained models viBERT and vELECTRA are\nreleased as open source to facilitate adoption and further research.",
    "num_pages": 8
}