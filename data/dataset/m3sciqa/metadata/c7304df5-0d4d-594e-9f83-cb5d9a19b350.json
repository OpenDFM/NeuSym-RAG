{
    "uuid": "c7304df5-0d4d-594e-9f83-cb5d9a19b350",
    "title": "Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Luyu Gao",
        "Zhuyun Dai",
        "Jamie Callan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.08751v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\c7304df5-0d4d-594e-9f83-cb5d9a19b350.pdf",
    "bibtex": "@misc{gao2021rethinktrainingofbertrerankers,\n    title = {Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline},\n    author = {Luyu Gao and Zhuyun Dai and Jamie Callan},\n    year = {2021},\n    eprint = {2101.08751},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.IR},\n    url = {http://arxiv.org/abs/2101.08751},\n}",
    "abstract": "Pre-trained deep language models~(LM) have advanced the state-of-the-art of\ntext retrieval. Rerankers fine-tuned from deep LM estimates candidate relevance\nbased on rich contextualized matching signals. Meanwhile, deep LMs can also be\nleveraged to improve search index, building retrievers with better recall. One\nwould expect a straightforward combination of both in a pipeline to have\nadditive performance gain. In this paper, we discover otherwise and that\npopular reranker cannot fully exploit the improved retrieval result. We,\ntherefore, propose a Localized Contrastive Estimation (LCE) for training\nrerankers and demonstrate it significantly improves deep two-stage models.",
    "num_pages": 8
}