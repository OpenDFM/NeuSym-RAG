{
    "uuid": "61add12c-1a79-5ef2-a38e-00e843271ad0",
    "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yang Liu",
        "Dan Iter",
        "Yichong Xu",
        "Shuohang Wang",
        "Ruochen Xu",
        "Chenguang Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.16634v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\61add12c-1a79-5ef2-a38e-00e843271ad0.pdf",
    "bibtex": "@misc{liu2023gevalnlgevaluationusinggpt4,\n    title = {G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment},\n    author = {Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},\n    year = {2023},\n    eprint = {2303.16634},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.16634},\n}",
    "abstract": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval",
    "num_pages": 11
}