{
    "uuid": "0b53b7e9-c74a-53e3-911d-f614a18dcb5b",
    "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Weijia Shi",
        "Sewon Min",
        "Michihiro Yasunaga",
        "Minjoon Seo",
        "Rich James",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Wen-tau Yih"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.12652v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\0b53b7e9-c74a-53e3-911d-f614a18dcb5b.pdf",
    "bibtex": "@misc{shi2023replugretrievalaugmentedblackboxlanguagemodels,\n    title = {REPLUG: Retrieval-Augmented Black-Box Language Models},\n    author = {Weijia Shi and Sewon Min and Michihiro Yasunaga and Minjoon Seo and Rich James and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},\n    year = {2023},\n    eprint = {2301.12652},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.12652},\n}",
    "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that\ntreats the language model (LM) as a black box and augments it with a tuneable\nretrieval model. Unlike prior retrieval-augmented LMs that train language\nmodels with special cross attention mechanisms to encode the retrieved text,\nREPLUG simply prepends retrieved documents to the input for the frozen\nblack-box LM. This simple design can be easily applied to any existing\nretrieval and language models. Furthermore, we show that the LM can be used to\nsupervise the retrieval model, which can then find documents that help the LM\nmake better predictions. Our experiments demonstrate that REPLUG with the tuned\nretriever significantly improves the performance of GPT-3 (175B) on language\nmodeling by 6.3%, as well as the performance of Codex on five-shot MMLU by\n5.1%.",
    "num_pages": 12
}