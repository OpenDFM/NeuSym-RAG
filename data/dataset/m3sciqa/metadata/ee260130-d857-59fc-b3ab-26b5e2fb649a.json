{
    "uuid": "ee260130-d857-59fc-b3ab-26b5e2fb649a",
    "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Xisen Jin",
        "Dejiao Zhang",
        "Henghui Zhu",
        "Wei Xiao",
        "Shang-Wen Li",
        "Xiaokai Wei",
        "Andrew Arnold",
        "Xiang Ren"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08534v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ee260130-d857-59fc-b3ab-26b5e2fb649a.pdf",
    "bibtex": "@misc{jin2022lifelongpretrainingcontinuallyadaptinglanguage,\n    title = {Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora},\n    author = {Xisen Jin and Dejiao Zhang and Henghui Zhu and Wei Xiao and Shang-Wen Li and Xiaokai Wei and Andrew Arnold and Xiang Ren},\n    year = {2022},\n    eprint = {2110.08534},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.08534},\n}",
    "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.",
    "num_pages": 18
}