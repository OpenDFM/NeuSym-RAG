{
    "uuid": "9d19a76a-1f8e-5170-b5d7-b2d07ad1c2ec",
    "title": "Large Language Models are Zero-Shot Reasoners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Takeshi Kojima",
        "Shixiang Shane Gu",
        "Machel Reid",
        "Yutaka Matsuo",
        "Yusuke Iwasawa"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.11916v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\9d19a76a-1f8e-5170-b5d7-b2d07ad1c2ec.pdf",
    "bibtex": "@misc{kojima2023largelanguagemodelsarezeroshot,\n    title = {Large Language Models are Zero-Shot Reasoners},\n    author = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},\n    year = {2023},\n    eprint = {2205.11916},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.11916},\n}",
    "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.",
    "num_pages": 42
}