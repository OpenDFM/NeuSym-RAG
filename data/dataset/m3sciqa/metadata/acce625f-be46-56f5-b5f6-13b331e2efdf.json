{
    "uuid": "acce625f-be46-56f5-b5f6-13b331e2efdf",
    "title": "Low-bit Quantization of Neural Networks for Efficient Inference",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yoni Choukroun",
        "Eli Kravchik",
        "Fan Yang",
        "Pavel Kisilev"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.06822v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\acce625f-be46-56f5-b5f6-13b331e2efdf.pdf",
    "bibtex": "@misc{choukroun2019lowbitquantizationofneuralnetworks,\n    title = {Low-bit Quantization of Neural Networks for Efficient Inference},\n    author = {Yoni Choukroun and Eli Kravchik and Fan Yang and Pavel Kisilev},\n    year = {2019},\n    eprint = {1902.06822},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1902.06822},\n}",
    "abstract": "Recent machine learning methods use increasingly large deep neural networks\nto achieve state of the art results in various tasks. The gains in performance\ncome at the cost of a substantial increase in computation and storage\nrequirements. This makes real-time implementations on limited resources\nhardware a challenging task. One popular approach to address this challenge is\nto perform low-bit precision computations via neural network quantization.\nHowever, aggressive quantization generally entails a severe penalty in terms of\naccuracy, and often requires retraining of the network, or resorting to higher\nbit precision quantization. In this paper, we formalize the linear quantization\ntask as a Minimum Mean Squared Error (MMSE) problem for both weights and\nactivations, allowing low-bit precision inference without the need for full\nnetwork retraining. The main contributions of our approach are the\noptimizations of the constrained MSE problem at each layer of the network, the\nhardware aware partitioning of the network parameters, and the use of multiple\nlow precision quantized tensors for poorly approximated layers. The proposed\napproach allows 4 bits integer (INT4) quantization for deployment of pretrained\nmodels on limited hardware resources. Multiple experiments on various network\narchitectures show that the suggested method yields state of the art results\nwith minimal loss of tasks accuracy.",
    "num_pages": 10
}