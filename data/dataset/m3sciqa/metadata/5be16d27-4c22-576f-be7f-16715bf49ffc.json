{
    "uuid": "5be16d27-4c22-576f-be7f-16715bf49ffc",
    "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jiacheng Chen",
        "Hexiang Hu",
        "Hao Wu",
        "Yuning Jiang",
        "Changhu Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04305v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\5be16d27-4c22-576f-be7f-16715bf49ffc.pdf",
    "bibtex": "@misc{chen2021learningthebestpoolingstrategy,\n    title = {Learning the Best Pooling Strategy for Visual Semantic Embedding},\n    author = {Jiacheng Chen and Hexiang Hu and Hao Wu and Yuning Jiang and Changhu Wang},\n    year = {2021},\n    eprint = {2011.04305},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2011.04305},\n}",
    "abstract": "Visual Semantic Embedding (VSE) is a dominant approach for vision-language\nretrieval, which aims at learning a deep embedding space such that visual data\nare embedded close to their semantic text labels or descriptions. Recent VSE\nmodels use complex methods to better contextualize and aggregate multi-modal\nfeatures into holistic embeddings. However, we discover that surprisingly\nsimple (but carefully selected) global pooling functions (e.g., max pooling)\noutperform those complex models, across different feature extractors. Despite\nits simplicity and effectiveness, seeking the best pooling function for\ndifferent data modality and feature extractor is costly and tedious, especially\nwhen the size of features varies (e.g., text, video). Therefore, we propose a\nGeneralized Pooling Operator (GPO), which learns to automatically adapt itself\nto the best pooling strategy for different features, requiring no manual tuning\nwhile staying effective and efficient. We extend the VSE model using this\nproposed GPO and denote it as VSE$\\infty$.\n  Without bells and whistles, VSE$\\infty$ outperforms previous VSE methods\nsignificantly on image-text retrieval benchmarks across popular feature\nextractors. With a simple adaptation, variants of VSE$\\infty$ further\ndemonstrate its strength by achieving the new state of the art on two\nvideo-text retrieval datasets. Comprehensive experiments and visualizations\nconfirm that GPO always discovers the best pooling strategy and can be a\nplug-and-play feature aggregation module for standard VSE models. Code and\npre-trained models are available at https://vse-infty.github.io.",
    "num_pages": 15
}