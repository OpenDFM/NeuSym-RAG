{
    "uuid": "157c2429-46a5-5b91-bb21-999bfcf07dfa",
    "title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Wang Zhu",
        "Hexiang Hu",
        "Jiacheng Chen",
        "Zhiwei Deng",
        "Vihan Jain",
        "Eugene Ie",
        "Fei Sha"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.04625v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\157c2429-46a5-5b91-bb21-999bfcf07dfa.pdf",
    "bibtex": "@misc{zhu2020babywalkgoingfartherinvisionandlanguage,\n    title = {BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps},\n    author = {Wang Zhu and Hexiang Hu and Jiacheng Chen and Zhiwei Deng and Vihan Jain and Eugene Ie and Fei Sha},\n    year = {2020},\n    eprint = {2005.04625},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2005.04625},\n}",
    "abstract": "Learning to follow instructions is of fundamental importance to autonomous\nagents for vision-and-language navigation (VLN). In this paper, we study how an\nagent can navigate long paths when learning from a corpus that consists of\nshorter ones. We show that existing state-of-the-art agents do not generalize\nwell. To this end, we propose BabyWalk, a new VLN agent that is learned to\nnavigate by decomposing long instructions into shorter ones (BabySteps) and\ncompleting them sequentially. A special design memory buffer is used by the\nagent to turn its past experiences into contexts for future steps. The learning\nprocess is composed of two phases. In the first phase, the agent uses imitation\nlearning from demonstration to accomplish BabySteps. In the second phase, the\nagent uses curriculum-based reinforcement learning to maximize rewards on\nnavigation tasks with increasingly longer instructions. We create two new\nbenchmark datasets (of long navigation tasks) and use them in conjunction with\nexisting ones to examine BabyWalk's generalization ability. Empirical results\nshow that BabyWalk achieves state-of-the-art results on several metrics, in\nparticular, is able to follow long instructions better. The codes and the\ndatasets are released on our project page https://github.com/Sha-Lab/babywalk.",
    "num_pages": 18
}