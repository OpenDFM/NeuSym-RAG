{
    "uuid": "dc69ab19-1148-5de0-88be-2c610277ffd8",
    "title": "Diversifying Neural Dialogue Generation via Negative Distillation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yiwei Li",
        "Shaoxiong Feng",
        "Bin Sun",
        "Kan Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.02795v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\dc69ab19-1148-5de0-88be-2c610277ffd8.pdf",
    "bibtex": "@misc{li2022diversifyingneuraldialoguegenerationvia,\n    title = {Diversifying Neural Dialogue Generation via Negative Distillation},\n    author = {Yiwei Li and Shaoxiong Feng and Bin Sun and Kan Li},\n    year = {2022},\n    eprint = {2205.02795},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.02795},\n}",
    "abstract": "Generative dialogue models suffer badly from the generic response problem,\nlimiting their applications to a few toy scenarios. Recently, an interesting\napproach, namely negative training, has been proposed to alleviate this problem\nby reminding the model not to generate high-frequency responses during\ntraining. However, its performance is hindered by two issues, ignoring\nlow-frequency but generic responses and bringing low-frequency but meaningless\nresponses. In this paper, we propose a novel negative training paradigm, called\nnegative distillation, to keep the model away from the undesirable generic\nresponses while avoiding the above problems. First, we introduce a negative\nteacher model that can produce query-wise generic responses, and then the\nstudent model is required to maximize the distance with multi-level negative\nknowledge. Empirical results show that our method outperforms previous negative\ntraining methods significantly.",
    "num_pages": 12
}