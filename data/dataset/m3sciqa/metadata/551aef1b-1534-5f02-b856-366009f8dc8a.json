{
    "uuid": "551aef1b-1534-5f02-b856-366009f8dc8a",
    "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Mor Geva",
        "Roei Schuster",
        "Jonathan Berant",
        "Omer Levy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.14913v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\551aef1b-1534-5f02-b856-366009f8dc8a.pdf",
    "bibtex": "@misc{geva2021transformerfeedforwardlayersarekeyvalue,\n    title = {Transformer Feed-Forward Layers Are Key-Value Memories},\n    author = {Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},\n    year = {2021},\n    eprint = {2012.14913},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.14913},\n}",
    "abstract": "Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.",
    "num_pages": 12
}