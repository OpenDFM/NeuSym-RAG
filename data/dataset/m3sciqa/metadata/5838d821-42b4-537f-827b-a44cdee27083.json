{
    "uuid": "5838d821-42b4-537f-827b-a44cdee27083",
    "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Swabha Swayamdipta",
        "Roy Schwartz",
        "Nicholas Lourie",
        "Yizhong Wang",
        "Hannaneh Hajishirzi",
        "Noah A. Smith",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.10795v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\5838d821-42b4-537f-827b-a44cdee27083.pdf",
    "bibtex": "@misc{swayamdipta2020datasetcartographymappinganddiagnosing,\n    title = {Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},\n    author = {Swabha Swayamdipta and Roy Schwartz and Nicholas Lourie and Yizhong Wang and Hannaneh Hajishirzi and Noah A. Smith and Yejin Choi},\n    year = {2020},\n    eprint = {2009.10795},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.10795},\n}",
    "abstract": "Large datasets have become commonplace in NLP research. However, the\nincreased emphasis on data quantity has made it challenging to assess the\nquality of data. We introduce Data Maps---a model-based tool to characterize\nand diagnose datasets. We leverage a largely ignored source of information: the\nbehavior of the model on individual instances during training (training\ndynamics) for building data maps. This yields two intuitive measures for each\nexample---the model's confidence in the true class, and the variability of this\nconfidence across epochs---obtained in a single run of training. Experiments\nacross four datasets show that these model-dependent measures reveal three\ndistinct regions in the data map, each with pronounced characteristics. First,\nour data maps show the presence of \"ambiguous\" regions with respect to the\nmodel, which contribute the most towards out-of-distribution generalization.\nSecond, the most populous regions in the data are \"easy to learn\" for the\nmodel, and play an important role in model optimization. Finally, data maps\nuncover a region with instances that the model finds \"hard to learn\"; these\noften correspond to labeling errors. Our results indicate that a shift in focus\nfrom quantity to quality of data could lead to robust models and improved\nout-of-distribution generalization.",
    "num_pages": 19
}