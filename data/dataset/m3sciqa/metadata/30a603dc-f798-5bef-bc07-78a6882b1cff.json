{
    "uuid": "30a603dc-f798-5bef-bc07-78a6882b1cff",
    "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Adam Roberts",
        "Colin Raffel",
        "Noam Shazeer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08910v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\30a603dc-f798-5bef-bc07-78a6882b1cff.pdf",
    "bibtex": "@misc{roberts2020howmuchknowledgecanyou,\n    title = {How Much Knowledge Can You Pack Into the Parameters of a Language Model?},\n    author = {Adam Roberts and Colin Raffel and Noam Shazeer},\n    year = {2020},\n    eprint = {2002.08910},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2002.08910},\n}",
    "abstract": "It has recently been observed that neural language models trained on\nunstructured text can implicitly store and retrieve knowledge using natural\nlanguage queries. In this short paper, we measure the practical utility of this\napproach by fine-tuning pre-trained models to answer questions without access\nto any external context or knowledge. We show that this approach scales with\nmodel size and performs competitively with open-domain systems that explicitly\nretrieve answers from an external knowledge source when answering questions. To\nfacilitate reproducibility and future work, we release our code and trained\nmodels at https://goo.gle/t5-cbqa.",
    "num_pages": 9
}