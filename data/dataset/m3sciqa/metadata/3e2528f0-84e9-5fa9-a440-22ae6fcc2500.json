{
    "uuid": "3e2528f0-84e9-5fa9-a440-22ae6fcc2500",
    "title": "Generating Natural Questions About an Image",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Nasrin Mostafazadeh",
        "Ishan Misra",
        "Jacob Devlin",
        "Margaret Mitchell",
        "Xiaodong He",
        "Lucy Vanderwende"
    ],
    "pdf_url": "http://arxiv.org/pdf/1603.06059v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\3e2528f0-84e9-5fa9-a440-22ae6fcc2500.pdf",
    "bibtex": "@misc{mostafazadeh2016generatingnaturalquestionsaboutan,\n    title = {Generating Natural Questions About an Image},\n    author = {Nasrin Mostafazadeh and Ishan Misra and Jacob Devlin and Margaret Mitchell and Xiaodong He and Lucy Vanderwende},\n    year = {2016},\n    eprint = {1603.06059},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1603.06059},\n}",
    "abstract": "There has been an explosion of work in the vision & language community during\nthe past few years from image captioning to video transcription, and answering\nquestions about images. These tasks have focused on literal descriptions of the\nimage. To move beyond the literal, we choose to explore how questions about an\nimage are often directed at commonsense inference and the abstract events\nevoked by objects in the image. In this paper, we introduce the novel task of\nVisual Question Generation (VQG), where the system is tasked with asking a\nnatural and engaging question when shown an image. We provide three datasets\nwhich cover a variety of images from object-centric to event-centric, with\nconsiderably more abstract training data than provided to state-of-the-art\ncaptioning systems thus far. We train and test several generative and retrieval\nmodels to tackle the task of VQG. Evaluation results show that while such\nmodels ask reasonable questions for a variety of images, there is still a wide\ngap with human performance which motivates further work on connecting images\nwith commonsense knowledge and pragmatics. Our proposed task offers a new\nchallenge to the community which we hope furthers interest in exploring deeper\nconnections between vision & language.",
    "num_pages": 12
}