{
    "uuid": "612006a5-2d37-53ab-ac56-7de3ead0ace4",
    "title": "SummEval: Re-evaluating Summarization Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Alexander R. Fabbri",
        "Wojciech Kryściński",
        "Bryan McCann",
        "Caiming Xiong",
        "Richard Socher",
        "Dragomir Radev"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.12626v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\612006a5-2d37-53ab-ac56-7de3ead0ace4.pdf",
    "bibtex": "@misc{fabbri2021summevalreevaluatingsummarizationevaluation,\n    title = {SummEval: Re-evaluating Summarization Evaluation},\n    author = {Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},\n    year = {2021},\n    eprint = {2007.12626},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2007.12626},\n}",
    "abstract": "The scarcity of comprehensive up-to-date studies on evaluation metrics for\ntext summarization and the lack of consensus regarding evaluation protocols\ncontinue to inhibit progress. We address the existing shortcomings of\nsummarization evaluation methods along five dimensions: 1) we re-evaluate 14\nautomatic evaluation metrics in a comprehensive and consistent fashion using\nneural summarization model outputs along with expert and crowd-sourced human\nannotations, 2) we consistently benchmark 23 recent summarization models using\nthe aforementioned automatic evaluation metrics, 3) we assemble the largest\ncollection of summaries generated by models trained on the CNN/DailyMail news\ndataset and share it in a unified format, 4) we implement and share a toolkit\nthat provides an extensible and unified API for evaluating summarization models\nacross a broad range of automatic metrics, 5) we assemble and share the largest\nand most diverse, in terms of model types, collection of human judgments of\nmodel-generated summaries on the CNN/Daily Mail dataset annotated by both\nexpert judges and crowd-source workers. We hope that this work will help\npromote a more complete evaluation protocol for text summarization as well as\nadvance research in developing evaluation metrics that better correlate with\nhuman judgments.",
    "num_pages": 18
}