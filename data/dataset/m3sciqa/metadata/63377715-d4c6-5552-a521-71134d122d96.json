{
    "uuid": "63377715-d4c6-5552-a521-71134d122d96",
    "title": "Learning to Compress Prompts with Gist Tokens",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Jesse Mu",
        "Xiang Lisa Li",
        "Noah Goodman"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.08467v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\63377715-d4c6-5552-a521-71134d122d96.pdf",
    "bibtex": "@misc{mu2024learningtocompresspromptswith,\n    title = {Learning to Compress Prompts with Gist Tokens},\n    author = {Jesse Mu and Xiang Lisa Li and Noah Goodman},\n    year = {2024},\n    eprint = {2304.08467},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.08467},\n}",
    "abstract": "Prompting is the primary way to utilize the multitask capabilities of\nlanguage models (LMs), but prompts occupy valuable space in the input context\nwindow, and repeatedly encoding the same prompt is computationally inefficient.\nFinetuning and distillation methods allow for specialization of LMs without\nprompting, but require retraining the model for each task. To avoid this\ntrade-off entirely, we present gisting, which trains an LM to compress prompts\ninto smaller sets of \"gist\" tokens which can be cached and reused for compute\nefficiency. Gist models can be trained with no additional cost over standard\ninstruction finetuning by simply modifying Transformer attention masks to\nencourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder\n(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting\nin up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,\nall with minimal loss in output quality.",
    "num_pages": 26
}