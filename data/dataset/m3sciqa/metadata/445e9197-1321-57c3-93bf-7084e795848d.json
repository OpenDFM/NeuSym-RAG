{
    "uuid": "445e9197-1321-57c3-93bf-7084e795848d",
    "title": "Deep reinforcement learning from human preferences",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Paul Christiano",
        "Jan Leike",
        "Tom B. Brown",
        "Miljan Martic",
        "Shane Legg",
        "Dario Amodei"
    ],
    "pdf_url": "http://arxiv.org/pdf/1706.03741v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\445e9197-1321-57c3-93bf-7084e795848d.pdf",
    "bibtex": "@misc{christiano2023deepreinforcementlearningfromhuman,\n    title = {Deep reinforcement learning from human preferences},\n    author = {Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},\n    year = {2023},\n    eprint = {1706.03741},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1706.03741},\n}",
    "abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.",
    "num_pages": 17
}