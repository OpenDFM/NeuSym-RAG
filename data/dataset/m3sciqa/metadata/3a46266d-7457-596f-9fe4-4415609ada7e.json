{
    "uuid": "3a46266d-7457-596f-9fe4-4415609ada7e",
    "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Qianhui Wu",
        "Huiqiang Jiang",
        "Haonan Yin",
        "Börje F. Karlsson",
        "Chin-Yew Lin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.11300v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\3a46266d-7457-596f-9fe4-4415609ada7e.pdf",
    "bibtex": "@misc{wu2023multilevelknowledgedistillationforoutofdistribution,\n    title = {Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text},\n    author = {Qianhui Wu and Huiqiang Jiang and Haonan Yin and Börje F. Karlsson and Chin-Yew Lin},\n    year = {2023},\n    eprint = {2211.11300},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.11300},\n}",
    "abstract": "Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.",
    "num_pages": 14
}