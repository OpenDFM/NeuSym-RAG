{
    "uuid": "aee71259-0a10-5694-bac5-7cbb85b3cba6",
    "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Liangming Pan",
        "Michael Saxon",
        "Wenda Xu",
        "Deepak Nathani",
        "Xinyi Wang",
        "William Yang Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2308.03188v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\aee71259-0a10-5694-bac5-7cbb85b3cba6.pdf",
    "bibtex": "@misc{pan2023automaticallycorrectinglargelanguagemodels,\n    title = {Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies},\n    author = {Liangming Pan and Michael Saxon and Wenda Xu and Deepak Nathani and Xinyi Wang and William Yang Wang},\n    year = {2023},\n    eprint = {2308.03188},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2308.03188},\n}",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\na wide array of NLP tasks. However, their efficacy is undermined by undesired\nand inconsistent behaviors, including hallucination, unfaithful reasoning, and\ntoxic content. A promising approach to rectify these flaws is self-correction,\nwhere the LLM itself is prompted or guided to fix problems in its own output.\nTechniques leveraging automated feedback -- either produced by the LLM itself\nor some external system -- are of particular interest as they are a promising\nway to make LLM-based solutions more practical and deployable with minimal\nhuman feedback. This paper presents a comprehensive review of this emerging\nclass of techniques. We analyze and taxonomize a wide array of recent work\nutilizing these strategies, including training-time, generation-time, and\npost-hoc correction. We also summarize the major applications of this strategy\nand conclude by discussing future directions and challenges.",
    "num_pages": 24
}