{
    "uuid": "e45897f5-4429-5750-a8fb-dcfa9a904b5f",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter J. Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.10683v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\e45897f5-4429-5750-a8fb-dcfa9a904b5f.pdf",
    "bibtex": "@misc{raffel2023exploringthelimitsoftransfer,\n    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n    year = {2023},\n    eprint = {1910.10683},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1910.10683},\n}",
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task\nbefore being fine-tuned on a downstream task, has emerged as a powerful\ntechnique in natural language processing (NLP). The effectiveness of transfer\nlearning has given rise to a diversity of approaches, methodology, and\npractice. In this paper, we explore the landscape of transfer learning\ntechniques for NLP by introducing a unified framework that converts all\ntext-based language problems into a text-to-text format. Our systematic study\ncompares pre-training objectives, architectures, unlabeled data sets, transfer\napproaches, and other factors on dozens of language understanding tasks. By\ncombining the insights from our exploration with scale and our new ``Colossal\nClean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks\ncovering summarization, question answering, text classification, and more. To\nfacilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.",
    "num_pages": 67
}