{
    "uuid": "5e549c77-a92b-5ee7-85bc-5a30491d7767",
    "title": "Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Siliang Tang",
        "Yuan Zhang",
        "Wenjie Liu",
        "Edward Lin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.13252v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\5e549c77-a92b-5ee7-85bc-5a30491d7767.pdf",
    "bibtex": "@misc{shen2022maskthecorrecttokensan,\n    title = {Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction},\n    author = {Kai Shen and Yichong Leng and Xu Tan and Siliang Tang and Yuan Zhang and Wenjie Liu and Edward Lin},\n    year = {2022},\n    eprint = {2211.13252},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.13252},\n}",
    "abstract": "Text error correction aims to correct the errors in text sequences such as\nthose typed by humans or generated by speech recognition models. Previous error\ncorrection methods usually take the source (incorrect) sentence as encoder\ninput and generate the target (correct) sentence through the decoder. Since the\nerror rate of the incorrect sentence is usually low (e.g., 10\\%), the\ncorrection model can only learn to correct on limited error tokens but\ntrivially copy on most tokens (correct tokens), which harms the effective\ntraining of error correction. In this paper, we argue that the correct tokens\nshould be better utilized to facilitate effective training and then propose a\nsimple yet effective masking strategy to achieve this goal. Specifically, we\nrandomly mask out a part of the correct tokens in the source sentence and let\nthe model learn to not only correct the original error tokens but also predict\nthe masked tokens based on their context information. Our method enjoys several\nadvantages: 1) it alleviates trivial copy; 2) it leverages effective training\nsignals from correct tokens; 3) it is a plug-and-play module and can be applied\nto different models and tasks. Experiments on spelling error correction and\nspeech recognition error correction on Mandarin datasets and grammar error\ncorrection on English datasets with both autoregressive and non-autoregressive\ngeneration models show that our method improves the correction accuracy\nconsistently.",
    "num_pages": 13
}