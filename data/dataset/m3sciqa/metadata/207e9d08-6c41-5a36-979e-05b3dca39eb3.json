{
    "uuid": "207e9d08-6c41-5a36-979e-05b3dca39eb3",
    "title": "Neural Architectures for Named Entity Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Guillaume Lample",
        "Miguel Ballesteros",
        "Sandeep Subramanian",
        "Kazuya Kawakami",
        "Chris Dyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/1603.01360v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\207e9d08-6c41-5a36-979e-05b3dca39eb3.pdf",
    "bibtex": "@misc{lample2016neuralarchitecturesfornamedentity,\n    title = {Neural Architectures for Named Entity Recognition},\n    author = {Guillaume Lample and Miguel Ballesteros and Sandeep Subramanian and Kazuya Kawakami and Chris Dyer},\n    year = {2016},\n    eprint = {1603.01360},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1603.01360},\n}",
    "abstract": "State-of-the-art named entity recognition systems rely heavily on\nhand-crafted features and domain-specific knowledge in order to learn\neffectively from the small, supervised training corpora that are available. In\nthis paper, we introduce two new neural architectures---one based on\nbidirectional LSTMs and conditional random fields, and the other that\nconstructs and labels segments using a transition-based approach inspired by\nshift-reduce parsers. Our models rely on two sources of information about\nwords: character-based word representations learned from the supervised corpus\nand unsupervised word representations learned from unannotated corpora. Our\nmodels obtain state-of-the-art performance in NER in four languages without\nresorting to any language-specific knowledge or resources such as gazetteers.",
    "num_pages": 11
}