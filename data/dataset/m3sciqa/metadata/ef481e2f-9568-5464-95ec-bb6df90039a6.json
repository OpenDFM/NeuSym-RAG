{
    "uuid": "ef481e2f-9568-5464-95ec-bb6df90039a6",
    "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zichun Yu",
        "Chenyan Xiong",
        "Shi Yu",
        "Zhiyuan Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.17331v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ef481e2f-9568-5464-95ec-bb6df90039a6.pdf",
    "bibtex": "@misc{yu2023augmentationadaptedretrieverimprovesgeneralizationof,\n    title = {Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In},\n    author = {Zichun Yu and Chenyan Xiong and Shi Yu and Zhiyuan Liu},\n    year = {2023},\n    eprint = {2305.17331},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.17331},\n}",
    "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive\ntasks by supplying them with external information. Prior works on retrieval\naugmentation usually jointly fine-tune the retriever and the LM, making them\nclosely coupled. In this paper, we explore the scheme of generic retrieval\nplug-in: the retriever is to assist target LMs that may not be known beforehand\nor are unable to be fine-tuned together. To retrieve useful documents for\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\nlearns LM's preferences obtained from a known source LM. Experiments on the\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\nis able to significantly improve the zero-shot generalization of larger target\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\nthat the preferences of different LMs overlap, enabling AAR trained with a\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
    "num_pages": 14
}