{
    "uuid": "8bbd5e00-c644-585d-97de-3b42b56bad8e",
    "title": "Adaptive Input Representations for Neural Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Alexei Baevski",
        "Michael Auli"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.10853v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\8bbd5e00-c644-585d-97de-3b42b56bad8e.pdf",
    "bibtex": "@misc{baevski2019adaptiveinputrepresentationsforneural,\n    title = {Adaptive Input Representations for Neural Language Modeling},\n    author = {Alexei Baevski and Michael Auli},\n    year = {2019},\n    eprint = {1809.10853},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1809.10853},\n}",
    "abstract": "We introduce adaptive input representations for neural language modeling\nwhich extend the adaptive softmax of Grave et al. (2017) to input\nrepresentations of variable capacity. There are several choices on how to\nfactorize the input and output layers, and whether to model words, characters\nor sub-word units. We perform a systematic comparison of popular choices for a\nself-attentional architecture. Our experiments show that models equipped with\nadaptive embeddings are more than twice as fast to train than the popular\ncharacter input CNN while having a lower number of parameters. On the\nWikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5\nperplexity compared to the previously best published result and on the Billion\nWord benchmark, we achieve 23.02 perplexity.",
    "num_pages": 13
}