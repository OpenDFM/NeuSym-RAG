{
    "uuid": "f0e5dee8-1409-5d41-9e15-e42bbe9c54ce",
    "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Maarten Sap",
        "Swabha Swayamdipta",
        "Laura Vianna",
        "Xuhui Zhou",
        "Yejin Choi",
        "Noah A. Smith"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.07997v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\f0e5dee8-1409-5d41-9e15-e42bbe9c54ce.pdf",
    "bibtex": "@misc{sap2022annotatorswithattitudeshowannotator,\n    title = {Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection},\n    author = {Maarten Sap and Swabha Swayamdipta and Laura Vianna and Xuhui Zhou and Yejin Choi and Noah A. Smith},\n    year = {2022},\n    eprint = {2111.07997},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2111.07997},\n}",
    "abstract": "The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.",
    "num_pages": 23
}