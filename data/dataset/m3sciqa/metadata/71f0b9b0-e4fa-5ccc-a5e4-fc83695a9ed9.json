{
    "uuid": "71f0b9b0-e4fa-5ccc-a5e4-fc83695a9ed9",
    "title": "Semiparametric Language Models Are Scalable Continual Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Guangyue Peng",
        "Tao Ge",
        "Si-Qing Chen",
        "Furu Wei",
        "Houfeng Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.01421v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\71f0b9b0-e4fa-5ccc-a5e4-fc83695a9ed9.pdf",
    "bibtex": "@misc{peng2023semiparametriclanguagemodelsarescalable,\n    title = {Semiparametric Language Models Are Scalable Continual Learners},\n    author = {Guangyue Peng and Tao Ge and Si-Qing Chen and Furu Wei and Houfeng Wang},\n    year = {2023},\n    eprint = {2303.01421},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.01421},\n}",
    "abstract": "Semiparametric language models (LMs) have shown promise in continuously\nlearning from new text data by combining a parameterized neural LM with a\ngrowable non-parametric memory for memorizing new content. However,\nconventional semiparametric LMs will finally become prohibitive for computing\nand storing if they are applied to continual learning over streaming data,\nbecause the non-parametric memory grows linearly with the amount of data they\nlearn from over time. To address the issue of scalability, we present a simple\nand intuitive approach called Selective Memorization (SeMem), which only\nmemorizes difficult samples that the model is likely to struggle with. We\ndemonstrate that SeMem improves the scalability of semiparametric LMs for\ncontinual learning over streaming data in two ways: (1) data-wise scalability:\nas the model becomes stronger through continual learning, it will encounter\nfewer difficult cases that need to be memorized, causing the growth of the\nnon-parametric memory to slow down over time rather than growing at a linear\nrate with the size of training data; (2) model-wise scalability: SeMem allows a\nlarger model to memorize fewer samples than its smaller counterpart because it\nis rarer for a larger model to encounter incomprehensible cases, resulting in a\nnon-parametric memory that does not scale linearly with model size. We conduct\nextensive experiments in language modeling and downstream tasks to test SeMem's\nresults, showing SeMem enables a semiparametric LM to be a scalable continual\nlearner with little forgetting.",
    "num_pages": 15
}