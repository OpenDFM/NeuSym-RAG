{
    "uuid": "30401aad-d103-567d-bdd2-afd4af2815e2",
    "title": "Lifelong Language Pretraining with Distribution-Specialized Experts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Wuyang Chen",
        "Yanqi Zhou",
        "Nan Du",
        "Yanping Huang",
        "James Laudon",
        "Zhifeng Chen",
        "Claire Cu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.12281v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\30401aad-d103-567d-bdd2-afd4af2815e2.pdf",
    "bibtex": "@misc{chen2023lifelonglanguagepretrainingwithdistributionspecialized,\n    title = {Lifelong Language Pretraining with Distribution-Specialized Experts},\n    author = {Wuyang Chen and Yanqi Zhou and Nan Du and Yanping Huang and James Laudon and Zhifeng Chen and Claire Cu},\n    year = {2023},\n    eprint = {2305.12281},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.12281},\n}",
    "abstract": "Pretraining on a large-scale corpus has become a standard method to build\ngeneral language models (LMs). Adapting a model to new data distributions\ntargeting different downstream tasks poses significant challenges. Naive\nfine-tuning may incur catastrophic forgetting when the over-parameterized LMs\noverfit the new data but fail to preserve the pretrained features. Lifelong\nlearning (LLL) aims to enable information systems to learn from a continuous\ndata stream across time. However, most prior work modifies the training recipe\nassuming a static fixed network architecture. We find that additional model\ncapacity and proper regularization are key elements to achieving strong LLL\nperformance. Thus, we propose Lifelong-MoE, an extensible MoE\n(Mixture-of-Experts) architecture that dynamically adds model capacity via\nadding experts with regularized pretraining. Our results show that by only\nintroducing a limited number of extra experts while keeping the computation\ncost constant, our model can steadily adapt to data distribution shifts while\npreserving the previous knowledge. Compared to existing lifelong learning\napproaches, Lifelong-MoE achieves better few-shot performance on 19 downstream\nNLP tasks.",
    "num_pages": 14
}