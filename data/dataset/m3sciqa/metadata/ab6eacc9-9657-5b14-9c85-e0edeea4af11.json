{
    "uuid": "ab6eacc9-9657-5b14-9c85-e0edeea4af11",
    "title": "Measuring Compositional Generalization: A Comprehensive Method on Realistic Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Daniel Keysers",
        "Nathanael Schärli",
        "Nathan Scales",
        "Hylke Buisman",
        "Daniel Furrer",
        "Sergii Kashubin",
        "Nikola Momchev",
        "Danila Sinopalnikov",
        "Lukasz Stafiniak",
        "Tibor Tihon",
        "Dmitry Tsarkov",
        "Xiao Wang",
        "Marc van Zee",
        "Olivier Bousquet"
    ],
    "pdf_url": "http://arxiv.org/pdf/1912.09713v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\ab6eacc9-9657-5b14-9c85-e0edeea4af11.pdf",
    "bibtex": "@misc{keysers2020measuringcompositionalgeneralizationacomprehensive,\n    title = {Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},\n    author = {Daniel Keysers and Nathanael Schärli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},\n    year = {2020},\n    eprint = {1912.09713},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1912.09713},\n}",
    "abstract": "State-of-the-art machine learning methods exhibit limited compositional\ngeneralization. At the same time, there is a lack of realistic benchmarks that\ncomprehensively measure this ability, which makes it challenging to find and\nevaluate improvements. We introduce a novel method to systematically construct\nsuch benchmarks by maximizing compound divergence while guaranteeing a small\natom divergence between train and test sets, and we quantitatively compare this\nmethod to other approaches for creating compositional generalization\nbenchmarks. We present a large and realistic natural language question\nanswering dataset that is constructed according to this method, and we use it\nto analyze the compositional generalization ability of three machine learning\narchitectures. We find that they fail to generalize compositionally and that\nthere is a surprisingly strong negative correlation between compound divergence\nand accuracy. We also demonstrate how our method can be used to create new\ncompositionality benchmarks on top of the existing SCAN dataset, which confirms\nthese findings.",
    "num_pages": 38
}