{
    "uuid": "d95266e3-9e89-592a-b1fe-b55d3279249d",
    "title": "Recurrent Neural Network Regularization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Wojciech Zaremba",
        "Ilya Sutskever",
        "Oriol Vinyals"
    ],
    "pdf_url": "http://arxiv.org/pdf/1409.2329v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\d95266e3-9e89-592a-b1fe-b55d3279249d.pdf",
    "bibtex": "@misc{zaremba2015recurrentneuralnetworkregularization,\n    title = {Recurrent Neural Network Regularization},\n    author = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},\n    year = {2015},\n    eprint = {1409.2329},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.NE},\n    url = {http://arxiv.org/abs/1409.2329},\n}",
    "abstract": "We present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful\ntechnique for regularizing neural networks, does not work well with RNNs and\nLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show\nthat it substantially reduces overfitting on a variety of tasks. These tasks\ninclude language modeling, speech recognition, image caption generation, and\nmachine translation.",
    "num_pages": 8
}