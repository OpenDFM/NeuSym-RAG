{
    "uuid": "916615ea-a2db-5994-815e-ff4c0b641987",
    "title": "Learned Token Pruning for Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Sehoon Kim",
        "Sheng Shen",
        "David Thorsley",
        "Amir Gholami",
        "Woosuk Kwon",
        "Joseph Hassoun",
        "Kurt Keutzer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00910v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\916615ea-a2db-5994-815e-ff4c0b641987.pdf",
    "bibtex": "@misc{kim2022learnedtokenpruningfortransformers,\n    title = {Learned Token Pruning for Transformers},\n    author = {Sehoon Kim and Sheng Shen and David Thorsley and Amir Gholami and Woosuk Kwon and Joseph Hassoun and Kurt Keutzer},\n    year = {2022},\n    eprint = {2107.00910},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2107.00910},\n}",
    "abstract": "Deploying transformer models in practice is challenging due to their\ninference cost, which scales quadratically with input sequence length. To\naddress this, we present a novel Learned Token Pruning (LTP) method which\nadaptively removes unimportant tokens as an input sequence passes through\ntransformer layers. In particular, LTP prunes tokens with an attention score\nbelow a threshold value which is learned for each layer during training. Our\nthreshold-based method allows the length of the pruned sequence to vary\nadaptively based on the input sequence, and avoids algorithmically expensive\noperations such as top-k token selection. We extensively test the performance\nof LTP on GLUE tasks and show that our method outperforms the prior\nstate-of-the-art token pruning methods by up to ~2.5% higher accuracy with the\nsame amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction\nwith less than 1% accuracy drop, which results in up to 1.9x and 2.0x\nthroughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,\nrespectively. Furthermore, we demonstrate that LTP is more robust than prior\nmethods to variations on input sentence lengths. Our code has been developed in\nPyTorch and has been open-sourced.",
    "num_pages": 11
}