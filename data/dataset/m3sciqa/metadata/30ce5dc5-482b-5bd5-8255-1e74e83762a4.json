{
    "uuid": "30ce5dc5-482b-5bd5-8255-1e74e83762a4",
    "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jinhao Jiang",
        "Kun Zhou",
        "Wayne Xin Zhao",
        "Yaliang Li",
        "Ji-Rong Wen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2401.00158v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\30ce5dc5-482b-5bd5-8255-1e74e83762a4.pdf",
    "bibtex": "@misc{jiang2023reasoninglmenablingstructuralsubgraphreasoning,\n    title = {ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph},\n    author = {Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yaliang Li and Ji-Rong Wen},\n    year = {2023},\n    eprint = {2401.00158},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2401.00158},\n}",
    "abstract": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities\nfor the natural language question from a large-scale Knowledge Graph~(KG). To\nbetter perform reasoning on KG, recent work typically adopts a pre-trained\nlanguage model~(PLM) to model the question, and a graph neural network~(GNN)\nbased module to perform multi-hop reasoning on the KG. Despite the\neffectiveness, due to the divergence in model architecture, the PLM and GNN are\nnot closely integrated, limiting the knowledge sharing and fine-grained feature\ninteractions. To solve it, we aim to simplify the above two-module approach,\nand develop a more capable PLM that can directly support subgraph reasoning for\nKGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware\nself-attention mechanism to imitate the GNN for performing structured\nreasoning, and also adopt an adaptation tuning strategy to adapt the model\nparameters with 20,000 subgraphs with synthesized questions. After adaptation,\nthe PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments\nshow that ReasoningLM surpasses state-of-the-art models by a large margin, even\nwith fewer updated parameters and less training data. Our codes and data are\npublicly available at~\\url{https://github.com/RUCAIBox/ReasoningLM}.",
    "num_pages": 15
}