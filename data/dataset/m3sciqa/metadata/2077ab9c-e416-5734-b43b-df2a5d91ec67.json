{
    "uuid": "2077ab9c-e416-5734-b43b-df2a5d91ec67",
    "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
    ],
    "pdf_url": "http://arxiv.org/pdf/1607.06520v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\2077ab9c-e416-5734-b43b-df2a5d91ec67.pdf",
    "bibtex": "@misc{bolukbasi2016manistocomputerprogrammer,\n    title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},\n    author = {Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},\n    year = {2016},\n    eprint = {1607.06520},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1607.06520},\n}",
    "abstract": "The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.",
    "num_pages": 25
}