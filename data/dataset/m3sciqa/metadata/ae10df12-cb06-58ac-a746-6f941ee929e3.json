{
    "uuid": "ae10df12-cb06-58ac-a746-6f941ee929e3",
    "title": "Adam: A Method for Stochastic Optimization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
    ],
    "pdf_url": "http://arxiv.org/pdf/1412.6980v9",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\ae10df12-cb06-58ac-a746-6f941ee929e3.pdf",
    "bibtex": "@misc{kingma2017adamamethodforstochastic,\n    title = {Adam: A Method for Stochastic Optimization},\n    author = {Diederik P. Kingma and Jimmy Ba},\n    year = {2017},\n    eprint = {1412.6980},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1412.6980},\n}",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
    "num_pages": 15
}