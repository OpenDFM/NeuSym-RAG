{
    "uuid": "ea143f49-405f-55c1-b49b-3c092e20eb19",
    "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Viet Dac Lai",
        "Nghia Trung Ngo",
        "Amir Pouran Ben Veyseh",
        "Hieu Man",
        "Franck Dernoncourt",
        "Trung Bui",
        "Thien Huu Nguyen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.05613v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ea143f49-405f-55c1-b49b-3c092e20eb19.pdf",
    "bibtex": "@misc{lai2023chatgptbeyondenglishtowardsa,\n    title = {ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning},\n    author = {Viet Dac Lai and Nghia Trung Ngo and Amir Pouran Ben Veyseh and Hieu Man and Franck Dernoncourt and Trung Bui and Thien Huu Nguyen},\n    year = {2023},\n    eprint = {2304.05613},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.05613},\n}",
    "abstract": "Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.",
    "num_pages": 21
}