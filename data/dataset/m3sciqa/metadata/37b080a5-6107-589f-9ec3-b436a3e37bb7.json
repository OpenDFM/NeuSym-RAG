{
    "uuid": "37b080a5-6107-589f-9ec3-b436a3e37bb7",
    "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Tu Vu",
        "Brian Lester",
        "Noah Constant",
        "Rami Al-Rfou",
        "Daniel Cer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.07904v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\37b080a5-6107-589f-9ec3-b436a3e37bb7.pdf",
    "bibtex": "@misc{vu2022spotbetterfrozenmodeladaptation,\n    title = {SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer},\n    author = {Tu Vu and Brian Lester and Noah Constant and Rami Al-Rfou and Daniel Cer},\n    year = {2022},\n    eprint = {2110.07904},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.07904},\n}",
    "abstract": "There has been growing interest in parameter-efficient methods to apply\npre-trained language models to downstream tasks. Building on the Prompt Tuning\napproach of Lester et al. (2021), which learns task-specific soft prompts to\ncondition a frozen pre-trained model to perform different tasks, we propose a\nnovel prompt-based transfer learning approach called SPoT: Soft Prompt\nTransfer. SPoT first learns a prompt on one or more source tasks and then uses\nit to initialize the prompt for a target task. We show that SPoT significantly\nboosts the performance of Prompt Tuning across many tasks. More remarkably,\nacross all model sizes, SPoT matches or outperforms standard Model Tuning\n(which fine-tunes all model parameters) on the SuperGLUE benchmark, while using\nup to 27,000x fewer task-specific parameters. To understand where SPoT is most\neffective, we conduct a large-scale study on task transferability with 26 NLP\ntasks in 160 combinations, and demonstrate that many tasks can benefit each\nother via prompt transfer. Finally, we propose an efficient retrieval approach\nthat interprets task prompts as task embeddings to identify similar tasks and\npredict the most transferable source tasks for a novel target task.",
    "num_pages": 21
}