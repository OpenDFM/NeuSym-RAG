{
    "uuid": "82a80b14-aa33-57ed-b666-c182235bc860",
    "title": "FUDGE: Controlled Text Generation With Future Discriminators",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Kevin Yang",
        "Dan Klein"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.05218v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\82a80b14-aa33-57ed-b666-c182235bc860.pdf",
    "bibtex": "@misc{yang2021fudgecontrolledtextgenerationwith,\n    title = {FUDGE: Controlled Text Generation With Future Discriminators},\n    author = {Kevin Yang and Dan Klein},\n    year = {2021},\n    eprint = {2104.05218},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.05218},\n}",
    "abstract": "We propose Future Discriminators for Generation (FUDGE), a flexible and\nmodular method for controlled text generation. Given a pre-existing model G for\ngenerating text from a distribution of interest, FUDGE enables conditioning on\na desired attribute a (for example, formality) while requiring access only to\nG's output logits. FUDGE learns an attribute predictor operating on a partial\nsequence, and uses this predictor's outputs to adjust G's original\nprobabilities. We show that FUDGE models terms corresponding to a Bayesian\ndecomposition of the conditional distribution of G given attribute a. Moreover,\nFUDGE can easily compose predictors for multiple desired attributes. We\nevaluate FUDGE on three tasks -- couplet completion in poetry, topic control in\nlanguage generation, and formality change in machine translation -- and observe\ngains in all three tasks.",
    "num_pages": 25
}