{
    "uuid": "2d4ccd49-4eeb-58ae-bd4f-134be8ae2c11",
    "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shayne Longpre",
        "Le Hou",
        "Tu Vu",
        "Albert Webson",
        "Hyung Won Chung",
        "Yi Tay",
        "Denny Zhou",
        "Quoc V. Le",
        "Barret Zoph",
        "Jason Wei",
        "Adam Roberts"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.13688v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\2d4ccd49-4eeb-58ae-bd4f-134be8ae2c11.pdf",
    "bibtex": "@misc{longpre2023theflancollectiondesigningdata,\n    title = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},\n    author = {Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},\n    year = {2023},\n    eprint = {2301.13688},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2301.13688},\n}",
    "abstract": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.",
    "num_pages": 22
}