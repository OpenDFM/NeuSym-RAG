{
    "uuid": "46c0d816-f49b-5890-b64f-5ff9e7ee016b",
    "title": "An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ilias Chalkidis",
        "Xiang Dai",
        "Manos Fergadiotis",
        "Prodromos Malakasiotis",
        "Desmond Elliott"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.05529v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\46c0d816-f49b-5890-b64f-5ff9e7ee016b.pdf",
    "bibtex": "@misc{chalkidis2022anexplorationofhierarchicalattention,\n    title = {An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification},\n    author = {Ilias Chalkidis and Xiang Dai and Manos Fergadiotis and Prodromos Malakasiotis and Desmond Elliott},\n    year = {2022},\n    eprint = {2210.05529},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.05529},\n}",
    "abstract": "Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.",
    "num_pages": 16
}