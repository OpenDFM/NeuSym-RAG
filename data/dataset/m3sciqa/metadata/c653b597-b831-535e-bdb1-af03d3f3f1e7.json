{
    "uuid": "c653b597-b831-535e-bdb1-af03d3f3f1e7",
    "title": "Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Wenyu Guo",
        "Qingkai Fang",
        "Dong Yu",
        "Yang Feng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.13361v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\c653b597-b831-535e-bdb1-af03d3f3f1e7.pdf",
    "bibtex": "@misc{guo2023bridgingthegapbetweensynthetic,\n    title = {Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation},\n    author = {Wenyu Guo and Qingkai Fang and Dong Yu and Yang Feng},\n    year = {2023},\n    eprint = {2310.13361},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2310.13361},\n}",
    "abstract": "Multimodal machine translation (MMT) simultaneously takes the source sentence\nand a relevant image as input for translation. Since there is no paired image\navailable for the input sentence in most cases, recent studies suggest\nutilizing powerful text-to-image generation models to provide image inputs.\nNevertheless, synthetic images generated by these models often follow different\ndistributions compared to authentic images. Consequently, using authentic\nimages for training and synthetic images for inference can introduce a\ndistribution shift, resulting in performance degradation during inference. To\ntackle this challenge, in this paper, we feed synthetic and authentic images to\nthe MMT model, respectively. Then we minimize the gap between the synthetic and\nauthentic images by drawing close the input image representations of the\nTransformer Encoder and the output distributions of the Transformer Decoder.\nTherefore, we mitigate the distribution disparity introduced by the synthetic\nimages during inference, thereby freeing the authentic images from the\ninference process.Experimental results show that our approach achieves\nstate-of-the-art performance on the Multi30K En-De and En-Fr datasets, while\nremaining independent of authentic images during inference.",
    "num_pages": 12
}