{
    "uuid": "f424edba-b48e-5654-bb56-533a4767fb27",
    "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jonas Pfeiffer",
        "Aishwarya Kamath",
        "Andreas Rücklé",
        "Kyunghyun Cho",
        "Iryna Gurevych"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00247v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f424edba-b48e-5654-bb56-533a4767fb27.pdf",
    "bibtex": "@misc{pfeiffer2021adapterfusionnondestructivetaskcompositionfor,\n    title = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},\n    author = {Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},\n    year = {2021},\n    eprint = {2005.00247},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.00247},\n}",
    "abstract": "Sequential fine-tuning and multi-task learning are methods aiming to\nincorporate knowledge from multiple tasks; however, they suffer from\ncatastrophic forgetting and difficulties in dataset balancing. To address these\nshortcomings, we propose AdapterFusion, a new two stage learning algorithm that\nleverages knowledge from multiple tasks. First, in the knowledge extraction\nstage we learn task specific parameters called adapters, that encapsulate the\ntask-specific information. We then combine the adapters in a separate knowledge\ncomposition step. We show that by separating the two stages, i.e., knowledge\nextraction and knowledge composition, the classifier can effectively exploit\nthe representations learned from multiple tasks in a non-destructive manner. We\nempirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it\neffectively combines various types of knowledge at different layers of the\nmodel. We show that our approach outperforms traditional strategies such as\nfull fine-tuning as well as multi-task learning. Our code and adapters are\navailable at AdapterHub.ml.",
    "num_pages": 17
}