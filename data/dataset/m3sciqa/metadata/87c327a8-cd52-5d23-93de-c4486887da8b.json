{
    "uuid": "87c327a8-cd52-5d23-93de-c4486887da8b",
    "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shih-yang Liu",
        "Zechun Liu",
        "Xijie Huang",
        "Pingcheng Dong",
        "Kwang-Ting Cheng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.16836v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\87c327a8-cd52-5d23-93de-c4486887da8b.pdf",
    "bibtex": "@misc{liu2023llmfp44bitfloatingpointquantizedtransformers,\n    title = {LLM-FP4: 4-Bit Floating-Point Quantized Transformers},\n    author = {Shih-yang Liu and Zechun Liu and Xijie Huang and Pingcheng Dong and Kwang-Ting Cheng},\n    year = {2023},\n    eprint = {2310.16836},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.16836},\n}",
    "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large\nlanguage models (LLMs) down to 4-bit floating-point values, in a post-training\nmanner. Existing post-training quantization (PTQ) solutions are primarily\ninteger-based and struggle with bit widths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization is more flexible and can better\nhandle long-tail or bell-shaped distributions, and it has emerged as a default\nchoice in many hardware platforms. One characteristic of FP quantization is\nthat its performance largely depends on the choice of exponent bits and\nclipping range. In this regard, we construct a strong FP-PTQ baseline by\nsearching for the optimal quantization parameters. Furthermore, we observe a\nhigh inter-channel variance and low intra-channel variance pattern in\nactivation distributions, which adds activation quantization difficulty. We\nrecognize this pattern to be consistent across a spectrum of transformer models\ndesigned for diverse tasks, such as LLMs, BERT, and Vision Transformer models.\nTo tackle this, we propose per-channel activation quantization and show that\nthese additional scaling factors can be reparameterized as exponential biases\nof weights, incurring a negligible cost. Our method, for the first time, can\nquantize both weights and activations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the common sense zero-shot reasoning\ntasks, which is only 5.8 lower than the full-precision model, significantly\noutperforming the previous state-of-the-art by 12.7 points. Code is available\nat: https://github.com/nbasyl/LLM-FP4.",
    "num_pages": 14
}