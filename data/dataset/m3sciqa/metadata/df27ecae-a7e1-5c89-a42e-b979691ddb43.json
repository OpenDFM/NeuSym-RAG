{
    "uuid": "df27ecae-a7e1-5c89-a42e-b979691ddb43",
    "title": "Understanding Black-box Predictions via Influence Functions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Pang Wei Koh",
        "Percy Liang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1703.04730v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\df27ecae-a7e1-5c89-a42e-b979691ddb43.pdf",
    "bibtex": "@misc{koh2020understandingblackboxpredictionsviainfluence,\n    title = {Understanding Black-box Predictions via Influence Functions},\n    author = {Pang Wei Koh and Percy Liang},\n    year = {2020},\n    eprint = {1703.04730},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1703.04730},\n}",
    "abstract": "How can we explain the predictions of a black-box model? In this paper, we\nuse influence functions -- a classic technique from robust statistics -- to\ntrace a model's prediction through the learning algorithm and back to its\ntraining data, thereby identifying training points most responsible for a given\nprediction. To scale up influence functions to modern machine learning\nsettings, we develop a simple, efficient implementation that requires only\noracle access to gradients and Hessian-vector products. We show that even on\nnon-convex and non-differentiable models where the theory breaks down,\napproximations to influence functions can still provide valuable information.\nOn linear models and convolutional neural networks, we demonstrate that\ninfluence functions are useful for multiple purposes: understanding model\nbehavior, debugging models, detecting dataset errors, and even creating\nvisually-indistinguishable training-set attacks.",
    "num_pages": 12
}