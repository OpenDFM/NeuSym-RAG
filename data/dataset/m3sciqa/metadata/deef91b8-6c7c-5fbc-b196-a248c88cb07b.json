{
    "uuid": "deef91b8-6c7c-5fbc-b196-a248c88cb07b",
    "title": "DialFact: A Benchmark for Fact-Checking in Dialogue",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Prakhar Gupta",
        "Chien-Sheng Wu",
        "Wenhao Liu",
        "Caiming Xiong"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08222v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\deef91b8-6c7c-5fbc-b196-a248c88cb07b.pdf",
    "bibtex": "@misc{gupta2022dialfactabenchmarkforfactchecking,\n    title = {DialFact: A Benchmark for Fact-Checking in Dialogue},\n    author = {Prakhar Gupta and Chien-Sheng Wu and Wenhao Liu and Caiming Xiong},\n    year = {2022},\n    eprint = {2110.08222},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.08222},\n}",
    "abstract": "Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation. We introduce the task of fact-checking in dialogue, which\nis a relatively unexplored area. We construct DialFact, a testing benchmark\ndataset of 22,245 annotated conversational claims, paired with pieces of\nevidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable\nclaim detection task distinguishes whether a response carries verifiable\nfactual information; 2) Evidence retrieval task retrieves the most relevant\nWikipedia snippets as evidence; 3) Claim verification task predicts a dialogue\nresponse to be supported, refuted, or not enough information. We found that\nexisting fact-checking models trained on non-dialogue data like FEVER fail to\nperform well on our task, and thus, we propose a simple yet data-efficient\nsolution to effectively improve fact-checking performance in dialogue. We point\nout unique challenges in DialFact such as handling the colloquialisms,\ncoreferences and retrieval ambiguities in the error analysis to shed light on\nfuture research in this direction.",
    "num_pages": 17
}