{
    "uuid": "2318a9a9-d21c-563f-9414-102fe10fd204",
    "title": "GeDi: Generative Discriminator Guided Sequence Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Ben Krause",
        "Akhilesh Deepak Gotmare",
        "Bryan McCann",
        "Nitish Shirish Keskar",
        "Shafiq Joty",
        "Richard Socher",
        "Nazneen Fatema Rajani"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.06367v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\2318a9a9-d21c-563f-9414-102fe10fd204.pdf",
    "bibtex": "@misc{krause2020gedigenerativediscriminatorguidedsequence,\n    title = {GeDi: Generative Discriminator Guided Sequence Generation},\n    author = {Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and Nitish Shirish Keskar and Shafiq Joty and Richard Socher and Nazneen Fatema Rajani},\n    year = {2020},\n    eprint = {2009.06367},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.06367},\n}",
    "abstract": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.",
    "num_pages": 30
}