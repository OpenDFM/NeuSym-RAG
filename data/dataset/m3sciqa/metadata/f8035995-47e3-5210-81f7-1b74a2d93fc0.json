{
    "uuid": "f8035995-47e3-5210-81f7-1b74a2d93fc0",
    "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Wenhu Chen",
        "Xueguang Ma",
        "Xinyi Wang",
        "William W. Cohen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.12588v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f8035995-47e3-5210-81f7-1b74a2d93fc0.pdf",
    "bibtex": "@misc{chen2023programofthoughtspromptingdisentangling,\n    title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},\n    author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},\n    year = {2023},\n    eprint = {2211.12588},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.12588},\n}",
    "abstract": "Recently, there has been significant progress in teaching language models to\nperform step-by-step reasoning to solve complex numerical reasoning tasks.\nChain-of-thoughts prompting (CoT) is by far the state-of-art method for these\ntasks. CoT uses language models to perform both reasoning and computation in\nthe multi-step `thought' process. To disentangle computation from reasoning, we\npropose `Program of Thoughts' (PoT), which uses language models (mainly Codex)\nto express the reasoning process as a program. The computation is relegated to\nan external computer, which executes the generated programs to derive the\nanswer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP,\nTabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA)\nfor both few-shot and zero-shot setups. Under both few-shot and zero-shot\nsettings, PoT can show an average performance gain over CoT by around 12\\%\nacross all the evaluated datasets. By combining PoT with self-consistency\ndecoding, we can achieve SoTA performance on all math problem datasets and\nnear-SoTA performance on financial datasets. All of our data and code are\nreleased in Github https://github.com/wenhuchen/Program-of-Thoughts",
    "num_pages": 20
}