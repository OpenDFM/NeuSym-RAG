{
    "uuid": "947f07f1-5d2a-554e-a441-9838afd4a779",
    "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Xuanting Chen",
        "Junjie Ye",
        "Can Zu",
        "Nuo Xu",
        "Rui Zheng",
        "Minlong Peng",
        "Jie Zhou",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.00293v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\947f07f1-5d2a-554e-a441-9838afd4a779.pdf",
    "bibtex": "@misc{chen2023howrobustisgpt35to,\n    title = {How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks},\n    author = {Xuanting Chen and Junjie Ye and Can Zu and Nuo Xu and Rui Zheng and Minlong Peng and Jie Zhou and Tao Gui and Qi Zhang and Xuanjing Huang},\n    year = {2023},\n    eprint = {2303.00293},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.00293},\n}",
    "abstract": "The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.",
    "num_pages": 45
}