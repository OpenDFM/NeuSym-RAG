{
    "uuid": "d11cd80b-ef61-5a64-a760-578f700a5170",
    "title": "Sequence-Level Knowledge Distillation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Yoon Kim",
        "Alexander M. Rush"
    ],
    "pdf_url": "http://arxiv.org/pdf/1606.07947v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\d11cd80b-ef61-5a64-a760-578f700a5170.pdf",
    "bibtex": "@misc{kim2016sequencelevelknowledgedistillation,\n    title = {Sequence-Level Knowledge Distillation},\n    author = {Yoon Kim and Alexander M. Rush},\n    year = {2016},\n    eprint = {1606.07947},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1606.07947},\n}",
    "abstract": "Neural machine translation (NMT) offers a novel alternative formulation of\ntranslation that is potentially simpler than statistical approaches. However to\nreach competitive performance, NMT models need to be exceedingly large. In this\npaper we consider applying knowledge distillation approaches (Bucila et al.,\n2006; Hinton et al., 2015) that have proven successful for reducing the size of\nneural models in other domains to the problem of NMT. We demonstrate that\nstandard knowledge distillation applied to word-level prediction can be\neffective for NMT, and also introduce two novel sequence-level versions of\nknowledge distillation that further improve performance, and somewhat\nsurprisingly, seem to eliminate the need for beam search (even when applied on\nthe original teacher model). Our best student model runs 10 times faster than\nits state-of-the-art teacher with little loss in performance. It is also\nsignificantly better than a baseline model trained without knowledge\ndistillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight\npruning on top of knowledge distillation results in a student model that has 13\ntimes fewer parameters than the original teacher model, with a decrease of 0.4\nBLEU.",
    "num_pages": 11
}