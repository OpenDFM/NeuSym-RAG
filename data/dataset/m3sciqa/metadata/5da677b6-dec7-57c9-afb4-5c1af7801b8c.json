{
    "uuid": "5da677b6-dec7-57c9-afb4-5c1af7801b8c",
    "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jack Hessel",
        "Ari Holtzman",
        "Maxwell Forbes",
        "Ronan Le Bras",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08718v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\5da677b6-dec7-57c9-afb4-5c1af7801b8c.pdf",
    "bibtex": "@misc{hessel2022clipscoreareferencefreeevaluationmetric,\n    title = {CLIPScore: A Reference-free Evaluation Metric for Image Captioning},\n    author = {Jack Hessel and Ari Holtzman and Maxwell Forbes and Ronan Le Bras and Yejin Choi},\n    year = {2022},\n    eprint = {2104.08718},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2104.08718},\n}",
    "abstract": "Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.",
    "num_pages": 15
}