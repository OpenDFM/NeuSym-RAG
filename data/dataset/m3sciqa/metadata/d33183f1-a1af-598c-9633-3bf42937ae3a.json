{
    "uuid": "d33183f1-a1af-598c-9633-3bf42937ae3a",
    "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Brian Lester",
        "Rami Al-Rfou",
        "Noah Constant"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.08691v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\d33183f1-a1af-598c-9633-3bf42937ae3a.pdf",
    "bibtex": "@misc{lester2021thepowerofscalefor,\n    title = {The Power of Scale for Parameter-Efficient Prompt Tuning},\n    author = {Brian Lester and Rami Al-Rfou and Noah Constant},\n    year = {2021},\n    eprint = {2104.08691},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.08691},\n}",
    "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.",
    "num_pages": 15
}