{
    "uuid": "81c6be03-577c-51d5-8e65-f63b3e709112",
    "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Alex Wang",
        "Yada Pruksachatkun",
        "Nikita Nangia",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R. Bowman"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.00537v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\81c6be03-577c-51d5-8e65-f63b3e709112.pdf",
    "bibtex": "@misc{wang2020superglueastickierbenchmarkfor,\n    title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},\n    author = {Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\n    year = {2020},\n    eprint = {1905.00537},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1905.00537},\n}",
    "abstract": "In the last year, new models and methods for pretraining and transfer\nlearning have driven striking performance improvements across a range of\nlanguage understanding tasks. The GLUE benchmark, introduced a little over one\nyear ago, offers a single-number metric that summarizes progress on a diverse\nset of such tasks, but performance on the benchmark has recently surpassed the\nlevel of non-expert humans, suggesting limited headroom for further research.\nIn this paper we present SuperGLUE, a new benchmark styled after GLUE with a\nnew set of more difficult language understanding tasks, a software toolkit, and\na public leaderboard. SuperGLUE is available at super.gluebenchmark.com.",
    "num_pages": 29
}