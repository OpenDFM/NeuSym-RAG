{
    "uuid": "be395337-c3e2-5e16-b2ef-1ed22e6736dd",
    "title": "Improving alignment of dialogue agents via targeted human judgements",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Amelia Glaese",
        "Nat McAleese",
        "Maja Trębacz",
        "John Aslanides",
        "Vlad Firoiu",
        "Timo Ewalds",
        "Maribeth Rauh",
        "Laura Weidinger",
        "Martin Chadwick",
        "Phoebe Thacker",
        "Lucy Campbell-Gillingham",
        "Jonathan Uesato",
        "Po-Sen Huang",
        "Ramona Comanescu",
        "Fan Yang",
        "Abigail See",
        "Sumanth Dathathri",
        "Rory Greig",
        "Charlie Chen",
        "Doug Fritz",
        "Jaume Sanchez Elias",
        "Richard Green",
        "Soňa Mokrá",
        "Nicholas Fernando",
        "Boxi Wu",
        "Rachel Foley",
        "Susannah Young",
        "Iason Gabriel",
        "William Isaac",
        "John Mellor",
        "Demis Hassabis",
        "Koray Kavukcuoglu",
        "Lisa Anne Hendricks",
        "Geoffrey Irving"
    ],
    "pdf_url": "http://arxiv.org/pdf/2209.14375v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\be395337-c3e2-5e16-b2ef-1ed22e6736dd.pdf",
    "bibtex": "@misc{glaese2022improvingalignmentofdialogueagents,\n    title = {Improving alignment of dialogue agents via targeted human judgements},\n    author = {Amelia Glaese and Nat McAleese and Maja Trębacz and John Aslanides and Vlad Firoiu and Timo Ewalds and Maribeth Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and Jonathan Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and Abigail See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Soňa Mokrá and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William Isaac and John Mellor and Demis Hassabis and Koray Kavukcuoglu and Lisa Anne Hendricks and Geoffrey Irving},\n    year = {2022},\n    eprint = {2209.14375},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2209.14375},\n}",
    "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more\nhelpful, correct, and harmless compared to prompted language model baselines.\nWe use reinforcement learning from human feedback to train our models with two\nnew additions to help human raters judge agent behaviour. First, to make our\nagent more helpful and harmless, we break down the requirements for good\ndialogue into natural language rules the agent should follow, and ask raters\nabout each rule separately. We demonstrate that this breakdown enables us to\ncollect more targeted human judgements of agent behaviour and allows for more\nefficient rule-conditional reward models. Second, our agent provides evidence\nfrom sources supporting factual claims when collecting preference judgements\nover model statements. For factual questions, evidence provided by Sparrow\nsupports the sampled response 78% of the time. Sparrow is preferred more often\nthan baselines while being more resilient to adversarial probing by humans,\nviolating our rules only 8% of the time when probed. Finally, we conduct\nextensive analyses showing that though our model learns to follow our rules it\ncan exhibit distributional biases.",
    "num_pages": 77
}