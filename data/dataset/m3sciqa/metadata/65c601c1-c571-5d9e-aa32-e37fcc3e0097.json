{
    "uuid": "65c601c1-c571-5d9e-aa32-e37fcc3e0097",
    "title": "Gradient based sample selection for online continual learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Rahaf Aljundi",
        "Min Lin",
        "Baptiste Goujaud",
        "Yoshua Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.08671v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\65c601c1-c571-5d9e-aa32-e37fcc3e0097.pdf",
    "bibtex": "@misc{aljundi2019gradientbasedsampleselectionfor,\n    title = {Gradient based sample selection for online continual learning},\n    author = {Rahaf Aljundi and Min Lin and Baptiste Goujaud and Yoshua Bengio},\n    year = {2019},\n    eprint = {1903.08671},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1903.08671},\n}",
    "abstract": "A continual learning agent learns online with a non-stationary and\nnever-ending stream of data. The key to such learning process is to overcome\nthe catastrophic forgetting of previously seen data, which is a well known\nproblem of neural networks. To prevent forgetting, a replay buffer is usually\nemployed to store the previous data for the purpose of rehearsal. Previous\nworks often depend on task boundary and i.i.d. assumptions to properly select\nsamples for the replay buffer. In this work, we formulate sample selection as a\nconstraint reduction problem based on the constrained optimization view of\ncontinual learning. The goal is to select a fixed subset of constraints that\nbest approximate the feasible region defined by the original constraints. We\nshow that it is equivalent to maximizing the diversity of samples in the replay\nbuffer with parameters gradient as the feature. We further develop a greedy\nalternative that is cheap and efficient. The advantage of the proposed method\nis demonstrated by comparing to other alternatives under the continual learning\nsetting. Further comparisons are made against state of the art methods that\nrely on task boundaries which show comparable or even better results for our\nmethod.",
    "num_pages": 13
}