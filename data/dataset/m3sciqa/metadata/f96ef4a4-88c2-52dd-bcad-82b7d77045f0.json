{
    "uuid": "f96ef4a4-88c2-52dd-bcad-82b7d77045f0",
    "title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Pranav Rajpurkar",
        "Robin Jia",
        "Percy Liang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1806.03822v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\f96ef4a4-88c2-52dd-bcad-82b7d77045f0.pdf",
    "bibtex": "@misc{rajpurkar2018knowwhatyoudontknow,\n    title = {Know What You Don't Know: Unanswerable Questions for SQuAD},\n    author = {Pranav Rajpurkar and Robin Jia and Percy Liang},\n    year = {2018},\n    eprint = {1806.03822},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1806.03822},\n}",
    "abstract": "Extractive reading comprehension systems can often locate the correct answer\nto a question in a context document, but they also tend to make unreliable\nguesses on questions for which the correct answer is not stated in the context.\nExisting datasets either focus exclusively on answerable questions, or use\nautomatically generated unanswerable questions that are easy to identify. To\naddress these weaknesses, we present SQuAD 2.0, the latest version of the\nStanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD\ndata with over 50,000 unanswerable questions written adversarially by\ncrowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,\nsystems must not only answer questions when possible, but also determine when\nno answer is supported by the paragraph and abstain from answering. SQuAD 2.0\nis a challenging natural language understanding task for existing models: a\nstrong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on\nSQuAD 2.0.",
    "num_pages": 9
}