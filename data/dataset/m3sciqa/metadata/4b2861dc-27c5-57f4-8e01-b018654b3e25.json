{
    "uuid": "4b2861dc-27c5-57f4-8e01-b018654b3e25",
    "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Tao Xu",
        "Pengchuan Zhang",
        "Qiuyuan Huang",
        "Han Zhang",
        "Zhe Gan",
        "Xiaolei Huang",
        "Xiaodong He"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.10485v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\4b2861dc-27c5-57f4-8e01-b018654b3e25.pdf",
    "bibtex": "@misc{xu2017attnganfinegrainedtexttoimage,\n    title = {AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks},\n    author = {Tao Xu and Pengchuan Zhang and Qiuyuan Huang and Han Zhang and Zhe Gan and Xiaolei Huang and Xiaodong He},\n    year = {2017},\n    eprint = {1711.10485},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1711.10485},\n}",
    "abstract": "In this paper, we propose an Attentional Generative Adversarial Network\n(AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained\ntext-to-image generation. With a novel attentional generative network, the\nAttnGAN can synthesize fine-grained details at different subregions of the\nimage by paying attentions to the relevant words in the natural language\ndescription. In addition, a deep attentional multimodal similarity model is\nproposed to compute a fine-grained image-text matching loss for training the\ngenerator. The proposed AttnGAN significantly outperforms the previous state of\nthe art, boosting the best reported inception score by 14.14% on the CUB\ndataset and 170.25% on the more challenging COCO dataset. A detailed analysis\nis also performed by visualizing the attention layers of the AttnGAN. It for\nthe first time shows that the layered attentional GAN is able to automatically\nselect the condition at the word level for generating different parts of the\nimage.",
    "num_pages": 9
}