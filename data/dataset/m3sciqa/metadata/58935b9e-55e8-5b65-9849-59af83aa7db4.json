{
    "uuid": "58935b9e-55e8-5b65-9849-59af83aa7db4",
    "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Minghao Wu",
        "Abdul Waheed",
        "Chiyu Zhang",
        "Muhammad Abdul-Mageed",
        "Alham Fikri Aji"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.14402v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\58935b9e-55e8-5b65-9849-59af83aa7db4.pdf",
    "bibtex": "@misc{wu2024laminilmadiverseherdof,\n    title = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},\n    author = {Minghao Wu and Abdul Waheed and Chiyu Zhang and Muhammad Abdul-Mageed and Alham Fikri Aji},\n    year = {2024},\n    eprint = {2304.14402},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.14402},\n}",
    "abstract": "Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being much smaller in size.",
    "num_pages": 21
}