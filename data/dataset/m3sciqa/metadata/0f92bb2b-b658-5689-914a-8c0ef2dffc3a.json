{
    "uuid": "0f92bb2b-b658-5689-914a-8c0ef2dffc3a",
    "title": "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Ke Tran",
        "Arianna Bisazza",
        "Christof Monz"
    ],
    "pdf_url": "http://arxiv.org/pdf/1803.03585v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\0f92bb2b-b658-5689-914a-8c0ef2dffc3a.pdf",
    "bibtex": "@misc{tran2018theimportanceofbeingrecurrent,\n    title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},\n    author = {Ke Tran and Arianna Bisazza and Christof Monz},\n    year = {2018},\n    eprint = {1803.03585},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1803.03585},\n}",
    "abstract": "Recent work has shown that recurrent neural networks (RNNs) can implicitly\ncapture and exploit hierarchical information when trained to solve common\nnatural language processing tasks such as language modeling (Linzen et al.,\n2016) and neural machine translation (Shi et al., 2016). In contrast, the\nability to model structured data with non-recurrent neural networks has\nreceived little attention despite their success in many NLP tasks (Gehring et\nal., 2017; Vaswani et al., 2017). In this work, we compare the two\narchitectures---recurrent versus non-recurrent---with respect to their ability\nto model hierarchical structure and find that recurrency is indeed important\nfor this purpose.",
    "num_pages": 6
}