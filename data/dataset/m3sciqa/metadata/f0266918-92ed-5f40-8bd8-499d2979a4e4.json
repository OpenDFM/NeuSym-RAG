{
    "uuid": "f0266918-92ed-5f40-8bd8-499d2979a4e4",
    "title": "On Vision Features in Multimodal Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Bei Li",
        "Chuanhao Lv",
        "Zefan Zhou",
        "Tao Zhou",
        "Tong Xiao",
        "Anxiang Ma",
        "JingBo Zhu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.09173v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\f0266918-92ed-5f40-8bd8-499d2979a4e4.pdf",
    "bibtex": "@misc{li2022onvisionfeaturesinmultimodal,\n    title = {On Vision Features in Multimodal Machine Translation},\n    author = {Bei Li and Chuanhao Lv and Zefan Zhou and Tao Zhou and Tong Xiao and Anxiang Ma and JingBo Zhu},\n    year = {2022},\n    eprint = {2203.09173},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.09173},\n}",
    "abstract": "Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.",
    "num_pages": 11
}