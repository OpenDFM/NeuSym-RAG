{
    "uuid": "ba614015-ae38-5939-8106-728d33b07d78",
    "title": "Evaluating Cross-Domain Text-to-SQL Models and Benchmarks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Mohammadreza Pourreza",
        "Davood Rafiei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.18538v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ba614015-ae38-5939-8106-728d33b07d78.pdf",
    "bibtex": "@misc{pourreza2023evaluatingcrossdomaintexttosqlmodelsand,\n    title = {Evaluating Cross-Domain Text-to-SQL Models and Benchmarks},\n    author = {Mohammadreza Pourreza and Davood Rafiei},\n    year = {2023},\n    eprint = {2310.18538},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.18538},\n}",
    "abstract": "Text-to-SQL benchmarks play a crucial role in evaluating the progress made in\nthe field and the ranking of different models. However, accurately matching a\nmodel-generated SQL query to a reference SQL query in a benchmark fails for\nvarious reasons, such as underspecified natural language queries, inherent\nassumptions in both model-generated and reference queries, and the\nnon-deterministic nature of SQL output under certain conditions. In this paper,\nwe conduct an extensive study of several prominent cross-domain text-to-SQL\nbenchmarks and re-evaluate some of the top-performing models within these\nbenchmarks, by both manually evaluating the SQL queries and rewriting them in\nequivalent expressions. Our evaluation reveals that attaining a perfect\nperformance on these benchmarks is unfeasible due to the multiple\ninterpretations that can be derived from the provided samples. Furthermore, we\nfind that the true performance of the models is underestimated and their\nrelative performance changes after a re-evaluation. Most notably, our\nevaluation reveals a surprising discovery: a recent GPT4-based model surpasses\nthe gold standard reference queries in the Spider benchmark in our human\nevaluation. This finding highlights the importance of interpreting benchmark\nevaluations cautiously, while also acknowledging the critical role of\nadditional independent evaluations in driving advancements in the field.",
    "num_pages": 11
}