{
    "uuid": "02ac9093-bf58-5560-8201-76013521e9e7",
    "title": "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jianmo Ni",
        "Gustavo Hernández Ábrego",
        "Noah Constant",
        "Ji Ma",
        "Keith B. Hall",
        "Daniel Cer",
        "Yinfei Yang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.08877v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\02ac9093-bf58-5560-8201-76013521e9e7.pdf",
    "bibtex": "@misc{ni2021sentencet5scalablesentenceencodersfrom,\n    title = {Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models},\n    author = {Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},\n    year = {2021},\n    eprint = {2108.08877},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2108.08877},\n}",
    "abstract": "We provide the first exploration of sentence embeddings from text-to-text\ntransformers (T5). Sentence embeddings are broadly useful for language\nprocessing tasks. While T5 achieves impressive performance on language tasks\ncast as sequence-to-sequence mapping problems, it is unclear how to produce\nsentence embeddings from encoder-decoder models. We investigate three methods\nfor extracting T5 sentence embeddings: two utilize only the T5 encoder and one\nuses the full T5 encoder-decoder model. To support our investigation, we\nestablish a new sentence representation transfer benchmark, SentGLUE, which\nextends the SentEval toolkit to nine tasks from the GLUE benchmark. Our\nencoder-only models outperforms Sentence-BERT and SimCSE sentence embeddings on\nboth SentEval and SentGLUE transfer tasks, including semantic textual\nsimilarity (STS). Scaling up T5 from millions to billions of parameters is\nfound to produce consistent further improvements. Finally, our encoder-decoder\nmethod achieves a new state-of-the-art on STS when using sentence embeddings.\nOur models are released at https://tfhub.dev/google/collections/sentence-t5/1.",
    "num_pages": 11
}