{
    "uuid": "79a11f77-2549-54a2-8f22-605de4f94ea2",
    "title": "BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Mohsen Fayyaz",
        "Ehsan Aghazadeh",
        "Ali Modarressi",
        "Mohammad Taher Pilehvar",
        "Yadollah Yaghoobzadeh",
        "Samira Ebrahimi Kahou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.05610v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\79a11f77-2549-54a2-8f22-605de4f94ea2.pdf",
    "bibtex": "@misc{fayyaz2022bertonadatadiet,\n    title = {BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning},\n    author = {Mohsen Fayyaz and Ehsan Aghazadeh and Ali Modarressi and Mohammad Taher Pilehvar and Yadollah Yaghoobzadeh and Samira Ebrahimi Kahou},\n    year = {2022},\n    eprint = {2211.05610},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.05610},\n}",
    "abstract": "Current pre-trained language models rely on large datasets for achieving\nstate-of-the-art performance. However, past research has shown that not all\nexamples in a dataset are equally important during training. In fact, it is\nsometimes possible to prune a considerable fraction of the training set while\nmaintaining the test performance. Established on standard vision benchmarks,\ntwo gradient-based scoring metrics for finding important examples are GraNd and\nits estimated version, EL2N. In this work, we employ these two metrics for the\nfirst time in NLP. We demonstrate that these metrics need to be computed after\nat least one epoch of fine-tuning and they are not reliable in early steps.\nFurthermore, we show that by pruning a small portion of the examples with the\nhighest GraNd/EL2N scores, we can not only preserve the test accuracy, but also\nsurpass it. This paper details adjustments and implementation choices which\nenable GraNd and EL2N to be applied to NLP.",
    "num_pages": 7
}