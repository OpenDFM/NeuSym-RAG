{
    "uuid": "2479f89b-b7c0-534f-b51d-d24093d3a1f9",
    "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zeyuan Yang",
        "Peng Li",
        "Yang Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.15746v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\2479f89b-b7c0-534f-b51d-d24093d3a1f9.pdf",
    "bibtex": "@misc{yang2023failurespavethewayenhancing,\n    title = {Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation},\n    author = {Zeyuan Yang and Peng Li and Yang Liu},\n    year = {2023},\n    eprint = {2310.15746},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.15746},\n}",
    "abstract": "Large Language Models (LLMs) have showcased impressive performance. However,\ndue to their inability to capture relationships among samples, these frozen\nLLMs inevitably keep repeating similar mistakes. In this work, we propose our\nTuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving\ntheir performance by learning from previous mistakes. Considering data arrives\nsequentially, LLMs gradually accumulate rules from incorrect cases, forming a\nrule collection. These rules are then utilized by the LLMs to avoid making\nsimilar mistakes when processing subsequent inputs. Moreover, the rules remain\nindependent of the primary prompts, seamlessly complementing prompt design\nstrategies. Experimentally, we show that TRAN improves over recent baselines by\na large margin.",
    "num_pages": 27
}