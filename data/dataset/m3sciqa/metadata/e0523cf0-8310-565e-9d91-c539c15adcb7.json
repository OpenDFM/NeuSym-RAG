{
    "uuid": "e0523cf0-8310-565e-9d91-c539c15adcb7",
    "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Kevin Yang",
        "Yuandong Tian",
        "Nanyun Peng",
        "Dan Klein"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.06774v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e0523cf0-8310-565e-9d91-c539c15adcb7.pdf",
    "bibtex": "@misc{yang2022re3generatinglongerstorieswith,\n    title = {Re3: Generating Longer Stories With Recursive Reprompting and Revision},\n    author = {Kevin Yang and Yuandong Tian and Nanyun Peng and Dan Klein},\n    year = {2022},\n    eprint = {2210.06774},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.06774},\n}",
    "abstract": "We consider the problem of automatically generating longer stories of over\ntwo thousand words. Compared to prior work on shorter stories, long-range plot\ncoherence and relevance are more central challenges here. We propose the\nRecursive Reprompting and Revision framework (Re3) to address these challenges\nby (a) prompting a general-purpose language model to construct a structured\noverarching plan, and (b) generating story passages by repeatedly injecting\ncontextual information from both the plan and current story state into a\nlanguage model prompt. We then revise by (c) reranking different continuations\nfor plot coherence and premise relevance, and finally (d) editing the best\ncontinuation for factual consistency. Compared to similar-length stories\ngenerated directly from the same base model, human evaluators judged\nsubstantially more of Re3's stories as having a coherent overarching plot (by\n14% absolute increase), and relevant to the given initial premise (by 20%).",
    "num_pages": 86
}