{
    "uuid": "8f13f0c9-2b92-5a09-8369-41969fc0924d",
    "title": "Out-of-Distribution Detection with Deep Nearest Neighbors",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yiyou Sun",
        "Yifei Ming",
        "Xiaojin Zhu",
        "Yixuan Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.06507v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\8f13f0c9-2b92-5a09-8369-41969fc0924d.pdf",
    "bibtex": "@misc{sun2022outofdistributiondetectionwithdeepnearest,\n    title = {Out-of-Distribution Detection with Deep Nearest Neighbors},\n    author = {Yiyou Sun and Yifei Ming and Xiaojin Zhu and Yixuan Li},\n    year = {2022},\n    eprint = {2204.06507},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2204.06507},\n}",
    "abstract": "Out-of-distribution (OOD) detection is a critical task for deploying machine\nlearning models in the open world. Distance-based methods have demonstrated\npromise, where testing samples are detected as OOD if they are relatively far\naway from in-distribution (ID) data. However, prior methods impose a strong\ndistributional assumption of the underlying feature space, which may not always\nhold. In this paper, we explore the efficacy of non-parametric nearest-neighbor\ndistance for OOD detection, which has been largely overlooked in the\nliterature. Unlike prior works, our method does not impose any distributional\nassumption, hence providing stronger flexibility and generality. We demonstrate\nthe effectiveness of nearest-neighbor-based OOD detection on several benchmarks\nand establish superior performance. Under the same model trained on\nImageNet-1k, our method substantially reduces the false positive rate\n(FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a\nparametric approach Mahalanobis distance in detection. Code is available:\nhttps://github.com/deeplearning-wisc/knn-ood.",
    "num_pages": 14
}