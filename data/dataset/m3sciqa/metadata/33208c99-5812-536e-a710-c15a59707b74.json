{
    "uuid": "33208c99-5812-536e-a710-c15a59707b74",
    "title": "Meta-Learning to Compositionally Generalize",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Henry Conklin",
        "Bailin Wang",
        "Kenny Smith",
        "Ivan Titov"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.04252v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\33208c99-5812-536e-a710-c15a59707b74.pdf",
    "bibtex": "@misc{conklin2021metalearningtocompositionallygeneralize,\n    title = {Meta-Learning to Compositionally Generalize},\n    author = {Henry Conklin and Bailin Wang and Kenny Smith and Ivan Titov},\n    year = {2021},\n    eprint = {2106.04252},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.04252},\n}",
    "abstract": "Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.",
    "num_pages": 14
}