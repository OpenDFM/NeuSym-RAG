{
    "uuid": "da90e201-7d72-5390-82f0-8549c85e4fba",
    "title": "Emergent Abilities of Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler",
        "Ed H. Chi",
        "Tatsunori Hashimoto",
        "Oriol Vinyals",
        "Percy Liang",
        "Jeff Dean",
        "William Fedus"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.07682v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\da90e201-7d72-5390-82f0-8549c85e4fba.pdf",
    "bibtex": "@misc{wei2022emergentabilitiesoflargelanguage,\n    title = {Emergent Abilities of Large Language Models},\n    author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},\n    year = {2022},\n    eprint = {2206.07682},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2206.07682},\n}",
    "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.",
    "num_pages": 30
}