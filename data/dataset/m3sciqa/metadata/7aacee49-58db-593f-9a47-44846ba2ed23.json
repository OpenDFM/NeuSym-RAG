{
    "uuid": "7aacee49-58db-593f-9a47-44846ba2ed23",
    "title": "Unifying Vision-and-Language Tasks via Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jaemin Cho",
        "Jie Lei",
        "Hao Tan",
        "Mohit Bansal"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.02779v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\7aacee49-58db-593f-9a47-44846ba2ed23.pdf",
    "bibtex": "@misc{cho2021unifyingvisionandlanguagetasksviatext,\n    title = {Unifying Vision-and-Language Tasks via Text Generation},\n    author = {Jaemin Cho and Jie Lei and Hao Tan and Mohit Bansal},\n    year = {2021},\n    eprint = {2102.02779},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2102.02779},\n}",
    "abstract": "Existing methods for vision-and-language learning typically require designing\ntask-specific architectures and objectives for each task. For example, a\nmulti-label answer classifier for visual question answering, a region scorer\nfor referring expression comprehension, and a language decoder for image\ncaptioning, etc. To alleviate these hassles, in this work, we propose a unified\nframework that learns different tasks in a single architecture with the same\nlanguage modeling objective, i.e., multimodal conditional text generation,\nwhere our models learn to generate labels in text based on the visual and\ntextual inputs. On 7 popular vision-and-language benchmarks, including visual\nquestion answering, referring expression comprehension, visual commonsense\nreasoning, most of which have been previously modeled as discriminative tasks,\nour generative approach (with a single unified architecture) reaches comparable\nperformance to recent task-specific state-of-the-art vision-and-language\nmodels. Moreover, our generative approach shows better generalization ability\non questions that have rare answers. Also, we show that our framework allows\nmulti-task learning in a single architecture with a single set of parameters,\nachieving similar performance to separately optimized single-task models. Our\ncode is publicly available at: https://github.com/j-min/VL-T5",
    "num_pages": 15
}