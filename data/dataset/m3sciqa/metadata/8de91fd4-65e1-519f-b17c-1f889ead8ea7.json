{
    "uuid": "8de91fd4-65e1-519f-b17c-1f889ead8ea7",
    "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Junxian He",
        "Daniel Spokoyny",
        "Graham Neubig",
        "Taylor Berg-Kirkpatrick"
    ],
    "pdf_url": "http://arxiv.org/pdf/1901.05534v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\8de91fd4-65e1-519f-b17c-1f889ead8ea7.pdf",
    "bibtex": "@misc{he2019lagginginferencenetworksandposterior,\n    title = {Lagging Inference Networks and Posterior Collapse in Variational Autoencoders},\n    author = {Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},\n    year = {2019},\n    eprint = {1901.05534},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1901.05534},\n}",
    "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent\nvariable model and accompanying variational learning technique. By using a\nneural inference network to approximate the model's posterior on latent\nvariables, VAEs efficiently parameterize a lower bound on marginal data\nlikelihood that can be optimized directly via gradient methods. In practice,\nhowever, VAE training often results in a degenerate local optimum known as\n\"posterior collapse\" where the model learns to ignore the latent variable and\nthe approximate posterior mimics the prior. In this paper, we investigate\nposterior collapse from the perspective of training dynamics. We find that\nduring the initial stages of training the inference network fails to\napproximate the model's true posterior, which is a moving target. As a result,\nthe model is encouraged to ignore the latent encoding and posterior collapse\noccurs. Based on this observation, we propose an extremely simple modification\nto VAE training to reduce inference lag: depending on the model's current\nmutual information between latent variable and observation, we aggressively\noptimize the inference network before performing each model update. Despite\nintroducing neither new model components nor significant complexity over basic\nVAE, our approach is able to avoid the problem of collapse that has plagued a\nlarge amount of previous work. Empirically, our approach outperforms strong\nautoregressive baselines on text and image benchmarks in terms of held-out\nlikelihood, and is competitive with more complex techniques for avoiding\ncollapse while being substantially faster.",
    "num_pages": 15
}