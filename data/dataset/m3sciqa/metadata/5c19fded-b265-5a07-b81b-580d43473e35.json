{
    "uuid": "5c19fded-b265-5a07-b81b-580d43473e35",
    "title": "On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Ethan Gotlieb Wilcox",
        "Jon Gauthier",
        "Jennifer Hu",
        "Peng Qian",
        "Roger Levy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.01912v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\5c19fded-b265-5a07-b81b-580d43473e35.pdf",
    "bibtex": "@misc{wilcox2020onthepredictivepowerof,\n    title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},\n    author = {Ethan Gotlieb Wilcox and Jon Gauthier and Jennifer Hu and Peng Qian and Roger Levy},\n    year = {2020},\n    eprint = {2006.01912},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2006.01912},\n}",
    "abstract": "Human reading behavior is tuned to the statistics of natural language: the\ntime it takes human subjects to read a word can be predicted from estimates of\nthe word's probability in context. However, it remains an open question what\ncomputational architecture best characterizes the expectations deployed in real\ntime by humans that determine the behavioral signatures of reading. Here we\ntest over two dozen models, independently manipulating computational\narchitecture and training dataset size, on how well their next-word\nexpectations predict human reading time behavior on naturalistic text corpora.\nWe find that across model architectures and training dataset sizes the\nrelationship between word log-probability and reading time is (near-)linear. We\nnext evaluate how features of these models determine their psychometric\npredictive power, or ability to predict human reading behavior. In general, the\nbetter a model's next-word expectations, the better its psychometric predictive\npower. However, we find nontrivial differences across model architectures. For\nany given perplexity, deep Transformer models and n-gram models generally show\nsuperior psychometric predictive power over LSTM or structurally supervised\nneural models, especially for eye movement data. Finally, we compare models'\npsychometric predictive power to the depth of their syntactic knowledge, as\nmeasured by a battery of syntactic generalization tests developed using methods\nfrom controlled psycholinguistic experiments. Once perplexity is controlled\nfor, we find no significant relationship between syntactic knowledge and\npredictive power. These results suggest that different approaches may be\nrequired to best model human real-time language comprehension behavior in\nnaturalistic reading versus behavior for controlled linguistic materials\ndesigned for targeted probing of syntactic knowledge.",
    "num_pages": 7
}