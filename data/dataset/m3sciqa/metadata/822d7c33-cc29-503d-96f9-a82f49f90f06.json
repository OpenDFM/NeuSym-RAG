{
    "uuid": "822d7c33-cc29-503d-96f9-a82f49f90f06",
    "title": "Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Shiyue Zhang",
        "Mohit Bansal"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.06356v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\822d7c33-cc29-503d-96f9-a82f49f90f06.pdf",
    "bibtex": "@misc{zhang2019addressingsemanticdriftinquestion,\n    title = {Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering},\n    author = {Shiyue Zhang and Mohit Bansal},\n    year = {2019},\n    eprint = {1909.06356},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.06356},\n}",
    "abstract": "Text-based Question Generation (QG) aims at generating natural and relevant\nquestions that can be answered by a given answer in some context. Existing QG\nmodels suffer from a \"semantic drift\" problem, i.e., the semantics of the\nmodel-generated question drifts away from the given context and answer. In this\npaper, we first propose two semantics-enhanced rewards obtained from downstream\nquestion paraphrasing and question answering tasks to regularize the QG model\nto generate semantically valid questions. Second, since the traditional\nevaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\ngenerated questions, we propose a QA-based evaluation method which measures the\nQG model's ability to mimic human annotators in generating QA training data.\nExperiments show that our method achieves the new state-of-the-art performance\nw.r.t. traditional metrics, and also performs best on our QA-based evaluation\nmetrics. Further, we investigate how to use our QG model to augment QA datasets\nand enable semi-supervised QA. We propose two ways to generate synthetic QA\npairs: generate new questions from existing articles or collect QA pairs from\nnew articles. We also propose two empirically effective strategies, a data\nfilter and mixing mini-batch training, to properly use the QG-generated data\nfor QA. Experiments show that our method improves over both BiDAF and BERT QA\nbaselines, even without introducing new articles.",
    "num_pages": 15
}