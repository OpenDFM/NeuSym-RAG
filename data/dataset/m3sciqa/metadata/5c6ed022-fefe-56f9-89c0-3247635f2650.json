{
    "uuid": "5c6ed022-fefe-56f9-89c0-3247635f2650",
    "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Xiaokai Wei",
        "Shen Wang",
        "Dejiao Zhang",
        "Parminder Bhatia",
        "Andrew Arnold"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08455v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\5c6ed022-fefe-56f9-89c0-3247635f2650.pdf",
    "bibtex": "@misc{wei2021knowledgeenhancedpretrainedlanguagemodels,\n    title = {Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey},\n    author = {Xiaokai Wei and Shen Wang and Dejiao Zhang and Parminder Bhatia and Andrew Arnold},\n    year = {2021},\n    eprint = {2110.08455},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.08455},\n}",
    "abstract": "Pretrained Language Models (PLM) have established a new paradigm through\nlearning informative contextualized representations on large-scale text corpus.\nThis new paradigm has revolutionized the entire field of natural language\nprocessing, and set the new state-of-the-art performance for a wide variety of\nNLP tasks. However, though PLMs could store certain knowledge/facts from\ntraining corpus, their knowledge awareness is still far from satisfactory. To\naddress this issue, integrating knowledge into PLMs have recently become a very\nactive research area and a variety of approaches have been developed. In this\npaper, we provide a comprehensive survey of the literature on this emerging and\nfast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs).\nWe introduce three taxonomies to categorize existing work. Besides, we also\nsurvey the various NLU and NLG applications on which KE-PLM has demonstrated\nsuperior performance over vanilla PLMs. Finally, we discuss challenges that\nface KE-PLMs and also promising directions for future research.",
    "num_pages": 17
}