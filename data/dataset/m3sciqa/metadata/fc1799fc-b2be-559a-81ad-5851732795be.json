{
    "uuid": "fc1799fc-b2be-559a-81ad-5851732795be",
    "title": "LAMBERT: Layout-Aware (Language) Modeling for information extraction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Łukasz Garncarek",
        "Rafał Powalski",
        "Tomasz Stanisławek",
        "Bartosz Topolski",
        "Piotr Halama",
        "Michał Turski",
        "Filip Graliński"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.08087v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\fc1799fc-b2be-559a-81ad-5851732795be.pdf",
    "bibtex": "@misc{garncarek2021lambertlayoutawarelanguagemodelingfor,\n    title = {LAMBERT: Layout-Aware (Language) Modeling for information extraction},\n    author = {Łukasz Garncarek and Rafał Powalski and Tomasz Stanisławek and Bartosz Topolski and Piotr Halama and Michał Turski and Filip Graliński},\n    year = {2021},\n    eprint = {2002.08087},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2002.08087},\n}",
    "abstract": "We introduce a simple new approach to the problem of understanding documents\nwhere non-trivial layout influences the local semantics. To this end, we modify\nthe Transformer encoder architecture in a way that allows it to use layout\nfeatures obtained from an OCR system, without the need to re-learn language\nsemantics from scratch. We only augment the input of the model with the\ncoordinates of token bounding boxes, avoiding, in this way, the use of raw\nimages. This leads to a layout-aware language model which can then be\nfine-tuned on downstream tasks.\n  The model is evaluated on an end-to-end information extraction task using\nfour publicly available datasets: Kleister NDA, Kleister Charity, SROIE and\nCORD. We show that our model achieves superior performance on datasets\nconsisting of visually rich documents, while also outperforming the baseline\nRoBERTa on documents with flat layout (NDA \\(F_{1}\\) increase from 78.50 to\n80.42). Our solution ranked first on the public leaderboard for the Key\nInformation Extraction from the SROIE dataset, improving the SOTA\n\\(F_{1}\\)-score from 97.81 to 98.17.",
    "num_pages": 16
}