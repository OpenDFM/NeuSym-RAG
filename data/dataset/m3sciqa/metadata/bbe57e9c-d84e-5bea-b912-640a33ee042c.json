{
    "uuid": "bbe57e9c-d84e-5bea-b912-640a33ee042c",
    "title": "Topic-Guided Variational Autoencoders for Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Wenlin Wang",
        "Zhe Gan",
        "Hongteng Xu",
        "Ruiyi Zhang",
        "Guoyin Wang",
        "Dinghan Shen",
        "Changyou Chen",
        "Lawrence Carin"
    ],
    "pdf_url": "http://arxiv.org/pdf/1903.07137v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\bbe57e9c-d84e-5bea-b912-640a33ee042c.pdf",
    "bibtex": "@misc{wang2019topicguidedvariationalautoencodersfortext,\n    title = {Topic-Guided Variational Autoencoders for Text Generation},\n    author = {Wenlin Wang and Zhe Gan and Hongteng Xu and Ruiyi Zhang and Guoyin Wang and Dinghan Shen and Changyou Chen and Lawrence Carin},\n    year = {2019},\n    eprint = {1903.07137},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1903.07137},\n}",
    "abstract": "We propose a topic-guided variational autoencoder (TGVAE) model for text\ngeneration. Distinct from existing variational autoencoder (VAE) based\napproaches, which assume a simple Gaussian prior for the latent code, our model\nspecifies the prior as a Gaussian mixture model (GMM) parametrized by a neural\ntopic module. Each mixture component corresponds to a latent topic, which\nprovides guidance to generate sentences under the topic. The neural topic\nmodule and the VAE-based neural sequence module in our model are learned\njointly. In particular, a sequence of invertible Householder transformations is\napplied to endow the approximate posterior of the latent code with high\nflexibility during model inference. Experimental results show that our TGVAE\noutperforms alternative approaches on both unconditional and conditional text\ngeneration, which can generate semantically-meaningful sentences with various\ntopics.",
    "num_pages": 12
}