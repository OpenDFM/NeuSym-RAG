{
    "uuid": "b3cf3805-1c84-5852-874b-c43830dfbb3c",
    "title": "Compositional Generalization for Primitive Substitutions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yuanpeng Li",
        "Liang Zhao",
        "Jianyu Wang",
        "Joel Hestness"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.02612v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\b3cf3805-1c84-5852-874b-c43830dfbb3c.pdf",
    "bibtex": "@misc{li2019compositionalgeneralizationforprimitivesubstitutions,\n    title = {Compositional Generalization for Primitive Substitutions},\n    author = {Yuanpeng Li and Liang Zhao and Jianyu Wang and Joel Hestness},\n    year = {2019},\n    eprint = {1910.02612},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1910.02612},\n}",
    "abstract": "Compositional generalization is a basic mechanism in human language learning,\nbut current neural networks lack such ability. In this paper, we conduct\nfundamental research for encoding compositionality in neural networks.\nConventional methods use a single representation for the input sentence, making\nit hard to apply prior knowledge of compositionality. In contrast, our approach\nleverages such knowledge with two representations, one generating attention\nmaps, and the other mapping attended input words to output symbols. We reduce\nthe entropy in each representation to improve generalization. Our experiments\ndemonstrate significant improvements over the conventional methods in five NLP\ntasks including instruction learning and machine translation. In the SCAN\ndomain, it boosts accuracies from 14.0% to 98.8% in Jump task, and from 92.0%\nto 99.7% in TurnLeft task. It also beats human performance on a few-shot\nlearning task. We hope the proposed approach can help ease future research\ntowards human-level compositional language learning.",
    "num_pages": 12
}