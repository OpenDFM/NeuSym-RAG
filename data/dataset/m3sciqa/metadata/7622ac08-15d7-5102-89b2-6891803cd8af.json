{
    "uuid": "7622ac08-15d7-5102-89b2-6891803cd8af",
    "title": "Proximal Policy Optimization Algorithms",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "John Schulman",
        "Filip Wolski",
        "Prafulla Dhariwal",
        "Alec Radford",
        "Oleg Klimov"
    ],
    "pdf_url": "http://arxiv.org/pdf/1707.06347v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\7622ac08-15d7-5102-89b2-6891803cd8af.pdf",
    "bibtex": "@misc{schulman2017proximalpolicyoptimizationalgorithms,\n    title = {Proximal Policy Optimization Algorithms},\n    author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},\n    year = {2017},\n    eprint = {1707.06347},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1707.06347},\n}",
    "abstract": "We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.",
    "num_pages": 12
}