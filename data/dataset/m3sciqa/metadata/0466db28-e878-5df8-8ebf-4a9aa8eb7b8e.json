{
    "uuid": "0466db28-e878-5df8-8ebf-4a9aa8eb7b8e",
    "title": "Dual-Level Collaborative Transformer for Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yunpeng Luo",
        "Jiayi Ji",
        "Xiaoshuai Sun",
        "Liujuan Cao",
        "Yongjian Wu",
        "Feiyue Huang",
        "Chia-Wen Lin",
        "Rongrong Ji"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.06462v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\0466db28-e878-5df8-8ebf-4a9aa8eb7b8e.pdf",
    "bibtex": "@misc{luo2021duallevelcollaborativetransformerforimage,\n    title = {Dual-Level Collaborative Transformer for Image Captioning},\n    author = {Yunpeng Luo and Jiayi Ji and Xiaoshuai Sun and Liujuan Cao and Yongjian Wu and Feiyue Huang and Chia-Wen Lin and Rongrong Ji},\n    year = {2021},\n    eprint = {2101.06462},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2101.06462},\n}",
    "abstract": "Descriptive region features extracted by object detection networks have\nplayed an important role in the recent advancements of image captioning.\nHowever, they are still criticized for the lack of contextual information and\nfine-grained details, which in contrast are the merits of traditional grid\nfeatures. In this paper, we introduce a novel Dual-Level Collaborative\nTransformer (DLCT) network to realize the complementary advantages of the two\nfeatures. Concretely, in DLCT, these two features are first processed by a\nnovelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a\nComprehensive Relation Attention component is also introduced to embed the\ngeometric information. In addition, we propose a Locality-Constrained Cross\nAttention module to address the semantic noises caused by the direct fusion of\nthese two features, where a geometric alignment graph is constructed to\naccurately align and reinforce region and grid features. To validate our model,\nwe conduct extensive experiments on the highly competitive MS-COCO dataset, and\nachieve new state-of-the-art performance on both local and online test sets,\ni.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split.\nCode is available at https://github.com/luo3300612/image-captioning-DLCT.",
    "num_pages": 8
}