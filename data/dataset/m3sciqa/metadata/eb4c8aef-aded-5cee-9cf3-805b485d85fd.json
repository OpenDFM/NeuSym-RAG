{
    "uuid": "eb4c8aef-aded-5cee-9cf3-805b485d85fd",
    "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Victor Sanh",
        "Albert Webson",
        "Colin Raffel",
        "Stephen H. Bach",
        "Lintang Sutawika",
        "Zaid Alyafeai",
        "Antoine Chaffin",
        "Arnaud Stiegler",
        "Teven Le Scao",
        "Arun Raja",
        "Manan Dey",
        "M Saiful Bari",
        "Canwen Xu",
        "Urmish Thakker",
        "Shanya Sharma Sharma",
        "Eliza Szczechla",
        "Taewoon Kim",
        "Gunjan Chhablani",
        "Nihal Nayak",
        "Debajyoti Datta",
        "Jonathan Chang",
        "Mike Tian-Jian Jiang",
        "Han Wang",
        "Matteo Manica",
        "Sheng Shen",
        "Zheng Xin Yong",
        "Harshit Pandey",
        "Rachel Bawden",
        "Thomas Wang",
        "Trishala Neeraj",
        "Jos Rozen",
        "Abheesht Sharma",
        "Andrea Santilli",
        "Thibault Fevry",
        "Jason Alan Fries",
        "Ryan Teehan",
        "Tali Bers",
        "Stella Biderman",
        "Leo Gao",
        "Thomas Wolf",
        "Alexander M. Rush"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08207v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\eb4c8aef-aded-5cee-9cf3-805b485d85fd.pdf",
    "bibtex": "@misc{sanh2022multitaskpromptedtrainingenableszeroshot,\n    title = {Multitask Prompted Training Enables Zero-Shot Task Generalization},\n    author = {Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},\n    year = {2022},\n    eprint = {2110.08207},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2110.08207},\n}",
    "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.",
    "num_pages": 216
}