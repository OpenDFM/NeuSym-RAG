{
    "uuid": "3e6ebb91-fb32-5754-abed-0d8518e037a8",
    "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jinhao Jiang",
        "Kun Zhou",
        "Zican Dong",
        "Keming Ye",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.09645v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\3e6ebb91-fb32-5754-abed-0d8518e037a8.pdf",
    "bibtex": "@misc{jiang2023structgptageneralframeworkfor,\n    title = {StructGPT: A General Framework for Large Language Model to Reason over Structured Data},\n    author = {Jinhao Jiang and Kun Zhou and Zican Dong and Keming Ye and Wayne Xin Zhao and Ji-Rong Wen},\n    year = {2023},\n    eprint = {2305.09645},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.09645},\n}",
    "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of\nlarge language models~(LLMs) over structured data in a unified way. Inspired by\nthe study on tool augmentation for LLMs, we develop an \\emph{Iterative\nReading-then-Reasoning~(IRR)} approach for solving question answering tasks\nbased on structured data, called \\textbf{StructGPT}. In our approach, we\nconstruct the specialized function to collect relevant evidence from structured\ndata (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on\nthe collected information (\\ie \\emph{reasoning}). Specially, we propose an\n\\emph{invoking-linearization-generation} procedure to support LLMs in reasoning\non the structured data with the help of the external interfaces. By iterating\nthis procedures with provided interfaces, our approach can gradually approach\nthe target answer to a given query. Extensive experiments conducted on three\ntypes of structured data demonstrate the effectiveness of our approach, which\ncan significantly boost the performance of ChatGPT and achieve comparable\nperformance against the full-data supervised-tuning baselines. Our codes and\ndata are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
    "num_pages": 15
}