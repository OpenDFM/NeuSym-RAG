{
    "uuid": "120aa43e-5a6b-5fc6-9e11-fecd0d3ca36e",
    "title": "Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zonghan Yang",
        "Xiaoyuan Yi",
        "Peng Li",
        "Yang Liu",
        "Xing Xie"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.04492v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\120aa43e-5a6b-5fc6-9e11-fecd0d3ca36e.pdf",
    "bibtex": "@misc{yang2023unifieddetoxifyinganddebiasingin,\n    title = {Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization},\n    author = {Zonghan Yang and Xiaoyuan Yi and Peng Li and Yang Liu and Xing Xie},\n    year = {2023},\n    eprint = {2210.04492},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.04492},\n}",
    "abstract": "Warning: this paper contains model outputs exhibiting offensiveness and\nbiases. Recently pre-trained language models (PLMs) have prospered in various\nnatural language generation (NLG) tasks due to their ability to generate fairly\nfluent text. Nevertheless, these models are observed to capture and reproduce\nharmful contents in training corpora, typically toxic language and social\nbiases, raising severe moral issues. Prior works on ethical NLG tackle\ndetoxifying and debiasing separately, which is problematic since we find\ndebiased models still exhibit toxicity while detoxified ones even exacerbate\nsocial biases. To address such a challenge, we propose the first unified\nframework of detoxifying and debiasing called UDDIA, which jointly formalizes\nthese two problems as rectifying the output space. We theoretically interpret\nour framework as learning a text distribution mixing weighted attributes.\nBesides, UDDIA conducts adaptive optimization of only a few parameters during\ndecoding based on a parameter-efficient tuning schema without any training\ndata. This leads to minimal generation quality loss and improved rectification\nperformance with acceptable computational cost. Experimental results\ndemonstrate that compared to several strong baselines, UDDIA achieves debiasing\nand detoxifying simultaneously and better balances efficiency and\neffectiveness, taking a further step towards practical ethical NLG.",
    "num_pages": 42
}