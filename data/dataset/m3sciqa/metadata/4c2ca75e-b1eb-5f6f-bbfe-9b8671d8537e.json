{
    "uuid": "4c2ca75e-b1eb-5f6f-bbfe-9b8671d8537e",
    "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Zhengyuan Yang",
        "Zhe Gan",
        "Jianfeng Wang",
        "Xiaowei Hu",
        "Yumao Lu",
        "Zicheng Liu",
        "Lijuan Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.05014v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\4c2ca75e-b1eb-5f6f-bbfe-9b8671d8537e.pdf",
    "bibtex": "@misc{yang2022anempiricalstudyofgpt3,\n    title = {An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA},\n    author = {Zhengyuan Yang and Zhe Gan and Jianfeng Wang and Xiaowei Hu and Yumao Lu and Zicheng Liu and Lijuan Wang},\n    year = {2022},\n    eprint = {2109.05014},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2109.05014},\n}",
    "abstract": "Knowledge-based visual question answering (VQA) involves answering questions\nthat require external knowledge not present in the image. Existing methods\nfirst retrieve knowledge from external resources, then reason over the selected\nknowledge, the input image, and question for answer prediction. However, this\ntwo-step approach could lead to mismatches that potentially limit the VQA\nperformance. For example, the retrieved knowledge might be noisy and irrelevant\nto the question, and the re-embedded knowledge features during reasoning might\ndeviate from their original meanings in the knowledge base (KB). To address\nthis challenge, we propose PICa, a simple yet effective method that Prompts\nGPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by\nGPT-3's power in knowledge retrieval and question answering, instead of using\nstructured KBs as in previous work, we treat GPT-3 as an implicit and\nunstructured KB that can jointly acquire and process relevant knowledge.\nSpecifically, we first convert the image into captions (or tags) that GPT-3 can\nunderstand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just\nproviding a few in-context VQA examples. We further boost performance by\ncarefully investigating: (i) what text formats best describe the image content,\nand (ii) how in-context examples can be better selected and used. PICa unlocks\nthe first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa\nsurpasses the supervised state of the art by an absolute +8.6 points on the\nOK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent\nfew-shot performance.",
    "num_pages": 10
}