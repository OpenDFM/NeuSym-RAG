{
    "uuid": "b24e7069-38ff-5103-b30a-c8d278132f0a",
    "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chenxu Wang",
        "Ping Jian",
        "Mu Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2311.00367v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b24e7069-38ff-5103-b30a-c8d278132f0a.pdf",
    "bibtex": "@misc{wang2023promptbasedlogicalsemanticsenhancementfor,\n    title = {Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition},\n    author = {Chenxu Wang and Ping Jian and Mu Huang},\n    year = {2023},\n    eprint = {2311.00367},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2311.00367},\n}",
    "abstract": "Implicit Discourse Relation Recognition (IDRR), which infers discourse\nrelations without the help of explicit connectives, is still a crucial and\nchallenging task for discourse parsing. Recent works tend to exploit the\nhierarchical structure information from the annotated senses, which demonstrate\nenhanced discourse relation representations can be obtained by integrating\nsense hierarchy. Nevertheless, the performance and robustness for IDRR are\nsignificantly constrained by the availability of annotated data. Fortunately,\nthere is a wealth of unannotated utterances with explicit connectives, that can\nbe utilized to acquire enriched discourse relation features. In light of such\nmotivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)\nmethod for IDRR. Essentially, our method seamlessly injects knowledge relevant\nto discourse relation into pre-trained language models through prompt-based\nconnective prediction. Furthermore, considering the prompt-based connective\nprediction exhibits local dependencies due to the deficiency of masked language\nmodel (MLM) in capturing global semantics, we design a novel self-supervised\nlearning objective based on mutual information maximization to derive enhanced\nrepresentations of logical semantics for IDRR. Experimental results on PDTB 2.0\nand CoNLL16 datasets demonstrate that our method achieves outstanding and\nconsistent performance against the current state-of-the-art models.",
    "num_pages": 13
}