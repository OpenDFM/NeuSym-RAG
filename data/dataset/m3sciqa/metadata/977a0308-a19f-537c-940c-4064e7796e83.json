{
    "uuid": "977a0308-a19f-537c-940c-4064e7796e83",
    "title": "Neural Text Generation with Unlikelihood Training",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Sean Welleck",
        "Ilia Kulikov",
        "Stephen Roller",
        "Emily Dinan",
        "Kyunghyun Cho",
        "Jason Weston"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.04319v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\977a0308-a19f-537c-940c-4064e7796e83.pdf",
    "bibtex": "@misc{welleck2019neuraltextgenerationwithunlikelihood,\n    title = {Neural Text Generation with Unlikelihood Training},\n    author = {Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},\n    year = {2019},\n    eprint = {1908.04319},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1908.04319},\n}",
    "abstract": "Neural text generation is a key tool in natural language applications, but it\nis well known there are major problems at its core. In particular, standard\nlikelihood training and decoding leads to dull and repetitive outputs. While\nsome post-hoc fixes have been proposed, in particular top-$k$ and nucleus\nsampling, they do not address the fact that the token-level probabilities\npredicted by the model are poor. In this paper we show that the likelihood\nobjective itself is at fault, resulting in a model that assigns too much\nprobability to sequences containing repeats and frequent words, unlike those\nfrom the human training distribution. We propose a new objective, unlikelihood\ntraining, which forces unlikely generations to be assigned lower probability by\nthe model. We show that both token and sequence level unlikelihood training\ngive less repetitive, less dull text while maintaining perplexity, giving\nsuperior generations using standard greedy or beam search. According to human\nevaluations, our approach with standard beam search also outperforms the\ncurrently popular decoding methods of nucleus sampling or beam blocking, thus\nproviding a strong alternative to existing techniques.",
    "num_pages": 17
}