{
    "uuid": "7aceb4e8-8d59-53ed-ad15-af3bb2315845",
    "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jason Hoelscher-Obermaier",
        "Julia Persson",
        "Esben Kran",
        "Ioannis Konstas",
        "Fazl Barez"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.17553v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\7aceb4e8-8d59-53ed-ad15-af3bb2315845.pdf",
    "bibtex": "@misc{hoelscherobermaier2023detectingeditfailuresinlarge,\n    title = {Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark},\n    author = {Jason Hoelscher-Obermaier and Julia Persson and Esben Kran and Ioannis Konstas and Fazl Barez},\n    year = {2023},\n    eprint = {2305.17553},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.17553},\n}",
    "abstract": "Recent model editing techniques promise to mitigate the problem of memorizing\nfalse or outdated associations during LLM training. However, we show that these\ntechniques can introduce large unwanted side effects which are not detected by\nexisting specificity benchmarks. We extend the existing CounterFact benchmark\nto include a dynamic component and dub our benchmark CounterFact+.\nAdditionally, we extend the metrics used for measuring specificity by a\nprincipled KL divergence-based metric. We use this improved benchmark to\nevaluate recent model editing techniques and find that they suffer from low\nspecificity. Our findings highlight the need for improved specificity\nbenchmarks that identify and prevent unwanted side effects.",
    "num_pages": 10
}