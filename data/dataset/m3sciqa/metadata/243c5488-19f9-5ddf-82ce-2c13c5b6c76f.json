{
    "uuid": "243c5488-19f9-5ddf-82ce-2c13c5b6c76f",
    "title": "Modelling General Properties of Nouns by Selectively Averaging Contextualised Embeddings",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Na Li",
        "Zied Bouraoui",
        "Jose Camacho Collados",
        "Luis Espinosa-Anke",
        "Qing Gu",
        "Steven Schockaert"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.07580v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\243c5488-19f9-5ddf-82ce-2c13c5b6c76f.pdf",
    "bibtex": "@misc{li2021modellinggeneralpropertiesofnouns,\n    title = {Modelling General Properties of Nouns by Selectively Averaging Contextualised Embeddings},\n    author = {Na Li and Zied Bouraoui and Jose Camacho Collados and Luis Espinosa-Anke and Qing Gu and Steven Schockaert},\n    year = {2021},\n    eprint = {2012.07580},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.07580},\n}",
    "abstract": "While the success of pre-trained language models has largely eliminated the\nneed for high-quality static word vectors in many NLP applications, such\nvectors continue to play an important role in tasks where words need to be\nmodelled in the absence of linguistic context. In this paper, we explore how\nthe contextualised embeddings predicted by BERT can be used to produce\nhigh-quality word vectors for such domains, in particular related to knowledge\nbase completion, where our focus is on capturing the semantic properties of\nnouns. We find that a simple strategy of averaging the contextualised\nembeddings of masked word mentions leads to vectors that outperform the static\nword vectors learned by BERT, as well as those from standard word embedding\nmodels, in property induction tasks. We notice in particular that masking\ntarget words is critical to achieve this strong performance, as the resulting\nvectors focus less on idiosyncratic properties and more on general semantic\nproperties. Inspired by this view, we propose a filtering strategy which is\naimed at removing the most idiosyncratic mention vectors, allowing us to obtain\nfurther performance gains in property induction.",
    "num_pages": 16
}