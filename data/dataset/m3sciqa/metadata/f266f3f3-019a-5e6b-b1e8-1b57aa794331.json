{
    "uuid": "f266f3f3-019a-5e6b-b1e8-1b57aa794331",
    "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ehsan Kamalloo",
        "Nouha Dziri",
        "Charles L. A. Clarke",
        "Davood Rafiei"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.06984v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f266f3f3-019a-5e6b-b1e8-1b57aa794331.pdf",
    "bibtex": "@misc{kamalloo2023evaluatingopendomainquestionansweringin,\n    title = {Evaluating Open-Domain Question Answering in the Era of Large Language Models},\n    author = {Ehsan Kamalloo and Nouha Dziri and Charles L. A. Clarke and Davood Rafiei},\n    year = {2023},\n    eprint = {2305.06984},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.06984},\n}",
    "abstract": "Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.",
    "num_pages": 14
}