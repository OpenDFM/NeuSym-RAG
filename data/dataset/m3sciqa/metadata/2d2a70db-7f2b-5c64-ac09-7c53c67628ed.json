{
    "uuid": "2d2a70db-7f2b-5c64-ac09-7c53c67628ed",
    "title": "Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Evgeny Lagutin",
        "Daniil Gavrilov",
        "Pavel Kalaidin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.04229v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\2d2a70db-7f2b-5c64-ac09-7c53c67628ed.pdf",
    "bibtex": "@misc{lagutin2021implicitunlikelihoodtrainingimprovingneural,\n    title = {Implicit Unlikelihood Training: Improving Neural Text Generation with Reinforcement Learning},\n    author = {Evgeny Lagutin and Daniil Gavrilov and Pavel Kalaidin},\n    year = {2021},\n    eprint = {2101.04229},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2101.04229},\n}",
    "abstract": "Likelihood training and maximization-based decoding result in dull and\nrepetitive generated texts even when using powerful language models (Holtzman\net al., 2019). Adding a loss function for regularization was shown to improve\ntext generation output by helping avoid unwanted properties, such as\ncontradiction or repetition (Li at al., 2020). In this work, we propose\nfine-tuning a language model by using policy gradient reinforcement learning,\ndirectly optimizing for better generation. We apply this approach to minimizing\nrepetition in generated text, and show that, when combined with unlikelihood\ntraining (Welleck et al., 2020), our method further reduces repetition without\nimpacting the language model quality. We also evaluate other methods for\nimproving generation at training and decoding time, and compare them using\nvarious metrics aimed at control for better text generation output.",
    "num_pages": 10
}