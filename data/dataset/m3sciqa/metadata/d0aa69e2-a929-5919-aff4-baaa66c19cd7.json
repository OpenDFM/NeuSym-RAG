{
    "uuid": "d0aa69e2-a929-5919-aff4-baaa66c19cd7",
    "title": "The birth of Romanian BERT",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Stefan Daniel Dumitrescu",
        "Andrei-Marius Avram",
        "Sampo Pyysalo"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.08712v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\d0aa69e2-a929-5919-aff4-baaa66c19cd7.pdf",
    "bibtex": "@misc{dumitrescu2020thebirthofromanianbert,\n    title = {The birth of Romanian BERT},\n    author = {Stefan Daniel Dumitrescu and Andrei-Marius Avram and Sampo Pyysalo},\n    year = {2020},\n    eprint = {2009.08712},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.08712},\n}",
    "abstract": "Large-scale pretrained language models have become ubiquitous in Natural\nLanguage Processing. However, most of these models are available either in\nhigh-resource languages, in particular English, or as multilingual models that\ncompromise performance on individual languages for coverage. This paper\nintroduces Romanian BERT, the first purely Romanian transformer-based language\nmodel, pretrained on a large text corpus. We discuss corpus composition and\ncleaning, the model training process, as well as an extensive evaluation of the\nmodel on various Romanian datasets. We open source not only the model itself,\nbut also a repository that contains information on how to obtain the corpus,\nfine-tune and use this model in production (with practical examples), and how\nto fully replicate the evaluation process.",
    "num_pages": 5
}