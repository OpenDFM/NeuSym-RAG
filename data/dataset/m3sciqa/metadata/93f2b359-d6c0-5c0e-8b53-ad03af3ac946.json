{
    "uuid": "93f2b359-d6c0-5c0e-8b53-ad03af3ac946",
    "title": "MoLE : Mixture of Language Experts for Multi-Lingual Automatic Speech Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yoohwan Kwon",
        "Soo-Whan Chung"
    ],
    "pdf_url": "http://arxiv.org/pdf/2302.13750v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\93f2b359-d6c0-5c0e-8b53-ad03af3ac946.pdf",
    "bibtex": "@misc{kwon2023molemixtureoflanguage,\n    title = {MoLE : Mixture of Language Experts for Multi-Lingual Automatic Speech Recognition},\n    author = {Yoohwan Kwon and Soo-Whan Chung},\n    year = {2023},\n    eprint = {2302.13750},\n    archivePrefix = {arXiv},\n    primaryClass = {eess.AS},\n    url = {http://arxiv.org/abs/2302.13750},\n}",
    "abstract": "Multi-lingual speech recognition aims to distinguish linguistic expressions\nin different languages and integrate acoustic processing simultaneously. In\ncontrast, current multi-lingual speech recognition research follows a\nlanguage-aware paradigm, mainly targeted to improve recognition performance\nrather than discriminate language characteristics. In this paper, we present a\nmulti-lingual speech recognition network named\nMixture-of-Language-Expert(MoLE), which digests speech in a variety of\nlanguages. Specifically, MoLE analyzes linguistic expression from input speech\nin arbitrary languages, activating a language-specific expert with a\nlightweight language tokenizer. The tokenizer not only activates experts, but\nalso estimates the reliability of the activation. Based on the reliability, the\nactivated expert and the language-agnostic expert are aggregated to represent\nlanguage-conditioned embedding for efficient speech recognition. Our proposed\nmodel is evaluated in 5 languages scenario, and the experimental results show\nthat our structure is advantageous on multi-lingual recognition, especially for\nspeech in low-resource language.",
    "num_pages": 5
}