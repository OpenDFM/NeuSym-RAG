{
    "uuid": "d3c84791-eee6-5f1b-8500-1c0a84375f47",
    "title": "Progressive Tree-Structured Prototype Network for End-to-End Image Captioning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Pengpeng Zeng",
        "Jinkuan Zhu",
        "Jingkuan Song",
        "Lianli Gao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.09460v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\d3c84791-eee6-5f1b-8500-1c0a84375f47.pdf",
    "bibtex": "@misc{zeng2022progressivetreestructuredprototypenetworkfor,\n    title = {Progressive Tree-Structured Prototype Network for End-to-End Image Captioning},\n    author = {Pengpeng Zeng and Jinkuan Zhu and Jingkuan Song and Lianli Gao},\n    year = {2022},\n    eprint = {2211.09460},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2211.09460},\n}",
    "abstract": "Studies of image captioning are shifting towards a trend of a fully\nend-to-end paradigm by leveraging powerful visual pre-trained models and\ntransformer-based generation architecture for more flexible model training and\nfaster inference speed. State-of-the-art approaches simply extract isolated\nconcepts or attributes to assist description generation. However, such\napproaches do not consider the hierarchical semantic structure in the textual\ndomain, which leads to an unpredictable mapping between visual representations\nand concept words. To this end, we propose a novel Progressive Tree-Structured\nprototype Network (dubbed PTSN), which is the first attempt to narrow down the\nscope of prediction words with appropriate semantics by modeling the\nhierarchical textual semantics. Specifically, we design a novel embedding\nmethod called tree-structured prototype, producing a set of hierarchical\nrepresentative embeddings which capture the hierarchical semantic structure in\ntextual space. To utilize such tree-structured prototypes into visual\ncognition, we also propose a progressive aggregation module to exploit semantic\nrelationships within the image and prototypes. By applying our PTSN to the\nend-to-end captioning framework, extensive experiments conducted on MSCOCO\ndataset show that our method achieves a new state-of-the-art performance with\n144.2% (single model) and 146.5% (ensemble of 4 models) CIDEr scores on\n`Karpathy' split and 141.4% (c5) and 143.9% (c40) CIDEr scores on the official\nonline test server. Trained models and source code have been released at:\nhttps://github.com/NovaMind-Z/PTSN.",
    "num_pages": 9
}