{
    "uuid": "97bf5a6b-8d1c-533a-a00c-d1ca31d3256d",
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Tri Dao",
        "Daniel Y. Fu",
        "Stefano Ermon",
        "Atri Rudra",
        "Christopher Ré"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.14135v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\97bf5a6b-8d1c-533a-a00c-d1ca31d3256d.pdf",
    "bibtex": "@misc{dao2022flashattentionfastandmemoryefficientexact,\n    title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},\n    author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},\n    year = {2022},\n    eprint = {2205.14135},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2205.14135},\n}",
    "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and\nmemory complexity of self-attention are quadratic in sequence length.\nApproximate attention methods have attempted to address this problem by trading\noff model quality to reduce the compute complexity, but often do not achieve\nwall-clock speedup. We argue that a missing principle is making attention\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\nuses tiling to reduce the number of memory reads/writes between GPU high\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\nFlashAttention, showing that it requires fewer HBM accesses than standard\nattention, and is optimal for a range of SRAM sizes. We also extend\nFlashAttention to block-sparse attention, yielding an approximate attention\nalgorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15%\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\nMLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K),\nand 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\nlong-document classification) and entirely new capabilities: the first\nTransformers to achieve better-than-chance performance on the Path-X challenge\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\naccuracy).",
    "num_pages": 34
}