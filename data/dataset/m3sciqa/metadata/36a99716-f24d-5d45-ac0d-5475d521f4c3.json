{
    "uuid": "36a99716-f24d-5d45-ac0d-5475d521f4c3",
    "title": "The Surprising Power of Graph Neural Networks with Random Node Initialization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Ralph Abboud",
        "İsmail İlkan Ceylan",
        "Martin Grohe",
        "Thomas Lukasiewicz"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01179v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\36a99716-f24d-5d45-ac0d-5475d521f4c3.pdf",
    "bibtex": "@misc{abboud2021thesurprisingpowerofgraph,\n    title = {The Surprising Power of Graph Neural Networks with Random Node Initialization},\n    author = {Ralph Abboud and İsmail İlkan Ceylan and Martin Grohe and Thomas Lukasiewicz},\n    year = {2021},\n    eprint = {2010.01179},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2010.01179},\n}",
    "abstract": "Graph neural networks (GNNs) are effective models for representation learning\non relational data. However, standard GNNs are limited in their expressive\npower, as they cannot distinguish graphs beyond the capability of the\nWeisfeiler-Leman graph isomorphism heuristic. In order to break this\nexpressiveness barrier, GNNs have been enhanced with random node initialization\n(RNI), where the idea is to train and run the models with randomized initial\nnode features. In this work, we analyze the expressive power of GNNs with RNI,\nand prove that these models are universal, a first such result for GNNs not\nrelying on computationally demanding higher-order properties. This universality\nresult holds even with partially randomized initial node features, and\npreserves the invariance properties of GNNs in expectation. We then empirically\nanalyze the effect of RNI on GNNs, based on carefully constructed datasets. Our\nempirical findings support the superior performance of GNNs with RNI over\nstandard GNNs.",
    "num_pages": 14
}