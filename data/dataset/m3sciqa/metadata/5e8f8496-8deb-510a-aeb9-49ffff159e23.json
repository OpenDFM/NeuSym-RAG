{
    "uuid": "5e8f8496-8deb-510a-aeb9-49ffff159e23",
    "title": "Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Haoyu Zhang",
        "Yu Wang",
        "Guanghao Yin",
        "Kejun Liu",
        "Yuanyuan Liu",
        "Tianshu Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.05804v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5e8f8496-8deb-510a-aeb9-49ffff159e23.pdf",
    "bibtex": "@misc{zhang2023learninglanguageguidedadaptivehypermodalityrepresentation,\n    title = {Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis},\n    author = {Haoyu Zhang and Yu Wang and Guanghao Yin and Kejun Liu and Yuanyuan Liu and Tianshu Yu},\n    year = {2023},\n    eprint = {2310.05804},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2310.05804},\n}",
    "abstract": "Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.",
    "num_pages": 12
}