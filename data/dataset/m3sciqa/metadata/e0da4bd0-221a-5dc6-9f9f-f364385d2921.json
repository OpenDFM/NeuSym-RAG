{
    "uuid": "e0da4bd0-221a-5dc6-9f9f-f364385d2921",
    "title": "XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Francesco Barbieri",
        "Luis Espinosa Anke",
        "Jose Camacho-Collados"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.12250v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e0da4bd0-221a-5dc6-9f9f-f364385d2921.pdf",
    "bibtex": "@misc{barbieri2022xlmtmultilinguallanguagemodelsin,\n    title = {XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond},\n    author = {Francesco Barbieri and Luis Espinosa Anke and Jose Camacho-Collados},\n    year = {2022},\n    eprint = {2104.12250},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.12250},\n}",
    "abstract": "Language models are ubiquitous in current NLP, and their multilingual\ncapacity has recently attracted considerable attention. However, current\nanalyses have almost exclusively focused on (multilingual variants of) standard\nbenchmarks, and have relied on clean pre-training and task-specific corpora as\nmultilingual signals. In this paper, we introduce XLM-T, a model to train and\nevaluate multilingual language models in Twitter. In this paper we provide: (1)\na new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020)\nmodel pre-trained on millions of tweets in over thirty languages, alongside\nstarter code to subsequently fine-tune on a target task; and (2) a set of\nunified sentiment analysis Twitter datasets in eight different languages and a\nXLM-T model fine-tuned on them.",
    "num_pages": 9
}