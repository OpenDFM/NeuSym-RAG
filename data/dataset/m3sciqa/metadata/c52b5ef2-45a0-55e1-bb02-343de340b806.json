{
    "uuid": "c52b5ef2-45a0-55e1-bb02-343de340b806",
    "title": "Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Yuxin Ren",
        "Zihan Zhong",
        "Xingjian Shi",
        "Yi Zhu",
        "Chun Yuan",
        "Mu Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.09651v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\c52b5ef2-45a0-55e1-bb02-343de340b806.pdf",
    "bibtex": "@misc{ren2024tailoringinstructionstostudentslearning,\n    title = {Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation},\n    author = {Yuxin Ren and Zihan Zhong and Xingjian Shi and Yi Zhu and Chun Yuan and Mu Li},\n    year = {2024},\n    eprint = {2305.09651},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.09651},\n}",
    "abstract": "It has been commonly observed that a teacher model with superior performance\ndoes not necessarily result in a stronger student, highlighting a discrepancy\nbetween current teacher training practices and effective knowledge transfer. In\norder to enhance the guidance of the teacher training process, we introduce the\nconcept of distillation influence to determine the impact of distillation from\neach training sample on the student's generalization ability. In this paper, we\npropose Learning Good Teacher Matters (LGTM), an efficient training technique\nfor incorporating distillation influence into the teacher's learning process.\nBy prioritizing samples that are likely to enhance the student's generalization\nability, our LGTM outperforms 10 common knowledge distillation baselines on 6\ntext classification tasks in the GLUE benchmark.",
    "num_pages": 15
}