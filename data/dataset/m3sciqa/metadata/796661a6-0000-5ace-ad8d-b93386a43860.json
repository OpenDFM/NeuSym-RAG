{
    "uuid": "796661a6-0000-5ace-ad8d-b93386a43860",
    "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Adams Wei Yu",
        "David Dohan",
        "Minh-Thang Luong",
        "Rui Zhao",
        "Kai Chen",
        "Mohammad Norouzi",
        "Quoc V. Le"
    ],
    "pdf_url": "http://arxiv.org/pdf/1804.09541v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\796661a6-0000-5ace-ad8d-b93386a43860.pdf",
    "bibtex": "@misc{yu2018qanetcombininglocalconvolutionwith,\n    title = {QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension},\n    author = {Adams Wei Yu and David Dohan and Minh-Thang Luong and Rui Zhao and Kai Chen and Mohammad Norouzi and Quoc V. Le},\n    year = {2018},\n    eprint = {1804.09541},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1804.09541},\n}",
    "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are\nprimarily based on recurrent neural networks (RNNs) with attention. Despite\ntheir success, these models are often slow for both training and inference due\nto the sequential nature of RNNs. We propose a new Q\\&A architecture called\nQANet, which does not require recurrent networks: Its encoder consists\nexclusively of convolution and self-attention, where convolution models local\ninteractions and self-attention models global interactions. On the SQuAD\ndataset, our model is 3x to 13x faster in training and 4x to 9x faster in\ninference, while achieving equivalent accuracy to recurrent models. The\nspeed-up gain allows us to train the model with much more data. We hence\ncombine our model with data generated by backtranslation from a neural machine\ntranslation model. On the SQuAD dataset, our single model, trained with\naugmented data, achieves 84.6 F1 score on the test set, which is significantly\nbetter than the best published F1 score of 81.8.",
    "num_pages": 16
}