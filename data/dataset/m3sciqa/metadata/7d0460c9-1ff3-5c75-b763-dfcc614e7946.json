{
    "uuid": "7d0460c9-1ff3-5c75-b763-dfcc614e7946",
    "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Nguyen Luong Tran",
        "Duong Minh Le",
        "Dat Quoc Nguyen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09701v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\7d0460c9-1ff3-5c75-b763-dfcc614e7946.pdf",
    "bibtex": "@misc{tran2022bartphopretrainedsequencetosequencemodelsfor,\n    title = {BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese},\n    author = {Nguyen Luong Tran and Duong Minh Le and Dat Quoc Nguyen},\n    year = {2022},\n    eprint = {2109.09701},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.09701},\n}",
    "abstract": "We present BARTpho with two versions, BARTpho-syllable and BARTpho-word,\nwhich are the first public large-scale monolingual sequence-to-sequence models\npre-trained for Vietnamese. BARTpho uses the \"large\" architecture and the\npre-training scheme of the sequence-to-sequence denoising autoencoder BART,\nthus it is especially suitable for generative NLP tasks. We conduct experiments\nto compare our BARTpho with its competitor mBART on a downstream task of\nVietnamese text summarization and show that: in both automatic and human\nevaluations, BARTpho outperforms the strong baseline mBART and improves the\nstate-of-the-art. We further evaluate and compare BARTpho and mBART on the\nVietnamese capitalization and punctuation restoration tasks and also find that\nBARTpho is more effective than mBART on these two tasks. We publicly release\nBARTpho to facilitate future research and applications of generative Vietnamese\nNLP tasks. Our BARTpho models are available at\nhttps://github.com/VinAIResearch/BARTpho",
    "num_pages": 5
}