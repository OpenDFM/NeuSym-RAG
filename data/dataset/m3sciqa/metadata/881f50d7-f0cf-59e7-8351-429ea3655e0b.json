{
    "uuid": "881f50d7-f0cf-59e7-8351-429ea3655e0b",
    "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Yuzhuang Xu",
        "Shuo Wang",
        "Peng Li",
        "Fuwen Luo",
        "Xiaolong Wang",
        "Weidong Liu",
        "Yang Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2309.04658v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\881f50d7-f0cf-59e7-8351-429ea3655e0b.pdf",
    "bibtex": "@misc{xu2024exploringlargelanguagemodelsfor,\n    title = {Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf},\n    author = {Yuzhuang Xu and Shuo Wang and Peng Li and Fuwen Luo and Xiaolong Wang and Weidong Liu and Yang Liu},\n    year = {2024},\n    eprint = {2309.04658},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2309.04658},\n}",
    "abstract": "Communication games, which we refer to as incomplete information games that\nheavily depend on natural language communication, hold significant research\nvalue in fields such as economics, social science, and artificial intelligence.\nIn this work, we explore the problem of how to engage large language models\n(LLMs) in communication games, and in response, propose a tuning-free\nframework. Our approach keeps LLMs frozen, and relies on the retrieval and\nreflection on past communications and experiences for improvement. An empirical\nstudy on the representative and widely-studied communication game,\n``Werewolf'', demonstrates that our framework can effectively play Werewolf\ngame without tuning the parameters of the LLMs. More importantly, strategic\nbehaviors begin to emerge in our experiments, suggesting that it will be a\nfruitful journey to engage LLMs in communication games and associated domains.",
    "num_pages": 23
}