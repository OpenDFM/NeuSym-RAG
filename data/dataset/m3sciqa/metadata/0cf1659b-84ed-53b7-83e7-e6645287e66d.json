{
    "uuid": "0cf1659b-84ed-53b7-83e7-e6645287e66d",
    "title": "Prototypical Representation Learning for Relation Extraction",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Ning Ding",
        "Xiaobin Wang",
        "Yao Fu",
        "Guangwei Xu",
        "Rui Wang",
        "Pengjun Xie",
        "Ying Shen",
        "Fei Huang",
        "Hai-Tao Zheng",
        "Rui Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.11647v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\0cf1659b-84ed-53b7-83e7-e6645287e66d.pdf",
    "bibtex": "@misc{ding2021prototypicalrepresentationlearningforrelation,\n    title = {Prototypical Representation Learning for Relation Extraction},\n    author = {Ning Ding and Xiaobin Wang and Yao Fu and Guangwei Xu and Rui Wang and Pengjun Xie and Ying Shen and Fei Huang and Hai-Tao Zheng and Rui Zhang},\n    year = {2021},\n    eprint = {2103.11647},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2103.11647},\n}",
    "abstract": "Recognizing relations between entities is a pivotal task of relational\nlearning. Learning relation representations from distantly-labeled datasets is\ndifficult because of the abundant label noise and complicated expressions in\nhuman language. This paper aims to learn predictive, interpretable, and robust\nrelation representations from distantly-labeled data that are effective in\ndifferent settings, including supervised, distantly supervised, and few-shot\nlearning. Instead of solely relying on the supervision from noisy labels, we\npropose to learn prototypes for each relation from contextual information to\nbest explore the intrinsic semantics of relations. Prototypes are\nrepresentations in the feature space abstracting the essential semantics of\nrelations between entities in sentences. We learn prototypes based on\nobjectives with clear geometric interpretation, where the prototypes are unit\nvectors uniformly dispersed in a unit ball, and statement embeddings are\ncentered at the end of their corresponding prototype vectors on the surface of\nthe ball. This approach allows us to learn meaningful, interpretable prototypes\nfor the final classification. Results on several relation learning tasks show\nthat our model significantly outperforms the previous state-of-the-art models.\nWe further demonstrate the robustness of the encoder and the interpretability\nof prototypes with extensive experiments.",
    "num_pages": 16
}