{
    "uuid": "8de2532b-6746-59ad-bb7d-7a42ce02682d",
    "title": "AMR Parsing with Action-Pointer Transformer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jiawei Zhou",
        "Tahira Naseem",
        "Ramón Fernandez Astudillo",
        "Radu Florian"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.14674v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\8de2532b-6746-59ad-bb7d-7a42ce02682d.pdf",
    "bibtex": "@misc{zhou2021amrparsingwithactionpointertransformer,\n    title = {AMR Parsing with Action-Pointer Transformer},\n    author = {Jiawei Zhou and Tahira Naseem and Ramón Fernandez Astudillo and Radu Florian},\n    year = {2021},\n    eprint = {2104.14674},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.14674},\n}",
    "abstract": "Abstract Meaning Representation parsing is a sentence-to-graph prediction\ntask where target nodes are not explicitly aligned to sentence tokens. However,\nsince graph nodes are semantically based on one or more sentence tokens,\nimplicit alignments can be derived. Transition-based parsers operate over the\nsentence from left to right, capturing this inductive bias via alignments at\nthe cost of limited expressiveness. In this work, we propose a transition-based\nsystem that combines hard-attention over sentences with a target-side action\npointer mechanism to decouple source tokens from node representations and\naddress alignments. We model the transitions as well as the pointer mechanism\nthrough straightforward modifications within a single Transformer architecture.\nParser state and graph structure information are efficiently encoded using\nattention heads. We show that our action-pointer approach leads to increased\nexpressiveness and attains large gains (+1.6 points) against the best\ntransition-based AMR parser in very similar conditions. While using no graph\nre-categorization, our single model yields the second best Smatch score on AMR\n2.0 (81.8), which is further improved to 83.4 with silver data and ensemble\ndecoding.",
    "num_pages": 14
}