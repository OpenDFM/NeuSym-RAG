{
    "uuid": "0b48d3a0-c352-5afd-8dc7-9c186316d538",
    "title": "Facebook FAIR's WMT19 News Translation Task Submission",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Nathan Ng",
        "Kyra Yee",
        "Alexei Baevski",
        "Myle Ott",
        "Michael Auli",
        "Sergey Edunov"
    ],
    "pdf_url": "http://arxiv.org/pdf/1907.06616v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\0b48d3a0-c352-5afd-8dc7-9c186316d538.pdf",
    "bibtex": "@misc{ng2019facebookfairswmt19newstranslation,\n    title = {Facebook FAIR's WMT19 News Translation Task Submission},\n    author = {Nathan Ng and Kyra Yee and Alexei Baevski and Myle Ott and Michael Auli and Sergey Edunov},\n    year = {2019},\n    eprint = {1907.06616},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1907.06616},\n}",
    "abstract": "This paper describes Facebook FAIR's submission to the WMT19 shared news\ntranslation task. We participate in two language pairs and four language\ndirections, English <-> German and English <-> Russian. Following our\nsubmission from last year, our baseline systems are large BPE-based transformer\nmodels trained with the Fairseq sequence modeling toolkit which rely on sampled\nback-translations. This year we experiment with different bitext data filtering\nschemes, as well as with adding filtered back-translated data. We also ensemble\nand fine-tune our models on domain-specific data, then decode using noisy\nchannel model reranking. Our submissions are ranked first in all four\ndirections of the human evaluation campaign. On En->De, our system\nsignificantly outperforms other systems as well as human translations. This\nsystem improves upon our WMT'18 submission by 4.5 BLEU points.",
    "num_pages": 7
}