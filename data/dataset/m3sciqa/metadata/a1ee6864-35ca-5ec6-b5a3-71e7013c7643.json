{
    "uuid": "a1ee6864-35ca-5ec6-b5a3-71e7013c7643",
    "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Tianbao Xie",
        "Chen Henry Wu",
        "Peng Shi",
        "Ruiqi Zhong",
        "Torsten Scholak",
        "Michihiro Yasunaga",
        "Chien-Sheng Wu",
        "Ming Zhong",
        "Pengcheng Yin",
        "Sida I. Wang",
        "Victor Zhong",
        "Bailin Wang",
        "Chengzu Li",
        "Connor Boyle",
        "Ansong Ni",
        "Ziyu Yao",
        "Dragomir Radev",
        "Caiming Xiong",
        "Lingpeng Kong",
        "Rui Zhang",
        "Noah A. Smith",
        "Luke Zettlemoyer",
        "Tao Yu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.05966v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\a1ee6864-35ca-5ec6-b5a3-71e7013c7643.pdf",
    "bibtex": "@misc{xie2022unifiedskgunifyingandmultitaskingstructured,\n    title = {UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models},\n    author = {Tianbao Xie and Chen Henry Wu and Peng Shi and Ruiqi Zhong and Torsten Scholak and Michihiro Yasunaga and Chien-Sheng Wu and Ming Zhong and Pengcheng Yin and Sida I. Wang and Victor Zhong and Bailin Wang and Chengzu Li and Connor Boyle and Ansong Ni and Ziyu Yao and Dragomir Radev and Caiming Xiong and Lingpeng Kong and Rui Zhang and Noah A. Smith and Luke Zettlemoyer and Tao Yu},\n    year = {2022},\n    eprint = {2201.05966},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.05966},\n}",
    "abstract": "Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.",
    "num_pages": 30
}