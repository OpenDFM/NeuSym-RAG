{
    "uuid": "ce99c350-ea64-53eb-b705-c33884619d64",
    "title": "Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Puyuan Liu",
        "Chenyang Huang",
        "Lili Mou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.14521v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ce99c350-ea64-53eb-b705-c33884619d64.pdf",
    "bibtex": "@misc{liu2022learningnonautoregressivemodelsfromsearch,\n    title = {Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization},\n    author = {Puyuan Liu and Chenyang Huang and Lili Mou},\n    year = {2022},\n    eprint = {2205.14521},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.14521},\n}",
    "abstract": "Text summarization aims to generate a short summary for an input text. In\nthis work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)\napproach, which does not require parallel data for training. Our NAUS first\nperforms edit-based search towards a heuristically defined score, and generates\na summary as pseudo-groundtruth. Then, we train an encoder-only\nnon-autoregressive Transformer based on the search result. We also propose a\ndynamic programming approach for length-control decoding, which is important\nfor the summarization task. Experiments on two datasets show that NAUS achieves\nstate-of-the-art performance for unsupervised summarization, yet largely\nimproving inference efficiency. Further, our algorithm is able to perform\nexplicit length-transfer summary generation.",
    "num_pages": 14
}