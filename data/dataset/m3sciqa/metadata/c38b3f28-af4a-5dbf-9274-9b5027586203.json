{
    "uuid": "c38b3f28-af4a-5dbf-9274-9b5027586203",
    "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Suchin Gururangan",
        "Mike Lewis",
        "Ari Holtzman",
        "Noah A. Smith",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.05036v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\c38b3f28-af4a-5dbf-9274-9b5027586203.pdf",
    "bibtex": "@misc{gururangan2021demixlayersdisentanglingdomainsfor,\n    title = {DEMix Layers: Disentangling Domains for Modular Language Modeling},\n    author = {Suchin Gururangan and Mike Lewis and Ari Holtzman and Noah A. Smith and Luke Zettlemoyer},\n    year = {2021},\n    eprint = {2108.05036},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2108.05036},\n}",
    "abstract": "We introduce a new domain expert mixture (DEMix) layer that enables\nconditioning a language model (LM) on the domain of the input text. A DEMix\nlayer is a collection of expert feedforward networks, each specialized to a\ndomain, that makes the LM modular: experts can be mixed, added or removed after\ninitial training. Extensive experiments with autoregressive transformer LMs (up\nto 1.3B parameters) show that DEMix layers reduce test-time perplexity,\nincrease training efficiency, and enable rapid adaptation with little overhead.\nWe show that mixing experts during inference, using a parameter-free weighted\nensemble, allows the model to better generalize to heterogeneous or unseen\ndomains. We also show that experts can be added to iteratively incorporate new\ndomains without forgetting older ones, and that experts can be removed to\nrestrict access to unwanted domains, without additional training. Overall,\nthese results demonstrate benefits of explicitly conditioning on textual\ndomains during language modeling.",
    "num_pages": 21
}