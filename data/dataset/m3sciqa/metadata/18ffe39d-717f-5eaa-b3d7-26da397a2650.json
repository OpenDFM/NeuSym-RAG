{
    "uuid": "18ffe39d-717f-5eaa-b3d7-26da397a2650",
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J. Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.00295v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\18ffe39d-717f-5eaa-b3d7-26da397a2650.pdf",
    "bibtex": "@misc{tsai2019multimodaltransformerforunalignedmultimodal,\n    title = {Multimodal Transformer for Unaligned Multimodal Language Sequences},\n    author = {Yao-Hung Hubert Tsai and Shaojie Bai and Paul Pu Liang and J. Zico Kolter and Louis-Philippe Morency and Ruslan Salakhutdinov},\n    year = {2019},\n    eprint = {1906.00295},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1906.00295},\n}",
    "abstract": "Human language is often multimodal, which comprehends a mixture of natural\nlanguage, facial gestures, and acoustic behaviors. However, two major\nchallenges in modeling such multimodal human language time-series data exist:\n1) inherent data non-alignment due to variable sampling rates for the sequences\nfrom each modality; and 2) long-range dependencies between elements across\nmodalities. In this paper, we introduce the Multimodal Transformer (MulT) to\ngenerically address the above issues in an end-to-end manner without explicitly\naligning the data. At the heart of our model is the directional pairwise\ncrossmodal attention, which attends to interactions between multimodal\nsequences across distinct time steps and latently adapt streams from one\nmodality to another. Comprehensive experiments on both aligned and non-aligned\nmultimodal time-series show that our model outperforms state-of-the-art methods\nby a large margin. In addition, empirical analysis suggests that correlated\ncrossmodal signals are able to be captured by the proposed crossmodal attention\nmechanism in MulT.",
    "num_pages": 12
}