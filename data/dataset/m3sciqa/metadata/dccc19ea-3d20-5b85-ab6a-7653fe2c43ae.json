{
    "uuid": "dccc19ea-3d20-5b85-ab6a-7653fe2c43ae",
    "title": "Empirical Study of Zero-Shot NER with ChatGPT",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Tingyu Xie",
        "Qi Li",
        "Jian Zhang",
        "Yan Zhang",
        "Zuozhu Liu",
        "Hongwei Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.10035v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\dccc19ea-3d20-5b85-ab6a-7653fe2c43ae.pdf",
    "bibtex": "@misc{xie2023empiricalstudyofzeroshotner,\n    title = {Empirical Study of Zero-Shot NER with ChatGPT},\n    author = {Tingyu Xie and Qi Li and Jian Zhang and Yan Zhang and Zuozhu Liu and Hongwei Wang},\n    year = {2023},\n    eprint = {2310.10035},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.10035},\n}",
    "abstract": "Large language models (LLMs) exhibited powerful capability in various natural\nlanguage processing tasks. This work focuses on exploring LLM performance on\nzero-shot information extraction, with a focus on the ChatGPT and named entity\nrecognition (NER) task. Inspired by the remarkable reasoning capability of LLM\non symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods\nto NER and propose reasoning strategies tailored for NER. First, we explore a\ndecomposed question-answering paradigm by breaking down the NER task into\nsimpler subproblems by labels. Second, we propose syntactic augmentation to\nstimulate the model's intermediate thinking in two ways: syntactic prompting,\nwhich encourages the model to analyze the syntactic structure itself, and tool\naugmentation, which provides the model with the syntactic information generated\nby a parsing tool. Besides, we adapt self-consistency to NER by proposing a\ntwo-stage majority voting strategy, which first votes for the most consistent\nmentions, then the most consistent types. The proposed methods achieve\nremarkable improvements for zero-shot NER across seven benchmarks, including\nChinese and English datasets, and on both domain-specific and general-domain\nscenarios. In addition, we present a comprehensive analysis of the error types\nwith suggestions for optimization directions. We also verify the effectiveness\nof the proposed methods on the few-shot setting and other LLMs.",
    "num_pages": 22
}