{
    "uuid": "5671a75b-4bcf-5446-82c0-481b1cd418f2",
    "title": "Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Zhiyong Wu",
        "Lingpeng Kong",
        "Wei Bi",
        "Xiang Li",
        "Ben Kao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.14462v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\5671a75b-4bcf-5446-82c0-481b1cd418f2.pdf",
    "bibtex": "@misc{wu2021goodformisconceivedreasonsan,\n    title = {Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation},\n    author = {Zhiyong Wu and Lingpeng Kong and Wei Bi and Xiang Li and Ben Kao},\n    year = {2021},\n    eprint = {2105.14462},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2105.14462},\n}",
    "abstract": "A neural multimodal machine translation (MMT) system is one that aims to\nperform better translation by extending conventional text-only translation\nmodels with multimodal information. Many recent studies report improvements\nwhen equipping their models with the multimodal module, despite the controversy\nof whether such improvements indeed come from the multimodal part. We revisit\nthe contribution of multimodal information in MMT by devising two interpretable\nMMT models. To our surprise, although our models replicate similar gains as\nrecently developed multimodal-integrated systems achieved, our models learn to\nignore the multimodal information. Upon further investigation, we discover that\nthe improvements achieved by the multimodal models over text-only counterparts\nare in fact results of the regularization effect. We report empirical findings\nthat highlight the importance of MMT models' interpretability, and discuss how\nour findings will benefit future research.",
    "num_pages": 14
}