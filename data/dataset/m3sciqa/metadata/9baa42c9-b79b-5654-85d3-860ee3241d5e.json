{
    "uuid": "9baa42c9-b79b-5654-85d3-860ee3241d5e",
    "title": "Unifying Multimodal Transformer for Bi-directional Image and Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yupan Huang",
        "Hongwei Xue",
        "Bei Liu",
        "Yutong Lu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.09753v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\9baa42c9-b79b-5654-85d3-860ee3241d5e.pdf",
    "bibtex": "@misc{huang2021unifyingmultimodaltransformerforbidirectional,\n    title = {Unifying Multimodal Transformer for Bi-directional Image and Text Generation},\n    author = {Yupan Huang and Hongwei Xue and Bei Liu and Yutong Lu},\n    year = {2021},\n    eprint = {2110.09753},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2110.09753},\n}",
    "abstract": "We study the joint learning of image-to-text and text-to-image generations,\nwhich are naturally bi-directional tasks. Typical existing works design two\nseparate task-specific models for each task, which impose expensive design\nefforts. In this work, we propose a unified image-and-text generative framework\nbased on a single multimodal model to jointly study the bi-directional tasks.\nWe adopt Transformer as our unified architecture for its strong performance and\ntask-agnostic design. Specifically, we formulate both tasks as sequence\ngeneration tasks, where we represent images and text as unified sequences of\ntokens, and the Transformer learns multimodal interactions to generate\nsequences. We further propose two-level granularity feature representations and\nsequence-level training to improve the Transformer-based unified framework.\nExperiments show that our approach significantly improves previous\nTransformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for\ntext-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for\nfine-tuned image-to-text generation on the MS-COCO dataset. Our code is\navailable online.",
    "num_pages": 10
}