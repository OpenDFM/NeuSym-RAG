{
    "uuid": "e30a2f94-a104-572f-b30f-3ae7ff87f6e7",
    "title": "Improving Image Captioning via Predicting Structured Concepts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ting Wang",
        "Weidong Chen",
        "Yuanhe Tian",
        "Yan Song",
        "Zhendong Mao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2311.08223v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\e30a2f94-a104-572f-b30f-3ae7ff87f6e7.pdf",
    "bibtex": "@misc{wang2023improvingimagecaptioningviapredicting,\n    title = {Improving Image Captioning via Predicting Structured Concepts},\n    author = {Ting Wang and Weidong Chen and Yuanhe Tian and Yan Song and Zhendong Mao},\n    year = {2023},\n    eprint = {2311.08223},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2311.08223},\n}",
    "abstract": "Having the difficulty of solving the semantic gap between images and texts\nfor the image captioning task, conventional studies in this area paid some\nattention to treating semantic concepts as a bridge between the two modalities\nand improved captioning performance accordingly. Although promising results on\nconcept prediction were obtained, the aforementioned studies normally ignore\nthe relationship among concepts, which relies on not only objects in the image,\nbut also word dependencies in the text, so that offers a considerable potential\nfor improving the process of generating good descriptions. In this paper, we\npropose a structured concept predictor (SCP) to predict concepts and their\nstructures, then we integrate them into captioning, so as to enhance the\ncontribution of visual signals in this task via concepts and further use their\nrelations to distinguish cross-modal semantics for better description\ngeneration. Particularly, we design weighted graph convolutional networks\n(W-GCN) to depict concept relations driven by word dependencies, and then\nlearns differentiated contributions from these concepts for following decoding\nprocess. Therefore, our approach captures potential relations among concepts\nand discriminatively learns different concepts, so that effectively facilitates\nimage captioning with inherited information across modalities. Extensive\nexperiments and their results demonstrate the effectiveness of our approach as\nwell as each proposed module in this work.",
    "num_pages": 13
}