{
    "uuid": "3960e993-aece-5afc-a9d9-a232feae755a",
    "title": "Fine-grained Disentangled Representation Learning for Multimodal Emotion Recognition",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Xuechen Wang",
        "Wenjia Zeng",
        "Yong Chen",
        "Yong Qin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2312.13567v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\3960e993-aece-5afc-a9d9-a232feae755a.pdf",
    "bibtex": "@misc{sun2023finegraineddisentangledrepresentationlearningfor,\n    title = {Fine-grained Disentangled Representation Learning for Multimodal Emotion Recognition},\n    author = {Haoqin Sun and Shiwan Zhao and Xuechen Wang and Wenjia Zeng and Yong Chen and Yong Qin},\n    year = {2023},\n    eprint = {2312.13567},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.SD},\n    url = {http://arxiv.org/abs/2312.13567},\n}",
    "abstract": "Multimodal emotion recognition (MMER) is an active research field that aims\nto accurately recognize human emotions by fusing multiple perceptual\nmodalities. However, inherent heterogeneity across modalities introduces\ndistribution gaps and information redundancy, posing significant challenges for\nMMER. In this paper, we propose a novel fine-grained disentangled\nrepresentation learning (FDRL) framework to address these challenges.\nSpecifically, we design modality-shared and modality-private encoders to\nproject each modality into modality-shared and modality-private subspaces,\nrespectively. In the shared subspace, we introduce a fine-grained alignment\ncomponent to learn modality-shared representations, thus capturing modal\nconsistency. Subsequently, we tailor a fine-grained disparity component to\nconstrain the private subspaces, thereby learning modality-private\nrepresentations and enhancing their diversity. Lastly, we introduce a\nfine-grained predictor component to ensure that the labels of the output\nrepresentations from the encoders remain unchanged. Experimental results on the\nIEMOCAP dataset show that FDRL outperforms the state-of-the-art methods,\nachieving 78.34% and 79.44% on WAR and UAR, respectively.",
    "num_pages": 5
}