{
    "uuid": "393e9431-63b0-589d-88bb-62335baaf8bc",
    "title": "DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yizhe Zhang",
        "Siqi Sun",
        "Michel Galley",
        "Yen-Chun Chen",
        "Chris Brockett",
        "Xiang Gao",
        "Jianfeng Gao",
        "Jingjing Liu",
        "Bill Dolan"
    ],
    "pdf_url": "http://arxiv.org/pdf/1911.00536v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\393e9431-63b0-589d-88bb-62335baaf8bc.pdf",
    "bibtex": "@misc{zhang2020dialogptlargescalegenerativepretrainingfor,\n    title = {DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},\n    author = {Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},\n    year = {2020},\n    eprint = {1911.00536},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1911.00536},\n}",
    "abstract": "We present a large, tunable neural conversational response generation model,\nDialoGPT (dialogue generative pre-trained transformer). Trained on 147M\nconversation-like exchanges extracted from Reddit comment chains over a period\nspanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to human both in terms of automatic\nand human evaluation in single-turn dialogue settings. We show that\nconversational systems that leverage DialoGPT generate more relevant,\ncontentful and context-consistent responses than strong baseline systems. The\npre-trained model and training pipeline are publicly released to facilitate\nresearch into neural response generation and the development of more\nintelligent open-domain dialogue systems.",
    "num_pages": 10
}