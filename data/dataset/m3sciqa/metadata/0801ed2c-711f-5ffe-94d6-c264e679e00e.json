{
    "uuid": "0801ed2c-711f-5ffe-94d6-c264e679e00e",
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jianlin Su",
        "Yu Lu",
        "Shengfeng Pan",
        "Ahmed Murtadha",
        "Bo Wen",
        "Yunfeng Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.09864v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\0801ed2c-711f-5ffe-94d6-c264e679e00e.pdf",
    "bibtex": "@misc{su2023roformerenhancedtransformerwithrotary,\n    title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},\n    year = {2023},\n    eprint = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.09864},\n}",
    "abstract": "Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\n\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
    "num_pages": 14
}