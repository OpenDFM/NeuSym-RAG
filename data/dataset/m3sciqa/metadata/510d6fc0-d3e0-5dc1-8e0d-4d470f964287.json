{
    "uuid": "510d6fc0-d3e0-5dc1-8e0d-4d470f964287",
    "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Lajanugen Logeswaran",
        "Honglak Lee",
        "Dragomir Radev"
    ],
    "pdf_url": "http://arxiv.org/pdf/1611.02654v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\510d6fc0-d3e0-5dc1-8e0d-4d470f964287.pdf",
    "bibtex": "@misc{logeswaran2017sentenceorderingandcoherencemodeling,\n    title = {Sentence Ordering and Coherence Modeling using Recurrent Neural Networks},\n    author = {Lajanugen Logeswaran and Honglak Lee and Dragomir Radev},\n    year = {2017},\n    eprint = {1611.02654},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1611.02654},\n}",
    "abstract": "Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.",
    "num_pages": 8
}