{
    "uuid": "20c2e833-7d1f-55b9-9ae9-9b39a2e1cbca",
    "title": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Chris Hokamp",
        "Qun Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/1704.07138v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\20c2e833-7d1f-55b9-9ae9-9b39a2e1cbca.pdf",
    "bibtex": "@misc{hokamp2017lexicallyconstraineddecodingforsequence,\n    title = {Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},\n    author = {Chris Hokamp and Qun Liu},\n    year = {2017},\n    eprint = {1704.07138},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1704.07138},\n}",
    "abstract": "We present Grid Beam Search (GBS), an algorithm which extends beam search to\nallow the inclusion of pre-specified lexical constraints. The algorithm can be\nused with any model that generates a sequence $ \\mathbf{\\hat{y}} =\n\\{y_{0}\\ldots y_{T}\\} $, by maximizing $ p(\\mathbf{y} | \\mathbf{x}) =\n\\prod\\limits_{t}p(y_{t} | \\mathbf{x}; \\{y_{0} \\ldots y_{t-1}\\}) $. Lexical\nconstraints take the form of phrases or words that must be present in the\noutput sequence. This is a very general way to incorporate additional knowledge\ninto a model's output without requiring any modification of the model\nparameters or training data. We demonstrate the feasibility and flexibility of\nLexically Constrained Decoding by conducting experiments on Neural\nInteractive-Predictive Translation, as well as Domain Adaptation for Neural\nMachine Translation. Experiments show that GBS can provide large improvements\nin translation quality in interactive scenarios, and that, even without any\nuser input, GBS can be used to achieve significant gains in performance in\ndomain adaptation scenarios.",
    "num_pages": 12
}