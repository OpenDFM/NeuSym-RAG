{
    "uuid": "770824c7-74c4-56a2-96fb-f660347c5ab0",
    "title": "Revisiting Entropy Rate Constancy in Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Vivek Verma",
        "Nicholas Tomlin",
        "Dan Klein"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.12084v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\770824c7-74c4-56a2-96fb-f660347c5ab0.pdf",
    "bibtex": "@misc{verma2023revisitingentropyrateconstancyin,\n    title = {Revisiting Entropy Rate Constancy in Text},\n    author = {Vivek Verma and Nicholas Tomlin and Dan Klein},\n    year = {2023},\n    eprint = {2305.12084},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.12084},\n}",
    "abstract": "The uniform information density (UID) hypothesis states that humans tend to\ndistribute information roughly evenly across an utterance or discourse. Early\nevidence in support of the UID hypothesis came from Genzel & Charniak (2002),\nwhich proposed an entropy rate constancy principle based on the probability of\nEnglish text under n-gram language models. We re-evaluate the claims of Genzel\n& Charniak (2002) with neural language models, failing to find clear evidence\nin support of entropy rate constancy. We conduct a range of experiments across\ndatasets, model sizes, and languages and discuss implications for the uniform\ninformation density hypothesis and linguistic theories of efficient\ncommunication more broadly.",
    "num_pages": 13
}