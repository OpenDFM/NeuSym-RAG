{
    "uuid": "88eaf40c-dc71-5736-b1a5-7e39d2d01725",
    "title": "RoBERTuito: a pre-trained language model for social media text in Spanish",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Juan Manuel Pérez",
        "Damián A. Furman",
        "Laura Alonso Alemany",
        "Franco Luque"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.09453v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\88eaf40c-dc71-5736-b1a5-7e39d2d01725.pdf",
    "bibtex": "@misc{prez2022robertuitoapretrainedlanguagemodel,\n    title = {RoBERTuito: a pre-trained language model for social media text in Spanish},\n    author = {Juan Manuel Pérez and Damián A. Furman and Laura Alonso Alemany and Franco Luque},\n    year = {2022},\n    eprint = {2111.09453},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2111.09453},\n}",
    "abstract": "Since BERT appeared, Transformer language models and transfer learning have\nbecome state-of-the-art for Natural Language Understanding tasks. Recently,\nsome works geared towards pre-training specially-crafted models for particular\ndomains, such as scientific papers, medical documents, user-generated texts,\namong others. These domain-specific models have been shown to improve\nperformance significantly in most tasks. However, for languages other than\nEnglish such models are not widely available.\n  In this work, we present RoBERTuito, a pre-trained language model for\nuser-generated text in Spanish, trained on over 500 million tweets. Experiments\non a benchmark of tasks involving user-generated text showed that RoBERTuito\noutperformed other pre-trained language models in Spanish. In addition to this,\nour model achieves top results for some English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and has also competitive\nperformance against monolingual models in English tasks. To facilitate further\nresearch, we make RoBERTuito publicly available at the HuggingFace model hub\ntogether with the dataset used to pre-train it.",
    "num_pages": 9
}