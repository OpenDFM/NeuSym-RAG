{
    "uuid": "2bc7f244-8607-5501-a3c5-916c66efc615",
    "title": "Language Models as Knowledge Bases?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Fabio Petroni",
        "Tim Rocktäschel",
        "Patrick Lewis",
        "Anton Bakhtin",
        "Yuxiang Wu",
        "Alexander H. Miller",
        "Sebastian Riedel"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01066v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\2bc7f244-8607-5501-a3c5-916c66efc615.pdf",
    "bibtex": "@misc{petroni2019languagemodelsasknowledgebases,\n    title = {Language Models as Knowledge Bases?},\n    author = {Fabio Petroni and Tim Rocktäschel and Patrick Lewis and Anton Bakhtin and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},\n    year = {2019},\n    eprint = {1909.01066},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.01066},\n}",
    "abstract": "Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.",
    "num_pages": 11
}