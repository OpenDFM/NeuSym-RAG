{
    "uuid": "f2c45532-0d1d-52f3-8293-a28562dca0c2",
    "title": "Visual Instruction Tuning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.08485v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f2c45532-0d1d-52f3-8293-a28562dca0c2.pdf",
    "bibtex": "@misc{liu2023visualinstructiontuning,\n    title = {Visual Instruction Tuning},\n    author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},\n    year = {2023},\n    eprint = {2304.08485},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2304.08485},\n}",
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.",
    "num_pages": 25
}