{
    "uuid": "2b8f1988-37a8-5fc1-b6c3-40b70dd2867f",
    "title": "CIDEr: Consensus-based Image Description Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Ramakrishna Vedantam",
        "C. Lawrence Zitnick",
        "Devi Parikh"
    ],
    "pdf_url": "http://arxiv.org/pdf/1411.5726v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\2b8f1988-37a8-5fc1-b6c3-40b70dd2867f.pdf",
    "bibtex": "@misc{vedantam2015ciderconsensusbasedimagedescriptionevaluation,\n    title = {CIDEr: Consensus-based Image Description Evaluation},\n    author = {Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},\n    year = {2015},\n    eprint = {1411.5726},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1411.5726},\n}",
    "abstract": "Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.",
    "num_pages": 17
}