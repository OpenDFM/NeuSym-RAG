{
    "uuid": "ca8591e1-3c1d-559b-8241-d10d621689a3",
    "title": "Token Merging: Your ViT But Faster",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Daniel Bolya",
        "Cheng-Yang Fu",
        "Xiaoliang Dai",
        "Peizhao Zhang",
        "Christoph Feichtenhofer",
        "Judy Hoffman"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.09461v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\ca8591e1-3c1d-559b-8241-d10d621689a3.pdf",
    "bibtex": "@misc{bolya2023tokenmergingyourvitbut,\n    title = {Token Merging: Your ViT But Faster},\n    author = {Daniel Bolya and Cheng-Yang Fu and Xiaoliang Dai and Peizhao Zhang and Christoph Feichtenhofer and Judy Hoffman},\n    year = {2023},\n    eprint = {2210.09461},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2210.09461},\n}",
    "abstract": "We introduce Token Merging (ToMe), a simple method to increase the throughput\nof existing ViT models without needing to train. ToMe gradually combines\nsimilar tokens in a transformer using a general and light-weight matching\nalgorithm that is as fast as pruning while being more accurate. Off-the-shelf,\nToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518\nmodels on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3%\naccuracy drop in each case. ToMe can also easily be applied during training,\nimproving in practice training speed up to 2x for MAE fine-tuning on video.\nTraining with ToMe further minimizes accuracy drop, leading to 2x the\nthroughput of ViT-B on audio for only a 0.4% mAP drop. Qualitatively, we find\nthat ToMe merges object parts into one token, even over multiple frames of\nvideo. Overall, ToMe's accuracy and speed are competitive with state-of-the-art\non images, video, and audio.",
    "num_pages": 20
}