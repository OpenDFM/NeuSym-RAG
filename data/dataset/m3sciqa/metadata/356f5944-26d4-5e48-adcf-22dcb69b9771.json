{
    "uuid": "356f5944-26d4-5e48-adcf-22dcb69b9771",
    "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yilun Zhao",
        "Yunxiang Li",
        "Chenying Li",
        "Rui Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.01347v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\356f5944-26d4-5e48-adcf-22dcb69b9771.pdf",
    "bibtex": "@misc{zhao2022multihierttnumericalreasoningovermulti,\n    title = {MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data},\n    author = {Yilun Zhao and Yunxiang Li and Chenying Li and Rui Zhang},\n    year = {2022},\n    eprint = {2206.01347},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.AI},\n    url = {http://arxiv.org/abs/2206.01347},\n}",
    "abstract": "Numerical reasoning over hybrid data containing both textual and tabular\ncontent (e.g., financial reports) has recently attracted much attention in the\nNLP community. However, existing question answering (QA) benchmarks over hybrid\ndata only include a single flat table in each document and thus lack examples\nof multi-step numerical reasoning across multiple hierarchical tables. To\nfacilitate data analytical progress, we construct a new large-scale benchmark,\nMultiHiertt, with QA pairs over Multi Hierarchical Tabular and Textual data.\nMultiHiertt is built from a wealth of financial reports and has the following\nunique characteristics: 1) each document contain multiple tables and longer\nunstructured texts; 2) most of tables contained are hierarchical; 3) the\nreasoning process required for each question is more complex and challenging\nthan existing benchmarks; and 4) fine-grained annotations of reasoning\nprocesses and supporting facts are provided to reveal complex numerical\nreasoning. We further introduce a novel QA model termed MT2Net, which first\napplies facts retrieving to extract relevant supporting facts from both tables\nand text and then uses a reasoning module to perform symbolic reasoning over\nretrieved facts. We conduct comprehensive experiments on various baselines. The\nexperimental results show that MultiHiertt presents a strong challenge for\nexisting baselines whose results lag far behind the performance of human\nexperts. The dataset and code are publicly available at\nhttps://github.com/psunlpgroup/MultiHiertt.",
    "num_pages": 13
}