{
    "uuid": "1398ab3c-cb2d-593e-9d68-02a0d484e643",
    "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Jiwei Li",
        "Michel Galley",
        "Chris Brockett",
        "Jianfeng Gao",
        "Bill Dolan"
    ],
    "pdf_url": "http://arxiv.org/pdf/1510.03055v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\1398ab3c-cb2d-593e-9d68-02a0d484e643.pdf",
    "bibtex": "@misc{li2016adiversitypromotingobjectivefunctionfor,\n    title = {A Diversity-Promoting Objective Function for Neural Conversation Models},\n    author = {Jiwei Li and Michel Galley and Chris Brockett and Jianfeng Gao and Bill Dolan},\n    year = {2016},\n    eprint = {1510.03055},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1510.03055},\n}",
    "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.",
    "num_pages": 11
}