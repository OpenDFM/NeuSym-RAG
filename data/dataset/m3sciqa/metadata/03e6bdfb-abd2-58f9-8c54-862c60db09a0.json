{
    "uuid": "03e6bdfb-abd2-58f9-8c54-862c60db09a0",
    "title": "Evaluating Rewards for Question Generation Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Tom Hosking",
        "Sebastian Riedel"
    ],
    "pdf_url": "http://arxiv.org/pdf/1902.11049v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\03e6bdfb-abd2-58f9-8c54-862c60db09a0.pdf",
    "bibtex": "@misc{hosking2019evaluatingrewardsforquestiongeneration,\n    title = {Evaluating Rewards for Question Generation Models},\n    author = {Tom Hosking and Sebastian Riedel},\n    year = {2019},\n    eprint = {1902.11049},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1902.11049},\n}",
    "abstract": "Recent approaches to question generation have used modifications to a Seq2Seq\narchitecture inspired by advances in machine translation. Models are trained\nusing teacher forcing to optimise only the one-step-ahead prediction. However,\nat test time, the model is asked to generate a whole sequence, causing errors\nto propagate through the generation process (exposure bias). A number of\nauthors have proposed countering this bias by optimising for a reward that is\nless tightly coupled to the training data, using reinforcement learning. We\noptimise directly for quality metrics, including a novel approach using a\ndiscriminator learned directly from the training data. We confirm that policy\ngradient methods can be used to decouple training from the ground truth,\nleading to increases in the metrics used as rewards. We perform a human\nevaluation, and show that although these metrics have previously been assumed\nto be good proxies for question quality, they are poorly aligned with human\njudgement and the model simply learns to exploit the weaknesses of the reward\nsource.",
    "num_pages": 6
}