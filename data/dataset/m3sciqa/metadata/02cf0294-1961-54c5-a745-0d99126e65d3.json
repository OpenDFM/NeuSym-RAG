{
    "uuid": "02cf0294-1961-54c5-a745-0d99126e65d3",
    "title": "How Can Self-Attention Networks Recognize Dyck-n Languages?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Javid Ebrahimi",
        "Dhruv Gelda",
        "Wei Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04303v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\02cf0294-1961-54c5-a745-0d99126e65d3.pdf",
    "bibtex": "@misc{ebrahimi2020howcanselfattentionnetworksrecognize,\n    title = {How Can Self-Attention Networks Recognize Dyck-n Languages?},\n    author = {Javid Ebrahimi and Dhruv Gelda and Wei Zhang},\n    year = {2020},\n    eprint = {2010.04303},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.04303},\n}",
    "abstract": "We focus on the recognition of Dyck-n ($\\mathcal{D}_n$) languages with\nself-attention (SA) networks, which has been deemed to be a difficult task for\nthese networks. We compare the performance of two variants of SA, one with a\nstarting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$\nis able to generalize to longer sequences and deeper dependencies. For\n$\\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences\nwhereas the accuracy of SA$^+$ is 58.82$\\%$. We find attention maps learned by\n$\\text{SA}{^+}$ to be amenable to interpretation and compatible with a\nstack-based language recognizer. Surprisingly, the performance of SA networks\nis at par with LSTMs, which provides evidence on the ability of SA to learn\nhierarchies without recursion.",
    "num_pages": 7
}