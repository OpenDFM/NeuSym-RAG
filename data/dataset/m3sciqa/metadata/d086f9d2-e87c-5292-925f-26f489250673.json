{
    "uuid": "d086f9d2-e87c-5292-925f-26f489250673",
    "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Li Dong",
        "Nan Yang",
        "Wenhui Wang",
        "Furu Wei",
        "Xiaodong Liu",
        "Yu Wang",
        "Jianfeng Gao",
        "Ming Zhou",
        "Hsiao-Wuen Hon"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.03197v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\d086f9d2-e87c-5292-925f-26f489250673.pdf",
    "bibtex": "@misc{dong2019unifiedlanguagemodelpretrainingfor,\n    title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},\n    author = {Li Dong and Nan Yang and Wenhui Wang and Furu Wei and Xiaodong Liu and Yu Wang and Jianfeng Gao and Ming Zhou and Hsiao-Wuen Hon},\n    year = {2019},\n    eprint = {1905.03197},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1905.03197},\n}",
    "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can\nbe fine-tuned for both natural language understanding and generation tasks. The\nmodel is pre-trained using three types of language modeling tasks:\nunidirectional, bidirectional, and sequence-to-sequence prediction. The unified\nmodeling is achieved by employing a shared Transformer network and utilizing\nspecific self-attention masks to control what context the prediction conditions\non. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0\nand CoQA question answering tasks. Moreover, UniLM achieves new\nstate-of-the-art results on five natural language generation datasets,\nincluding improving the CNN/DailyMail abstractive summarization ROUGE-L to\n40.51 (2.04 absolute improvement), the Gigaword abstractive summarization\nROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question\nanswering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question\ngeneration BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7\ndocument-grounded dialog response generation NIST-4 to 2.67 (human performance\nis 2.65). The code and pre-trained models are available at\nhttps://github.com/microsoft/unilm.",
    "num_pages": 14
}