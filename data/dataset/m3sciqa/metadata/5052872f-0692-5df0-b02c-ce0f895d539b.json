{
    "uuid": "5052872f-0692-5df0-b02c-ce0f895d539b",
    "title": "Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Ru Peng",
        "Yawen Zeng",
        "Junbo Zhao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.04468v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5052872f-0692-5df0-b02c-ce0f895d539b.pdf",
    "bibtex": "@misc{peng2023distilltheimagetonowhere,\n    title = {Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation},\n    author = {Ru Peng and Yawen Zeng and Junbo Zhao},\n    year = {2023},\n    eprint = {2210.04468},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.04468},\n}",
    "abstract": "Past works on multimodal machine translation (MMT) elevate bilingual setup by\nincorporating additional aligned vision information. However, an image-must\nrequirement of the multimodal dataset largely hinders MMT's development --\nnamely that it demands an aligned form of [image, source text, target text].\nThis limitation is generally troublesome during the inference phase especially\nwhen the aligned image is not provided as in the normal NMT setup. Thus, in\nthis work, we introduce IKD-MMT, a novel MMT framework to support the\nimage-free inference phase via an inversion knowledge distillation scheme. In\nparticular, a multimodal feature generator is executed with a knowledge\ndistillation module, which directly generates the multimodal feature from\n(only) source texts as the input. While there have been a few prior works\nentertaining the possibility to support image-free inference for machine\ntranslation, their performances have yet to rival the image-must translation.\nIn our experiments, we identify our method as the first image-free approach to\ncomprehensively rival or even surpass (almost) all image-must frameworks, and\nachieved the state-of-the-art result on the often-used Multi30k benchmark. Our\ncode and data are available at: https://github.com/pengr/IKD-mmt/tree/master..",
    "num_pages": 12
}