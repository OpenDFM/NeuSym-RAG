{
    "uuid": "81da51d9-10f0-55fa-a1a0-9c33f0f62d60",
    "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yao Fu",
        "Litu Ou",
        "Mingyu Chen",
        "Yuhao Wan",
        "Hao Peng",
        "Tushar Khot"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.17306v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\81da51d9-10f0-55fa-a1a0-9c33f0f62d60.pdf",
    "bibtex": "@misc{fu2023chainofthoughthubacontinuouseffort,\n    title = {Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},\n    author = {Yao Fu and Litu Ou and Mingyu Chen and Yuhao Wan and Hao Peng and Tushar Khot},\n    year = {2023},\n    eprint = {2305.17306},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.17306},\n}",
    "abstract": "As large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF.",
    "num_pages": 6
}