{
    "uuid": "c97315f8-2f29-5e3e-bbf4-f0848b855663",
    "title": "Discourse structure interacts with reference but not syntax in neural language models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Forrest Davis",
        "Marten van Schijndel"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.04887v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\c97315f8-2f29-5e3e-bbf4-f0848b855663.pdf",
    "bibtex": "@misc{davis2020discoursestructureinteractswithreference,\n    title = {Discourse structure interacts with reference but not syntax in neural language models},\n    author = {Forrest Davis and Marten van Schijndel},\n    year = {2020},\n    eprint = {2010.04887},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.04887},\n}",
    "abstract": "Language models (LMs) trained on large quantities of text have been claimed\nto acquire abstract linguistic representations. Our work tests the robustness\nof these abstractions by focusing on the ability of LMs to learn interactions\nbetween different linguistic representations. In particular, we utilized\nstimuli from psycholinguistic studies showing that humans can condition\nreference (i.e. coreference resolution) and syntactic processing on the same\ndiscourse structure (implicit causality). We compared both transformer and long\nshort-term memory LMs to find that, contrary to humans, implicit causality only\ninfluences LM behavior for reference, not syntax, despite model representations\nthat encode the necessary discourse information. Our results further suggest\nthat LM behavior can contradict not only learned representations of discourse\nbut also syntactic agreement, pointing to shortcomings of standard language\nmodeling.",
    "num_pages": 12
}