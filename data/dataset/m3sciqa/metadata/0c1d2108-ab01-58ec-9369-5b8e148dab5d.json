{
    "uuid": "0c1d2108-ab01-58ec-9369-5b8e148dab5d",
    "title": "Long Range Arena: A Benchmark for Efficient Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yi Tay",
        "Mostafa Dehghani",
        "Samira Abnar",
        "Yikang Shen",
        "Dara Bahri",
        "Philip Pham",
        "Jinfeng Rao",
        "Liu Yang",
        "Sebastian Ruder",
        "Donald Metzler"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.04006v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\0c1d2108-ab01-58ec-9369-5b8e148dab5d.pdf",
    "bibtex": "@misc{tay2020longrangearenaabenchmark,\n    title = {Long Range Arena: A Benchmark for Efficient Transformers},\n    author = {Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},\n    year = {2020},\n    eprint = {2011.04006},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2011.04006},\n}",
    "abstract": "Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.",
    "num_pages": 16
}