{
    "uuid": "5bf0d02e-6fc3-5d7e-8b7e-79a0113a37e6",
    "title": "Towards Out-Of-Distribution Generalization: A Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jiashuo Liu",
        "Zheyan Shen",
        "Yue He",
        "Xingxuan Zhang",
        "Renzhe Xu",
        "Han Yu",
        "Peng Cui"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.13624v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5bf0d02e-6fc3-5d7e-8b7e-79a0113a37e6.pdf",
    "bibtex": "@misc{liu2023towardsoutofdistributiongeneralizationasurvey,\n    title = {Towards Out-Of-Distribution Generalization: A Survey},\n    author = {Jiashuo Liu and Zheyan Shen and Yue He and Xingxuan Zhang and Renzhe Xu and Han Yu and Peng Cui},\n    year = {2023},\n    eprint = {2108.13624},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2108.13624},\n}",
    "abstract": "Traditional machine learning paradigms are based on the assumption that both\ntraining and test data follow the same statistical pattern, which is\nmathematically referred to as Independent and Identically Distributed\n($i.i.d.$). However, in real-world applications, this $i.i.d.$ assumption often\nfails to hold due to unforeseen distributional shifts, leading to considerable\ndegradation in model performance upon deployment. This observed discrepancy\nindicates the significance of investigating the Out-of-Distribution (OOD)\ngeneralization problem. OOD generalization is an emerging topic of machine\nlearning research that focuses on complex scenarios wherein the distributions\nof the test data differ from those of the training data. This paper represents\nthe first comprehensive, systematic review of OOD generalization, encompassing\na spectrum of aspects from problem definition, methodological development, and\nevaluation procedures, to the implications and future directions of the field.\nOur discussion begins with a precise, formal characterization of the OOD\ngeneralization problem. Following that, we categorize existing methodologies\ninto three segments: unsupervised representation learning, supervised model\nlearning, and optimization, according to their positions within the overarching\nlearning process. We provide an in-depth discussion on representative\nmethodologies for each category, further elucidating the theoretical links\nbetween them. Subsequently, we outline the prevailing benchmark datasets\nemployed in OOD generalization studies. To conclude, we overview the existing\nbody of work in this domain and suggest potential avenues for future research\non OOD generalization. A summary of the OOD generalization methodologies\nsurveyed in this paper can be accessed at\nhttp://out-of-distribution-generalization.com.",
    "num_pages": 51
}