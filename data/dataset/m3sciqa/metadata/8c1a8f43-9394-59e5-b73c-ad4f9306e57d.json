{
    "uuid": "8c1a8f43-9394-59e5-b73c-ad4f9306e57d",
    "title": "Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Chenxu Yang",
        "Zheng Lin",
        "Lanrui Wang",
        "Chong Tian",
        "Liang Pang",
        "Jiangnan Li",
        "Qirong Ho",
        "Yanan Cao",
        "Weiping Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.08943v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\8c1a8f43-9394-59e5-b73c-ad4f9306e57d.pdf",
    "bibtex": "@misc{yang2023multileveladaptivecontrastivelearningfor,\n    title = {Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation},\n    author = {Chenxu Yang and Zheng Lin and Lanrui Wang and Chong Tian and Liang Pang and Jiangnan Li and Qirong Ho and Yanan Cao and Weiping Wang},\n    year = {2023},\n    eprint = {2310.08943},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.08943},\n}",
    "abstract": "Knowledge-grounded dialogue generation aims to mitigate the issue of text\ndegeneration by incorporating external knowledge to supplement the context.\nHowever, the model often fails to internalize this information into responses\nin a human-like manner. Instead, it simply inserts segments of the provided\nknowledge into generic responses. As a result, the generated responses tend to\nbe tedious, incoherent, and in lack of interactivity which means the\ndegeneration problem is still unsolved. In this work, we first find that such\ncopying-style degeneration is primarily due to the weak likelihood objective,\nwhich allows the model to \"cheat\" the objective by merely duplicating knowledge\nsegments in a superficial pattern matching based on overlap. To overcome this\nchallenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL)\nframework that dynamically samples negative examples and subsequently penalizes\ndegeneration behaviors at both the token-level and sequence-level. Extensive\nexperiments on the WoW dataset demonstrate the effectiveness of our approach\nacross various pre-trained models.",
    "num_pages": 14
}