{
    "uuid": "b5ebcdd6-2a29-55fa-ad8f-1d52ede2025f",
    "title": "How Do In-Context Examples Affect Compositional Generalization?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shengnan An",
        "Zeqi Lin",
        "Qiang Fu",
        "Bei Chen",
        "Nanning Zheng",
        "Jian-Guang Lou",
        "Dongmei Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.04835v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\b5ebcdd6-2a29-55fa-ad8f-1d52ede2025f.pdf",
    "bibtex": "@misc{an2023howdoincontextexamplesaffect,\n    title = {How Do In-Context Examples Affect Compositional Generalization?},\n    author = {Shengnan An and Zeqi Lin and Qiang Fu and Bei Chen and Nanning Zheng and Jian-Guang Lou and Dongmei Zhang},\n    year = {2023},\n    eprint = {2305.04835},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.04835},\n}",
    "abstract": "Compositional generalization--understanding unseen combinations of seen\nprimitives--is an essential reasoning capability in human intelligence. The AI\ncommunity mainly studies this capability by fine-tuning neural networks on lots\nof training samples, while it is still unclear whether and how in-context\nlearning--the prevailing few-shot paradigm based on large language\nmodels--exhibits compositional generalization. In this paper, we present CoFe,\na test suite to investigate in-context compositional generalization. We find\nthat the compositional generalization performance can be easily affected by the\nselection of in-context examples, thus raising the research question what the\nkey factors are to make good in-context examples for compositional\ngeneralization. We study three potential factors: similarity, diversity and\ncomplexity. Our systematic experiments indicate that in-context examples should\nbe structurally similar to the test case, diverse from each other, and\nindividually simple. Furthermore, two strong limitations are observed:\nin-context compositional generalization on fictional words is much weaker than\nthat on commonly used ones; it is still critical that the in-context examples\nshould cover required linguistic structures, even though the backbone model has\nbeen pre-trained on large corpus. We hope our analysis would facilitate the\nunderstanding and utilization of in-context learning paradigm.",
    "num_pages": 24
}