{
    "uuid": "d537f355-a76b-5894-856f-7fbd3328d16e",
    "title": "Very Deep Transformers for Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Xiaodong Liu",
        "Kevin Duh",
        "Liyuan Liu",
        "Jianfeng Gao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2008.07772v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\d537f355-a76b-5894-856f-7fbd3328d16e.pdf",
    "bibtex": "@misc{liu2020verydeeptransformersforneural,\n    title = {Very Deep Transformers for Neural Machine Translation},\n    author = {Xiaodong Liu and Kevin Duh and Liyuan Liu and Jianfeng Gao},\n    year = {2020},\n    eprint = {2008.07772},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2008.07772},\n}",
    "abstract": "We explore the application of very deep Transformer models for Neural Machine\nTranslation (NMT). Using a simple yet effective initialization technique that\nstabilizes training, we show that it is feasible to build standard\nTransformer-based models with up to 60 encoder layers and 12 decoder layers.\nThese deep models outperform their baseline 6-layer counterparts by as much as\n2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14\nEnglish-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14\nEnglish-German (30.1 BLEU).The code and trained models will be publicly\navailable at: https://github.com/namisan/exdeep-nmt.",
    "num_pages": 7
}