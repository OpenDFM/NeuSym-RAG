{
    "uuid": "99ac7d3d-eedb-58ea-99c9-cfefa71aba7b",
    "title": "Document-Level Machine Translation with Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Longyue Wang",
        "Chenyang Lyu",
        "Tianbo Ji",
        "Zhirui Zhang",
        "Dian Yu",
        "Shuming Shi",
        "Zhaopeng Tu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2304.02210v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\99ac7d3d-eedb-58ea-99c9-cfefa71aba7b.pdf",
    "bibtex": "@misc{wang2023documentlevelmachinetranslationwithlarge,\n    title = {Document-Level Machine Translation with Large Language Models},\n    author = {Longyue Wang and Chenyang Lyu and Tianbo Ji and Zhirui Zhang and Dian Yu and Shuming Shi and Zhaopeng Tu},\n    year = {2023},\n    eprint = {2304.02210},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2304.02210},\n}",
    "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.",
    "num_pages": 16
}