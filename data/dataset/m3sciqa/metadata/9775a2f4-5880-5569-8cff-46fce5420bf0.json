{
    "uuid": "9775a2f4-5880-5569-8cff-46fce5420bf0",
    "title": "Can RNNs learn Recursive Nested Subject-Verb Agreements?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yair Lakretz",
        "Théo Desbordes",
        "Jean-Rémi King",
        "Benoît Crabbé",
        "Maxime Oquab",
        "Stanislas Dehaene"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.02258v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\9775a2f4-5880-5569-8cff-46fce5420bf0.pdf",
    "bibtex": "@misc{lakretz2021canrnnslearnrecursivenested,\n    title = {Can RNNs learn Recursive Nested Subject-Verb Agreements?},\n    author = {Yair Lakretz and Théo Desbordes and Jean-Rémi King and Benoît Crabbé and Maxime Oquab and Stanislas Dehaene},\n    year = {2021},\n    eprint = {2101.02258},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2101.02258},\n}",
    "abstract": "One of the fundamental principles of contemporary linguistics states that\nlanguage processing requires the ability to extract recursively nested tree\nstructures. However, it remains unclear whether and how this code could be\nimplemented in neural circuits. Recent advances in Recurrent Neural Networks\n(RNNs), which achieve near-human performance in some language tasks, provide a\ncompelling model to address such questions. Here, we present a new framework to\nstudy recursive processing in RNNs, using subject-verb agreement as a probe\ninto the representations of the neural network. We trained six distinct types\nof RNNs on a simplified probabilistic context-free grammar designed to\nindependently manipulate the length of a sentence and the depth of its\nsyntactic tree. All RNNs generalized to subject-verb dependencies longer than\nthose seen during training. However, none systematically generalized to deeper\ntree structures, even those with a structural bias towards learning nested tree\n(i.e., stack-RNNs). In addition, our analyses revealed primacy and recency\neffects in the generalization patterns of LSTM-based models, showing that these\nmodels tend to perform well on the outer- and innermost parts of a\ncenter-embedded tree structure, but poorly on its middle levels. Finally,\nprobing the internal states of the model during the processing of sentences\nwith nested tree structures, we found a complex encoding of grammatical\nagreement information (e.g. grammatical number), in which all the information\nfor multiple words nouns was carried by a single unit. Taken together, these\nresults indicate how neural networks may extract bounded nested tree\nstructures, without learning a systematic recursive rule.",
    "num_pages": 26
}