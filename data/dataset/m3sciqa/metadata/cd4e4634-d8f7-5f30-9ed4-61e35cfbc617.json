{
    "uuid": "cd4e4634-d8f7-5f30-9ed4-61e35cfbc617",
    "title": "Sequence-Level Mixed Sample Data Augmentation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Demi Guo",
        "Yoon Kim",
        "Alexander M. Rush"
    ],
    "pdf_url": "http://arxiv.org/pdf/2011.09039v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\cd4e4634-d8f7-5f30-9ed4-61e35cfbc617.pdf",
    "bibtex": "@misc{guo2020sequencelevelmixedsampledataaugmentation,\n    title = {Sequence-Level Mixed Sample Data Augmentation},\n    author = {Demi Guo and Yoon Kim and Alexander M. Rush},\n    year = {2020},\n    eprint = {2011.09039},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2011.09039},\n}",
    "abstract": "Despite their empirical success, neural networks still have difficulty\ncapturing compositional aspects of natural language. This work proposes a\nsimple data augmentation approach to encourage compositional behavior in neural\nmodels for sequence-to-sequence problems. Our approach, SeqMix, creates new\nsynthetic examples by softly combining input/output sequences from the training\nset. We connect this approach to existing techniques such as SwitchOut and word\ndropout, and show that these techniques are all approximating variants of a\nsingle objective. SeqMix consistently yields approximately 1.0 BLEU improvement\non five different translation datasets over strong Transformer baselines. On\ntasks that require strong compositional generalization such as SCAN and\nsemantic parsing, SeqMix also offers further improvements.",
    "num_pages": 6
}