{
    "uuid": "cb042120-1319-5a51-a270-eb61170f4631",
    "title": "Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Yang Liu",
        "Sujian Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/1609.06380v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\cb042120-1319-5a51-a270-eb61170f4631.pdf",
    "bibtex": "@misc{liu2016recognizingimplicitdiscourserelationsvia,\n    title = {Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention},\n    author = {Yang Liu and Sujian Li},\n    year = {2016},\n    eprint = {1609.06380},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1609.06380},\n}",
    "abstract": "Recognizing implicit discourse relations is a challenging but important task\nin the field of Natural Language Processing. For such a complex text processing\ntask, different from previous studies, we argue that it is necessary to\nrepeatedly read the arguments and dynamically exploit the efficient features\nuseful for recognizing discourse relations. To mimic the repeated reading\nstrategy, we propose the neural networks with multi-level attention (NNMA),\ncombining the attention mechanism and external memories to gradually fix the\nattention on some specific words helpful to judging the discourse relations.\nExperiments on the PDTB dataset show that our proposed method achieves the\nstate-of-art results. The visualization of the attention weights also\nillustrates the progress that our model observes the arguments on each level\nand progressively locates the important words.",
    "num_pages": 10
}