{
    "uuid": "f554c584-7f48-587f-9f9b-4e4debbc5cad",
    "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Kimin Lee",
        "Kibok Lee",
        "Honglak Lee",
        "Jinwoo Shin"
    ],
    "pdf_url": "http://arxiv.org/pdf/1807.03888v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\f554c584-7f48-587f-9f9b-4e4debbc5cad.pdf",
    "bibtex": "@misc{lee2018asimpleunifiedframeworkfor,\n    title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},\n    author = {Kimin Lee and Kibok Lee and Honglak Lee and Jinwoo Shin},\n    year = {2018},\n    eprint = {1807.03888},\n    archivePrefix = {arXiv},\n    primaryClass = {stat.ML},\n    url = {http://arxiv.org/abs/1807.03888},\n}",
    "abstract": "Detecting test samples drawn sufficiently far away from the training\ndistribution statistically or adversarially is a fundamental requirement for\ndeploying a good classifier in many real-world machine learning applications.\nHowever, deep neural networks with the softmax classifier are known to produce\nhighly overconfident posterior distributions even for such abnormal samples. In\nthis paper, we propose a simple yet effective method for detecting any abnormal\nsamples, which is applicable to any pre-trained softmax neural classifier. We\nobtain the class conditional Gaussian distributions with respect to (low- and\nupper-level) features of the deep models under Gaussian discriminant analysis,\nwhich result in a confidence score based on the Mahalanobis distance. While\nmost prior methods have been evaluated for detecting either out-of-distribution\nor adversarial samples, but not both, the proposed method achieves the\nstate-of-the-art performances for both cases in our experiments. Moreover, we\nfound that our proposed method is more robust in harsh cases, e.g., when the\ntraining dataset has noisy labels or small number of samples. Finally, we show\nthat the proposed method enjoys broader usage by applying it to\nclass-incremental learning: whenever out-of-distribution samples are detected,\nour classification rule can incorporate new classes well without further\ntraining deep models.",
    "num_pages": 20
}