{
    "uuid": "6b474476-57a6-5296-8664-c0cad7717810",
    "title": "Scalable and Efficient MoE Training for Multitask Multilingual Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Young Jin Kim",
        "Ammar Ahmad Awan",
        "Alexandre Muzio",
        "Andres Felipe Cruz Salinas",
        "Liyang Lu",
        "Amr Hendy",
        "Samyam Rajbhandari",
        "Yuxiong He",
        "Hany Hassan Awadalla"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.10465v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\6b474476-57a6-5296-8664-c0cad7717810.pdf",
    "bibtex": "@misc{kim2021scalableandefficientmoetraining,\n    title = {Scalable and Efficient MoE Training for Multitask Multilingual Models},\n    author = {Young Jin Kim and Ammar Ahmad Awan and Alexandre Muzio and Andres Felipe Cruz Salinas and Liyang Lu and Amr Hendy and Samyam Rajbhandari and Yuxiong He and Hany Hassan Awadalla},\n    year = {2021},\n    eprint = {2109.10465},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.10465},\n}",
    "abstract": "The Mixture of Experts (MoE) models are an emerging class of sparsely\nactivated deep learning models that have sublinear compute costs with respect\nto their parameters. In contrast with dense models, the sparse architecture of\nMoE offers opportunities for drastically growing model size with significant\naccuracy gain while consuming much lower compute budget. However, supporting\nlarge scale MoE training also has its own set of system and modeling\nchallenges. To overcome the challenges and embrace the opportunities of MoE, we\nfirst develop a system capable of scaling MoE models efficiently to trillions\nof parameters. It combines multi-dimensional parallelism and heterogeneous\nmemory technologies harmoniously with MoE to empower 8x larger models on the\nsame hardware compared with existing work. Besides boosting system efficiency,\nwe also present new training methods to improve MoE sample efficiency and\nleverage expert pruning strategy to improve inference time efficiency. By\ncombining the efficient system and training methods, we are able to\nsignificantly scale up large multitask multilingual models for language\ngeneration which results in a great improvement in model accuracy. A model\ntrained with 10 billion parameters on 50 languages can achieve state-of-the-art\nperformance in Machine Translation (MT) and multilingual natural language\ngeneration tasks. The system support of efficient MoE training has been\nimplemented and open-sourced with the DeepSpeed library.",
    "num_pages": 16
}