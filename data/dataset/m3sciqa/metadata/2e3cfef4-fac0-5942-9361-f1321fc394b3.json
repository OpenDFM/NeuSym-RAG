{
    "uuid": "2e3cfef4-fac0-5942-9361-f1321fc394b3",
    "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Andrej Karpathy",
        "Li Fei-Fei"
    ],
    "pdf_url": "http://arxiv.org/pdf/1412.2306v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\2e3cfef4-fac0-5942-9361-f1321fc394b3.pdf",
    "bibtex": "@misc{karpathy2015deepvisualsemanticalignmentsforgenerating,\n    title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},\n    author = {Andrej Karpathy and Li Fei-Fei},\n    year = {2015},\n    eprint = {1412.2306},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1412.2306},\n}",
    "abstract": "We present a model that generates natural language descriptions of images and\ntheir regions. Our approach leverages datasets of images and their sentence\ndescriptions to learn about the inter-modal correspondences between language\nand visual data. Our alignment model is based on a novel combination of\nConvolutional Neural Networks over image regions, bidirectional Recurrent\nNeural Networks over sentences, and a structured objective that aligns the two\nmodalities through a multimodal embedding. We then describe a Multimodal\nRecurrent Neural Network architecture that uses the inferred alignments to\nlearn to generate novel descriptions of image regions. We demonstrate that our\nalignment model produces state of the art results in retrieval experiments on\nFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generated\ndescriptions significantly outperform retrieval baselines on both full images\nand on a new dataset of region-level annotations.",
    "num_pages": 17
}