{
    "uuid": "f330aa35-8578-5231-bb8b-ecc1a839b0d5",
    "title": "Deep Learning Through the Lens of Example Difficulty",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Robert J. N. Baldock",
        "Hartmut Maennel",
        "Behnam Neyshabur"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09647v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\f330aa35-8578-5231-bb8b-ecc1a839b0d5.pdf",
    "bibtex": "@misc{baldock2021deeplearningthroughthelens,\n    title = {Deep Learning Through the Lens of Example Difficulty},\n    author = {Robert J. N. Baldock and Hartmut Maennel and Behnam Neyshabur},\n    year = {2021},\n    eprint = {2106.09647},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2106.09647},\n}",
    "abstract": "Existing work on understanding deep learning often employs measures that\ncompress all data-dependent information into a few numbers. In this work, we\nadopt a perspective based on the role of individual examples. We introduce a\nmeasure of the computational difficulty of making a prediction for a given\ninput: the (effective) prediction depth. Our extensive investigation reveals\nsurprising yet simple relationships between the prediction depth of a given\ninput and the model's uncertainty, confidence, accuracy and speed of learning\nfor that data point. We further categorize difficult examples into three\ninterpretable groups, demonstrate how these groups are processed differently\ninside deep models and showcase how this understanding allows us to improve\nprediction accuracy. Insights from our study lead to a coherent view of a\nnumber of separately reported phenomena in the literature: early layers\ngeneralize while later layers memorize; early layers converge faster and\nnetworks learn easy data and simple functions first.",
    "num_pages": 46
}