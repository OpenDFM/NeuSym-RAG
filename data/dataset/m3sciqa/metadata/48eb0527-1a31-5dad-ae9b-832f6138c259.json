{
    "uuid": "48eb0527-1a31-5dad-ae9b-832f6138c259",
    "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Xiang Hu",
        "Haitao Mi",
        "Zujie Wen",
        "Yafang Wang",
        "Yi Su",
        "Jing Zheng",
        "Gerard de Melo"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.00967v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\48eb0527-1a31-5dad-ae9b-832f6138c259.pdf",
    "bibtex": "@misc{hu2022r2d2recursivetransformerbasedon,\n    title = {R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling},\n    author = {Xiang Hu and Haitao Mi and Zujie Wen and Yafang Wang and Yi Su and Jing Zheng and Gerard de Melo},\n    year = {2022},\n    eprint = {2107.00967},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2107.00967},\n}",
    "abstract": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
    "num_pages": 12
}