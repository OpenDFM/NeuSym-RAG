{
    "uuid": "be09434f-a531-569f-bd6b-bfc25cc9ceab",
    "title": "Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Chenze Shao",
        "Yang Feng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.03953v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\be09434f-a531-569f-bd6b-bfc25cc9ceab.pdf",
    "bibtex": "@misc{shao2022nonmonotoniclatentalignmentsforctcbased,\n    title = {Non-Monotonic Latent Alignments for CTC-Based Non-Autoregressive Machine Translation},\n    author = {Chenze Shao and Yang Feng},\n    year = {2022},\n    eprint = {2210.03953},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.03953},\n}",
    "abstract": "Non-autoregressive translation (NAT) models are typically trained with the\ncross-entropy loss, which forces the model outputs to be aligned verbatim with\nthe target sentence and will highly penalize small shifts in word positions.\nLatent alignment models relax the explicit alignment by marginalizing out all\nmonotonic latent alignments with the CTC loss. However, they cannot handle\nnon-monotonic alignments, which is non-negligible as there is typically global\nword reordering in machine translation. In this work, we explore non-monotonic\nlatent alignments for NAT. We extend the alignment space to non-monotonic\nalignments to allow for the global word reordering and further consider all\nalignments that overlap with the target sentence. We non-monotonically match\nthe alignments to the target sentence and train the latent alignment model to\nmaximize the F1 score of non-monotonic matching. Extensive experiments on major\nWMT benchmarks show that our method substantially improves the translation\nperformance of CTC-based models. Our best model achieves 30.06 BLEU on WMT14\nEn-De with only one-iteration decoding, closing the gap between\nnon-autoregressive and autoregressive models.",
    "num_pages": 19
}