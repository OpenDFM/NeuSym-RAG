{
    "uuid": "18bb551c-6c39-5d99-b143-34974536e02d",
    "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Wenhu Chen",
        "Hanwen Zha",
        "Zhiyu Chen",
        "Wenhan Xiong",
        "Hong Wang",
        "William Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.07347v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\18bb551c-6c39-5d99-b143-34974536e02d.pdf",
    "bibtex": "@misc{chen2021hybridqaadatasetofmultihop,\n    title = {HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data},\n    author = {Wenhu Chen and Hanwen Zha and Zhiyu Chen and Wenhan Xiong and Hong Wang and William Wang},\n    year = {2021},\n    eprint = {2004.07347},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.07347},\n}",
    "abstract": "Existing question answering datasets focus on dealing with homogeneous\ninformation, based either only on text or KB/Table information alone. However,\nas human knowledge is distributed over heterogeneous forms, using homogeneous\ninformation alone might lead to severe coverage problems. To fill in the gap,\nwe present HybridQA https://github.com/wenhuchen/HybridQA, a new large-scale\nquestion-answering dataset that requires reasoning on heterogeneous\ninformation. Each question is aligned with a Wikipedia table and multiple\nfree-form corpora linked with the entities in the table. The questions are\ndesigned to aggregate both tabular information and text information, i.e., lack\nof either form would render the question unanswerable. We test with three\ndifferent models: 1) a table-only model. 2) text-only model. 3) a hybrid model\nthat combines heterogeneous information to find the answer. The experimental\nresults show that the EM scores obtained by two baselines are below 20\\%, while\nthe hybrid model can achieve an EM over 40\\%. This gap suggests the necessity\nto aggregate heterogeneous information in HybridQA. However, the hybrid model's\nscore is still far behind human performance. Hence, HybridQA can serve as a\nchallenging benchmark to study question answering with heterogeneous\ninformation.",
    "num_pages": 11
}