{
    "uuid": "5b08f953-b4e3-5d1d-9a7f-4c3728a4b0b4",
    "title": "On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Victor Prokhorov",
        "Ehsan Shareghi",
        "Yingzhen Li",
        "Mohammad Taher Pilehvar",
        "Nigel Collier"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.13668v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\5b08f953-b4e3-5d1d-9a7f-4c3728a4b0b4.pdf",
    "bibtex": "@misc{prokhorov2019ontheimportanceofthe,\n    title = {On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation},\n    author = {Victor Prokhorov and Ehsan Shareghi and Yingzhen Li and Mohammad Taher Pilehvar and Nigel Collier},\n    year = {2019},\n    eprint = {1909.13668},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.13668},\n}",
    "abstract": "Variational Autoencoders (VAEs) are known to suffer from learning\nuninformative latent representation of the input due to issues such as\napproximated posterior collapse, or entanglement of the latent space. We impose\nan explicit constraint on the Kullback-Leibler (KL) divergence term inside the\nVAE objective function. While the explicit constraint naturally avoids\nposterior collapse, we use it to further understand the significance of the KL\nterm in controlling the information transmitted through the VAE channel. Within\nthis framework, we explore different properties of the estimated posterior\ndistribution, and highlight the trade-off between the amount of information\nencoded in a latent code during training, and the generative capacity of the\nmodel.",
    "num_pages": 10
}