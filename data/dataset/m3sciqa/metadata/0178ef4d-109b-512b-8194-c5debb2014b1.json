{
    "uuid": "0178ef4d-109b-512b-8194-c5debb2014b1",
    "title": "Universal Adversarial Triggers for Attacking and Analyzing NLP",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Eric Wallace",
        "Shi Feng",
        "Nikhil Kandpal",
        "Matt Gardner",
        "Sameer Singh"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.07125v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\0178ef4d-109b-512b-8194-c5debb2014b1.pdf",
    "bibtex": "@misc{wallace2021universaladversarialtriggersforattacking,\n    title = {Universal Adversarial Triggers for Attacking and Analyzing NLP},\n    author = {Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},\n    year = {2021},\n    eprint = {1908.07125},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1908.07125},\n}",
    "abstract": "Adversarial examples highlight model vulnerabilities and are useful for\nevaluation and interpretation. We define universal adversarial triggers:\ninput-agnostic sequences of tokens that trigger a model to produce a specific\nprediction when concatenated to any input from a dataset. We propose a\ngradient-guided search over tokens which finds short trigger sequences (e.g.,\none word for classification and four words for language modeling) that\nsuccessfully trigger the target prediction. For example, triggers cause SNLI\nentailment accuracy to drop from 89.94% to 0.55%, 72% of \"why\" questions in\nSQuAD to be answered \"to kill american people\", and the GPT-2 language model to\nspew racist output even when conditioned on non-racial contexts. Furthermore,\nalthough the triggers are optimized using white-box access to a specific model,\nthey transfer to other models for all tasks we consider. Finally, since\ntriggers are input-agnostic, they provide an analysis of global model behavior.\nFor instance, they confirm that SNLI models exploit dataset biases and help to\ndiagnose heuristics learned by reading comprehension models.",
    "num_pages": 15
}