{
    "uuid": "f1ea3e20-3942-5edd-9c70-f73b10f2da1e",
    "title": "Improving Cross-modal Alignment for Text-Guided Image Inpainting",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yucheng Zhou",
        "Guodong Long"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.11362v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f1ea3e20-3942-5edd-9c70-f73b10f2da1e.pdf",
    "bibtex": "@misc{zhou2023improvingcrossmodalalignmentfortextguided,\n    title = {Improving Cross-modal Alignment for Text-Guided Image Inpainting},\n    author = {Yucheng Zhou and Guodong Long},\n    year = {2023},\n    eprint = {2301.11362},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2301.11362},\n}",
    "abstract": "Text-guided image inpainting (TGII) aims to restore missing regions based on\na given text in a damaged image. Existing methods are based on a strong vision\nencoder and a cross-modal fusion model to integrate cross-modal features.\nHowever, these methods allocate most of the computation to visual encoding,\nwhile light computation on modeling modality interactions. Moreover, they take\ncross-modal fusion for depth features, which ignores a fine-grained alignment\nbetween text and image. Recently, vision-language pre-trained models (VLPM),\nencapsulating rich cross-modal alignment knowledge, have advanced in most\nmultimodal tasks. In this work, we propose a novel model for TGII by improving\ncross-modal alignment (CMA). CMA model consists of a VLPM as a vision-language\nencoder, an image generator and global-local discriminators. To explore\ncross-modal alignment knowledge for image restoration, we introduce cross-modal\nalignment distillation and in-sample distribution distillation. In addition, we\nemploy adversarial training to enhance the model to fill the missing region in\ncomplicated structures effectively. Experiments are conducted on two popular\nvision-language datasets. Results show that our model achieves state-of-the-art\nperformance compared with other strong competitors.",
    "num_pages": 12
}