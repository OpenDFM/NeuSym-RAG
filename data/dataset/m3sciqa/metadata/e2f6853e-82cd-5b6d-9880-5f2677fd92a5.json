{
    "uuid": "e2f6853e-82cd-5b6d-9880-5f2677fd92a5",
    "title": "Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Byeongchang Kim",
        "Jaewoo Ahn",
        "Gunhee Kim"
    ],
    "pdf_url": "http://arxiv.org/pdf/2002.07510v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\e2f6853e-82cd-5b6d-9880-5f2677fd92a5.pdf",
    "bibtex": "@misc{kim2020sequentiallatentknowledgeselectionfor,\n    title = {Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue},\n    author = {Byeongchang Kim and Jaewoo Ahn and Gunhee Kim},\n    year = {2020},\n    eprint = {2002.07510},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2002.07510},\n}",
    "abstract": "Knowledge-grounded dialogue is a task of generating an informative response\nbased on both discourse context and external knowledge. As we focus on better\nmodeling the knowledge selection in the multi-turn knowledge-grounded dialogue,\nwe propose a sequential latent variable model as the first approach to this\nmatter. The model named sequential knowledge transformer (SKT) can keep track\nof the prior and posterior distribution over knowledge; as a result, it can not\nonly reduce the ambiguity caused from the diversity in knowledge selection of\nconversation but also better leverage the response information for proper\nchoice of knowledge. Our experimental results show that the proposed model\nimproves the knowledge selection accuracy and subsequently the performance of\nutterance generation. We achieve the new state-of-the-art performance on Wizard\nof Wikipedia (Dinan et al., 2019) as one of the most large-scale and\nchallenging benchmarks. We further validate the effectiveness of our model over\nexisting conversation methods in another knowledge-based dialogue Holl-E\ndataset (Moghe et al., 2018).",
    "num_pages": 14
}