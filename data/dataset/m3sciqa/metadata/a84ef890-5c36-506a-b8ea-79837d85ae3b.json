{
    "uuid": "a84ef890-5c36-506a-b8ea-79837d85ae3b",
    "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Michael Ahn",
        "Anthony Brohan",
        "Noah Brown",
        "Yevgen Chebotar",
        "Omar Cortes",
        "Byron David",
        "Chelsea Finn",
        "Chuyuan Fu",
        "Keerthana Gopalakrishnan",
        "Karol Hausman",
        "Alex Herzog",
        "Daniel Ho",
        "Jasmine Hsu",
        "Julian Ibarz",
        "Brian Ichter",
        "Alex Irpan",
        "Eric Jang",
        "Rosario Jauregui Ruano",
        "Kyle Jeffrey",
        "Sally Jesmonth",
        "Nikhil J Joshi",
        "Ryan Julian",
        "Dmitry Kalashnikov",
        "Yuheng Kuang",
        "Kuang-Huei Lee",
        "Sergey Levine",
        "Yao Lu",
        "Linda Luu",
        "Carolina Parada",
        "Peter Pastor",
        "Jornell Quiambao",
        "Kanishka Rao",
        "Jarek Rettinghouse",
        "Diego Reyes",
        "Pierre Sermanet",
        "Nicolas Sievers",
        "Clayton Tan",
        "Alexander Toshev",
        "Vincent Vanhoucke",
        "Fei Xia",
        "Ted Xiao",
        "Peng Xu",
        "Sichun Xu",
        "Mengyuan Yan",
        "Andy Zeng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.01691v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\a84ef890-5c36-506a-b8ea-79837d85ae3b.pdf",
    "bibtex": "@misc{ahn2022doasicannot,\n    title = {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},\n    author = {Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},\n    year = {2022},\n    eprint = {2204.01691},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.RO},\n    url = {http://arxiv.org/abs/2204.01691},\n}",
    "abstract": "Large language models can encode a wealth of semantic knowledge about the\nworld. Such knowledge could be extremely useful to robots aiming to act upon\nhigh-level, temporally extended instructions expressed in natural language.\nHowever, a significant weakness of language models is that they lack real-world\nexperience, which makes it difficult to leverage them for decision making\nwithin a given embodiment. For example, asking a language model to describe how\nto clean a spill might result in a reasonable narrative, but it may not be\napplicable to a particular agent, such as a robot, that needs to perform this\ntask in a particular environment. We propose to provide real-world grounding by\nmeans of pretrained skills, which are used to constrain the model to propose\nnatural language actions that are both feasible and contextually appropriate.\nThe robot can act as the language model's \"hands and eyes,\" while the language\nmodel supplies high-level semantic knowledge about the task. We show how\nlow-level skills can be combined with large language models so that the\nlanguage model provides high-level knowledge about the procedures for\nperforming complex and temporally-extended instructions, while value functions\nassociated with these skills provide the grounding necessary to connect this\nknowledge to a particular physical environment. We evaluate our method on a\nnumber of real-world robotic tasks, where we show the need for real-world\ngrounding and that this approach is capable of completing long-horizon,\nabstract, natural language instructions on a mobile manipulator. The project's\nwebsite and the video can be found at https://say-can.github.io/.",
    "num_pages": 34
}