{
    "uuid": "d527d6bb-30ef-5662-971b-78311f082434",
    "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Liyan Tang",
        "Tanya Goyal",
        "Alexander R. Fabbri",
        "Philippe Laban",
        "Jiacheng Xu",
        "Semih Yavuz",
        "Wojciech Kryściński",
        "Justin F. Rousseau",
        "Greg Durrett"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.12854v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\d527d6bb-30ef-5662-971b-78311f082434.pdf",
    "bibtex": "@misc{tang2023understandingfactualerrorsinsummarization,\n    title = {Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors},\n    author = {Liyan Tang and Tanya Goyal and Alexander R. Fabbri and Philippe Laban and Jiacheng Xu and Semih Yavuz and Wojciech Kryściński and Justin F. Rousseau and Greg Durrett},\n    year = {2023},\n    eprint = {2205.12854},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.12854},\n}",
    "abstract": "The propensity of abstractive summarization models to make factual errors has\nbeen studied extensively, including design of metrics to detect factual errors\nand annotation of errors in current systems' outputs. However, the\never-evolving nature of summarization systems, metrics, and annotated\nbenchmarks makes factuality evaluation a moving target, and drawing clear\ncomparisons among metrics has become increasingly difficult. In this work, we\naggregate factuality error annotations from nine existing datasets and stratify\nthem according to the underlying summarization model. We compare performance of\nstate-of-the-art factuality metrics, including recent ChatGPT-based metrics, on\nthis stratified benchmark and show that their performance varies significantly\nacross different types of summarization models. Critically, our analysis shows\nthat much of the recent improvement in the factuality detection space has been\non summaries from older (pre-Transformer) models instead of more relevant\nrecent summarization models. We further perform a finer-grained analysis per\nerror-type and find similar performance variance across error types for\ndifferent factuality metrics. Our results show that no one metric is superior\nin all settings or for all error types, and we provide recommendations for best\npractices given these insights.",
    "num_pages": 17
}