{
    "uuid": "a1e01326-a4d0-540c-92f8-037a3dd93c8e",
    "title": "Transformer-Patcher: One Mistake worth One Neuron",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zeyu Huang",
        "Yikang Shen",
        "Xiaofeng Zhang",
        "Jie Zhou",
        "Wenge Rong",
        "Zhang Xiong"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.09785v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\a1e01326-a4d0-540c-92f8-037a3dd93c8e.pdf",
    "bibtex": "@misc{huang2023transformerpatcheronemistakeworthone,\n    title = {Transformer-Patcher: One Mistake worth One Neuron},\n    author = {Zeyu Huang and Yikang Shen and Xiaofeng Zhang and Jie Zhou and Wenge Rong and Zhang Xiong},\n    year = {2023},\n    eprint = {2301.09785},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.09785},\n}",
    "abstract": "Large Transformer-based Pretrained Language Models (PLMs) dominate almost all\nNatural Language Processing (NLP) tasks. Nevertheless, they still make mistakes\nfrom time to time. For a model deployed in an industrial environment, fixing\nthese mistakes quickly and robustly is vital to improve user experiences.\nPrevious works formalize such problems as Model Editing (ME) and mostly focus\non fixing one mistake. However, the one-mistake-fixing scenario is not an\naccurate abstraction of the real-world challenge. In the deployment of AI\nservices, there are ever-emerging mistakes, and the same mistake may recur if\nnot corrected in time. Thus a preferable solution is to rectify the mistakes as\nsoon as they appear nonstop. Therefore, we extend the existing ME into\nSequential Model Editing (SME) to help develop more practical editing methods.\nOur study shows that most current ME methods could yield unsatisfying results\nin this scenario. We then introduce Transformer-Patcher, a novel model editor\nthat can shift the behavior of transformer-based models by simply adding and\ntraining a few neurons in the last Feed-Forward Network layer. Experimental\nresults on both classification and generation tasks show that\nTransformer-Patcher can successively correct up to thousands of errors\n(Reliability) and generalize to their equivalent inputs (Generality) while\nretaining the model's accuracy on irrelevant inputs (Locality). Our method\noutperforms previous fine-tuning and HyperNetwork-based methods and achieves\nstate-of-the-art performance for Sequential Model Editing (SME). The code is\navailable at https://github.com/ZeroYuHuang/Transformer-Patcher.",
    "num_pages": 16
}