{
    "uuid": "b1509b39-7013-5138-af63-9057839e7476",
    "title": "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Arabella Sinclair",
        "Jaap Jumelet",
        "Willem Zuidema",
        "Raquel Fernández"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.14989v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\b1509b39-7013-5138-af63-9057839e7476.pdf",
    "bibtex": "@misc{sinclair2022structuralpersistenceinlanguagemodels,\n    title = {Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations},\n    author = {Arabella Sinclair and Jaap Jumelet and Willem Zuidema and Raquel Fernández},\n    year = {2022},\n    eprint = {2109.14989},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.14989},\n}",
    "abstract": "We investigate the extent to which modern, neural language models are\nsusceptible to structural priming, the phenomenon whereby the structure of a\nsentence makes the same structure more probable in a follow-up sentence. We\nexplore how priming can be used to study the potential of these models to learn\nabstract structural information, which is a prerequisite for good performance\non tasks that require natural language understanding skills. We introduce a\nnovel metric and release Prime-LM, a large corpus where we control for various\nlinguistic factors which interact with priming strength. We find that\nTransformer models indeed show evidence of structural priming, but also that\nthe generalisations they learned are to some extent modulated by semantic\ninformation. Our experiments also show that the representations acquired by the\nmodels may not only encode abstract sequential structure but involve certain\nlevel of hierarchical syntactic information. More generally, our study shows\nthat the priming paradigm is a useful, additional tool for gaining insights\ninto the capacities of language models and opens the door to future\npriming-based investigations that probe the model's internal states.",
    "num_pages": 19
}