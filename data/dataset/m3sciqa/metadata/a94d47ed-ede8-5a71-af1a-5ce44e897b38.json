{
    "uuid": "a94d47ed-ede8-5a71-af1a-5ce44e897b38",
    "title": "Pointer Sentinel Mixture Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Stephen Merity",
        "Caiming Xiong",
        "James Bradbury",
        "Richard Socher"
    ],
    "pdf_url": "http://arxiv.org/pdf/1609.07843v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\a94d47ed-ede8-5a71-af1a-5ce44e897b38.pdf",
    "bibtex": "@misc{merity2016pointersentinelmixturemodels,\n    title = {Pointer Sentinel Mixture Models},\n    author = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},\n    year = {2016},\n    eprint = {1609.07843},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1609.07843},\n}",
    "abstract": "Recent neural network sequence models with softmax classifiers have achieved\ntheir best language modeling performance only with very large hidden states and\nlarge vocabularies. Even then they struggle to predict rare or unseen words\neven if the context makes the prediction unambiguous. We introduce the pointer\nsentinel mixture architecture for neural sequence models which has the ability\nto either reproduce a word from the recent context or produce a word from a\nstandard softmax classifier. Our pointer sentinel-LSTM model achieves state of\nthe art language modeling performance on the Penn Treebank (70.9 perplexity)\nwhile using far fewer parameters than a standard softmax LSTM. In order to\nevaluate how well language models can exploit longer contexts and deal with\nmore realistic vocabularies and larger corpora we also introduce the freely\navailable WikiText corpus.",
    "num_pages": 13
}