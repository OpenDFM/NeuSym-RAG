{
    "uuid": "0dbcbe8e-2d15-50c7-a52a-9aea7f74fa95",
    "title": "Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Daniel Furrer",
        "Marc van Zee",
        "Nathan Scales",
        "Nathanael Schärli"
    ],
    "pdf_url": "http://arxiv.org/pdf/2007.08970v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\0dbcbe8e-2d15-50c7-a52a-9aea7f74fa95.pdf",
    "bibtex": "@misc{furrer2021compositionalgeneralizationinsemanticparsing,\n    title = {Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures},\n    author = {Daniel Furrer and Marc van Zee and Nathan Scales and Nathanael Schärli},\n    year = {2021},\n    eprint = {2007.08970},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2007.08970},\n}",
    "abstract": "While mainstream machine learning methods are known to have limited ability\nto compositionally generalize, new architectures and techniques continue to be\nproposed to address this limitation. We investigate state-of-the-art techniques\nand architectures in order to assess their effectiveness in improving\ncompositional generalization in semantic parsing tasks based on the SCAN and\nCFQ datasets. We show that masked language model (MLM) pre-training rivals\nSCAN-inspired architectures on primitive holdout splits. On a more complex\ncompositional task, we show that pre-training leads to significant improvements\nin performance vs. comparable non-pre-trained models, whereas architectures\nproposed to encourage compositional generalization on SCAN or in the area of\nalgorithm learning fail to lead to significant improvements. We establish a new\nstate of the art on the CFQ compositional generalization benchmark using MLM\npre-training together with an intermediate representation.",
    "num_pages": 16
}