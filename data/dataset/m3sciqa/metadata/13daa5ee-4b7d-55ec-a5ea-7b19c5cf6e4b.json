{
    "uuid": "13daa5ee-4b7d-55ec-a5ea-7b19c5cf6e4b",
    "title": "An Investigation of Evaluation Metrics for Automated Medical Note Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Asma Ben Abacha",
        "Wen-wai Yim",
        "George Michalopoulos",
        "Thomas Lin"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.17364v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\13daa5ee-4b7d-55ec-a5ea-7b19c5cf6e4b.pdf",
    "bibtex": "@misc{abacha2023aninvestigationofevaluationmetrics,\n    title = {An Investigation of Evaluation Metrics for Automated Medical Note Generation},\n    author = {Asma Ben Abacha and Wen-wai Yim and George Michalopoulos and Thomas Lin},\n    year = {2023},\n    eprint = {2305.17364},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.17364},\n}",
    "abstract": "Recent studies on automatic note generation have shown that doctors can save\nsignificant amounts of time when using automatic clinical note generation\n(Knoll et al., 2022). Summarization models have been used for this task to\ngenerate clinical notes as summaries of doctor-patient conversations (Krishna\net al., 2021; Cai et al., 2022). However, assessing which model would best\nserve clinicians in their daily practice is still a challenging task due to the\nlarge set of possible correct summaries, and the potential limitations of\nautomatic evaluation metrics. In this paper, we study evaluation methods and\nmetrics for the automatic generation of clinical notes from medical\nconversations. In particular, we propose new task-specific metrics and we\ncompare them to SOTA evaluation metrics in text summarization and generation,\nincluding: (i) knowledge-graph embedding-based metrics, (ii) customized\nmodel-based metrics, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble\nmetrics. To study the correlation between the automatic metrics and manual\njudgments, we evaluate automatic notes/summaries by comparing the system and\nreference facts and computing the factual correctness, and the hallucination\nand omission rates for critical medical facts. This study relied on seven\ndatasets manually annotated by domain experts. Our experiments show that\nautomatic evaluation metrics can have substantially different behaviors on\ndifferent types of clinical notes datasets. However, the results highlight one\nstable subset of metrics as the most correlated with human judgments with a\nrelevant aggregation of different evaluation criteria.",
    "num_pages": 12
}