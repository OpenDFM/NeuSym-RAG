{
    "uuid": "e2b71dfc-0678-5451-82f6-42650b5ca0dc",
    "title": "Molecular Contrastive Learning of Representations via Graph Neural Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yuyang Wang",
        "Jianren Wang",
        "Zhonglin Cao",
        "Amir Barati Farimani"
    ],
    "pdf_url": "http://arxiv.org/pdf/2102.10056v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e2b71dfc-0678-5451-82f6-42650b5ca0dc.pdf",
    "bibtex": "@misc{wang2022molecularcontrastivelearningofrepresentations,\n    title = {Molecular Contrastive Learning of Representations via Graph Neural Networks},\n    author = {Yuyang Wang and Jianren Wang and Zhonglin Cao and Amir Barati Farimani},\n    year = {2022},\n    eprint = {2102.10056},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2102.10056},\n}",
    "abstract": "Molecular Machine Learning (ML) bears promise for efficient molecule property\nprediction and drug discovery. However, labeled molecule data can be expensive\nand time-consuming to acquire. Due to the limited labeled data, it is a great\nchallenge for supervised-learning ML models to generalize to the giant chemical\nspace. In this work, we present MolCLR: Molecular Contrastive Learning of\nRepresentations via Graph Neural Networks (GNNs), a self-supervised learning\nframework that leverages large unlabeled data (~10M unique molecules). In\nMolCLR pre-training, we build molecule graphs and develop GNN encoders to learn\ndifferentiable representations. Three molecule graph augmentations are\nproposed: atom masking, bond deletion, and subgraph removal. A contrastive\nestimator maximizes the agreement of augmentations from the same molecule while\nminimizing the agreement of different molecules. Experiments show that our\ncontrastive learning framework significantly improves the performance of GNNs\non various molecular property benchmarks including both classification and\nregression tasks. Benefiting from pre-training on the large unlabeled database,\nMolCLR even achieves state-of-the-art on several challenging benchmarks after\nfine-tuning. Additionally, further investigations demonstrate that MolCLR\nlearns to embed molecules into representations that can distinguish chemically\nreasonable molecular similarities.",
    "num_pages": 19
}