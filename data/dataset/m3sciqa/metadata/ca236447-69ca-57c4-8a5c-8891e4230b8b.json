{
    "uuid": "ca236447-69ca-57c4-8a5c-8891e4230b8b",
    "title": "Publicly Available Clinical BERT Embeddings",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Emily Alsentzer",
        "John R. Murphy",
        "Willie Boag",
        "Wei-Hung Weng",
        "Di Jin",
        "Tristan Naumann",
        "Matthew B. A. McDermott"
    ],
    "pdf_url": "http://arxiv.org/pdf/1904.03323v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\ca236447-69ca-57c4-8a5c-8891e4230b8b.pdf",
    "bibtex": "@misc{alsentzer2019publiclyavailableclinicalbertembeddings,\n    title = {Publicly Available Clinical BERT Embeddings},\n    author = {Emily Alsentzer and John R. Murphy and Willie Boag and Wei-Hung Weng and Di Jin and Tristan Naumann and Matthew B. A. McDermott},\n    year = {2019},\n    eprint = {1904.03323},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1904.03323},\n}",
    "abstract": "Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT\n(Devlin et al., 2018) have dramatically improved performance for many natural\nlanguage processing (NLP) tasks in recent months. However, these models have\nbeen minimally explored on specialty corpora, such as clinical text; moreover,\nin the clinical domain, no publicly-available pre-trained BERT models yet\nexist. In this work, we address this need by exploring and releasing BERT\nmodels for clinical text: one for generic clinical text and another for\ndischarge summaries specifically. We demonstrate that using a domain-specific\nmodel yields performance improvements on three common clinical NLP tasks as\ncompared to nonspecific embeddings. These domain-specific models are not as\nperformant on two clinical de-identification tasks, and argue that this is a\nnatural consequence of the differences between de-identified source text and\nsynthetically non de-identified task text.",
    "num_pages": 7
}