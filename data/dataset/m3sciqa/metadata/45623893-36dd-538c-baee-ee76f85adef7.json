{
    "uuid": "45623893-36dd-538c-baee-ee76f85adef7",
    "title": "Recipes for building an open-domain chatbot",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Stephen Roller",
        "Emily Dinan",
        "Naman Goyal",
        "Da Ju",
        "Mary Williamson",
        "Yinhan Liu",
        "Jing Xu",
        "Myle Ott",
        "Kurt Shuster",
        "Eric M. Smith",
        "Y-Lan Boureau",
        "Jason Weston"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.13637v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\45623893-36dd-538c-baee-ee76f85adef7.pdf",
    "bibtex": "@misc{roller2020recipesforbuildinganopendomain,\n    title = {Recipes for building an open-domain chatbot},\n    author = {Stephen Roller and Emily Dinan and Naman Goyal and Da Ju and Mary Williamson and Yinhan Liu and Jing Xu and Myle Ott and Kurt Shuster and Eric M. Smith and Y-Lan Boureau and Jason Weston},\n    year = {2020},\n    eprint = {2004.13637},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.13637},\n}",
    "abstract": "Building open-domain chatbots is a challenging area for machine learning\nresearch. While prior work has shown that scaling neural models in the number\nof parameters and the size of the data they are trained on gives improved\nresults, we show that other ingredients are important for a high-performing\nchatbot. Good conversation requires a number of skills that an expert\nconversationalist blends in a seamless way: providing engaging talking points\nand listening to their partners, and displaying knowledge, empathy and\npersonality appropriately, while maintaining a consistent persona. We show that\nlarge scale models can learn these skills when given appropriate training data\nand choice of generation strategy. We build variants of these recipes with 90M,\n2.7B and 9.4B parameter models, and make our models and code publicly\navailable. Human evaluations show our best models are superior to existing\napproaches in multi-turn dialogue in terms of engagingness and humanness\nmeasurements. We then discuss the limitations of this work by analyzing failure\ncases of our models.",
    "num_pages": 25
}