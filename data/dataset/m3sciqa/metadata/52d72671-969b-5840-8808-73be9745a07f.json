{
    "uuid": "52d72671-969b-5840-8808-73be9745a07f",
    "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yelysei Bondarenko",
        "Markus Nagel",
        "Tijmen Blankevoort"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.12948v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\52d72671-969b-5840-8808-73be9745a07f.pdf",
    "bibtex": "@misc{bondarenko2021understandingandovercomingthechallenges,\n    title = {Understanding and Overcoming the Challenges of Efficient Transformer Quantization},\n    author = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},\n    year = {2021},\n    eprint = {2109.12948},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2109.12948},\n}",
    "abstract": "Transformer-based architectures have become the de-facto standard models for\na wide range of Natural Language Processing tasks. However, their memory\nfootprint and high latency are prohibitive for efficient deployment and\ninference on resource-limited devices. In this work, we explore quantization\nfor transformers. We show that transformers have unique quantization challenges\n-- namely, high dynamic activation ranges that are difficult to represent with\na low bit fixed-point format. We establish that these activations contain\nstructured outliers in the residual connections that encourage specific\nattention patterns, such as attending to the special separator token. To combat\nthese challenges, we present three solutions based on post-training\nquantization and quantization-aware training, each with a different set of\ncompromises for accuracy, model size, and ease of use. In particular, we\nintroduce a novel quantization scheme -- per-embedding-group quantization. We\ndemonstrate the effectiveness of our methods on the GLUE benchmark using BERT,\nestablishing state-of-the-art results for post-training quantization. Finally,\nwe show that transformer weights and embeddings can be quantized to ultra-low\nbit-widths, leading to significant memory savings with a minimum accuracy loss.\nOur source code is available\nat~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.",
    "num_pages": 23
}