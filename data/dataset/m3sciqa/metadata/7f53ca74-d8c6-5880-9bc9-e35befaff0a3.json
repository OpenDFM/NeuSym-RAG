{
    "uuid": "7f53ca74-d8c6-5880-9bc9-e35befaff0a3",
    "title": "PhoBERT: Pre-trained language models for Vietnamese",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Dat Quoc Nguyen",
        "Anh Tuan Nguyen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2003.00744v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\7f53ca74-d8c6-5880-9bc9-e35befaff0a3.pdf",
    "bibtex": "@misc{nguyen2020phobertpretrainedlanguagemodelsfor,\n    title = {PhoBERT: Pre-trained language models for Vietnamese},\n    author = {Dat Quoc Nguyen and Anh Tuan Nguyen},\n    year = {2020},\n    eprint = {2003.00744},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2003.00744},\n}",
    "abstract": "We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the\nfirst public large-scale monolingual language models pre-trained for\nVietnamese. Experimental results show that PhoBERT consistently outperforms the\nrecent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and\nimproves the state-of-the-art in multiple Vietnamese-specific NLP tasks\nincluding Part-of-speech tagging, Dependency parsing, Named-entity recognition\nand Natural language inference. We release PhoBERT to facilitate future\nresearch and downstream applications for Vietnamese NLP. Our PhoBERT models are\navailable at https://github.com/VinAIResearch/PhoBERT",
    "num_pages": 6
}