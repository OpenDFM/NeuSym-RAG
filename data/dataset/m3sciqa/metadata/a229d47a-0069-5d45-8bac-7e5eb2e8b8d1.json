{
    "uuid": "a229d47a-0069-5d45-8bac-7e5eb2e8b8d1",
    "title": "Text Detoxification using Large Pre-trained Neural Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "David Dale",
        "Anton Voronov",
        "Daryna Dementieva",
        "Varvara Logacheva",
        "Olga Kozlova",
        "Nikita Semenov",
        "Alexander Panchenko"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.08914v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\a229d47a-0069-5d45-8bac-7e5eb2e8b8d1.pdf",
    "bibtex": "@misc{dale2021textdetoxificationusinglargepretrained,\n    title = {Text Detoxification using Large Pre-trained Neural Models},\n    author = {David Dale and Anton Voronov and Daryna Dementieva and Varvara Logacheva and Olga Kozlova and Nikita Semenov and Alexander Panchenko},\n    year = {2021},\n    eprint = {2109.08914},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.08914},\n}",
    "abstract": "We present two novel unsupervised methods for eliminating toxicity in text.\nOur first method combines two recent ideas: (1) guidance of the generation\nprocess with small style-conditional language models and (2) use of\nparaphrasing models to perform style transfer. We use a well-performing\nparaphraser guided by style-trained language models to keep the text content\nand remove toxicity. Our second method uses BERT to replace toxic words with\ntheir non-offensive synonyms. We make the method more flexible by enabling BERT\nto replace mask tokens with a variable number of words. Finally, we present the\nfirst large-scale comparative study of style transfer models on the task of\ntoxicity removal. We compare our models with a number of methods for style\ntransfer. The models are evaluated in a reference-free way using a combination\nof unsupervised style transfer metrics. Both methods we suggest yield new SOTA\nresults.",
    "num_pages": 18
}