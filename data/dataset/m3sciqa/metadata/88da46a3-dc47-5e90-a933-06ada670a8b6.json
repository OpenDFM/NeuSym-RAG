{
    "uuid": "88da46a3-dc47-5e90-a933-06ada670a8b6",
    "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Uri Alon",
        "Frank F. Xu",
        "Junxian He",
        "Sudipta Sengupta",
        "Dan Roth",
        "Graham Neubig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.12431v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\88da46a3-dc47-5e90-a933-06ada670a8b6.pdf",
    "bibtex": "@misc{alon2022neurosymboliclanguagemodelingwithautomatonaugmented,\n    title = {Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval},\n    author = {Uri Alon and Frank F. Xu and Junxian He and Sudipta Sengupta and Dan Roth and Graham Neubig},\n    year = {2022},\n    eprint = {2201.12431},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2201.12431},\n}",
    "abstract": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .",
    "num_pages": 18
}