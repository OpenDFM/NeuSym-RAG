{
    "uuid": "c74cfa25-c4e3-5642-9de4-343770a00dc3",
    "title": "Discovering Discrete Latent Topics with Neural Variational Inference",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Yishu Miao",
        "Edward Grefenstette",
        "Phil Blunsom"
    ],
    "pdf_url": "http://arxiv.org/pdf/1706.00359v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\c74cfa25-c4e3-5642-9de4-343770a00dc3.pdf",
    "bibtex": "@misc{miao2018discoveringdiscretelatenttopicswith,\n    title = {Discovering Discrete Latent Topics with Neural Variational Inference},\n    author = {Yishu Miao and Edward Grefenstette and Phil Blunsom},\n    year = {2018},\n    eprint = {1706.00359},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1706.00359},\n}",
    "abstract": "Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models.",
    "num_pages": 11
}