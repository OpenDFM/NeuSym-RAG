{
    "uuid": "97a1532b-9bde-55b3-815d-290e4190c11f",
    "title": "Adapting Language Models to Compress Contexts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Alexis Chevalier",
        "Alexander Wettig",
        "Anirudh Ajith",
        "Danqi Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.14788v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\97a1532b-9bde-55b3-815d-290e4190c11f.pdf",
    "bibtex": "@misc{chevalier2023adaptinglanguagemodelstocompress,\n    title = {Adapting Language Models to Compress Contexts},\n    author = {Alexis Chevalier and Alexander Wettig and Anirudh Ajith and Danqi Chen},\n    year = {2023},\n    eprint = {2305.14788},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.14788},\n}",
    "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These language models are capable\nof compressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments, and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show\nthat AutoCompressors can utilize long contexts to improve perplexity. We\nevaluate AutoCompressors on in-context learning by compressing task\ndemonstrations and find that summary vectors are good substitutes for\nplain-text demonstrations, increasing accuracy while reducing inference costs.\nFinally, we explore the benefits of pre-computing summary vectors for large\ncorpora by applying summary vectors to retrievalaugmented language modeling and\na passage re-ranking task. Overall, AutoCompressors emerge as a simple and\ninexpensive solution to extend the context window of LMs while speeding up\ninference over long contexts.",
    "num_pages": 18
}