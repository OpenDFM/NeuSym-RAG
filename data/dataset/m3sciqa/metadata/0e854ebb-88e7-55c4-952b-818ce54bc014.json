{
    "uuid": "0e854ebb-88e7-55c4-952b-818ce54bc014",
    "title": "Imagination improves Multimodal Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Desmond Elliott",
        "Ákos Kádár"
    ],
    "pdf_url": "http://arxiv.org/pdf/1705.04350v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\0e854ebb-88e7-55c4-952b-818ce54bc014.pdf",
    "bibtex": "@misc{elliott2017imaginationimprovesmultimodaltranslation,\n    title = {Imagination improves Multimodal Translation},\n    author = {Desmond Elliott and Ákos Kádár},\n    year = {2017},\n    eprint = {1705.04350},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1705.04350},\n}",
    "abstract": "We decompose multimodal translation into two sub-tasks: learning to translate\nand learning visually grounded representations. In a multitask learning\nframework, translations are learned in an attention-based encoder-decoder, and\ngrounded representations are learned through image representation prediction.\nOur approach improves translation performance compared to the state of the art\non the Multi30K dataset. Furthermore, it is equally effective if we train the\nimage prediction task on the external MS COCO dataset, and we find improvements\nif we train the translation model on the external News Commentary parallel\ntext.",
    "num_pages": 12
}