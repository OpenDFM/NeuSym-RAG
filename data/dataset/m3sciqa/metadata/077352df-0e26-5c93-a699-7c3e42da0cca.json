{
    "uuid": "077352df-0e26-5c93-a699-7c3e42da0cca",
    "title": "Learning to Write with Cooperative Discriminators",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Ari Holtzman",
        "Jan Buys",
        "Maxwell Forbes",
        "Antoine Bosselut",
        "David Golub",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/1805.06087v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\077352df-0e26-5c93-a699-7c3e42da0cca.pdf",
    "bibtex": "@misc{holtzman2018learningtowritewithcooperative,\n    title = {Learning to Write with Cooperative Discriminators},\n    author = {Ari Holtzman and Jan Buys and Maxwell Forbes and Antoine Bosselut and David Golub and Yejin Choi},\n    year = {2018},\n    eprint = {1805.06087},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1805.06087},\n}",
    "abstract": "Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models,\nbut when used to generate natural language their output tends to be overly\ngeneric, repetitive, and self-contradictory. We postulate that the objective\nfunction optimized by RNN language models, which amounts to the overall\nperplexity of a text, is not expressive enough to capture the notion of\ncommunicative goals described by linguistic principles such as Grice's Maxims.\nWe propose learning a mixture of multiple discriminative models that can be\nused to complement the RNN generator and guide the decoding process. Human\nevaluation demonstrates that text generated by our system is preferred over\nthat of baselines by a large margin and significantly enhances the overall\ncoherence, style, and information content of the generated text.",
    "num_pages": 17
}