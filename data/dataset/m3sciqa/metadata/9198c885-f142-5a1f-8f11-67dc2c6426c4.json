{
    "uuid": "9198c885-f142-5a1f-8f11-67dc2c6426c4",
    "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Bo Du",
        "Dacheng Tao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2302.10198v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\9198c885-f142-5a1f-8f11-67dc2c6426c4.pdf",
    "bibtex": "@misc{zhong2023canchatgptunderstandtooa,\n    title = {Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT},\n    author = {Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Dacheng Tao},\n    year = {2023},\n    eprint = {2302.10198},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2302.10198},\n}",
    "abstract": "Recently, ChatGPT has attracted great attention, as it can generate fluent\nand high-quality responses to human inquiries. Several prior studies have shown\nthat ChatGPT attains remarkable generation ability compared with existing\nmodels. However, the quantitative analysis of ChatGPT's understanding ability\nhas been given little attention. In this report, we explore the understanding\nability of ChatGPT by evaluating it on the most popular GLUE benchmark, and\ncomparing it with 4 representative fine-tuned BERT-style models. We find that:\n1) ChatGPT falls short in handling paraphrase and similarity tasks; 2) ChatGPT\noutperforms all BERT models on inference tasks by a large margin; 3) ChatGPT\nachieves comparable performance compared with BERT on sentiment analysis and\nquestion-answering tasks. Additionally, by combining some advanced prompting\nstrategies, we show that the understanding ability of ChatGPT can be further\nimproved.",
    "num_pages": 19
}