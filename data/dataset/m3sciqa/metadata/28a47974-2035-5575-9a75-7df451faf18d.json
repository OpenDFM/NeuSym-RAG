{
    "uuid": "28a47974-2035-5575-9a75-7df451faf18d",
    "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2006.03654v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\28a47974-2035-5575-9a75-7df451faf18d.pdf",
    "bibtex": "@misc{he2021debertadecodingenhancedbertwithdisentangled,\n    title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n    author = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n    year = {2021},\n    eprint = {2006.03654},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2006.03654},\n}",
    "abstract": "Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).",
    "num_pages": 23
}