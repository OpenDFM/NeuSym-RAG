{
    "uuid": "cb79f244-a7d6-5793-9199-512a482911f8",
    "title": "One Student Knows All Experts Know: From Sparse to Dense",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Fuzhao Xue",
        "Xiaoxin He",
        "Xiaozhe Ren",
        "Yuxuan Lou",
        "Yang You"
    ],
    "pdf_url": "http://arxiv.org/pdf/2201.10890v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\cb79f244-a7d6-5793-9199-512a482911f8.pdf",
    "bibtex": "@misc{xue2022onestudentknowsallexperts,\n    title = {One Student Knows All Experts Know: From Sparse to Dense},\n    author = {Fuzhao Xue and Xiaoxin He and Xiaozhe Ren and Yuxuan Lou and Yang You},\n    year = {2022},\n    eprint = {2201.10890},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2201.10890},\n}",
    "abstract": "Human education system trains one student by multiple experts.\nMixture-of-experts (MoE) is a powerful sparse architecture including multiple\nexperts. However, sparse MoE model is easy to overfit, hard to deploy, and not\nhardware-friendly for practitioners. In this work, inspired by the human\neducation model, we propose a novel task, knowledge integration, to obtain a\ndense student model (OneS) as knowledgeable as one sparse MoE. We investigate\nthis task by proposing a general training framework including knowledge\ngathering and knowledge distillation. Specifically, to gather key knowledge\nfrom different pre-trained experts, we first investigate four different\npossible knowledge gathering methods, \\ie summation, averaging, Top-K Knowledge\nGathering (Top-KG), and Singular Value Decomposition Knowledge Gathering\n(SVD-KG) proposed in this paper. We then refine the dense student model by\nknowledge distillation to offset the noise from gathering. On ImageNet, our\nOneS preserves $61.7\\%$ benefits from MoE and achieves $78.4\\%$ top-1 accuracy\nImageNet with only $15$M parameters. On four natural language processing\ndatasets, OneS obtains $88.2\\%$ MoE benefits and outperforms the best baseline\nby $51.7\\%$ using the same architecture and training data. In addition,\ncompared with the MoE counterpart, OneS can achieve $3.7 \\times$ inference\nspeedup due to less computation and hardware-friendly architecture.",
    "num_pages": 13
}