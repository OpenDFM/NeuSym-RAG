{
    "uuid": "95c63efc-ff02-585d-bcdf-75e933c5816a",
    "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Christoph Schuhmann",
        "Richard Vencu",
        "Romain Beaumont",
        "Robert Kaczmarczyk",
        "Clayton Mullis",
        "Aarush Katta",
        "Theo Coombes",
        "Jenia Jitsev",
        "Aran Komatsuzaki"
    ],
    "pdf_url": "http://arxiv.org/pdf/2111.02114v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\95c63efc-ff02-585d-bcdf-75e933c5816a.pdf",
    "bibtex": "@misc{schuhmann2021laion400mopendatasetofclipfiltered,\n    title = {LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},\n    author = {Christoph Schuhmann and Richard Vencu and Romain Beaumont and Robert Kaczmarczyk and Clayton Mullis and Aarush Katta and Theo Coombes and Jenia Jitsev and Aran Komatsuzaki},\n    year = {2021},\n    eprint = {2111.02114},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2111.02114},\n}",
    "abstract": "Multi-modal language-vision models trained on hundreds of millions of\nimage-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable\ncapability to perform zero- or few-shot learning and transfer even in absence\nof per-sample labels on target image data. Despite this trend, to date there\nhas been no publicly available datasets of sufficient scale for training such\nmodels from scratch. To address this issue, in a community effort we build and\nrelease for public LAION-400M, a dataset with CLIP-filtered 400 million\nimage-text pairs, their CLIP embeddings and kNN indices that allow efficient\nsimilarity search.",
    "num_pages": 5
}