{
    "uuid": "08f4ba03-f60b-5eed-b60f-e94ba143379e",
    "title": "Making Pre-trained Language Models Better Few-shot Learners",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Tianyu Gao",
        "Adam Fisch",
        "Danqi Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.15723v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\08f4ba03-f60b-5eed-b60f-e94ba143379e.pdf",
    "bibtex": "@misc{gao2021makingpretrainedlanguagemodelsbetter,\n    title = {Making Pre-trained Language Models Better Few-shot Learners},\n    author = {Tianyu Gao and Adam Fisch and Danqi Chen},\n    year = {2021},\n    eprint = {2012.15723},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.15723},\n}",
    "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot\nperformance solely by leveraging a natural-language prompt and a few task\ndemonstrations as input context. Inspired by their findings, we study few-shot\nlearning in a more practical scenario, where we use smaller language models for\nwhich fine-tuning is computationally efficient. We present LM-BFF--better\nfew-shot fine-tuning of language models--a suite of simple and complementary\ntechniques for fine-tuning language models on a small number of annotated\nexamples. Our approach includes (1) prompt-based fine-tuning together with a\nnovel pipeline for automating prompt generation; and (2) a refined strategy for\ndynamically and selectively incorporating demonstrations into each context.\nFinally, we present a systematic evaluation for analyzing few-shot performance\non a range of NLP tasks, including classification and regression. Our\nexperiments demonstrate that our methods combine to dramatically outperform\nstandard fine-tuning procedures in this low resource setting, achieving up to\n30% absolute improvement, and 11% on average across all tasks. Our approach\nmakes minimal assumptions on task resources and domain expertise, and hence\nconstitutes a strong task-agnostic method for few-shot learning.",
    "num_pages": 15
}