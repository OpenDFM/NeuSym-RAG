{
    "uuid": "dedf27e6-1e46-5256-a1ff-ef86b7b9795a",
    "title": "VIBE: Topic-Driven Temporal Adaptation for Twitter Classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yuji Zhang",
        "Jing Li",
        "Wenjie Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.10191v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\dedf27e6-1e46-5256-a1ff-ef86b7b9795a.pdf",
    "bibtex": "@misc{zhang2023vibetopicdriventemporaladaptationfor,\n    title = {VIBE: Topic-Driven Temporal Adaptation for Twitter Classification},\n    author = {Yuji Zhang and Jing Li and Wenjie Li},\n    year = {2023},\n    eprint = {2310.10191},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.10191},\n}",
    "abstract": "Language features are evolving in real-world social media, resulting in the\ndeteriorating performance of text classification in dynamics. To address this\nchallenge, we study temporal adaptation, where models trained on past data are\ntested in the future. Most prior work focused on continued pretraining or\nknowledge updating, which may compromise their performance on noisy social\nmedia data. To tackle this issue, we reflect feature change via modeling latent\ntopic evolution and propose a novel model, VIBE: Variational Information\nBottleneck for Evolutions. Concretely, we first employ two Information\nBottleneck (IB) regularizers to distinguish past and future topics. Then, the\ndistinguished topics work as adaptive features via multi-task training with\ntimestamp and class label prediction. In adaptive learning, VIBE utilizes\nretrieved unlabeled data from online streams created posterior to training data\ntime. Substantial Twitter experiments on three classification tasks show that\nour model, with only 3% of data, significantly outperforms previous\nstate-of-the-art continued-pretraining methods.",
    "num_pages": 14
}