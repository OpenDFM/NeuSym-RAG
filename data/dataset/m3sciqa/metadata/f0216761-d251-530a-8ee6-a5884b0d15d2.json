{
    "uuid": "f0216761-d251-530a-8ee6-a5884b0d15d2",
    "title": "Learning Factorized Multimodal Representations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
    ],
    "pdf_url": "http://arxiv.org/pdf/1806.06176v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\f0216761-d251-530a-8ee6-a5884b0d15d2.pdf",
    "bibtex": "@misc{tsai2019learningfactorizedmultimodalrepresentations,\n    title = {Learning Factorized Multimodal Representations},\n    author = {Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency and Ruslan Salakhutdinov},\n    year = {2019},\n    eprint = {1806.06176},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1806.06176},\n}",
    "abstract": "Learning multimodal representations is a fundamentally complex research\nproblem due to the presence of multiple heterogeneous sources of information.\nAlthough the presence of multiple modalities provides additional valuable\ninformation, there are two key challenges to address when learning from\nmultimodal data: 1) models must learn the complex intra-modal and cross-modal\ninteractions for prediction and 2) models must be robust to unexpected missing\nor noisy modalities during testing. In this paper, we propose to optimize for a\njoint generative-discriminative objective across multimodal data and labels. We\nintroduce a model that factorizes representations into two sets of independent\nfactors: multimodal discriminative and modality-specific generative factors.\nMultimodal discriminative factors are shared across all modalities and contain\njoint multimodal features required for discriminative tasks such as sentiment\nprediction. Modality-specific generative factors are unique for each modality\nand contain the information required for generating data. Experimental results\nshow that our model is able to learn meaningful multimodal representations that\nachieve state-of-the-art or competitive performance on six multimodal datasets.\nOur model demonstrates flexible generative capabilities by conditioning on\nindependent factors and can reconstruct missing modalities without\nsignificantly impacting performance. Lastly, we interpret our factorized\nrepresentations to understand the interactions that influence multimodal\nlearning.",
    "num_pages": 20
}