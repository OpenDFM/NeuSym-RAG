{
    "uuid": "234e1d3a-aab1-5133-8362-f4ae804b268e",
    "title": "Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Wei Shi",
        "Vera Demberg"
    ],
    "pdf_url": "http://arxiv.org/pdf/1811.01697v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\234e1d3a-aab1-5133-8362-f4ae804b268e.pdf",
    "bibtex": "@misc{shi2019learningtoexplicitateconnectiveswith,\n    title = {Learning to Explicitate Connectives with Seq2Seq Network for Implicit Discourse Relation Classification},\n    author = {Wei Shi and Vera Demberg},\n    year = {2019},\n    eprint = {1811.01697},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1811.01697},\n}",
    "abstract": "Implicit discourse relation classification is one of the most difficult steps\nin discourse parsing. The difficulty stems from the fact that the coherence\nrelation must be inferred based on the content of the discourse relational\narguments. Therefore, an effective encoding of the relational arguments is of\ncrucial importance. We here propose a new model for implicit discourse relation\nclassification, which consists of a classifier, and a sequence-to-sequence\nmodel which is trained to generate a representation of the discourse relational\narguments by trying to predict the relational arguments including a suitable\nimplicit connective. Training is possible because such implicit connectives\nhave been annotated as part of the PDTB corpus. Along with a memory network,\nour model could generate more refined representations for the task. And on the\nnow standard 11-way classification, our method outperforms previous state of\nthe art systems on the PDTB benchmark on multiple settings including cross\nvalidation.",
    "num_pages": 12
}