{
    "uuid": "2ca6dd33-bf5b-50cf-b091-3282a484d4a8",
    "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Dustin Schwenk",
        "Apoorv Khandelwal",
        "Christopher Clark",
        "Kenneth Marino",
        "Roozbeh Mottaghi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2206.01718v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\2ca6dd33-bf5b-50cf-b091-3282a484d4a8.pdf",
    "bibtex": "@misc{schwenk2022aokvqaabenchmarkforvisual,\n    title = {A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge},\n    author = {Dustin Schwenk and Apoorv Khandelwal and Christopher Clark and Kenneth Marino and Roozbeh Mottaghi},\n    year = {2022},\n    eprint = {2206.01718},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2206.01718},\n}",
    "abstract": "The Visual Question Answering (VQA) task aspires to provide a meaningful\ntestbed for the development of AI models that can jointly reason over visual\nand natural language inputs. Despite a proliferation of VQA datasets, this goal\nis hindered by a set of common limitations. These include a reliance on\nrelatively simplistic questions that are repetitive in both concepts and\nlinguistic structure, little world knowledge needed outside of the paired\nimage, and limited reasoning required to arrive at the correct answer. We\nintroduce A-OKVQA, a crowdsourced dataset composed of a diverse set of about\n25K questions requiring a broad base of commonsense and world knowledge to\nanswer. In contrast to the existing knowledge-based VQA datasets, the questions\ngenerally cannot be answered by simply querying a knowledge base, and instead\nrequire some form of commonsense reasoning about the scene depicted in the\nimage. We demonstrate the potential of this new dataset through a detailed\nanalysis of its contents and baseline performance measurements over a variety\nof state-of-the-art vision-language models. Project page:\nhttp://a-okvqa.allenai.org/",
    "num_pages": 20
}