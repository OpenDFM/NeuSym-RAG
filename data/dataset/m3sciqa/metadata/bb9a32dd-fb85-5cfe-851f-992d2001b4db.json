{
    "uuid": "bb9a32dd-fb85-5cfe-851f-992d2001b4db",
    "title": "Training Compute-Optimal Large Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jordan Hoffmann",
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Elena Buchatskaya",
        "Trevor Cai",
        "Eliza Rutherford",
        "Diego de Las Casas",
        "Lisa Anne Hendricks",
        "Johannes Welbl",
        "Aidan Clark",
        "Tom Hennigan",
        "Eric Noland",
        "Katie Millican",
        "George van den Driessche",
        "Bogdan Damoc",
        "Aurelia Guy",
        "Simon Osindero",
        "Karen Simonyan",
        "Erich Elsen",
        "Jack W. Rae",
        "Oriol Vinyals",
        "Laurent Sifre"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.15556v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\bb9a32dd-fb85-5cfe-851f-992d2001b4db.pdf",
    "bibtex": "@misc{hoffmann2022trainingcomputeoptimallargelanguagemodels,\n    title = {Training Compute-Optimal Large Language Models},\n    author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},\n    year = {2022},\n    eprint = {2203.15556},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.15556},\n}",
    "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
    "num_pages": 36
}