{
    "uuid": "e5c8fae9-734a-54d1-bde4-03ec07d10c86",
    "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Guanghui Qin",
        "Jason Eisner"
    ],
    "pdf_url": "http://arxiv.org/pdf/2104.06599v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\e5c8fae9-734a-54d1-bde4-03ec07d10c86.pdf",
    "bibtex": "@misc{qin2021learninghowtoaskquerying,\n    title = {Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},\n    author = {Guanghui Qin and Jason Eisner},\n    year = {2021},\n    eprint = {2104.06599},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2104.06599},\n}",
    "abstract": "Natural-language prompts have recently been used to coax pretrained language\nmodels into performing other AI tasks, using a fill-in-the-blank paradigm\n(Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,\n2020). For example, language models retain factual knowledge from their\ntraining corpora that can be extracted by asking them to \"fill in the blank\" in\na sentential prompt. However, where does this prompt come from? We explore the\nidea of learning prompts by gradient descent -- either fine-tuning prompts\ntaken from previous work, or starting from random initialization. Our prompts\nconsist of \"soft words,\" i.e., continuous vectors that are not necessarily word\ntype embeddings from the language model. Furthermore, for each task, we\noptimize a mixture of prompts, learning which prompts are most effective and\nhow to ensemble them. Across multiple English LMs and tasks, our approach\nhugely outperforms previous methods, showing that the implicit factual\nknowledge in language models was previously underestimated. Moreover, this\nknowledge is cheap to elicit: random initialization is nearly as good as\ninformed initialization.",
    "num_pages": 11
}