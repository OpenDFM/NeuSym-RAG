{
    "uuid": "118cac21-d91b-55b6-bfce-0742348b4c2d",
    "title": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Chao Yu",
        "Akash Velu",
        "Eugene Vinitsky",
        "Jiaxuan Gao",
        "Yu Wang",
        "Alexandre Bayen",
        "Yi Wu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2103.01955v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\118cac21-d91b-55b6-bfce-0742348b4c2d.pdf",
    "bibtex": "@misc{yu2022thesurprisingeffectivenessofppo,\n    title = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},\n    author = {Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},\n    year = {2022},\n    eprint = {2103.01955},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2103.01955},\n}",
    "abstract": "Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due to the belief that PPO is\nsignificantly less sample efficient than off-policy methods in multi-agent\nsystems. In this work, we carefully study the performance of PPO in cooperative\nmulti-agent settings. We show that PPO-based multi-agent algorithms achieve\nsurprisingly strong performance in four popular multi-agent testbeds: the\nparticle-world environments, the StarCraft multi-agent challenge, Google\nResearch Football, and the Hanabi challenge, with minimal hyperparameter tuning\nand without any domain-specific algorithmic modifications or architectures.\nImportantly, compared to competitive off-policy methods, PPO often achieves\ncompetitive or superior results in both final returns and sample efficiency.\nFinally, through ablation studies, we analyze implementation and hyperparameter\nfactors that are critical to PPO's empirical performance, and give concrete\npractical suggestions regarding these factors. Our results show that when using\nthese practices, simple PPO-based methods can be a strong baseline in\ncooperative multi-agent reinforcement learning. Source code is released at\n\\url{https://github.com/marlbenchmark/on-policy}.",
    "num_pages": 30
}