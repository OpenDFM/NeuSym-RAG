{
    "uuid": "206fe373-bd70-5bb7-ad16-6151168a2cc7",
    "title": "MetaICL: Learning to Learn In Context",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Sewon Min",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15943v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\206fe373-bd70-5bb7-ad16-6151168a2cc7.pdf",
    "bibtex": "@misc{min2022metaicllearningtolearnin,\n    title = {MetaICL: Learning to Learn In Context},\n    author = {Sewon Min and Mike Lewis and Luke Zettlemoyer and Hannaneh Hajishirzi},\n    year = {2022},\n    eprint = {2110.15943},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.15943},\n}",
    "abstract": "We introduce MetaICL (Meta-training for In-Context Learning), a new\nmeta-training framework for few-shot learning where a pretrained language model\nis tuned to do in-context learning on a large set of training tasks. This\nmeta-training enables the model to more effectively learn a new task in context\nat test time, by simply conditioning on a few training examples with no\nparameter updates or task-specific templates. We experiment on a large, diverse\ncollection of tasks consisting of 142 NLP datasets including classification,\nquestion answering, natural language inference, paraphrase detection and more,\nacross seven different meta-training/target splits. MetaICL outperforms a range\nof baselines including in-context learning without meta-training and multi-task\nlearning followed by zero-shot transfer. We find that the gains are\nparticularly significant for target tasks that have domain shifts from the\nmeta-training tasks, and that using a diverse set of the meta-training tasks is\nkey to improvements. We also show that MetaICL approaches (and sometimes beats)\nthe performance of models fully finetuned on the target task, and outperforms\nmuch bigger models with nearly 8x parameters. Finally, we show that MetaICL is\ncomplementary to human-written instructions, and the best performance can be\nachieved by combining both approaches.",
    "num_pages": 19
}