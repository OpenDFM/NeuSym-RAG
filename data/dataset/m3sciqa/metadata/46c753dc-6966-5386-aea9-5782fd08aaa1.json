{
    "uuid": "46c753dc-6966-5386-aea9-5782fd08aaa1",
    "title": "A Cognitive Regularizer for Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jason Wei",
        "Clara Meister",
        "Ryan Cotterell"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.07144v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\46c753dc-6966-5386-aea9-5782fd08aaa1.pdf",
    "bibtex": "@misc{wei2021acognitiveregularizerforlanguage,\n    title = {A Cognitive Regularizer for Language Modeling},\n    author = {Jason Wei and Clara Meister and Ryan Cotterell},\n    year = {2021},\n    eprint = {2105.07144},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2105.07144},\n}",
    "abstract": "The uniform information density (UID) hypothesis, which posits that speakers\nbehaving optimally tend to distribute information uniformly across a linguistic\nsignal, has gained traction in psycholinguistics as an explanation for certain\nsyntactic, morphological, and prosodic choices. In this work, we explore\nwhether the UID hypothesis can be operationalized as an inductive bias for\nstatistical language modeling. Specifically, we augment the canonical MLE\nobjective for training language models with a regularizer that encodes UID. In\nexperiments on ten languages spanning five language families, we find that\nusing UID regularization consistently improves perplexity in language models,\nhaving a larger effect when training data is limited. Moreover, via an analysis\nof generated sequences, we find that UID-regularized language models have other\ndesirable properties, e.g., they generate text that is more lexically diverse.\nOur results not only suggest that UID is a reasonable inductive bias for\nlanguage modeling, but also provide an alternative validation of the UID\nhypothesis using modern-day NLP tools.",
    "num_pages": 12
}