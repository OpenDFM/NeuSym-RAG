{
    "uuid": "900a2a6f-561b-528a-98d2-b3db6994132a",
    "title": "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Cheng-Fu Yang",
        "Yen-Chun Chen",
        "Jianwei Yang",
        "Xiyang Dai",
        "Lu Yuan",
        "Yu-Chiang Frank Wang",
        "Kai-Wei Chang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2310.12344v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\900a2a6f-561b-528a-98d2-b3db6994132a.pdf",
    "bibtex": "@misc{yang2023lacmalanguagealigningcontrastivelearningwith,\n    title = {LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following},\n    author = {Cheng-Fu Yang and Yen-Chun Chen and Jianwei Yang and Xiyang Dai and Lu Yuan and Yu-Chiang Frank Wang and Kai-Wei Chang},\n    year = {2023},\n    eprint = {2310.12344},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2310.12344},\n}",
    "abstract": "End-to-end Transformers have demonstrated an impressive success rate for\nEmbodied Instruction Following when the environment has been seen in training.\nHowever, they tend to struggle when deployed in an unseen environment. This\nlack of generalizability is due to the agent's insensitivity to subtle changes\nin natural language instructions. To mitigate this issue, we propose explicitly\naligning the agent's hidden states with the instructions via contrastive\nlearning. Nevertheless, the semantic gap between high-level language\ninstructions and the agent's low-level action space remains an obstacle.\nTherefore, we further introduce a novel concept of meta-actions to bridge the\ngap. Meta-actions are ubiquitous action patterns that can be parsed from the\noriginal action sequence. These patterns represent higher-level semantics that\nare intuitively aligned closer to the instructions. When meta-actions are\napplied as additional training signals, the agent generalizes better to unseen\nenvironments. Compared to a strong multi-modal Transformer baseline, we achieve\na significant 4.5% absolute gain in success rate in unseen environments of\nALFRED Embodied Instruction Following. Additional analysis shows that the\ncontrastive objective and meta-actions are complementary in achieving the best\nresults, and the resulting agent better aligns its states with corresponding\ninstructions, making it more suitable for real-world embodied agents. The code\nis available at: https://github.com/joeyy5588/LACMA.",
    "num_pages": 15
}