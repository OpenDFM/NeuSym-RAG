{
    "uuid": "608838ef-8a4c-50f0-a78e-db6f2709edc5",
    "title": "Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Ramesh Nallapati",
        "Bowen Zhou",
        "Cicero Nogueira dos santos",
        "Caglar Gulcehre",
        "Bing Xiang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1602.06023v5",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\608838ef-8a4c-50f0-a78e-db6f2709edc5.pdf",
    "bibtex": "@misc{nallapati2016abstractivetextsummarizationusingsequencetosequence,\n    title = {Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond},\n    author = {Ramesh Nallapati and Bowen Zhou and Cicero Nogueira dos santos and Caglar Gulcehre and Bing Xiang},\n    year = {2016},\n    eprint = {1602.06023},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1602.06023},\n}",
    "abstract": "In this work, we model abstractive text summarization using Attentional\nEncoder-Decoder Recurrent Neural Networks, and show that they achieve\nstate-of-the-art performance on two different corpora. We propose several novel\nmodels that address critical problems in summarization that are not adequately\nmodeled by the basic architecture, such as modeling key-words, capturing the\nhierarchy of sentence-to-word structure, and emitting words that are rare or\nunseen at training time. Our work shows that many of our proposed models\ncontribute to further improvement in performance. We also propose a new dataset\nconsisting of multi-sentence summaries, and establish performance benchmarks\nfor further research.",
    "num_pages": 12
}