{
    "uuid": "884e57aa-78e0-5621-958c-8681a2d6f61f",
    "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Aditya Ramesh",
        "Prafulla Dhariwal",
        "Alex Nichol",
        "Casey Chu",
        "Mark Chen"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.06125v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\884e57aa-78e0-5621-958c-8681a2d6f61f.pdf",
    "bibtex": "@misc{ramesh2022hierarchicaltextconditionalimagegenerationwith,\n    title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},\n    author = {Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},\n    year = {2022},\n    eprint = {2204.06125},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2204.06125},\n}",
    "abstract": "Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.",
    "num_pages": 27
}