{
    "uuid": "63ac5232-d7e8-5f0e-ab4c-f2378c7c2a8b",
    "title": "Complexity-Based Prompting for Multi-Step Reasoning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yao Fu",
        "Hao Peng",
        "Ashish Sabharwal",
        "Peter Clark",
        "Tushar Khot"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.00720v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\63ac5232-d7e8-5f0e-ab4c-f2378c7c2a8b.pdf",
    "bibtex": "@misc{fu2023complexitybasedpromptingformultistepreasoning,\n    title = {Complexity-Based Prompting for Multi-Step Reasoning},\n    author = {Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},\n    year = {2023},\n    eprint = {2210.00720},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.00720},\n}",
    "abstract": "We study the task of prompting large-scale language models to perform\nmulti-step reasoning. Existing work shows that when prompted with a chain of\nthoughts (CoT), sequences of short sentences describing intermediate reasoning\nsteps towards a final answer, large language models can generate new reasoning\nchains and predict answers for new inputs. A central question is which\nreasoning examples make the most effective prompts. In this work, we propose\ncomplexity-based prompting, a simple and effective example selection scheme for\nmulti-step reasoning. We show that prompts with higher reasoning complexity,\ni.e., chains with more reasoning steps, achieve substantially better\nperformance on multi-step reasoning tasks over strong baselines. We further\nextend our complexity-based criteria from prompting (selecting inputs) to\ndecoding (selecting outputs), where we sample multiple reasoning chains from\nthe model, then choose the majority of generated answers from complex reasoning\nchains (over simple chains). When used to prompt GPT-3 and Codex, our approach\nsubstantially improves multi-step reasoning accuracy and achieves new\nstate-of-the-art (SOTA) performance on three math benchmarks (GSM8K,\nMultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and\nPenguins), with an average +5.3 and up to +18 accuracy improvements. Compared\nwith existing example selection schemes like manual tuning or retrieval-based\nselection, selection based on reasoning complexity is intuitive, easy to\nimplement, and annotation-efficient. Further results demonstrate the robustness\nof performance gains from complex prompts under format perturbation and\ndistribution shift.",
    "num_pages": 14
}