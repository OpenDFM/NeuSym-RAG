{
    "uuid": "6a2a3889-79ed-5bfc-8448-331e3e0ff259",
    "title": "Episodic Transformer for Vision-and-Language Navigation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Alexander Pashevich",
        "Cordelia Schmid",
        "Chen Sun"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06453v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\6a2a3889-79ed-5bfc-8448-331e3e0ff259.pdf",
    "bibtex": "@misc{pashevich2021episodictransformerforvisionandlanguagenavigation,\n    title = {Episodic Transformer for Vision-and-Language Navigation},\n    author = {Alexander Pashevich and Cordelia Schmid and Chen Sun},\n    year = {2021},\n    eprint = {2105.06453},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2105.06453},\n}",
    "abstract": "Interaction and navigation defined by natural language instructions in\ndynamic environments pose significant challenges for neural agents. This paper\nfocuses on addressing two challenges: handling long sequence of subtasks, and\nunderstanding complex human instructions. We propose Episodic Transformer\n(E.T.), a multimodal transformer that encodes language inputs and the full\nepisode history of visual observations and actions. To improve training, we\nleverage synthetic instructions as an intermediate representation that\ndecouples understanding the visual appearance of an environment from the\nvariations of natural language instructions. We demonstrate that encoding the\nhistory with a transformer is critical to solve compositional tasks, and that\npretraining and joint training with synthetic instructions further improve the\nperformance. Our approach sets a new state of the art on the challenging ALFRED\nbenchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test\nsplits.",
    "num_pages": 19
}