{
    "uuid": "b9efb3d8-b346-518b-a6b9-d6fe8a8c9a20",
    "title": "Revisiting the Compositional Generalization Abilities of Neural Sequence Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Arkil Patel",
        "Satwik Bhattamishra",
        "Phil Blunsom",
        "Navin Goyal"
    ],
    "pdf_url": "http://arxiv.org/pdf/2203.07402v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\b9efb3d8-b346-518b-a6b9-d6fe8a8c9a20.pdf",
    "bibtex": "@misc{patel2022revisitingthecompositionalgeneralizationabilities,\n    title = {Revisiting the Compositional Generalization Abilities of Neural Sequence Models},\n    author = {Arkil Patel and Satwik Bhattamishra and Phil Blunsom and Navin Goyal},\n    year = {2022},\n    eprint = {2203.07402},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2203.07402},\n}",
    "abstract": "Compositional generalization is a fundamental trait in humans, allowing us to\neffortlessly combine known phrases to form novel sentences. Recent works have\nclaimed that standard seq-to-seq models severely lack the ability to\ncompositionally generalize. In this paper, we focus on one-shot primitive\ngeneralization as introduced by the popular SCAN benchmark. We demonstrate that\nmodifying the training distribution in simple and intuitive ways enables\nstandard seq-to-seq models to achieve near-perfect generalization performance,\nthereby showing that their compositional generalization abilities were\npreviously underestimated. We perform detailed empirical analysis of this\nphenomenon. Our results indicate that the generalization performance of models\nis highly sensitive to the characteristics of the training data which should be\ncarefully considered while designing such benchmarks in future.",
    "num_pages": 11
}