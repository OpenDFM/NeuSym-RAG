{
    "uuid": "c18fb95e-8ca9-5822-bd59-666d91a6554d",
    "title": "Generating Sentences from a Continuous Space",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Samuel R. Bowman",
        "Luke Vilnis",
        "Oriol Vinyals",
        "Andrew M. Dai",
        "Rafal Jozefowicz",
        "Samy Bengio"
    ],
    "pdf_url": "http://arxiv.org/pdf/1511.06349v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\c18fb95e-8ca9-5822-bd59-666d91a6554d.pdf",
    "bibtex": "@misc{bowman2016generatingsentencesfromacontinuous,\n    title = {Generating Sentences from a Continuous Space},\n    author = {Samuel R. Bowman and Luke Vilnis and Oriol Vinyals and Andrew M. Dai and Rafal Jozefowicz and Samy Bengio},\n    year = {2016},\n    eprint = {1511.06349},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1511.06349},\n}",
    "abstract": "The standard recurrent neural network language model (RNNLM) generates\nsentences one word at a time and does not work from an explicit global sentence\nrepresentation. In this work, we introduce and study an RNN-based variational\nautoencoder generative model that incorporates distributed latent\nrepresentations of entire sentences. This factorization allows it to explicitly\nmodel holistic properties of sentences such as style, topic, and high-level\nsyntactic features. Samples from the prior over these sentence representations\nremarkably produce diverse and well-formed sentences through simple\ndeterministic decoding. By examining paths through this latent space, we are\nable to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate its effectiveness in imputing missing\nwords, explore many interesting properties of the model's latent sentence\nspace, and present negative results on the use of the model in language\nmodeling.",
    "num_pages": 12
}