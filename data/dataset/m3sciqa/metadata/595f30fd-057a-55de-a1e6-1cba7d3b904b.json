{
    "uuid": "595f30fd-057a-55de-a1e6-1cba7d3b904b",
    "title": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Anchun Gui",
        "Han Xiao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.04573v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\595f30fd-057a-55de-a1e6-1cba7d3b904b.pdf",
    "bibtex": "@misc{gui2023hifihighinformationattentionheadshold,\n    title = {HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation},\n    author = {Anchun Gui and Han Xiao},\n    year = {2023},\n    eprint = {2305.04573},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.04573},\n}",
    "abstract": "To fully leverage the advantages of large-scale pre-trained language models\n(PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to\nfine-tune the entire parameters of PLMs. However, this paradigm poses issues of\ninefficient updating and resource over-consuming for fine-tuning in data-scarce\nand resource-limited scenarios, because of the large scale of parameters in\nPLMs. To alleviate these concerns, in this paper, we propose a\nparameter-efficient fine-tuning method HiFi, that is, only the highly\ninformative and strongly correlated attention heads for the specific task are\nfine-tuned. To search for those significant attention heads, we develop a novel\nframework to analyze the effectiveness of heads. Specifically, we first model\nthe relationship between heads into a graph from two perspectives of\ninformation richness and correlation, and then apply PageRank algorithm to\ndetermine the relative importance of each head. Extensive experiments on the\nGLUE benchmark demonstrate the effectiveness of our method, and show that HiFi\nobtains state-of-the-art performance over the prior baselines.",
    "num_pages": 15
}