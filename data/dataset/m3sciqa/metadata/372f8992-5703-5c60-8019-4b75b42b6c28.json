{
    "uuid": "372f8992-5703-5c60-8019-4b75b42b6c28",
    "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Jianfeng Wang",
        "Kevin Lin",
        "Ehsan Azarnasab",
        "Faisal Ahmed",
        "Zicheng Liu",
        "Ce Liu",
        "Michael Zeng",
        "Lijuan Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.11381v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\372f8992-5703-5c60-8019-4b75b42b6c28.pdf",
    "bibtex": "@misc{yang2023mmreactpromptingchatgptformultimodal,\n    title = {MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action},\n    author = {Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang},\n    year = {2023},\n    eprint = {2303.11381},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2303.11381},\n}",
    "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/",
    "num_pages": 31
}