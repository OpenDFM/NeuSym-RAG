{
    "uuid": "886d1f58-ef58-5ea0-ab1d-c64e94c61410",
    "title": "Modifying Memories in Transformer Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Chen Zhu",
        "Ankit Singh Rawat",
        "Manzil Zaheer",
        "Srinadh Bhojanapalli",
        "Daliang Li",
        "Felix Yu",
        "Sanjiv Kumar"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.00363v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\886d1f58-ef58-5ea0-ab1d-c64e94c61410.pdf",
    "bibtex": "@misc{zhu2020modifyingmemoriesintransformermodels,\n    title = {Modifying Memories in Transformer Models},\n    author = {Chen Zhu and Ankit Singh Rawat and Manzil Zaheer and Srinadh Bhojanapalli and Daliang Li and Felix Yu and Sanjiv Kumar},\n    year = {2020},\n    eprint = {2012.00363},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.00363},\n}",
    "abstract": "Large Transformer models have achieved impressive performance in many natural\nlanguage tasks. In particular, Transformer based language models have been\nshown to have great capabilities in encoding factual knowledge in their vast\namount of parameters. While the tasks of improving the memorization and\ngeneralization of Transformers have been widely studied, it is not well known\nhow to make transformers forget specific old facts and memorize new ones. In\nthis paper, we propose a new task of \\emph{explicitly modifying specific\nfactual knowledge in Transformer models while ensuring the model performance\ndoes not degrade on the unmodified facts}. This task is useful in many\nscenarios, such as updating stale knowledge, protecting privacy, and\neliminating unintended biases stored in the models. We benchmarked several\napproaches that provide natural baseline performances on this task. This leads\nto the discovery of key components of a Transformer model that are especially\neffective for knowledge modifications. The work also provides insights into the\nrole that different training phases (such as pretraining and fine-tuning) play\ntowards memorization and knowledge modification.",
    "num_pages": 21
}