{
    "uuid": "4837c79a-37bb-5d5c-9e26-57d3a5840ca0",
    "title": "Improving AMR Parsing with Sequence-to-Sequence Pre-training",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Dongqin Xu",
        "Junhui Li",
        "Muhua Zhu",
        "Min Zhang",
        "Guodong Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2010.01771v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\4837c79a-37bb-5d5c-9e26-57d3a5840ca0.pdf",
    "bibtex": "@misc{xu2020improvingamrparsingwithsequencetosequence,\n    title = {Improving AMR Parsing with Sequence-to-Sequence Pre-training},\n    author = {Dongqin Xu and Junhui Li and Muhua Zhu and Min Zhang and Guodong Zhou},\n    year = {2020},\n    eprint = {2010.01771},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2010.01771},\n}",
    "abstract": "In the literature, the research on abstract meaning representation (AMR)\nparsing is much restricted by the size of human-curated dataset which is\ncritical to build an AMR parser with good performance. To alleviate such data\nsize restriction, pre-trained models have been drawing more and more attention\nin AMR parsing. However, previous pre-trained models, like BERT, are\nimplemented for general purpose which may not work as expected for the specific\ntask of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq)\nAMR parsing and propose a seq2seq pre-training approach to build pre-trained\nmodels in both single and joint way on three relevant tasks, i.e., machine\ntranslation, syntactic parsing, and AMR parsing itself. Moreover, we extend the\nvanilla fine-tuning method to a multi-task learning fine-tuning method that\noptimizes for the performance of AMR parsing while endeavors to preserve the\nresponse of pre-trained models. Extensive experimental results on two English\nbenchmark datasets show that both the single and joint pre-trained models\nsignificantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0),\nwhich reaches the state of the art. The result is very encouraging since we\nachieve this with seq2seq models rather than complex models. We make our code\nand model available at https://github.com/xdqkid/S2S-AMR-Parser.",
    "num_pages": 11
}