{
    "uuid": "bf28f877-f161-5569-a7ac-0ff9a8d9f89f",
    "title": "Teaching Small Language Models to Reason",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Lucie Charlotte Magister",
        "Jonathan Mallinson",
        "Jakub Adamek",
        "Eric Malmi",
        "Aliaksei Severyn"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.08410v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\bf28f877-f161-5569-a7ac-0ff9a8d9f89f.pdf",
    "bibtex": "@misc{magister2023teachingsmalllanguagemodelsto,\n    title = {Teaching Small Language Models to Reason},\n    author = {Lucie Charlotte Magister and Jonathan Mallinson and Jakub Adamek and Eric Malmi and Aliaksei Severyn},\n    year = {2023},\n    eprint = {2212.08410},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.08410},\n}",
    "abstract": "Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.",
    "num_pages": 7
}