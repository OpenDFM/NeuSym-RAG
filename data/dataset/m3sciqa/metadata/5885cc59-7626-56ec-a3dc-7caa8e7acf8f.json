{
    "uuid": "5885cc59-7626-56ec-a3dc-7caa8e7acf8f",
    "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jiawei Liu",
        "Chunqiu Steven Xia",
        "Yuyao Wang",
        "Lingming Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.01210v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5885cc59-7626-56ec-a3dc-7caa8e7acf8f.pdf",
    "bibtex": "@misc{liu2023isyourcodegeneratedby,\n    title = {Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},\n    author = {Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},\n    year = {2023},\n    eprint = {2305.01210},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.SE},\n    url = {http://arxiv.org/abs/2305.01210},\n}",
    "abstract": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.",
    "num_pages": 15
}