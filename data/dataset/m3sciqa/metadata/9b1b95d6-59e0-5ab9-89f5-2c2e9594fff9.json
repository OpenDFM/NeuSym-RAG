{
    "uuid": "9b1b95d6-59e0-5ab9-89f5-2c2e9594fff9",
    "title": "An Unsupervised Sentence Embedding Method by Mutual Information Maximization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yan Zhang",
        "Ruidan He",
        "Zuozhu Liu",
        "Kwan Hui Lim",
        "Lidong Bing"
    ],
    "pdf_url": "http://arxiv.org/pdf/2009.12061v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\9b1b95d6-59e0-5ab9-89f5-2c2e9594fff9.pdf",
    "bibtex": "@misc{zhang2021anunsupervisedsentenceembeddingmethod,\n    title = {An Unsupervised Sentence Embedding Method by Mutual Information Maximization},\n    author = {Yan Zhang and Ruidan He and Zuozhu Liu and Kwan Hui Lim and Lidong Bing},\n    year = {2021},\n    eprint = {2009.12061},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2009.12061},\n}",
    "abstract": "BERT is inefficient for sentence-pair tasks such as clustering or semantic\nsearch as it needs to evaluate combinatorially many sentence pairs which is\nvery time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by\nlearning semantically meaningful representations of single sentences, such that\nsimilarity comparison can be easily accessed. However, SBERT is trained on\ncorpus with high-quality labeled sentence pairs, which limits its application\nto tasks where labeled data is extremely scarce. In this paper, we propose a\nlightweight extension on top of BERT and a novel self-supervised learning\nobjective based on mutual information maximization strategies to derive\nmeaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our\nmethod is not restricted by the availability of labeled data, such that it can\nbe applied on different domain-specific corpus. Experimental results show that\nthe proposed method significantly outperforms other unsupervised sentence\nembedding baselines on common semantic textual similarity (STS) tasks and\ndownstream supervised tasks. It also outperforms SBERT in a setting where\nin-domain labeled data is not available, and achieves performance competitive\nwith supervised methods on various tasks.",
    "num_pages": 10
}