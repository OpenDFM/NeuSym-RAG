{
    "uuid": "af424bb3-4227-5815-9e9b-ebbc9e9b961b",
    "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Md Tahmid Rahman Laskar",
        "M Saiful Bari",
        "Mizanur Rahman",
        "Md Amran Hossen Bhuiyan",
        "Shafiq Joty",
        "Jimmy Xiangji Huang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.18486v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\af424bb3-4227-5815-9e9b-ebbc9e9b961b.pdf",
    "bibtex": "@misc{laskar2023asystematicstudyandcomprehensive,\n    title = {A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets},\n    author = {Md Tahmid Rahman Laskar and M Saiful Bari and Mizanur Rahman and Md Amran Hossen Bhuiyan and Shafiq Joty and Jimmy Xiangji Huang},\n    year = {2023},\n    eprint = {2305.18486},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.18486},\n}",
    "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.",
    "num_pages": 37
}