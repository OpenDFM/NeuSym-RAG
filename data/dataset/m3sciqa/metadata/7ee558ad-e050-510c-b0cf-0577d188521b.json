{
    "uuid": "7ee558ad-e050-510c-b0cf-0577d188521b",
    "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Junxian He",
        "Chunting Zhou",
        "Xuezhe Ma",
        "Taylor Berg-Kirkpatrick",
        "Graham Neubig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04366v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\7ee558ad-e050-510c-b0cf-0577d188521b.pdf",
    "bibtex": "@misc{he2022towardsaunifiedviewof,\n    title = {Towards a Unified View of Parameter-Efficient Transfer Learning},\n    author = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},\n    year = {2022},\n    eprint = {2110.04366},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.04366},\n}",
    "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.",
    "num_pages": 15
}