{
    "uuid": "d4de27d2-08b9-57ae-b0a7-a4b9b26047bf",
    "title": "Do Language Models Know When They're Hallucinating References?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2024,
    "authors": [
        "Ayush Agrawal",
        "Mirac Suzgun",
        "Lester Mackey",
        "Adam Tauman Kalai"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.18248v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2024\\d4de27d2-08b9-57ae-b0a7-a4b9b26047bf.pdf",
    "bibtex": "@misc{agrawal2024dolanguagemodelsknowwhen,\n    title = {Do Language Models Know When They're Hallucinating References?},\n    author = {Ayush Agrawal and Mirac Suzgun and Lester Mackey and Adam Tauman Kalai},\n    year = {2024},\n    eprint = {2305.18248},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.18248},\n}",
    "abstract": "State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.",
    "num_pages": 17
}