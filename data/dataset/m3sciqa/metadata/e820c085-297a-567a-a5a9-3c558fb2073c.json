{
    "uuid": "e820c085-297a-567a-a5a9-3c558fb2073c",
    "title": "DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ella Neeman",
        "Roee Aharoni",
        "Or Honovich",
        "Leshem Choshen",
        "Idan Szpektor",
        "Omri Abend"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.05655v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\e820c085-297a-567a-a5a9-3c558fb2073c.pdf",
    "bibtex": "@misc{neeman2022disentqadisentanglingparametricandcontextual,\n    title = {DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering},\n    author = {Ella Neeman and Roee Aharoni and Or Honovich and Leshem Choshen and Idan Szpektor and Omri Abend},\n    year = {2022},\n    eprint = {2211.05655},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.05655},\n}",
    "abstract": "Question answering models commonly have access to two sources of \"knowledge\"\nduring inference time: (1) parametric knowledge - the factual knowledge encoded\nin the model weights, and (2) contextual knowledge - external knowledge (e.g.,\na Wikipedia passage) given to the model to generate a grounded answer. Having\nthese two sources of knowledge entangled together is a core issue for\ngenerative QA models as it is unclear whether the answer stems from the given\nnon-parametric knowledge or not. This unclarity has implications on issues of\ntrust, interpretability and factuality. In this work, we propose a new paradigm\nin which QA models are trained to disentangle the two sources of knowledge.\nUsing counterfactual data augmentation, we introduce a model that predicts two\nanswers for a given question: one based on given contextual knowledge and one\nbased on parametric knowledge. Our experiments on the Natural Questions dataset\nshow that this approach improves the performance of QA models by making them\nmore robust to knowledge conflicts between the two knowledge sources, while\ngenerating useful disentangled answers.",
    "num_pages": 12
}