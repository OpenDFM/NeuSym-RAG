{
    "uuid": "4f82b0a7-782b-5d83-abfc-146affe81aea",
    "title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Ioannis Konstas",
        "Srinivasan Iyer",
        "Mark Yatskar",
        "Yejin Choi",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "http://arxiv.org/pdf/1704.08381v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\4f82b0a7-782b-5d83-abfc-146affe81aea.pdf",
    "bibtex": "@misc{konstas2017neuralamrsequencetosequencemodelsfor,\n    title = {Neural AMR: Sequence-to-Sequence Models for Parsing and Generation},\n    author = {Ioannis Konstas and Srinivasan Iyer and Mark Yatskar and Yejin Choi and Luke Zettlemoyer},\n    year = {2017},\n    eprint = {1704.08381},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1704.08381},\n}",
    "abstract": "Sequence-to-sequence models have shown strong performance across a broad\nrange of applications. However, their application to parsing and generating\ntext usingAbstract Meaning Representation (AMR)has been limited, due to the\nrelatively limited amount of labeled data and the non-sequential nature of the\nAMR graphs. We present a novel training procedure that can lift this limitation\nusing millions of unlabeled sentences and careful preprocessing of the AMR\ngraphs. For AMR parsing, our model achieves competitive results of 62.1SMATCH,\nthe current best score reported without significant use of external semantic\nresources. For AMR generation, our model establishes a new state-of-the-art\nperformance of BLEU 33.8. We present extensive ablative and qualitative\nanalysis including strong evidence that sequence-based AMR models are robust\nagainst ordering variations of graph-to-sequence conversions.",
    "num_pages": 11
}