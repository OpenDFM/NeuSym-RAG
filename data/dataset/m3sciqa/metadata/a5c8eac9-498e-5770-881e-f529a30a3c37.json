{
    "uuid": "a5c8eac9-498e-5770-881e-f529a30a3c37",
    "title": "XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yaobo Liang",
        "Nan Duan",
        "Yeyun Gong",
        "Ning Wu",
        "Fenfei Guo",
        "Weizhen Qi",
        "Ming Gong",
        "Linjun Shou",
        "Daxin Jiang",
        "Guihong Cao",
        "Xiaodong Fan",
        "Ruofei Zhang",
        "Rahul Agrawal",
        "Edward Cui",
        "Sining Wei",
        "Taroon Bharti",
        "Ying Qiao",
        "Jiun-Hung Chen",
        "Winnie Wu",
        "Shuguang Liu",
        "Fan Yang",
        "Daniel Campos",
        "Rangan Majumder",
        "Ming Zhou"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.01401v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\a5c8eac9-498e-5770-881e-f529a30a3c37.pdf",
    "bibtex": "@misc{liang2020xglueanewbenchmarkdataset,\n    title = {XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},\n    author = {Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},\n    year = {2020},\n    eprint = {2004.01401},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2004.01401},\n}",
    "abstract": "In this paper, we introduce XGLUE, a new benchmark dataset that can be used\nto train large-scale cross-lingual pre-trained models using multilingual and\nbilingual corpora and evaluate their performance across a diverse set of\ncross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in\nEnglish for natural language understanding tasks only, XGLUE has two main\nadvantages: (1) it provides 11 diversified tasks that cover both natural\nlanguage understanding and generation scenarios; (2) for each task, it provides\nlabeled data in multiple languages. We extend a recent cross-lingual\npre-trained model Unicoder(Huang et al., 2019) to cover both understanding and\ngeneration tasks, which is evaluated on XGLUE as a strong baseline. We also\nevaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for\ncomparison.",
    "num_pages": 11
}