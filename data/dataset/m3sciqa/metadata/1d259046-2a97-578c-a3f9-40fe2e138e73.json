{
    "uuid": "1d259046-2a97-578c-a3f9-40fe2e138e73",
    "title": "Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Marcelo O. R. Prates",
        "Pedro H. C. Avelar",
        "Luis Lamb"
    ],
    "pdf_url": "http://arxiv.org/pdf/1809.02208v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\1d259046-2a97-578c-a3f9-40fe2e138e73.pdf",
    "bibtex": "@misc{prates2019assessinggenderbiasinmachine,\n    title = {Assessing Gender Bias in Machine Translation -- A Case Study with Google Translate},\n    author = {Marcelo O. R. Prates and Pedro H. C. Avelar and Luis Lamb},\n    year = {2019},\n    eprint = {1809.02208},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CY},\n    url = {http://arxiv.org/abs/1809.02208},\n}",
    "abstract": "Recently there has been a growing concern about machine bias, where trained\nstatistical models grow to reflect controversial societal asymmetries, such as\ngender or racial bias. A significant number of AI tools have recently been\nsuggested to be harmfully biased towards some minority, with reports of racist\ncriminal behavior predictors, Iphone X failing to differentiate between two\nAsian people and Google photos' mistakenly classifying black people as\ngorillas. Although a systematic study of such biases can be difficult, we\nbelieve that automated translation tools can be exploited through gender\nneutral languages to yield a window into the phenomenon of gender bias in AI.\n  In this paper, we start with a comprehensive list of job positions from the\nU.S. Bureau of Labor Statistics (BLS) and used it to build sentences in\nconstructions like \"He/She is an Engineer\" in 12 different gender neutral\nlanguages such as Hungarian, Chinese, Yoruba, and several others. We translate\nthese sentences into English using the Google Translate API, and collect\nstatistics about the frequency of female, male and gender-neutral pronouns in\nthe translated output. We show that GT exhibits a strong tendency towards male\ndefaults, in particular for fields linked to unbalanced gender distribution\nsuch as STEM jobs. We ran these statistics against BLS' data for the frequency\nof female participation in each job position, showing that GT fails to\nreproduce a real-world distribution of female workers. We provide experimental\nevidence that even if one does not expect in principle a 50:50 pronominal\ngender distribution, GT yields male defaults much more frequently than what\nwould be expected from demographic data alone.\n  We are hopeful that this work will ignite a debate about the need to augment\ncurrent statistical translation tools with debiasing techniques which can\nalready be found in the scientific literature.",
    "num_pages": 33
}