{
    "uuid": "9ada7bff-c684-55ab-ae9b-04f836247ddc",
    "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2016,
    "authors": [
        "Pranav Rajpurkar",
        "Jian Zhang",
        "Konstantin Lopyrev",
        "Percy Liang"
    ],
    "pdf_url": "http://arxiv.org/pdf/1606.05250v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2016\\9ada7bff-c684-55ab-ae9b-04f836247ddc.pdf",
    "bibtex": "@misc{rajpurkar2016squad100000questionsformachine,\n    title = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},\n    author = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},\n    year = {2016},\n    eprint = {1606.05250},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1606.05250},\n}",
    "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading\ncomprehension dataset consisting of 100,000+ questions posed by crowdworkers on\na set of Wikipedia articles, where the answer to each question is a segment of\ntext from the corresponding reading passage. We analyze the dataset to\nunderstand the types of reasoning required to answer the questions, leaning\nheavily on dependency and constituency trees. We build a strong logistic\nregression model, which achieves an F1 score of 51.0%, a significant\nimprovement over a simple baseline (20%). However, human performance (86.8%) is\nmuch higher, indicating that the dataset presents a good challenge problem for\nfuture research.\n  The dataset is freely available at https://stanford-qa.com",
    "num_pages": 10
}