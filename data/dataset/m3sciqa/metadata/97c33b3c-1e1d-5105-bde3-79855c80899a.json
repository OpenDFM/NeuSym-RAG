{
    "uuid": "97c33b3c-1e1d-5105-bde3-79855c80899a",
    "title": "TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Fan Zhou",
        "Mengkang Hu",
        "Haoyu Dong",
        "Zhoujun Cheng",
        "Shi Han",
        "Dongmei Zhang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.12682v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\97c33b3c-1e1d-5105-bde3-79855c80899a.pdf",
    "bibtex": "@misc{zhou2022tacubeprecomputingdatacubesfor,\n    title = {TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data},\n    author = {Fan Zhou and Mengkang Hu and Haoyu Dong and Zhoujun Cheng and Shi Han and Dongmei Zhang},\n    year = {2022},\n    eprint = {2205.12682},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.IR},\n    url = {http://arxiv.org/abs/2205.12682},\n}",
    "abstract": "Existing auto-regressive pre-trained language models (PLMs) like T5 and BART,\nhave been well applied to table question answering by UNIFIEDSKG and TAPEX,\nrespectively, and demonstrated state-of-the-art results on multiple benchmarks.\nHowever, auto-regressive PLMs are challenged by recent emerging numerical\nreasoning datasets, such as TAT-QA, due to the error-prone implicit\ncalculation. In this paper, we present TaCube, to pre-compute\naggregation/arithmetic results for the table in advance, so that they are handy\nand readily available for PLMs to answer numerical reasoning questions. TaCube\nsystematically and comprehensively covers a collection of computational\noperations over table segments. By simply concatenating TaCube to the input\nsequence of PLMs, it shows significant experimental effectiveness. TaCube\npromotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new\nstate-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube's\nimprovements on numerical reasoning cases are even more notable: on TAT-QA,\nTaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5%\non average, 36.6% on substraction, and 22.2% on division. We believe that\nTaCube is a general and portable pre-computation solution that can be\npotentially integrated to various numerical reasoning frameworks",
    "num_pages": 13
}