{
    "uuid": "8f35bc2f-2d51-5e00-8fe2-6a44259cfa7c",
    "title": "Hidden Markov Transformer for Simultaneous Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Shaolei Zhang",
        "Yang Feng"
    ],
    "pdf_url": "http://arxiv.org/pdf/2303.00257v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\8f35bc2f-2d51-5e00-8fe2-6a44259cfa7c.pdf",
    "bibtex": "@misc{zhang2023hiddenmarkovtransformerforsimultaneous,\n    title = {Hidden Markov Transformer for Simultaneous Machine Translation},\n    author = {Shaolei Zhang and Yang Feng},\n    year = {2023},\n    eprint = {2303.00257},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2303.00257},\n}",
    "abstract": "Simultaneous machine translation (SiMT) outputs the target sequence while\nreceiving the source sequence, and hence learning when to start translating\neach target token is the core challenge for SiMT task. However, it is\nnon-trivial to learn the optimal moment among many possible moments of starting\ntranslating, as the moments of starting translating always hide inside the\nmodel and can only be supervised with the observed target sequence. In this\npaper, we propose a Hidden Markov Transformer (HMT), which treats the moments\nof starting translating as hidden events and the target sequence as the\ncorresponding observed events, thereby organizing them as a hidden Markov\nmodel. HMT explicitly models multiple moments of starting translating as the\ncandidate hidden events, and then selects one to generate the target token.\nDuring training, by maximizing the marginal likelihood of the target sequence\nover multiple moments of starting translating, HMT learns to start translating\nat the moments that target tokens can be generated more accurately. Experiments\non multiple SiMT benchmarks show that HMT outperforms strong baselines and\nachieves state-of-the-art performance.",
    "num_pages": 22
}