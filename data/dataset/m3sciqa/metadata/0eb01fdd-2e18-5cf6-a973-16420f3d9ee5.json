{
    "uuid": "0eb01fdd-2e18-5cf6-a973-16420f3d9ee5",
    "title": "RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yizhe Zhang",
        "Siqi Sun",
        "Xiang Gao",
        "Yuwei Fang",
        "Chris Brockett",
        "Michel Galley",
        "Jianfeng Gao",
        "Bill Dolan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2105.06597v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\0eb01fdd-2e18-5cf6-a973-16420f3d9ee5.pdf",
    "bibtex": "@misc{zhang2022retgenajointframeworkfor,\n    title = {RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling},\n    author = {Yizhe Zhang and Siqi Sun and Xiang Gao and Yuwei Fang and Chris Brockett and Michel Galley and Jianfeng Gao and Bill Dolan},\n    year = {2022},\n    eprint = {2105.06597},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2105.06597},\n}",
    "abstract": "Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where information-relevant documents are\nprovided for context. We propose a framework that alleviates this data\nconstraint by jointly training a grounded generator and document retriever on\nthe language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.",
    "num_pages": 15
}