{
    "uuid": "f92811a6-b9ce-519e-9346-f3b892d7f535",
    "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Hyunwoo Kim",
        "Jack Hessel",
        "Liwei Jiang",
        "Peter West",
        "Ximing Lu",
        "Youngjae Yu",
        "Pei Zhou",
        "Ronan Le Bras",
        "Malihe Alikhani",
        "Gunhee Kim",
        "Maarten Sap",
        "Yejin Choi"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.10465v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\f92811a6-b9ce-519e-9346-f3b892d7f535.pdf",
    "bibtex": "@misc{kim2023sodamillionscaledialoguedistillationwith,\n    title = {SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization},\n    author = {Hyunwoo Kim and Jack Hessel and Liwei Jiang and Peter West and Ximing Lu and Youngjae Yu and Pei Zhou and Ronan Le Bras and Malihe Alikhani and Gunhee Kim and Maarten Sap and Yejin Choi},\n    year = {2023},\n    eprint = {2212.10465},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.10465},\n}",
    "abstract": "Data scarcity has been a long standing issue in the field of open-domain\nsocial dialogue. To quench this thirst, we present SODA: the first publicly\navailable, million-scale high-quality social dialogue dataset. By\ncontextualizing social commonsense knowledge from a knowledge graph, we are\nable to distill an exceptionally broad spectrum of social interactions from a\nlarge language model. Human evaluation shows that conversations in SODA are\nmore consistent, specific, and (surprisingly) natural than those in prior\nhuman-authored datasets.\n  Using SODA, we train COSMO: a generalizable conversation model that is\nsignificantly more natural and consistent on unseen datasets than\nbest-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).\nExperiments reveal COSMO is sometimes even preferred to the original\nhuman-written gold responses. Additionally, our results shed light on the\ndistinction between knowledge-enriched conversations and natural social\nchitchats. We plan to make our data, model, and code public.",
    "num_pages": 20
}