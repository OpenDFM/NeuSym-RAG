{
    "uuid": "d1586d70-b85f-506c-bf13-16816c8debdc",
    "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Arman Cohan",
        "Franck Dernoncourt",
        "Doo Soon Kim",
        "Trung Bui",
        "Seokhwan Kim",
        "Walter Chang",
        "Nazli Goharian"
    ],
    "pdf_url": "http://arxiv.org/pdf/1804.05685v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\d1586d70-b85f-506c-bf13-16816c8debdc.pdf",
    "bibtex": "@misc{cohan2018adiscourseawareattentionmodelfor,\n    title = {A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents},\n    author = {Arman Cohan and Franck Dernoncourt and Doo Soon Kim and Trung Bui and Seokhwan Kim and Walter Chang and Nazli Goharian},\n    year = {2018},\n    eprint = {1804.05685},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1804.05685},\n}",
    "abstract": "Neural abstractive summarization models have led to promising results in\nsummarizing relatively short documents. We propose the first model for\nabstractive summarization of single, longer-form documents (e.g., research\npapers). Our approach consists of a new hierarchical encoder that models the\ndiscourse structure of a document, and an attentive discourse-aware decoder to\ngenerate the summary. Empirical results on two large-scale datasets of\nscientific papers show that our model significantly outperforms\nstate-of-the-art models.",
    "num_pages": 7
}