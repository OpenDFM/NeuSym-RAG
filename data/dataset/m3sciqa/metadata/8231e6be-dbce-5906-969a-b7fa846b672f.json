{
    "uuid": "8231e6be-dbce-5906-969a-b7fa846b672f",
    "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Nan Du",
        "Yanping Huang",
        "Andrew M. Dai",
        "Simon Tong",
        "Dmitry Lepikhin",
        "Yuanzhong Xu",
        "Maxim Krikun",
        "Yanqi Zhou",
        "Adams Wei Yu",
        "Orhan Firat",
        "Barret Zoph",
        "Liam Fedus",
        "Maarten Bosma",
        "Zongwei Zhou",
        "Tao Wang",
        "Yu Emma Wang",
        "Kellie Webster",
        "Marie Pellat",
        "Kevin Robinson",
        "Kathleen Meier-Hellstern",
        "Toju Duke",
        "Lucas Dixon",
        "Kun Zhang",
        "Quoc V Le",
        "Yonghui Wu",
        "Zhifeng Chen",
        "Claire Cui"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.06905v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\8231e6be-dbce-5906-969a-b7fa846b672f.pdf",
    "bibtex": "@misc{du2022glamefficientscalingoflanguage,\n    title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},\n    author = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},\n    year = {2022},\n    eprint = {2112.06905},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2112.06905},\n}",
    "abstract": "Scaling language models with more data, compute and parameters has driven\nsignificant progress in natural language processing. For example, thanks to\nscaling, GPT-3 was able to achieve strong results on in-context learning tasks.\nHowever, training these large dense models requires significant amounts of\ncomputing resources. In this paper, we propose and develop a family of language\nmodels named GLaM (Generalist Language Model), which uses a sparsely activated\nmixture-of-experts architecture to scale the model capacity while also\nincurring substantially less training cost compared to dense variants. The\nlargest GLaM has 1.2 trillion parameters, which is approximately 7x larger than\nGPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half\nof the computation flops for inference, while still achieving better overall\nzero-shot and one-shot performance across 29 NLP tasks.",
    "num_pages": 23
}