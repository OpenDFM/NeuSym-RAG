{
    "uuid": "df2afc9e-2480-567e-84f6-bf4fb97fc1f5",
    "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Afra Feyza Aky端rek",
        "Ekin Aky端rek",
        "Aman Madaan",
        "Ashwin Kalyan",
        "Peter Clark",
        "Derry Wijaya",
        "Niket Tandon"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.08844v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\df2afc9e-2480-567e-84f6-bf4fb97fc1f5.pdf",
    "bibtex": "@misc{akyrek2023rl4fgeneratingnaturallanguagefeedback,\n    title = {RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs},\n    author = {Afra Feyza Aky端rek and Ekin Aky端rek and Aman Madaan and Ashwin Kalyan and Peter Clark and Derry Wijaya and Niket Tandon},\n    year = {2023},\n    eprint = {2305.08844},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.08844},\n}",
    "abstract": "Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.",
    "num_pages": 16
}