{
    "uuid": "21f40b37-516d-5ea9-9d17-1562757a5445",
    "title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Zeming Chen",
        "Gail Weiss",
        "Eric Mitchell",
        "Asli Celikyilmaz",
        "Antoine Bosselut"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.06349v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\21f40b37-516d-5ea9-9d17-1562757a5445.pdf",
    "bibtex": "@misc{chen2023reckoningreasoningthroughdynamicknowledge,\n    title = {RECKONING: Reasoning through Dynamic Knowledge Encoding},\n    author = {Zeming Chen and Gail Weiss and Eric Mitchell and Asli Celikyilmaz and Antoine Bosselut},\n    year = {2023},\n    eprint = {2305.06349},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.06349},\n}",
    "abstract": "Recent studies on transformer-based language models show that they can answer\nquestions by reasoning over knowledge provided as part of the context (i.e.,\nin-context reasoning). However, since the available knowledge is often not\nfiltered for a particular question, in-context reasoning can be sensitive to\ndistractor facts, additional content that is irrelevant to a question but that\nmay be relevant for a different question (i.e., not necessarily random noise).\nIn these situations, the model fails to distinguish the knowledge that is\nnecessary to answer the question, leading to spurious reasoning and degraded\nperformance. This reasoning failure contrasts with the model's apparent ability\nto distinguish its contextual knowledge from all the knowledge it has memorized\nduring pre-training. Following this observation, we propose teaching the model\nto reason more robustly by folding the provided contextual knowledge into the\nmodel's parameters before presenting it with a question. Our method, RECKONING,\nis a bi-level learning algorithm that teaches language models to reason by\nupdating their parametric knowledge through back-propagation, allowing them to\nthen answer questions using the updated parameters. During training, the inner\nloop rapidly adapts a copy of the model weights to encode contextual knowledge\ninto its parameters. In the outer loop, the model learns to use the updated\nweights to reproduce and answer reasoning questions about the memorized\nknowledge. Our experiments on two multi-hop reasoning datasets show that\nRECKONING's performance improves over the in-context reasoning baseline (by up\nto 4.5%). We also find that compared to in-context reasoning, RECKONING\ngeneralizes better to longer reasoning chains unseen during training, is more\nrobust to distractors in the context, and is more computationally efficient\nwhen multiple questions are asked about the same knowledge.",
    "num_pages": 22
}