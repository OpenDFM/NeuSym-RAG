{
    "uuid": "3e23a677-9f88-5a36-a751-7eaaca56c05f",
    "title": "Text Summarization with Pretrained Encoders",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Yang Liu",
        "Mirella Lapata"
    ],
    "pdf_url": "http://arxiv.org/pdf/1908.08345v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\3e23a677-9f88-5a36-a751-7eaaca56c05f.pdf",
    "bibtex": "@misc{liu2019textsummarizationwithpretrainedencoders,\n    title = {Text Summarization with Pretrained Encoders},\n    author = {Yang Liu and Mirella Lapata},\n    year = {2019},\n    eprint = {1908.08345},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1908.08345},\n}",
    "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the\nlatest incarnation of pretrained language models which have recently advanced a\nwide range of natural language processing tasks. In this paper, we showcase how\nBERT can be usefully applied in text summarization and propose a general\nframework for both extractive and abstractive models. We introduce a novel\ndocument-level encoder based on BERT which is able to express the semantics of\na document and obtain representations for its sentences. Our extractive model\nis built on top of this encoder by stacking several inter-sentence Transformer\nlayers. For abstractive summarization, we propose a new fine-tuning schedule\nwhich adopts different optimizers for the encoder and the decoder as a means of\nalleviating the mismatch between the two (the former is pretrained while the\nlatter is not). We also demonstrate that a two-staged fine-tuning approach can\nfurther boost the quality of the generated summaries. Experiments on three\ndatasets show that our model achieves state-of-the-art results across the board\nin both extractive and abstractive settings. Our code is available at\nhttps://github.com/nlpyang/PreSumm",
    "num_pages": 11
}