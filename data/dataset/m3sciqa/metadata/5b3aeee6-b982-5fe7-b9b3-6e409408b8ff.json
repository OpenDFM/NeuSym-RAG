{
    "uuid": "5b3aeee6-b982-5fe7-b9b3-6e409408b8ff",
    "title": "Incorporating Graph Information in Transformer-based AMR Parsing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Pavlo Vasylenko",
        "Pere-Lluís Huguet Cabot",
        "Abelardo Carlos Martínez Lorenzo",
        "Roberto Navigli"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.13467v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5b3aeee6-b982-5fe7-b9b3-6e409408b8ff.pdf",
    "bibtex": "@misc{vasylenko2023incorporatinggraphinformationintransformerbased,\n    title = {Incorporating Graph Information in Transformer-based AMR Parsing},\n    author = {Pavlo Vasylenko and Pere-Lluís Huguet Cabot and Abelardo Carlos Martínez Lorenzo and Roberto Navigli},\n    year = {2023},\n    eprint = {2306.13467},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.13467},\n}",
    "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that\naims at providing a semantic graph abstraction representing a given text.\nCurrent approaches are based on autoregressive language models such as BART or\nT5, fine-tuned through Teacher Forcing to obtain a linearized version of the\nAMR graph from a sentence. In this paper, we present LeakDistill, a model and\nmethod that explores a modification to the Transformer architecture, using\nstructural adapters to explicitly incorporate graph information into the\nlearned representations and improve AMR parsing performance. Our experiments\nshow how, by employing word-to-node alignment to embed graph structural\ninformation into the encoder at training time, we can obtain state-of-the-art\nAMR parsing through self-knowledge distillation, even without the use of\nadditional data. We release the code at\n\\url{http://www.github.com/sapienzanlp/LeakDistill}.",
    "num_pages": 15
}