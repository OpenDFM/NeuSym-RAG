{
    "uuid": "a2fb00d6-b601-5198-9a52-531df4454399",
    "title": "Auto-Encoding Variational Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Bryan Eikema",
        "Wilker Aziz"
    ],
    "pdf_url": "http://arxiv.org/pdf/1807.10564v4",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\a2fb00d6-b601-5198-9a52-531df4454399.pdf",
    "bibtex": "@misc{eikema2019autoencodingvariationalneuralmachinetranslation,\n    title = {Auto-Encoding Variational Neural Machine Translation},\n    author = {Bryan Eikema and Wilker Aziz},\n    year = {2019},\n    eprint = {1807.10564},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1807.10564},\n}",
    "abstract": "We present a deep generative model of bilingual sentence pairs for machine\ntranslation. The model generates source and target sentences jointly from a\nshared latent representation and is parameterised by neural networks. We\nperform efficient training using amortised variational inference and\nreparameterised gradients. Additionally, we discuss the statistical\nimplications of joint modelling and propose an efficient approximation to\nmaximum a posteriori decoding for fast test-time predictions. We demonstrate\nthe effectiveness of our model in three machine translation scenarios:\nin-domain training, mixed-domain training, and learning from a mix of\ngold-standard and synthetic data. Our experiments show consistently that our\njoint formulation outperforms conditional modelling (i.e. standard neural\nmachine translation) in all such scenarios.",
    "num_pages": 18
}