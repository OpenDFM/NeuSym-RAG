{
    "uuid": "0f9c5f89-7c6f-5bd2-bd90-727a9980a494",
    "title": "Zero-Shot Learning Through Cross-Modal Transfer",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2013,
    "authors": [
        "Richard Socher",
        "Milind Ganjoo",
        "Hamsa Sridhar",
        "Osbert Bastani",
        "Christopher D. Manning",
        "Andrew Y. Ng"
    ],
    "pdf_url": "http://arxiv.org/pdf/1301.3666v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2013\\0f9c5f89-7c6f-5bd2-bd90-727a9980a494.pdf",
    "bibtex": "@misc{socher2013zeroshotlearningthroughcrossmodaltransfer,\n    title = {Zero-Shot Learning Through Cross-Modal Transfer},\n    author = {Richard Socher and Milind Ganjoo and Hamsa Sridhar and Osbert Bastani and Christopher D. Manning and Andrew Y. Ng},\n    year = {2013},\n    eprint = {1301.3666},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1301.3666},\n}",
    "abstract": "This work introduces a model that can recognize objects in images even if no\ntraining data is available for the objects. The only necessary knowledge about\nthe unseen categories comes from unsupervised large text corpora. In our\nzero-shot framework distributional information in language can be seen as\nspanning a semantic basis for understanding what objects look like. Most\nprevious zero-shot learning models can only differentiate between unseen\nclasses. In contrast, our model can both obtain state of the art performance on\nclasses that have thousands of training images and obtain reasonable\nperformance on unseen classes. This is achieved by first using outlier\ndetection in the semantic space and then two separate recognition models.\nFurthermore, our model does not require any manually defined semantic features\nfor either words or images.",
    "num_pages": 7
}