{
    "uuid": "cb764bcf-9005-5541-8cf0-55e47ee0ff91",
    "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Thibault Formal",
        "Benjamin Piwowarski",
        "Stéphane Clinchant"
    ],
    "pdf_url": "http://arxiv.org/pdf/2107.05720v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\cb764bcf-9005-5541-8cf0-55e47ee0ff91.pdf",
    "bibtex": "@misc{formal2021spladesparselexicalandexpansion,\n    title = {SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking},\n    author = {Thibault Formal and Benjamin Piwowarski and Stéphane Clinchant},\n    year = {2021},\n    eprint = {2107.05720},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.IR},\n    url = {http://arxiv.org/abs/2107.05720},\n}",
    "abstract": "In neural Information Retrieval, ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\nsparse representations for documents and queries, that could inherit from the\ndesirable properties of bag-of-words models such as the exact matching of terms\nand the efficiency of inverted indexes. In this work, we present a new\nfirst-stage ranker based on explicit sparsity regularization and a\nlog-saturation effect on term weights, leading to highly sparse representations\nand competitive results with respect to state-of-the-art dense and sparse\nmethods. Our approach is simple, trained end-to-end in a single stage. We also\nexplore the trade-off between effectiveness and efficiency, by controlling the\ncontribution of the sparsity regularization.",
    "num_pages": 5
}