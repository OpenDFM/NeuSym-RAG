{
    "uuid": "9c9e036b-576a-5a70-923f-4d33da527760",
    "title": "Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Bingzhi Li",
        "Guillaume Wisniewski",
        "Benoît Crabbé"
    ],
    "pdf_url": "http://arxiv.org/pdf/2212.04523v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\9c9e036b-576a-5a70-923f-4d33da527760.pdf",
    "bibtex": "@misc{li2023assessingthecapacityoftransformer,\n    title = {Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement},\n    author = {Bingzhi Li and Guillaume Wisniewski and Benoît Crabbé},\n    year = {2023},\n    eprint = {2212.04523},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2212.04523},\n}",
    "abstract": "The long-distance agreement, evidence for syntactic structure, is\nincreasingly used to assess the syntactic generalization of Neural Language\nModels. Much work has shown that transformers are capable of high accuracy in\nvaried agreement tasks, but the mechanisms by which the models accomplish this\nbehavior are still not well understood. To better understand transformers'\ninternal working, this work contrasts how they handle two superficially similar\nbut theoretically distinct agreement phenomena: subject-verb and object-past\nparticiple agreement in French. Using probing and counterfactual analysis\nmethods, our experiments show that i) the agreement task suffers from several\nconfounders which partially question the conclusions drawn so far and ii)\ntransformers handle subject-verb and object-past participle agreements in a way\nthat is consistent with their modeling in theoretical linguistics.",
    "num_pages": 15
}