{
    "uuid": "de0470bf-d650-5f5c-9fb8-926b7ed2c806",
    "title": "Meta-Learning Representations for Continual Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Khurram Javed",
        "Martha White"
    ],
    "pdf_url": "http://arxiv.org/pdf/1905.12588v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\de0470bf-d650-5f5c-9fb8-926b7ed2c806.pdf",
    "bibtex": "@misc{javed2019metalearningrepresentationsforcontinuallearning,\n    title = {Meta-Learning Representations for Continual Learning},\n    author = {Khurram Javed and Martha White},\n    year = {2019},\n    eprint = {1905.12588},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1905.12588},\n}",
    "abstract": "A continual learning agent should be able to build on top of existing\nknowledge to learn on new data quickly while minimizing forgetting. Current\nintelligent systems based on neural network function approximators arguably do\nthe opposite---they are highly prone to forgetting and rarely trained to\nfacilitate future learning. One reason for this poor behavior is that they\nlearn from a representation that is not explicitly trained for these two goals.\nIn this paper, we propose OML, an objective that directly minimizes\ncatastrophic interference by learning representations that accelerate future\nlearning and are robust to forgetting under online updates in continual\nlearning. We show that it is possible to learn naturally sparse representations\nthat are more effective for online updating. Moreover, our algorithm is\ncomplementary to existing continual learning strategies, such as MER and GEM.\nFinally, we demonstrate that a basic online updating strategy on\nrepresentations learned by OML is competitive with rehearsal based methods for\ncontinual learning. We release an implementation of our method at\nhttps://github.com/khurramjaved96/mrcl .",
    "num_pages": 15
}