{
    "uuid": "8433f894-a217-54d3-9ae5-5c17f951797a",
    "title": "Large Language Models with Controllable Working Memory",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Daliang Li",
        "Ankit Singh Rawat",
        "Manzil Zaheer",
        "Xin Wang",
        "Michal Lukasik",
        "Andreas Veit",
        "Felix Yu",
        "Sanjiv Kumar"
    ],
    "pdf_url": "http://arxiv.org/pdf/2211.05110v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\8433f894-a217-54d3-9ae5-5c17f951797a.pdf",
    "bibtex": "@misc{li2022largelanguagemodelswithcontrollable,\n    title = {Large Language Models with Controllable Working Memory},\n    author = {Daliang Li and Ankit Singh Rawat and Manzil Zaheer and Xin Wang and Michal Lukasik and Andreas Veit and Felix Yu and Sanjiv Kumar},\n    year = {2022},\n    eprint = {2211.05110},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2211.05110},\n}",
    "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural\nlanguage processing (NLP), owing to their excellent understanding and\ngeneration abilities. Remarkably, what further sets these models apart is the\nmassive amounts of world knowledge they internalize during pretraining. While\nmany downstream applications provide the model with an informational context to\naid its performance on the underlying task, how the model's world knowledge\ninteracts with the factual information presented in the context remains under\nexplored. As a desirable behavior, an LLM should give precedence to the context\nwhenever it contains task-relevant information that conflicts with the model's\nmemorized knowledge. This enables model predictions to be grounded in the\ncontext, which can then be used to update or correct specific model predictions\nwithout frequent retraining. By contrast, when the context is irrelevant to the\ntask, the model should ignore it and fall back on its internal knowledge. In\nthis paper, we undertake a first joint study of the aforementioned two\nproperties, namely controllability and robustness, in the context of LLMs. We\ndemonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned)\ncould exhibit poor controllability and robustness, which do not scale with\nincreasing model size. As a solution, we propose a novel method - Knowledge\nAware FineTuning (KAFT) - to strengthen both controllability and robustness by\nincorporating counterfactual and irrelevant contexts to standard supervised\ndatasets. Our comprehensive evaluation showcases the utility of KAFT across\nmodel architectures and sizes.",
    "num_pages": 17
}