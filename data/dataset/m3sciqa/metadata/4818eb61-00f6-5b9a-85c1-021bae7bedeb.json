{
    "uuid": "4818eb61-00f6-5b9a-85c1-021bae7bedeb",
    "title": "Multimodal Subtask Graph Generation from Instructional Videos",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Yunseok Jang",
        "Sungryull Sohn",
        "Lajanugen Logeswaran",
        "Tiange Luo",
        "Moontae Lee",
        "Honglak Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/2302.08672v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\4818eb61-00f6-5b9a-85c1-021bae7bedeb.pdf",
    "bibtex": "@misc{jang2023multimodalsubtaskgraphgenerationfrom,\n    title = {Multimodal Subtask Graph Generation from Instructional Videos},\n    author = {Yunseok Jang and Sungryull Sohn and Lajanugen Logeswaran and Tiange Luo and Moontae Lee and Honglak Lee},\n    year = {2023},\n    eprint = {2302.08672},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2302.08672},\n}",
    "abstract": "Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty\npan needs to be washed before it can be used for cooking). In this work, we aim\nto model the causal dependencies between such subtasks from instructional\nvideos describing the task. This is a challenging problem since complete\ninformation about the world is often inaccessible from videos, which demands\nrobust learning mechanisms to understand the causal structure of events. We\npresent Multimodal Subtask Graph Generation (MSG2), an approach that constructs\na Subtask Graph defining the dependency between a task's subtasks relevant to a\ntask from noisy web videos. Graphs generated by our multimodal approach are\ncloser to human-annotated graphs compared to prior approaches. MSG2 further\nperforms the downstream task of next subtask prediction 85% and 30% more\naccurately than recent video transformer models in the ProceL and CrossTask\ndatasets, respectively.",
    "num_pages": 18
}