{
    "uuid": "e8b94d55-54d3-5c7e-ab41-e6af7c9b8b03",
    "title": "SelfDoc: Self-Supervised Document Representation Learning",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Peizhao Li",
        "Jiuxiang Gu",
        "Jason Kuen",
        "Vlad I. Morariu",
        "Handong Zhao",
        "Rajiv Jain",
        "Varun Manjunatha",
        "Hongfu Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.03331v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\e8b94d55-54d3-5c7e-ab41-e6af7c9b8b03.pdf",
    "bibtex": "@misc{li2021selfdocselfsuperviseddocumentrepresentationlearning,\n    title = {SelfDoc: Self-Supervised Document Representation Learning},\n    author = {Peizhao Li and Jiuxiang Gu and Jason Kuen and Vlad I. Morariu and Handong Zhao and Rajiv Jain and Varun Manjunatha and Hongfu Liu},\n    year = {2021},\n    eprint = {2106.03331},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2106.03331},\n}",
    "abstract": "We propose SelfDoc, a task-agnostic pre-training framework for document image\nunderstanding. Because documents are multimodal and are intended for sequential\nreading, our framework exploits the positional, textual, and visual information\nof every semantically meaningful component in a document, and it models the\ncontextualization between each block of content. Unlike existing document\npre-training models, our model is coarse-grained instead of treating individual\nwords as input, therefore avoiding an overly fine-grained with excessive\ncontextualization. Beyond that, we introduce cross-modal learning in the model\npre-training phase to fully leverage multimodal information from unlabeled\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\nmechanism for multimodal feature fusion by adaptively emphasizing language and\nvision signals. Our framework benefits from self-supervised pre-training on\ndocuments without requiring annotations by a feature masking training strategy.\nIt achieves superior performance on multiple downstream tasks with\nsignificantly fewer document images used in the pre-training stage compared to\nprevious works.",
    "num_pages": 10
}