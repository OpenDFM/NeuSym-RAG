{
    "uuid": "3f4d5a18-292d-5165-95fa-8aaf6c2ec2fd",
    "title": "Meta-Learning in Neural Networks: A Survey",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Timothy Hospedales",
        "Antreas Antoniou",
        "Paul Micaelli",
        "Amos Storkey"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.05439v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\3f4d5a18-292d-5165-95fa-8aaf6c2ec2fd.pdf",
    "bibtex": "@misc{hospedales2020metalearninginneuralnetworksa,\n    title = {Meta-Learning in Neural Networks: A Survey},\n    author = {Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},\n    year = {2020},\n    eprint = {2004.05439},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2004.05439},\n}",
    "abstract": "The field of meta-learning, or learning-to-learn, has seen a dramatic rise in\ninterest in recent years. Contrary to conventional approaches to AI where tasks\nare solved from scratch using a fixed learning algorithm, meta-learning aims to\nimprove the learning algorithm itself, given the experience of multiple\nlearning episodes. This paradigm provides an opportunity to tackle many\nconventional challenges of deep learning, including data and computation\nbottlenecks, as well as generalization. This survey describes the contemporary\nmeta-learning landscape. We first discuss definitions of meta-learning and\nposition it with respect to related fields, such as transfer learning and\nhyperparameter optimization. We then propose a new taxonomy that provides a\nmore comprehensive breakdown of the space of meta-learning methods today. We\nsurvey promising applications and successes of meta-learning such as few-shot\nlearning and reinforcement learning. Finally, we discuss outstanding challenges\nand promising areas for future research.",
    "num_pages": 20
}