{
    "uuid": "3caa80e4-e308-5fd8-b45f-6b55c8a7a835",
    "title": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Meng Cao",
        "Yue Dong",
        "Jackie Chi Kit Cheung"
    ],
    "pdf_url": "http://arxiv.org/pdf/2109.09784v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\3caa80e4-e308-5fd8-b45f-6b55c8a7a835.pdf",
    "bibtex": "@misc{cao2021hallucinatedbutfactualinspectingthe,\n    title = {Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization},\n    author = {Meng Cao and Yue Dong and Jackie Chi Kit Cheung},\n    year = {2021},\n    eprint = {2109.09784},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2109.09784},\n}",
    "abstract": "State-of-the-art abstractive summarization systems often generate\n\\emph{hallucinations}; i.e., content that is not directly inferable from the\nsource text. Despite being assumed incorrect, we find that much hallucinated\ncontent is factual, namely consistent with world knowledge. These factual\nhallucinations can be beneficial in a summary by providing useful background\ninformation. In this work, we propose a novel detection approach that separates\nfactual from non-factual hallucinations of entities. Our method utilizes an\nentity's prior and posterior probabilities according to pre-trained and\nfinetuned masked language models, respectively. Empirical results suggest that\nour approach vastly outperforms two baselines %in both accuracy and F1 scores\nand strongly correlates with human judgments. % on factuality classification\ntasks. Furthermore, we show that our detector, when used as a reward signal in\nan off-line reinforcement learning (RL) algorithm, significantly improves the\nfactuality of summaries while maintaining the level of abstractiveness.",
    "num_pages": 14
}