{
    "uuid": "2d8a2245-33f4-506c-9b85-aab7d7f6d8b2",
    "title": "Explanations from Large Language Models Make Small Reasoners Better",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Shiyang Li",
        "Jianshu Chen",
        "Yelong Shen",
        "Zhiyu Chen",
        "Xinlu Zhang",
        "Zekun Li",
        "Hong Wang",
        "Jing Qian",
        "Baolin Peng",
        "Yi Mao",
        "Wenhu Chen",
        "Xifeng Yan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.06726v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\2d8a2245-33f4-506c-9b85-aab7d7f6d8b2.pdf",
    "bibtex": "@misc{li2022explanationsfromlargelanguagemodels,\n    title = {Explanations from Large Language Models Make Small Reasoners Better},\n    author = {Shiyang Li and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jing Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},\n    year = {2022},\n    eprint = {2210.06726},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.06726},\n}",
    "abstract": "Integrating free-text explanations to in-context learning of large language\nmodels (LLM) is shown to elicit strong reasoning capabilities along with\nreasonable explanations. In this paper, we consider the problem of leveraging\nthe explanations generated by LLM to improve the training of small reasoners,\nwhich are more favorable in real-production deployment due to their low cost.\nWe systematically explore three explanation generation approaches from LLM and\nutilize a multi-task learning framework to facilitate small models to acquire\nstrong reasoning power together with explanation generation capabilities.\nExperiments on multiple reasoning tasks show that our method can consistently\nand significantly outperform finetuning baselines across different settings,\nand even perform better than finetuning/prompting a 60x larger GPT-3 (175B)\nmodel by up to 9.5% in accuracy. As a side benefit, human evaluation further\nshows that our method can generate high-quality explanations to justify its\npredictions, moving towards the goal of explainable AI.",
    "num_pages": 16
}