{
    "uuid": "85ba33aa-25d1-526b-a87d-42cfd55a08c9",
    "title": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Thomas Scialom",
        "Sylvain Lamprier",
        "Benjamin Piwowarski",
        "Jacopo Staiano"
    ],
    "pdf_url": "http://arxiv.org/pdf/1909.01610v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\85ba33aa-25d1-526b-a87d-42cfd55a08c9.pdf",
    "bibtex": "@misc{scialom2019answersuniteunsupervisedmetricsfor,\n    title = {Answers Unite! Unsupervised Metrics for Reinforced Summarization Models},\n    author = {Thomas Scialom and Sylvain Lamprier and Benjamin Piwowarski and Jacopo Staiano},\n    year = {2019},\n    eprint = {1909.01610},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1909.01610},\n}",
    "abstract": "Abstractive summarization approaches based on Reinforcement Learning (RL)\nhave recently been proposed to overcome classical likelihood maximization. RL\nenables to consider complex, possibly non-differentiable, metrics that globally\nassess the quality and relevance of the generated outputs. ROUGE, the most used\nsummarization metric, is known to suffer from bias towards lexical similarity\nas well as from suboptimal accounting for fluency and readability of the\ngenerated abstracts. We thus explore and propose alternative evaluation\nmeasures: the reported human-evaluation analysis shows that the proposed\nmetrics, based on Question Answering, favorably compares to ROUGE -- with the\nadditional property of not requiring reference summaries. Training a RL-based\nmodel on these metrics leads to improvements (both in terms of human or\nautomated metrics) over current approaches that use ROUGE as a reward.",
    "num_pages": 11
}