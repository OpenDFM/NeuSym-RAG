{
    "uuid": "45decdf4-f524-52d5-8385-3da8d839888b",
    "title": "Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Boxin Wang",
        "Wei Ping",
        "Chaowei Xiao",
        "Peng Xu",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bo Li",
        "Anima Anandkumar",
        "Bryan Catanzaro"
    ],
    "pdf_url": "http://arxiv.org/pdf/2202.04173v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\45decdf4-f524-52d5-8385-3da8d839888b.pdf",
    "bibtex": "@misc{wang2022exploringthelimitsofdomainadaptive,\n    title = {Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models},\n    author = {Boxin Wang and Wei Ping and Chaowei Xiao and Peng Xu and Mostofa Patwary and Mohammad Shoeybi and Bo Li and Anima Anandkumar and Bryan Catanzaro},\n    year = {2022},\n    eprint = {2202.04173},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2202.04173},\n}",
    "abstract": "Pre-trained language models (LMs) are shown to easily generate toxic\nlanguage. In this work, we systematically explore domain-adaptive training to\nreduce the toxicity of language models. We conduct this study on three\ndimensions: training corpus, model size, and parameter efficiency. For the\ntraining corpus, we propose to leverage the generative power of LMs and\ngenerate nontoxic datasets for domain-adaptive training, which mitigates the\nexposure bias and is shown to be more data-efficient than using a curated\npre-training corpus. We demonstrate that the self-generation method\nconsistently outperforms the existing baselines across various model sizes on\nboth automatic and human evaluations, even when it uses a 1/3 smaller training\ncorpus. We then comprehensively study detoxifying LMs with parameter sizes\nranging from 126M up to 530B (3x larger than GPT-3), a scale that has never\nbeen studied before. We find that i) large LMs have similar toxicity levels as\nsmaller ones given the same pre-training corpus, and ii) large LMs require more\nendeavor to detoxify. We also explore parameter-efficient training methods for\ndetoxification. We demonstrate that adding and training adapter-only layers in\nLMs not only saves a lot of parameters but also achieves a better trade-off\nbetween toxicity and perplexity than whole model adaptation for the large-scale\nmodels.",
    "num_pages": 27
}