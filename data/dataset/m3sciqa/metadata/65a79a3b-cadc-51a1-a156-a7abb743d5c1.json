{
    "uuid": "65a79a3b-cadc-51a1-a156-a7abb743d5c1",
    "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Leo Gao",
        "Stella Biderman",
        "Sid Black",
        "Laurence Golding",
        "Travis Hoppe",
        "Charles Foster",
        "Jason Phang",
        "Horace He",
        "Anish Thite",
        "Noa Nabeshima",
        "Shawn Presser",
        "Connor Leahy"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00027v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\65a79a3b-cadc-51a1-a156-a7abb743d5c1.pdf",
    "bibtex": "@misc{gao2020thepilean800gbdataset,\n    title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},\n    author = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},\n    year = {2020},\n    eprint = {2101.00027},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2101.00027},\n}",
    "abstract": "Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.",
    "num_pages": 39
}