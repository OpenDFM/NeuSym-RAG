{
    "uuid": "5b0d44de-0d1d-5f43-a9b0-89b5338732d7",
    "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jinheon Baek",
        "Alham Fikri Aji",
        "Amir Saffari"
    ],
    "pdf_url": "http://arxiv.org/pdf/2306.04136v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\5b0d44de-0d1d-5f43-a9b0-89b5338732d7.pdf",
    "bibtex": "@misc{baek2023knowledgeaugmentedlanguagemodelpromptingfor,\n    title = {Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering},\n    author = {Jinheon Baek and Alham Fikri Aji and Amir Saffari},\n    year = {2023},\n    eprint = {2306.04136},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2306.04136},\n}",
    "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.",
    "num_pages": 29
}