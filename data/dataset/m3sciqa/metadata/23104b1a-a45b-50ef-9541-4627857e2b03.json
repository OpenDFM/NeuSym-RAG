{
    "uuid": "23104b1a-a45b-50ef-9541-4627857e2b03",
    "title": "SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Daniel Cer",
        "Mona Diab",
        "Eneko Agirre",
        "Iñigo Lopez-Gazpio",
        "Lucia Specia"
    ],
    "pdf_url": "http://arxiv.org/pdf/1708.00055v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\23104b1a-a45b-50ef-9541-4627857e2b03.pdf",
    "bibtex": "@misc{cer2017semeval2017task1semantictextual,\n    title = {SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation},\n    author = {Daniel Cer and Mona Diab and Eneko Agirre and Iñigo Lopez-Gazpio and Lucia Specia},\n    year = {2017},\n    eprint = {1708.00055},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1708.00055},\n}",
    "abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of\nsentences. Applications include machine translation (MT), summarization,\ngeneration, question answering (QA), short answer grading, semantic search,\ndialog and conversational systems. The STS shared task is a venue for assessing\nthe current state-of-the-art. The 2017 task focuses on multilingual and\ncross-lingual pairs with one sub-track exploring MT quality estimation (MTQE)\ndata. The task obtained strong participation from 31 teams, with 17\nparticipating in all language tracks. We summarize performance and review a\nselection of well performing methods. Analysis highlights common errors,\nproviding insight into the limitations of existing models. To support ongoing\nwork on semantic representations, the STS Benchmark is introduced as a new\nshared training and evaluation set carefully selected from the corpus of\nEnglish STS shared task data (2012-2017).",
    "num_pages": 14
}