{
    "uuid": "5aab03c9-0e38-5818-90e3-3b5f882ce28d",
    "title": "Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yiding Hao",
        "Dana Angluin",
        "Robert Frank"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.06618v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\5aab03c9-0e38-5818-90e3-3b5f882ce28d.pdf",
    "bibtex": "@misc{hao2022formallanguagerecognitionbyhard,\n    title = {Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity},\n    author = {Yiding Hao and Dana Angluin and Robert Frank},\n    year = {2022},\n    eprint = {2204.06618},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CC},\n    url = {http://arxiv.org/abs/2204.06618},\n}",
    "abstract": "This paper analyzes three formal models of Transformer encoders that differ\nin the form of their self-attention mechanism: unique hard attention (UHAT);\ngeneralized unique hard attention (GUHAT), which generalizes UHAT; and\naveraging hard attention (AHAT). We show that UHAT and GUHAT Transformers,\nviewed as string acceptors, can only recognize formal languages in the\ncomplexity class AC$^0$, the class of languages recognizable by families of\nBoolean circuits of constant depth and polynomial size. This upper bound\nsubsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages\nor the PARITY language, since those languages are outside AC$^0$ (Furst et al.,\n1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are\nrecognizable by AHAT networks, implying that AHAT can recognize languages that\nUHAT and GUHAT cannot.",
    "num_pages": 11
}