{
    "uuid": "7b1f8efd-f0dd-566d-8e39-b67f95acb72e",
    "title": "Embodied Language Grounding with 3D Visual Feature Representations",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Mihir Prabhudesai",
        "Hsiao-Yu Fish Tung",
        "Syed Ashar Javed",
        "Maximilian Sieb",
        "Adam W. Harley",
        "Katerina Fragkiadaki"
    ],
    "pdf_url": "http://arxiv.org/pdf/1910.01210v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\7b1f8efd-f0dd-566d-8e39-b67f95acb72e.pdf",
    "bibtex": "@misc{prabhudesai2021embodiedlanguagegroundingwith3d,\n    title = {Embodied Language Grounding with 3D Visual Feature Representations},\n    author = {Mihir Prabhudesai and Hsiao-Yu Fish Tung and Syed Ashar Javed and Maximilian Sieb and Adam W. Harley and Katerina Fragkiadaki},\n    year = {2021},\n    eprint = {1910.01210},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1910.01210},\n}",
    "abstract": "We propose associating language utterances to 3D visual abstractions of the\nscene they describe. The 3D visual abstractions are encoded as 3-dimensional\nvisual feature maps. We infer these 3D visual scene feature maps from RGB\nimages of the scene via view prediction: when the generated 3D scene feature\nmap is neurally projected from a camera viewpoint, it should match the\ncorresponding RGB image. We present generative models that condition on the\ndependency tree of an utterance and generate a corresponding visual 3D feature\nmap as well as reason about its plausibility, and detector models that\ncondition on both the dependency tree of an utterance and a related image and\nlocalize the object referents in the 3D feature map inferred from the image.\nOur model outperforms models of language and vision that associate language\nwith 2D CNN activations or 2D images by a large margin in a variety of tasks,\nsuch as, classifying plausibility of utterances, detecting referential\nexpressions, and supplying rewards for trajectory optimization of object\nplacement policies from language instructions. We perform numerous ablations\nand show the improved performance of our detectors is due to its better\ngeneralization across camera viewpoints and lack of object interferences in the\ninferred 3D feature space, and the improved performance of our generators is\ndue to their ability to spatially reason about objects and their configurations\nin 3D when mapping from language to scenes.",
    "num_pages": 21
}