{
    "uuid": "c497371c-17dd-5bac-a8f4-a047d15a0ee7",
    "title": "Taming Sparsely Activated Transformer with Stochastic Experts",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Simiao Zuo",
        "Xiaodong Liu",
        "Jian Jiao",
        "Young Jin Kim",
        "Hany Hassan",
        "Ruofei Zhang",
        "Tuo Zhao",
        "Jianfeng Gao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.04260v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c497371c-17dd-5bac-a8f4-a047d15a0ee7.pdf",
    "bibtex": "@misc{zuo2022tamingsparselyactivatedtransformerwith,\n    title = {Taming Sparsely Activated Transformer with Stochastic Experts},\n    author = {Simiao Zuo and Xiaodong Liu and Jian Jiao and Young Jin Kim and Hany Hassan and Ruofei Zhang and Tuo Zhao and Jianfeng Gao},\n    year = {2022},\n    eprint = {2110.04260},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.04260},\n}",
    "abstract": "Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at:\nhttps://github.com/microsoft/Stochastic-Mixture-of-Experts.",
    "num_pages": 17
}