{
    "uuid": "239edc4a-ccdc-5065-96a3-9e612c1afcc1",
    "title": "BARTScore: Evaluating Generated Text as Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Weizhe Yuan",
        "Graham Neubig",
        "Pengfei Liu"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.11520v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\239edc4a-ccdc-5065-96a3-9e612c1afcc1.pdf",
    "bibtex": "@misc{yuan2021bartscoreevaluatinggeneratedtextas,\n    title = {BARTScore: Evaluating Generated Text as Text Generation},\n    author = {Weizhe Yuan and Graham Neubig and Pengfei Liu},\n    year = {2021},\n    eprint = {2106.11520},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.11520},\n}",
    "abstract": "A wide variety of NLP applications, such as machine translation,\nsummarization, and dialog, involve text generation. One major challenge for\nthese applications is how to evaluate whether such generated texts are actually\nfluent, accurate, or effective. In this work, we conceptualize the evaluation\nof generated text as a text generation problem, modeled using pre-trained\nsequence-to-sequence models. The general idea is that models trained to convert\nthe generated text to/from a reference output or the source text will achieve\nhigher scores when the generated text is better. We operationalize this idea\nusing BART, an encoder-decoder based pre-trained model, and propose a metric\nBARTScore with a number of variants that can be flexibly applied in an\nunsupervised fashion to evaluation of text from different perspectives (e.g.\ninformativeness, fluency, or factuality). BARTScore is conceptually simple and\nempirically effective. It can outperform existing top-scoring metrics in 16 of\n22 test settings, covering evaluation of 16 datasets (e.g., machine\ntranslation, text summarization) and 7 different perspectives (e.g.,\ninformativeness, factuality). Code to calculate BARTScore is available at\nhttps://github.com/neulab/BARTScore, and we have released an interactive\nleaderboard for meta-evaluation at\nhttp://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard\nplatform, which allows us to interactively understand the strengths,\nweaknesses, and complementarity of each metric.",
    "num_pages": 18
}