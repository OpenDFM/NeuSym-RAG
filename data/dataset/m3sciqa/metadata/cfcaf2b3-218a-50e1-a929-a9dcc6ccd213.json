{
    "uuid": "cfcaf2b3-218a-50e1-a929-a9dcc6ccd213",
    "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Cheng-Han Chiang",
        "Hung-yi Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/2305.01937v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\cfcaf2b3-218a-50e1-a929-a9dcc6ccd213.pdf",
    "bibtex": "@misc{chiang2023canlargelanguagemodelsbe,\n    title = {Can Large Language Models Be an Alternative to Human Evaluations?},\n    author = {Cheng-Han Chiang and Hung-yi Lee},\n    year = {2023},\n    eprint = {2305.01937},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2305.01937},\n}",
    "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.",
    "num_pages": 23
}