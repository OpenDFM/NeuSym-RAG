{
    "uuid": "2916ff3a-f5cc-5c0e-8960-e693e402ade5",
    "title": "DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Hanqing Zhang",
        "Dawei Song"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.09551v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\2916ff3a-f5cc-5c0e-8960-e693e402ade5.pdf",
    "bibtex": "@misc{zhang2022discupdiscriminatorcooperativeunlikelihoodprompttuning,\n    title = {DisCup: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation},\n    author = {Hanqing Zhang and Dawei Song},\n    year = {2022},\n    eprint = {2210.09551},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.09551},\n}",
    "abstract": "Prompt learning with immensely large Casual Language Models (CLMs) has been\nshown promising for attribute-controllable text generation (CTG). However,\nvanilla prompt tuning tends to imitate training corpus characteristics beyond\nthe control attributes, resulting in a poor generalization ability. Moreover,\nit is less able to capture the relationship between different attributes,\nfurther limiting the control performance. In this paper, we propose a new CTG\napproach, namely DisCup, which incorporates the attribute knowledge of\ndiscriminator to optimize the control-prompts, steering a frozen CLM to produce\nattribute-specific texts. Specifically, the frozen CLM model, capable of\nproducing multitudinous texts, is first used to generate the next-token\ncandidates based on the context, so as to ensure the diversity of tokens to be\npredicted. Then, we leverage an attribute-discriminator to select\ndesired/undesired tokens from those candidates, providing the inter-attribute\nknowledge. Finally, we bridge the above two traits by an unlikelihood objective\nfor prompt-tuning. Extensive experimental results show that DisCup can achieve\na new state-of-the-art control performance while maintaining an efficient and\nhigh-quality text generation, only relying on around 10 virtual tokens.",
    "num_pages": 15
}