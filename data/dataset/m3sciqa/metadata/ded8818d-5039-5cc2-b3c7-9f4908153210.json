{
    "uuid": "ded8818d-5039-5cc2-b3c7-9f4908153210",
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Pengchuan Zhang",
        "Xiujun Li",
        "Xiaowei Hu",
        "Jianwei Yang",
        "Lei Zhang",
        "Lijuan Wang",
        "Yejin Choi",
        "Jianfeng Gao"
    ],
    "pdf_url": "http://arxiv.org/pdf/2101.00529v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\ded8818d-5039-5cc2-b3c7-9f4908153210.pdf",
    "bibtex": "@misc{zhang2021vinvlrevisitingvisualrepresentationsin,\n    title = {VinVL: Revisiting Visual Representations in Vision-Language Models},\n    author = {Pengchuan Zhang and Xiujun Li and Xiaowei Hu and Jianwei Yang and Lei Zhang and Lijuan Wang and Yejin Choi and Jianfeng Gao},\n    year = {2021},\n    eprint = {2101.00529},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2101.00529},\n}",
    "abstract": "This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.",
    "num_pages": 30
}