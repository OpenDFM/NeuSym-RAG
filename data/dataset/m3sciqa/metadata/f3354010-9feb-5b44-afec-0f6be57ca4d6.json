{
    "uuid": "f3354010-9feb-5b44-afec-0f6be57ca4d6",
    "title": "Faithful to the Original: Fact Aware Neural Abstractive Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Ziqiang Cao",
        "Furu Wei",
        "Wenjie Li",
        "Sujian Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/1711.04434v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\f3354010-9feb-5b44-afec-0f6be57ca4d6.pdf",
    "bibtex": "@misc{cao2017faithfultotheoriginalfact,\n    title = {Faithful to the Original: Fact Aware Neural Abstractive Summarization},\n    author = {Ziqiang Cao and Furu Wei and Wenjie Li and Sujian Li},\n    year = {2017},\n    eprint = {1711.04434},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.IR},\n    url = {http://arxiv.org/abs/1711.04434},\n}",
    "abstract": "Unlike extractive summarization, abstractive summarization has to fuse\ndifferent parts of the source text, which inclines to create fake facts. Our\npreliminary study reveals nearly 30% of the outputs from a state-of-the-art\nneural summarization system suffer from this problem. While previous\nabstractive summarization approaches usually focus on the improvement of\ninformativeness, we argue that faithfulness is also a vital prerequisite for a\npractical abstractive summarization system. To avoid generating fake facts in a\nsummary, we leverage open information extraction and dependency parse\ntechnologies to extract actual fact descriptions from the source text. The\ndual-attention sequence-to-sequence framework is then proposed to force the\ngeneration conditioned on both the source text and the extracted fact\ndescriptions. Experiments on the Gigaword benchmark dataset demonstrate that\nour model can greatly reduce fake summaries by 80%. Notably, the fact\ndescriptions also bring significant improvement on informativeness since they\noften condense the meaning of the source text.",
    "num_pages": 8
}