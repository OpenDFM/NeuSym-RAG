{
    "uuid": "6297fbcc-4cda-5e1d-b3ab-9036a2192dcd",
    "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Chelsea Finn",
        "Pieter Abbeel",
        "Sergey Levine"
    ],
    "pdf_url": "http://arxiv.org/pdf/1703.03400v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\6297fbcc-4cda-5e1d-b3ab-9036a2192dcd.pdf",
    "bibtex": "@misc{finn2017modelagnosticmetalearningforfastadaptation,\n    title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},\n    author = {Chelsea Finn and Pieter Abbeel and Sergey Levine},\n    year = {2017},\n    eprint = {1703.03400},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1703.03400},\n}",
    "abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.",
    "num_pages": 13
}