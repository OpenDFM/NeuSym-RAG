{
    "uuid": "8f02441e-aa43-5abf-bd9b-cfd760b7cc20",
    "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yoojin Choi",
        "Mostafa El-Khamy",
        "Jungwon Lee"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.09835v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\8f02441e-aa43-5abf-bd9b-cfd760b7cc20.pdf",
    "bibtex": "@misc{choi2021dualteacherclassincrementallearningwithdatafree,\n    title = {Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay},\n    author = {Yoojin Choi and Mostafa El-Khamy and Jungwon Lee},\n    year = {2021},\n    eprint = {2106.09835},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2106.09835},\n}",
    "abstract": "This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.",
    "num_pages": 13
}