{
    "uuid": "4774dafb-8266-57ac-ba3b-3fb7302f43e8",
    "title": "POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Yujian Liu",
        "Xinliang Frederick Zhang",
        "David Wegsman",
        "Nick Beauchamp",
        "Lu Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.00619v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\4774dafb-8266-57ac-ba3b-3fb7302f43e8.pdf",
    "bibtex": "@misc{liu2022politicspretrainingwithsamestoryarticle,\n    title = {POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection},\n    author = {Yujian Liu and Xinliang Frederick Zhang and David Wegsman and Nick Beauchamp and Lu Wang},\n    year = {2022},\n    eprint = {2205.00619},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.00619},\n}",
    "abstract": "Ideology is at the core of political science research. Yet, there still does\nnot exist general-purpose tools to characterize and predict ideology across\ndifferent genres of text. To this end, we study Pretrained Language Models\nusing novel ideology-driven pretraining objectives that rely on the comparison\nof articles on the same story written by media of different ideologies. We\nfurther collect a large-scale dataset, consisting of more than 3.6M political\nnews articles, for pretraining. Our model POLITICS outperforms strong baselines\nand the previous state-of-the-art models on ideology prediction and stance\ndetection tasks. Further analyses show that POLITICS is especially good at\nunderstanding long or formally written texts, and is also robust in few-shot\nlearning scenarios.",
    "num_pages": 21
}