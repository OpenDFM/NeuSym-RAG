{
    "uuid": "1ffa6ac4-663b-5498-84b3-3148948fcba4",
    "title": "Up or Down? Adaptive Rounding for Post-Training Quantization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Markus Nagel",
        "Rana Ali Amjad",
        "Mart van Baalen",
        "Christos Louizos",
        "Tijmen Blankevoort"
    ],
    "pdf_url": "http://arxiv.org/pdf/2004.10568v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\1ffa6ac4-663b-5498-84b3-3148948fcba4.pdf",
    "bibtex": "@misc{nagel2020upordownadaptiverounding,\n    title = {Up or Down? Adaptive Rounding for Post-Training Quantization},\n    author = {Markus Nagel and Rana Ali Amjad and Mart van Baalen and Christos Louizos and Tijmen Blankevoort},\n    year = {2020},\n    eprint = {2004.10568},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/2004.10568},\n}",
    "abstract": "When quantizing neural networks, assigning each floating-point weight to its\nnearest fixed-point value is the predominant approach. We find that, perhaps\nsurprisingly, this is not the best we can do. In this paper, we propose\nAdaRound, a better weight-rounding mechanism for post-training quantization\nthat adapts to the data and the task loss. AdaRound is fast, does not require\nfine-tuning of the network, and only uses a small amount of unlabelled data. We\nstart by theoretically analyzing the rounding problem for a pre-trained neural\nnetwork. By approximating the task loss with a Taylor series expansion, the\nrounding task is posed as a quadratic unconstrained binary optimization\nproblem. We simplify this to a layer-wise local loss and propose to optimize\nthis loss with a soft relaxation. AdaRound not only outperforms\nrounding-to-nearest by a significant margin but also establishes a new\nstate-of-the-art for post-training quantization on several networks and tasks.\nWithout fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4\nbits while staying within an accuracy loss of 1%.",
    "num_pages": 12
}