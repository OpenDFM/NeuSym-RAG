{
    "uuid": "28c91c0b-4918-5ece-a008-5c539282c189",
    "title": "Large Language Models Can Self-Improve",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Jiaxin Huang",
        "Shixiang Shane Gu",
        "Le Hou",
        "Yuexin Wu",
        "Xuezhi Wang",
        "Hongkun Yu",
        "Jiawei Han"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.11610v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\28c91c0b-4918-5ece-a008-5c539282c189.pdf",
    "bibtex": "@misc{huang2022largelanguagemodelscanselfimprove,\n    title = {Large Language Models Can Self-Improve},\n    author = {Jiaxin Huang and Shixiang Shane Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},\n    year = {2022},\n    eprint = {2210.11610},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.11610},\n}",
    "abstract": "Large Language Models (LLMs) have achieved excellent performances in various\ntasks. However, fine-tuning an LLM requires extensive supervision. Human, on\nthe other hand, may improve their reasoning abilities by self-thinking without\nexternal inputs. In this work, we demonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets. We use a pre-trained LLM to\ngenerate \"high-confidence\" rationale-augmented answers for unlabeled questions\nusing Chain-of-Thought prompting and self-consistency, and fine-tune the LLM\nusing those self-generated solutions as target outputs. We show that our\napproach improves the general reasoning ability of a 540B-parameter LLM\n(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and\n63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,\nwithout any ground truth label. We conduct ablation studies and show that\nfine-tuning on reasoning is critical for self-improvement.",
    "num_pages": 19
}