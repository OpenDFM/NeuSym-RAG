{
    "uuid": "76df96e7-5638-5f68-b1be-b1051c1e8730",
    "title": "Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Kawin Ethayarajh",
        "Yejin Choi",
        "Swabha Swayamdipta"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.08420v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\76df96e7-5638-5f68-b1be-b1051c1e8730.pdf",
    "bibtex": "@misc{ethayarajh2022understandingdatasetdifficultywithmathcalvusable,\n    title = {Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information},\n    author = {Kawin Ethayarajh and Yejin Choi and Swabha Swayamdipta},\n    year = {2022},\n    eprint = {2110.08420},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.08420},\n}",
    "abstract": "Estimating the difficulty of a dataset typically involves comparing\nstate-of-the-art models to humans; the bigger the performance gap, the harder\nthe dataset is said to be. However, this comparison provides little\nunderstanding of how difficult each instance in a given distribution is, or\nwhat attributes make the dataset difficult for a given model. To address these\nquestions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as\nthe lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019),\nwhere a lower value indicates a more difficult dataset for $\\mathcal{V}$. We\nfurther introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for\nmeasuring the difficulty of individual instances w.r.t. a given distribution.\nWhile standard evaluation metrics typically only compare different models for\nthe same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also\npermit the converse: for a given model $\\mathcal{V}$, we can compare different\ndatasets, as well as different instances/slices of the same dataset.\nFurthermore, our framework allows for the interpretability of different input\nattributes via transformations of the input, which we use to discover\nannotation artefacts in widely-used NLP benchmarks.",
    "num_pages": 21
}