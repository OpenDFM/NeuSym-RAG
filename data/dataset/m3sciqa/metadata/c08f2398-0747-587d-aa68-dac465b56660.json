{
    "uuid": "c08f2398-0747-587d-aa68-dac465b56660",
    "title": "Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Alessandro Suglia",
        "Qiaozi Gao",
        "Jesse Thomason",
        "Govind Thattai",
        "Gaurav Sukhatme"
    ],
    "pdf_url": "http://arxiv.org/pdf/2108.04927v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\c08f2398-0747-587d-aa68-dac465b56660.pdf",
    "bibtex": "@misc{suglia2021embodiedbertatransformermodel,\n    title = {Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion},\n    author = {Alessandro Suglia and Qiaozi Gao and Jesse Thomason and Govind Thattai and Gaurav Sukhatme},\n    year = {2021},\n    eprint = {2108.04927},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2108.04927},\n}",
    "abstract": "Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.",
    "num_pages": 13
}