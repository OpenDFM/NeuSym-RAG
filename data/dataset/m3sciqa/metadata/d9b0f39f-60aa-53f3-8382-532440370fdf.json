{
    "uuid": "d9b0f39f-60aa-53f3-8382-532440370fdf",
    "title": "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Iacer Calixto",
        "Qun Liu",
        "Nick Campbell"
    ],
    "pdf_url": "http://arxiv.org/pdf/1702.01287v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\d9b0f39f-60aa-53f3-8382-532440370fdf.pdf",
    "bibtex": "@misc{calixto2017doublyattentivedecoderformultimodalneural,\n    title = {Doubly-Attentive Decoder for Multi-modal Neural Machine Translation},\n    author = {Iacer Calixto and Qun Liu and Nick Campbell},\n    year = {2017},\n    eprint = {1702.01287},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1702.01287},\n}",
    "abstract": "We introduce a Multi-modal Neural Machine Translation model in which a\ndoubly-attentive decoder naturally incorporates spatial visual features\nobtained using pre-trained convolutional neural networks, bridging the gap\nbetween image description and translation. Our decoder learns to attend to\nsource-language words and parts of an image independently by means of two\nseparate attention mechanisms as it generates words in the target language. We\nfind that our model can efficiently exploit not just back-translated in-domain\nmulti-modal data but also large general-domain text-only MT corpora. We also\nreport state-of-the-art results on the Multi30k data set.",
    "num_pages": 11
}