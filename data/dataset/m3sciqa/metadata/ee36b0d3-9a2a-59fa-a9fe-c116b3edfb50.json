{
    "uuid": "ee36b0d3-9a2a-59fa-a9fe-c116b3edfb50",
    "title": "When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ankur Sikarwar",
        "Arkil Patel",
        "Navin Goyal"
    ],
    "pdf_url": "http://arxiv.org/pdf/2210.12786v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\ee36b0d3-9a2a-59fa-a9fe-c116b3edfb50.pdf",
    "bibtex": "@misc{sikarwar2022whencantransformersgroundand,\n    title = {When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks},\n    author = {Ankur Sikarwar and Arkil Patel and Navin Goyal},\n    year = {2022},\n    eprint = {2210.12786},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2210.12786},\n}",
    "abstract": "Humans can reason compositionally whilst grounding language utterances to the\nreal world. Recent benchmarks like ReaSCAN use navigation tasks grounded in a\ngrid world to assess whether neural models exhibit similar capabilities. In\nthis work, we present a simple transformer-based model that outperforms\nspecialized architectures on ReaSCAN and a modified version of gSCAN. On\nanalyzing the task, we find that identifying the target location in the grid\nworld is the main challenge for the models. Furthermore, we show that a\nparticular split in ReaSCAN, which tests depth generalization, is unfair. On an\namended version of this split, we show that transformers can generalize to\ndeeper input structures. Finally, we design a simpler grounded compositional\ngeneralization task, RefEx, to investigate how transformers reason\ncompositionally. We show that a single self-attention layer with a single head\ngeneralizes to novel combinations of object attributes. Moreover, we derive a\nprecise mathematical construction of the transformer's computations from the\nlearned network. Overall, we provide valuable insights about the grounded\ncompositional generalization task and the behaviour of transformers on it,\nwhich would be useful for researchers working in this area.",
    "num_pages": 22
}