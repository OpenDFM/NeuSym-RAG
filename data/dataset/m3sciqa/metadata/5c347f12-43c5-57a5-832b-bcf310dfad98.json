{
    "uuid": "5c347f12-43c5-57a5-832b-bcf310dfad98",
    "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Martin Heusel",
        "Hubert Ramsauer",
        "Thomas Unterthiner",
        "Bernhard Nessler",
        "Sepp Hochreiter"
    ],
    "pdf_url": "http://arxiv.org/pdf/1706.08500v6",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\5c347f12-43c5-57a5-832b-bcf310dfad98.pdf",
    "bibtex": "@misc{heusel2018ganstrainedbyatwo,\n    title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},\n    author = {Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},\n    year = {2018},\n    eprint = {1706.08500},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG},\n    url = {http://arxiv.org/abs/1706.08500},\n}",
    "abstract": "Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark.",
    "num_pages": 38
}