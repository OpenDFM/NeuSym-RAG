{
    "uuid": "9610c218-3fc1-50c1-9412-849cb6c143c0",
    "title": "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Li Wang",
        "Junlin Yao",
        "Yunzhe Tao",
        "Li Zhong",
        "Wei Liu",
        "Qiang Du"
    ],
    "pdf_url": "http://arxiv.org/pdf/1805.03616v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\9610c218-3fc1-50c1-9412-849cb6c143c0.pdf",
    "bibtex": "@misc{wang2020areinforcedtopicawareconvolutionalsequencetosequence,\n    title = {A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization},\n    author = {Li Wang and Junlin Yao and Yunzhe Tao and Li Zhong and Wei Liu and Qiang Du},\n    year = {2020},\n    eprint = {1805.03616},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1805.03616},\n}",
    "abstract": "In this paper, we propose a deep learning approach to tackle the automatic\nsummarization tasks by incorporating topic information into the convolutional\nsequence-to-sequence (ConvS2S) model and using self-critical sequence training\n(SCST) for optimization. Through jointly attending to topics and word-level\nalignment, our approach can improve coherence, diversity, and informativeness\nof generated summaries via a biased probability generation mechanism. On the\nother hand, reinforcement training, like SCST, directly optimizes the proposed\nmodel with respect to the non-differentiable metric ROUGE, which also avoids\nthe exposure bias during inference. We carry out the experimental evaluation\nwith state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets.\nThe empirical results demonstrate the superiority of our proposed method in the\nabstractive summarization.",
    "num_pages": 8
}