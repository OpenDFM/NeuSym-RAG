{
    "uuid": "c48ce7f0-0f60-568a-a32a-61dac182a786",
    "title": "TellMeWhy: A Dataset for Answering Why-Questions in Narratives",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Yash Kumar Lal",
        "Nathanael Chambers",
        "Raymond Mooney",
        "Niranjan Balasubramanian"
    ],
    "pdf_url": "http://arxiv.org/pdf/2106.06132v3",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\c48ce7f0-0f60-568a-a32a-61dac182a786.pdf",
    "bibtex": "@misc{lal2021tellmewhyadatasetforanswering,\n    title = {TellMeWhy: A Dataset for Answering Why-Questions in Narratives},\n    author = {Yash Kumar Lal and Nathanael Chambers and Raymond Mooney and Niranjan Balasubramanian},\n    year = {2021},\n    eprint = {2106.06132},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2106.06132},\n}",
    "abstract": "Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.",
    "num_pages": 15
}