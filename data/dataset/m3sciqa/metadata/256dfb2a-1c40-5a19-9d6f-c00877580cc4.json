{
    "uuid": "256dfb2a-1c40-5a19-9d6f-c00877580cc4",
    "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2017,
    "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
    ],
    "pdf_url": "http://arxiv.org/pdf/1707.07250v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2017\\256dfb2a-1c40-5a19-9d6f-c00877580cc4.pdf",
    "bibtex": "@misc{zadeh2017tensorfusionnetworkformultimodal,\n    title = {Tensor Fusion Network for Multimodal Sentiment Analysis},\n    author = {Amir Zadeh and Minghai Chen and Soujanya Poria and Erik Cambria and Louis-Philippe Morency},\n    year = {2017},\n    eprint = {1707.07250},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1707.07250},\n}",
    "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which\nextends the conventional language-based definition of sentiment analysis to a\nmultimodal setup where other relevant modalities accompany language. In this\npaper, we pose the problem of multimodal sentiment analysis as modeling\nintra-modality and inter-modality dynamics. We introduce a novel model, termed\nTensor Fusion Network, which learns both such dynamics end-to-end. The proposed\napproach is tailored for the volatile nature of spoken language in online\nvideos as well as accompanying gestures and voice. In the experiments, our\nmodel outperforms state-of-the-art approaches for both multimodal and unimodal\nsentiment analysis.",
    "num_pages": 12
}