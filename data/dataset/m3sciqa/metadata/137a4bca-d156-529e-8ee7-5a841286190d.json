{
    "uuid": "137a4bca-d156-529e-8ee7-5a841286190d",
    "title": "POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Yizhe Zhang",
        "Guoyin Wang",
        "Chunyuan Li",
        "Zhe Gan",
        "Chris Brockett",
        "Bill Dolan"
    ],
    "pdf_url": "http://arxiv.org/pdf/2005.00558v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\137a4bca-d156-529e-8ee7-5a841286190d.pdf",
    "bibtex": "@misc{zhang2020pointerconstrainedprogressivetextgeneration,\n    title = {POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training},\n    author = {Yizhe Zhang and Guoyin Wang and Chunyuan Li and Zhe Gan and Chris Brockett and Bill Dolan},\n    year = {2020},\n    eprint = {2005.00558},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2005.00558},\n}",
    "abstract": "Large-scale pre-trained language models, such as BERT and GPT-2, have\nachieved excellent performance in language representation learning and\nfree-form text generation. However, these models cannot be directly employed to\ngenerate text under specified lexical constraints. To address this challenge,\nwe present POINTER (PrOgressive INsertion-based TransformER), a simple yet\nnovel insertion-based approach for hard-constrained text generation. The\nproposed method operates by progressively inserting new tokens between existing\ntokens in a parallel manner. This procedure is recursively applied until a\nsequence is completed. The resulting coarse-to-fine hierarchy makes the\ngeneration process intuitive and interpretable. We pre-train our model with the\nproposed progressive insertion-based objective on a 12GB Wikipedia dataset, and\nfine-tune it on downstream hard-constrained generation tasks.\nNon-autoregressive decoding yields an empirically logarithmic time complexity\nduring inference time. Experimental results on both News and Yelp datasets\ndemonstrate that POINTER achieves state-of-the-art performance on constrained\ntext generation. We released the pre-trained models and the source code to\nfacilitate future research (https://github.com/dreasysnail/POINTER).",
    "num_pages": 22
}