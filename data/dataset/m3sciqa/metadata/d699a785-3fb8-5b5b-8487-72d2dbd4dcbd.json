{
    "uuid": "d699a785-3fb8-5b5b-8487-72d2dbd4dcbd",
    "title": "Prompt Consistency for Zero-Shot Task Generalization",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Chunting Zhou",
        "Junxian He",
        "Xuezhe Ma",
        "Taylor Berg-Kirkpatrick",
        "Graham Neubig"
    ],
    "pdf_url": "http://arxiv.org/pdf/2205.00049v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\d699a785-3fb8-5b5b-8487-72d2dbd4dcbd.pdf",
    "bibtex": "@misc{zhou2022promptconsistencyforzeroshottask,\n    title = {Prompt Consistency for Zero-Shot Task Generalization},\n    author = {Chunting Zhou and Junxian He and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},\n    year = {2022},\n    eprint = {2205.00049},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2205.00049},\n}",
    "abstract": "One of the most impressive results of recent NLP history is the ability of\npre-trained language models to solve new tasks in a zero-shot setting. To\nachieve this, NLP tasks are framed as natural language prompts, generating a\nresponse indicating the predicted output. Nonetheless, the performance in such\nsettings often lags far behind its supervised counterpart, suggesting a large\nspace for potential improvement. In this paper, we explore methods to utilize\nunlabeled data to improve zero-shot performance. Specifically, we take\nadvantage of the fact that multiple prompts can be used to specify a single\ntask, and propose to regularize prompt consistency, encouraging consistent\npredictions over this diverse set of prompts. Our method makes it possible to\nfine-tune the model either with extra unlabeled training data, or directly on\ntest input at inference time in an unsupervised manner. In experiments, our\napproach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,\n2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points\nin terms of accuracy. The gains are often attained with a small number of\nunlabeled examples.",
    "num_pages": 14
}