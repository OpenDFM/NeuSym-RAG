{
    "uuid": "837e7fdc-f434-546f-9a83-81afc1529b90",
    "title": "Targeted Syntactic Evaluation of Language Models",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2018,
    "authors": [
        "Rebecca Marvin",
        "Tal Linzen"
    ],
    "pdf_url": "http://arxiv.org/pdf/1808.09031v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2018\\837e7fdc-f434-546f-9a83-81afc1529b90.pdf",
    "bibtex": "@misc{marvin2018targetedsyntacticevaluationoflanguage,\n    title = {Targeted Syntactic Evaluation of Language Models},\n    author = {Rebecca Marvin and Tal Linzen},\n    year = {2018},\n    eprint = {1808.09031},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1808.09031},\n}",
    "abstract": "We present a dataset for evaluating the grammaticality of the predictions of\na language model. We automatically construct a large number of minimally\ndifferent pairs of English sentences, each consisting of a grammatical and an\nungrammatical sentence. The sentence pairs represent different variations of\nstructure-sensitive phenomena: subject-verb agreement, reflexive anaphora and\nnegative polarity items. We expect a language model to assign a higher\nprobability to the grammatical sentence than the ungrammatical one. In an\nexperiment using this data set, an LSTM language model performed poorly on many\nof the constructions. Multi-task training with a syntactic objective (CCG\nsupertagging) improved the LSTM's accuracy, but a large gap remained between\nits performance and the accuracy of human participants recruited online. This\nsuggests that there is considerable room for improvement over LSTMs in\ncapturing syntax in a language model.",
    "num_pages": 18
}