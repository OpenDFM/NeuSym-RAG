{
    "uuid": "c481bb3e-8bda-50d0-a076-eb7c6cbffc3f",
    "title": "Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Niket Tandon",
        "Aman Madaan",
        "Peter Clark",
        "Yiming Yang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2112.09737v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\c481bb3e-8bda-50d0-a076-eb7c6cbffc3f.pdf",
    "bibtex": "@misc{tandon2022learningtorepairrepairingmodel,\n    title = {Learning to Repair: Repairing model output errors after deployment using a dynamic memory of feedback},\n    author = {Niket Tandon and Aman Madaan and Peter Clark and Yiming Yang},\n    year = {2022},\n    eprint = {2112.09737},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2112.09737},\n}",
    "abstract": "Large language models (LMs), while powerful, are not immune to mistakes, but\ncan be difficult to retrain. Our goal is for an LM to continue to improve after\ndeployment, without retraining, using feedback from the user. Our approach\npairs an LM with (i) a growing memory of cases where the user identified an\noutput error and provided general feedback on how to correct it (ii) a\ncorrector model, trained to translate this general feedback into specific edits\nto repair the model output. Given a new, unseen input, our model can then use\nfeedback from similar, past cases to repair output errors that may occur. We\ninstantiate our approach using an existing, fixed model for script generation,\nthat takes a goal (e.g., \"bake a cake\") and generates a partially ordered\nsequence of actions to achieve that goal, sometimes containing errors. Our\nmemory-enhanced system, FBNet, learns to apply user feedback to repair such\nerrors (up to 30 points improvement), while making a start at avoiding similar\npast mistakes on new, unseen examples (up to 7 points improvement in a\ncontrolled setting). This is a first step towards strengthening deployed\nmodels, potentially broadening their utility. Our code and data is available at\nhttps://github.com/allenai/interscript/.",
    "num_pages": 14
}