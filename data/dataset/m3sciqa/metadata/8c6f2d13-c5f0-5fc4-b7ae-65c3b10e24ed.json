{
    "uuid": "8c6f2d13-c5f0-5fc4-b7ae-65c3b10e24ed",
    "title": "Synthetic QA Corpora Generation with Roundtrip Consistency",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2019,
    "authors": [
        "Chris Alberti",
        "Daniel Andor",
        "Emily Pitler",
        "Jacob Devlin",
        "Michael Collins"
    ],
    "pdf_url": "http://arxiv.org/pdf/1906.05416v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2019\\8c6f2d13-c5f0-5fc4-b7ae-65c3b10e24ed.pdf",
    "bibtex": "@misc{alberti2019syntheticqacorporagenerationwith,\n    title = {Synthetic QA Corpora Generation with Roundtrip Consistency},\n    author = {Chris Alberti and Daniel Andor and Emily Pitler and Jacob Devlin and Michael Collins},\n    year = {2019},\n    eprint = {1906.05416},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/1906.05416},\n}",
    "abstract": "We introduce a novel method of generating synthetic question answering\ncorpora by combining models of question generation and answer extraction, and\nby filtering the results to ensure roundtrip consistency. By pretraining on the\nresulting corpora we obtain significant improvements on SQuAD2 and NQ,\nestablishing a new state-of-the-art on the latter. Our synthetic data\ngeneration models, for both question generation and answer extraction, can be\nfully reproduced by finetuning a publicly available BERT model on the\nextractive subsets of SQuAD2 and NQ. We also describe a more powerful variant\nthat does full sequence-to-sequence pretraining for question generation,\nobtaining exact match and F1 at less than 0.1% and 0.4% from human performance\non SQuAD2.",
    "num_pages": 7
}