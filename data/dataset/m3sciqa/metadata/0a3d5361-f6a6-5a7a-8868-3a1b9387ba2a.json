{
    "uuid": "0a3d5361-f6a6-5a7a-8868-3a1b9387ba2a",
    "title": "Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2021,
    "authors": [
        "Jiawei Zhou",
        "Tahira Naseem",
        "Ramón Fernandez Astudillo",
        "Young-Suk Lee",
        "Radu Florian",
        "Salim Roukos"
    ],
    "pdf_url": "http://arxiv.org/pdf/2110.15534v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2021\\0a3d5361-f6a6-5a7a-8868-3a1b9387ba2a.pdf",
    "bibtex": "@misc{zhou2021structureawarefinetuningofsequencetosequencetransformers,\n    title = {Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing},\n    author = {Jiawei Zhou and Tahira Naseem and Ramón Fernandez Astudillo and Young-Suk Lee and Radu Florian and Salim Roukos},\n    year = {2021},\n    eprint = {2110.15534},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2110.15534},\n}",
    "abstract": "Predicting linearized Abstract Meaning Representation (AMR) graphs using\npre-trained sequence-to-sequence Transformer models has recently led to large\nimprovements on AMR parsing benchmarks. These parsers are simple and avoid\nexplicit modeling of structure but lack desirable properties such as graph\nwell-formedness guarantees or built-in graph-sentence alignments. In this work\nwe explore the integration of general pre-trained sequence-to-sequence language\nmodels and a structure-aware transition-based approach. We depart from a\npointer-based transition system and propose a simplified transition set,\ndesigned to better exploit pre-trained language models for structured\nfine-tuning. We also explore modeling the parser state within the pre-trained\nencoder-decoder architecture and different vocabulary strategies for the same\npurpose. We provide a detailed comparison with recent progress in AMR parsing\nand show that the proposed parser retains the desirable properties of previous\ntransition-based approaches, while being simpler and reaching the new parsing\nstate of the art for AMR 2.0, without the need for graph re-categorization.",
    "num_pages": 12
}