{
    "uuid": "c811c5a5-c22a-5fd6-9f1c-1ffa1bde7c47",
    "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2015,
    "authors": [
        "Yukun Zhu",
        "Ryan Kiros",
        "Richard Zemel",
        "Ruslan Salakhutdinov",
        "Raquel Urtasun",
        "Antonio Torralba",
        "Sanja Fidler"
    ],
    "pdf_url": "http://arxiv.org/pdf/1506.06724v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2015\\c811c5a5-c22a-5fd6-9f1c-1ffa1bde7c47.pdf",
    "bibtex": "@misc{zhu2015aligningbooksandmoviestowards,\n    title = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},\n    author = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},\n    year = {2015},\n    eprint = {1506.06724},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/1506.06724},\n}",
    "abstract": "Books are a rich source of both fine-grained information, how a character, an\nobject or a scene looks like, as well as high-level semantics, what someone is\nthinking, feeling and how these states evolve through a story. This paper aims\nto align books to their movie releases in order to provide rich descriptive\nexplanations for visual content that go semantically far beyond the captions\navailable in current datasets. To align movies and books we exploit a neural\nsentence embedding that is trained in an unsupervised way from a large corpus\nof books, as well as a video-text neural embedding for computing similarities\nbetween movie clips and sentences in the book. We propose a context-aware CNN\nto combine information from multiple sources. We demonstrate good quantitative\nperformance for movie/book alignment and show several qualitative examples that\nshowcase the diversity of tasks our model can be used for.",
    "num_pages": 23
}