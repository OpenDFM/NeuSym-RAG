{
    "uuid": "a0ce147c-98bf-52ba-8365-84983999c80a",
    "title": "Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2023,
    "authors": [
        "Jinyang Li",
        "Binyuan Hui",
        "Reynold Cheng",
        "Bowen Qin",
        "Chenhao Ma",
        "Nan Huo",
        "Fei Huang",
        "Wenyu Du",
        "Luo Si",
        "Yongbin Li"
    ],
    "pdf_url": "http://arxiv.org/pdf/2301.07507v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2023\\a0ce147c-98bf-52ba-8365-84983999c80a.pdf",
    "bibtex": "@misc{li2023graphixt5mixingpretrainedtransformerswith,\n    title = {Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for Text-to-SQL Parsing},\n    author = {Jinyang Li and Binyuan Hui and Reynold Cheng and Bowen Qin and Chenhao Ma and Nan Huo and Fei Huang and Wenyu Du and Luo Si and Yongbin Li},\n    year = {2023},\n    eprint = {2301.07507},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2301.07507},\n}",
    "abstract": "The task of text-to-SQL parsing, which aims at converting natural language\nquestions into executable SQL queries, has garnered increasing attention in\nrecent years, as it can assist end users in efficiently extracting vital\ninformation from databases without the need for technical background. One of\nthe major challenges in text-to-SQL parsing is domain generalization, i.e., how\nto generalize well to unseen databases. Recently, the pre-trained text-to-text\ntransformer model, namely T5, though not specialized for text-to-SQL parsing,\nhas achieved state-of-the-art performance on standard benchmarks targeting\ndomain generalization. In this work, we explore ways to further augment the\npre-trained T5 model with specialized components for text-to-SQL parsing. Such\ncomponents are expected to introduce structural inductive bias into text-to-SQL\nparsers thus improving model's capacity on (potentially multi-hop) reasoning,\nwhich is critical for generating structure-rich SQLs. To this end, we propose a\nnew architecture GRAPHIX-T5, a mixed model with the standard pre-trained\ntransformer model augmented by some specially-designed graph-aware layers.\nExtensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5\nacross four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5\nsurpass all other T5-based parsers with a significant margin, achieving new\nstate-of-the-art performance. Notably, GRAPHIX-T5-large reach performance\nsuperior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6%\non execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and\n1.5% on EX.",
    "num_pages": 10
}