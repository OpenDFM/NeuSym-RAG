{
    "uuid": "49329a9c-736a-5aa7-a0e2-b91cf6e3b2c1",
    "title": "Extractive Opinion Summarization in Quantized Transformer Spaces",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2020,
    "authors": [
        "Stefanos Angelidis",
        "Reinald Kim Amplayo",
        "Yoshihiko Suhara",
        "Xiaolan Wang",
        "Mirella Lapata"
    ],
    "pdf_url": "http://arxiv.org/pdf/2012.04443v1",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2020\\49329a9c-736a-5aa7-a0e2-b91cf6e3b2c1.pdf",
    "bibtex": "@misc{angelidis2020extractiveopinionsummarizationinquantized,\n    title = {Extractive Opinion Summarization in Quantized Transformer Spaces},\n    author = {Stefanos Angelidis and Reinald Kim Amplayo and Yoshihiko Suhara and Xiaolan Wang and Mirella Lapata},\n    year = {2020},\n    eprint = {2012.04443},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL},\n    url = {http://arxiv.org/abs/2012.04443},\n}",
    "abstract": "We present the Quantized Transformer (QT), an unsupervised system for\nextractive opinion summarization. QT is inspired by Vector-Quantized\nVariational Autoencoders, which we repurpose for popularity-driven\nsummarization. It uses a clustering interpretation of the quantized space and a\nnovel extraction algorithm to discover popular opinions among hundreds of\nreviews, a significant step towards opinion summarization of practical scope.\nIn addition, QT enables controllable summarization without further training, by\nutilizing properties of the quantized space to extract aspect-specific\nsummaries. We also make publicly available SPACE, a large-scale evaluation\nbenchmark for opinion summarizers, comprising general and aspect-specific\nsummaries for 50 hotels. Experiments demonstrate the promise of our approach,\nwhich is validated by human studies where judges showed clear preference for\nour method over competitive baselines.",
    "num_pages": 16
}