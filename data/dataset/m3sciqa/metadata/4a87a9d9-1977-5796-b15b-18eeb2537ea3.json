{
    "uuid": "4a87a9d9-1977-5796-b15b-18eeb2537ea3",
    "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers",
    "conference": "arxiv",
    "conference_full": "ArXiv",
    "volume": null,
    "year": 2022,
    "authors": [
        "Ming Ding",
        "Wendi Zheng",
        "Wenyi Hong",
        "Jie Tang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2204.14217v2",
    "pdf_path": "data\\dataset\\m3sciqa\\papers\\arxiv2022\\4a87a9d9-1977-5796-b15b-18eeb2537ea3.pdf",
    "bibtex": "@misc{ding2022cogview2fasterandbettertexttoimage,\n    title = {CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers},\n    author = {Ming Ding and Wendi Zheng and Wenyi Hong and Jie Tang},\n    year = {2022},\n    eprint = {2204.14217},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV},\n    url = {http://arxiv.org/abs/2204.14217},\n}",
    "abstract": "The development of the transformer-based text-to-image models are impeded by\nits slow generation and complexity for high-resolution images. In this work, we\nput forward a solution based on hierarchical transformers and local parallel\nauto-regressive generation. We pretrain a 6B-parameter transformer with a\nsimple and flexible self-supervised task, Cross-modal general language model\n(CogLM), and finetune it for fast super-resolution. The new text-to-image\nsystem, CogView2, shows very competitive generation compared to concurrent\nstate-of-the-art DALL-E-2, and naturally supports interactive text-guided\nediting on images.",
    "num_pages": 15
}