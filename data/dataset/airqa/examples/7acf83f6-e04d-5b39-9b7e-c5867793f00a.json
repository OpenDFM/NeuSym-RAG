{
    "uuid": "7acf83f6-e04d-5b39-9b7e-c5867793f00a",
    "question": "Which model gains the highest acc in the Table 1 under the dataset of XNLI in the paper 'InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training', and in the original paper of this model, how many layers the author designes in the base size?",
    "answer_format": "Your answer should be a Python list of 2 strings, the name of the model, and the number of layers of this model's base size.",
    "tags": [
        "multiple",
        "table",
        "objective",
        "text"
    ],
    "anchor_pdf": [
        "9b44dc24-27be-52e4-b766-cc01787f167f"
    ],
    "reference_pdf": [
        "284e8b01-876b-5318-9e0d-c6bc486f3f9d"
    ],
    "conference": [],
    "reasoning_steps": [],
    "evaluator": {
        "eval_func": "eval_structured_object_exact_match",
        "eval_kwargs": {
            "gold": [
                "InfoXLM",
                "12"
            ],
            "ignore_order": true,
            "lowercase": true
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "human"
}