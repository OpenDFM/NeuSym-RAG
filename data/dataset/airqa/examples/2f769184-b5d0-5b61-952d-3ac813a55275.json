{
    "uuid": "2f769184-b5d0-5b61-952d-3ac813a55275",
    "question": "What assumption does Deja Vu make to accelerate LLM inference? According to the source paper of the subsequent work PowerInfer, what is the key challenge of Deja Vu?",
    "answer_format": "Your answer should be a python list of two strings",
    "tags": ["multiple","subjective","text"],
    "anchor_pdf": ["ef22ae5d-6800-5707-b8af-6c8984e27c8a","deb2d033-fbed-5014-93b1-528996e5c9cc"],
    "reference_pdf": [],
    "conference": [],
    "reasoning_steps": [
        "First, locate Introduction in the source paper of Deja Vu",
        "Find the assumption made to accelerate LLM inference",
        "Next, turn to the source paper of PowerInfer",
        "Locate Section 2.2",
        "Find the key challenge mentioned about Deja Vu."
    ],
    "evaluator": {
        "eval_func": "eval_conjunction",
        "eval_kwargs": {
            "eval_func_list": [
                "eval_reference_answer_with_llm",
                "eval_reference_answer_with_llm"
            ],
            "eval_kwargs_list": [
                {
                    "reference_answer": "Contextual sparsity exists given any input.",
                    "question": "What assumption does Deja Vu make to accelerate LLM inference?"
                },
                {
                    "reference_answer": "The key challenge with DejaVu in such contexts stems from the need to frequently transfer activated neurons from the CPU to the GPU during runtime.",
                    "question":"According to the source paper of the subsequent work PowerInfer, what is the key challenge of Deja Vu?"
                }
            ]

        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "human"
}