{
    "uuid": "9a8224a4-c359-553a-b334-e1339c47a8a7",
    "question": "What are the main differences between the backbone model used in the main experiments of the paper \"VISION-BY-LANGUAGE FOR TRAINING-FREE COMPOSITIONAL IMAGE RETRIEVAL\" and the standard Transformer?",
    "answer_format": "Your answer should be a python strings.",
    "tags": [
        "multiple",
        "text",
        "subjective"
    ],
    "anchor_pdf": [
        "069d474f-f4de-5eb8-a6a9-97aad73f0e74"
    ],
    "reference_pdf": [
        "6ea15eac-4060-5f1d-8629-76330e0b67c5"
    ],
    "conference": [],
    "reasoning_steps": [
        "Find the backbone model used in the main experiments of the paper \"VISION-BY-LANGUAGE FOR TRAINING-FREE COMPOSITIONAL IMAGE RETRIEVAL\".",
        "Locate the relevant paper about the backbone model.",
        "Retrieve the main difference or core features of the backbone model."
    ],
    "evaluator": {
        "eval_func": "eval_scoring_points_with_llm",
        "eval_kwargs": {
            "scoring_points":[
                "The input of the backbone model is a sequence of flattened 2D patches x_p\\in \\mathbb{R}^{N\\times (P^2 \\cdot C)}.",
                "We prepend a learnable embedding to the sequence of embedded patches (z^0_0 = x_{class}), whose state at the output of the Transformer encoder (z^0_L) serves as the image representation y."
            ],
            "question":"What are the main differences between the backbone model used in the main experiments of the paper \"VISION-BY-LANGUAGE FOR TRAINING-FREE COMPOSITIONAL IMAGE RETRIEVAL\" and the standard Transformer?"
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "human"
}