{
    "uuid": "e832b0e3-17f5-50a1-abbb-e600da0c3c95",
    "question": "Someone told me that a human aligned large language model will be destroyed by fine-tuning and incur toxic statements or safety issues. I wonder whether this conclusion can be proved in any published paper in ICLR 2024? And if the fine-tuning data does not contain any harmful information, can we still guarantee the safety or preserve the safety score of the model?",
    "answer_format": "Your answer should be a Python list of length 2. The first one is the most relevant paper title (str), and the second one is a boolean value (`True` or `False`).",
    "tags": [
        "retrieval",
        "text",
        "objective"
    ],
    "anchor_pdf": [],
    "reference_pdf": [],
    "conference": [
        "iclr2024"
    ],
    "reasoning_steps": [],
    "evaluator": {
        "eval_func": "eval_conjunction",
        "eval_kwargs": {
            "eval_func_list": [
                "eval_paper_relevance_with_llm_and_reference_answer",
                "eval_bool_exact_match"
            ],
            "eval_kwargs_list": [
                {
                    "question": "Someone told me that a human aligned large language model will be destroyed by fine-tuning and incur toxic statements or safety issues. I wonder whether this conclusion can be proved in any published paper in ICLR 2024? And if the fine-tuning data does not contain any harmful information, can we still guarantee the safety or preserve the safety score of the model?",
                    "reference_answer": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
                },
                {
                    "gold": false
                }
            ]
        }
    },
    "state": {
        "gui-gpt-4o-2024-11-20": false
    },
    "annotator": "human"
}