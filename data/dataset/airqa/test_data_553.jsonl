{"uuid": "00608f20-e3f5-5fdc-8979-4efeb0756d8e", "question": "What distance function and transfer function do the author use for their method?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Table 2 and get the final answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The author use distance function $d(u,v) = -u^Tv/\\tau$ and transfer function $f(h) = \\frac{h}{||h||}$ for their method.", "question": "What distance function and transfer function do the author use for their method?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["16f7e6e6-6bfa-5c74-9c8b-1adc5bf7e3b9"], "reference_pdf": []}
{"uuid": "00b28687-3ea1-5974-a1ec-80d7f6cd3424", "question": "What datasets were used to train the default embedding model for the retriever in the experiment of the anchor PDF?", "answer_format": "Your answer should be a python list of the dataset names, e.g. [\"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["7e09164d-5713-56a1-8210-d92aa98d512a"], "reference_pdf": ["413e7de9-03c4-5c1f-9e42-cd48030c9369"], "conference": [], "reasoning_steps": ["Retrieve the default embedding model for the retriever in the experiment section of the anchor PDF.", "Locate the relevant paper about the model.", "Identify the datasets on which the model was trained, which are usually mentioned in the dataset section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Wikipedia", "CCNet"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "0100f339-d8b0-5277-a73f-e0b3f6b10d0c", "question": "For the dataset where \"Before we dive into the answer.\" performs the best in specific setting in anchor paper, what deficiencies of previous datasets were raised by the authors of the dataset? Additionally, what variations did the authors propose to address these deficiencies?", "answer_format": "Your answer should be a Python list of 2 elements, the first is a string, the deficiencies, the second is a python list of strings, the categories of variations in general as proposed in the paper.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["0f6aee28-1439-5d69-9173-7d21f9bb0daa"], "reference_pdf": ["e05cbd04-192e-5761-97ce-7250058cf895", "ad5ecb28-5270-5e7b-b161-6d994db6c2f7", "a87a7490-623a-54af-bad6-ef68b0757499", "c2c5bf1a-3d4a-508e-a217-b3e4b78ce7f7"], "conference": [], "reasoning_steps": ["Read the sections to find the correspondence of prompts and indexs.", "Find the table that compares different prompts on different datasets.", "Identify the dataset where the prompt performs the best.", "Read the corresponding paper to find the deficiencies and variations."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_reference_answer_with_llm", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"reference_answer": "A large number of the problems in ASDiv-A and MAWPS can be correctly answered without even looking at the question. This suggests the presence of patterns in the bodies of MWPs in these datasets that have a direct correlation with the output equation.", "question": "What deficiencies of previous datasets were raised?"}, {"gold": ["Question Sensitivity", "Reasoning Ability", "Structural Invariance"], "ignore_order": true, "ignore_blank": true, "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "01db3056-b961-59bf-8b58-8b8ee0c70060", "question": "Which paper first published a real-world Chinese-English text image translation dataset?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first published a real-world Chinese-English text image translation dataset?", "reference_answer": "Exploring Better Text Image Translation with Multimodal Codebook"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0212a0ea-5029-52d9-bd26-cdcf61a1ff42", "question": "How can we get the bias attribution without skill knowledge according to formula (2)?", "answer_format": "Your answer should be a Python strings of the calculation method of bias attribution without skill knowledge.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["Find the formula (2) in the Methods section and the surrounding text to understand the calculation method of the bias attribution.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The bias attribution without skill knowledge can be calculated by the difference of between the the attribution score between the output text ^y and the golden label text y. Because A(i,x,^y)(h) includes skill knowledge in addition to biased knowledge since estimating the biased text also contains the knowledge of language modeling, such as understanding instruction knowledge. Therefore, we should disentangle the skill knowledge to compute the clean bias attribution.", "question": "How can we get the bias attribution without skill knowledge according to formula (2)?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["2b003f7e-a995-57d1-af78-78432ed96561"], "reference_pdf": []}
{"uuid": "0318f9e2-625a-5fab-8933-cb1b817faee5", "question": "In the paper \"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads\", on which two benchmarks was the long text capability tested? Which institutions proposed the Retrieval head used in this paper?", "answer_format": "Your answer should be a python list of two items. The first item is a python list of two strings, the two benchmarks. The second item is a python list strings, the institutions.", "tags": ["multiple", "objective", "text", "metadata"], "anchor_pdf": ["cd0d4b90-0516-5e63-9e18-2ede1bcc6dda", "287db02d-6d1e-5059-89f8-eb6973610a6b"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Section 3.2 Long-context Benchmarks and extract the two benchmarks.", "Turn to the source paper of the Retrieval head.", "Locate the beginning of the paper to extract the meta data, which will provide the institutions associated with the paper."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_element_list_included", "eval_element_list_included"], "eval_kwargs_list": [{"gold": ["Needle-in-a-Haystack", "LongBench"], "ignore_order": true}, {"gold": ["MIT", "Tsinghua University", "SJTU", "University of Edingburgh", "NVIDIA"], "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "041a256e-75f2-5b75-9edb-2077b7779235", "question": "What is the formula of the loss function used to align the feature spaces of the visual and text transformers in the anchor PDF?", "answer_format": "Your answer should be a python strings about the exact formula given in the reference paper, you don't need to explain the variables in the formula, e.g., \"loss_formula\".", "tags": ["multiple", "subjective", "text", "formula"], "conference": [], "reasoning_steps": ["Find the section mentioned the loss function used to align the feature spaces of the visual and text transformers in the anchor PDF.", "Locate the respective paper about the loss function", "Find the definition formula of the loss function in the reference paper."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": "\\mathcal{L}_{itc} = \\frac{1}{2} \\mathbb{E}_{(I,T)\\sim D}[H(\\mathbf{y}^{i2t}(I), \\mathbf{p}^{i2t}(I)) + H(\\mathbf{y}^{t2i}(T), \\mathbf{p}^{t2i}(T))]", "question": "What is the formula of the loss function used to align the feature spaces of the visual and text transformers in the anchor PDF?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["13a0c782-cda2-55db-b796-550f810c68c8"], "reference_pdf": ["5649e82f-57f5-5b42-960a-98ddc7716d45"]}
{"uuid": "04f6fcad-edd9-577c-b089-ae167567ef47", "question": "What is the most appropriate evaluation metric for this paper?", "answer_format": "Your answer should be a python strings of the exact name of the evaluation metric.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Find the corresponding sections about evaluation metric.", "Compare different evaluation metrics to find the most appropriate one based on the context."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "refinement", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["cc28a219-3ac4-5614-ba45-59d6aabf1af4"], "reference_pdf": []}
{"uuid": "05548a18-0a57-54e2-a7d6-58a5a8cdca72", "question": "Is there any paper that performs adversarial training on frame level for audio-visual representation learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that performs adversarial training on frame level for audio-visual representation learning?", "reference_answer": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "05729273-17c6-5641-8197-1b1f7ccd4b86", "question": "Which paper first use the attention weights to guide the simultaneous inference of speech translation models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first use the attention weights to guide the simultaneous inference of speech translation models?", "reference_answer": "Attention as a Guide for Simultaneous Speech Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0612502c-5e7f-59c3-83d6-bd4a867c22d7", "question": "Why the anchor PDF proposes a new Bias Score? What's the formula of the original Score?", "answer_format": "Your answer should be Python list of two strings, the first is the reason why the authors propose a new Bias Score. The second is the formula in LaTeX format.", "tags": ["multiple", "text", "table", "formula", "subjective"], "anchor_pdf": ["2b640ff7-e466-55c9-821a-4a8e20189660"], "reference_pdf": ["2ef41545-f0f2-57cc-adc4-1313b5e4875a", "4cd8cb70-48e8-51f7-ab8c-2ee1fc05ee56"], "conference": [], "reasoning_steps": ["Locate the section that introduces the Bias Score.", "Find on which dataset the Bias Score is used.", "Read the original dataset paper to identity the formula."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_reference_answer_with_llm", "eval_complex_math_formula_with_llm"], "eval_kwargs_list": [{"reference_answer": "The CrowSPairs score represents the percentage of instance pairs in which the stereotypical sentence has lower perplexity than the anti-stereotypical sentence in the total number of instance pairs. Since it is not easy to understand that the scores closer to 50 indicate less bias, we instead utilized the Bias score, which is defined as equal to |CrowSPairs score −50|, to replace the CrowSPairs score. A lower score represents lower bias.", "question": "Why the anchor PDF proposes a new Bias Score?"}, {"formulas": "\\text{score}(S) = \\sum_{i=0}^{|C|} \\log P(u_i \\in U \\mid U_{\\setminus u_i}, M, \\theta)", "question": "What's the formula of the original Score?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "0696ecf8-b39a-5288-b3ee-c6b3dcf2e420", "question": "What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?", "reference_answer": "A fine-grained comparison of pragmatic language understanding in humans and language models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "06e6f397-5d3d-5493-8d94-f40caefc91c1", "question": "Which implicit bias mentioned in the anchor PDF is not addressed in the works that directly overlap with the anchor work, particularly those exploring LLMs' capabilities as evaluators?", "answer_format": "Your answer should be a string, the name of the implicit bias as mentioned in the anchod PDF.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["3d218d94-1aa0-5a70-b23e-accb254141bd"], "reference_pdf": ["16945bc4-e1c2-55bf-8bcd-7203262db0aa", "95c4da59-2aea-5163-9044-3554ca09aa83"], "conference": [], "reasoning_steps": ["Read the section that introduces the benchmark to identify the implicit biases mentioned.", "Read the section that discusses the related works to identify the works that directly overlap with the anchor work.", "Identify the implicit biase mentioned in the anchor PDF that is not addressed in the corresponding works."], "evaluator": {"eval_func": "eval_string_fuzzy_match", "eval_kwargs": {"gold": "Compassion Fade (Naming)", "fuzz_method": "partial_ratio", "ignore_blank": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "079bf850-1cba-5b82-a432-8cfc8e2e28ff", "question": "Which paper presents an easy to implement and high performing method for OOD detection with language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper presents an easy to implement and high performing method for OOD detection with language models?", "reference_answer": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "07f7da2d-c8d9-581d-bf9e-ba8f9a562968", "question": "What are the meanings of different loss items in equation (9)? i.e. $\\mathcal{L}_{c}^{A}$, $\\mathcal{L}_{s}^{A}$, $\\mathcal{L}_{a}^{A}$, and $\\mathcal{L}_{c}^{B}$", "answer_format": "Your answer should be a Python list of four elements, where each of the item represent the meaning of the corresponding loss item.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of equation (9).", "Second, get the related content of each loss item in the paper.", "Third, analyze the meaning of each loss item."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["$\\mathcal{L}_{c}^{A}$: The content reconstruction loss, which ensures that the model can accurately reconstruct the input text from the latent representations.", "$\\mathcal{L}_{s}^{A}$: The stance classification loss, which guides the model to predict the correct stance of the text. It typically uses cross-entropy loss to align predictions with true stance labels.", "$\\mathcal{L}_{a}^{A}$: The aspect span reconstruction loss, which helps the model generate a specific aspect-related text span using a Gaussian negative log-likelihood (NLL) loss.", "$\\mathcal{L}_{c}^{B}$: The swapped reconstruction loss, derived from a swapping autoencoder mechanism. It ensures that the disentanglement is maintained even after swapping aspect embeddings across samples with the same aspect but different stances."], "question": "What are the meanings of different loss items in equation (9)? i.e. $\\mathcal{L}_{c}^{A}$, $\\mathcal{L}_{s}^{A}$, $\\mathcal{L}_{a}^{A}$, and $\\mathcal{L}_{c}^{B}$", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["c51a33ed-cd35-5426-b37e-6864a1b66a35"], "reference_pdf": []}
{"uuid": "082f325a-ee6d-5d63-9ca0-a6953640027e", "question": "What are the main components of ERRA model?", "answer_format": "Your answer should be a python list of strings, every element of the list is the name of the component directly mentionned in this paper.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) or page(s) about the description of the model.", "Then identify the main components of ERRA model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Retrieval Enhancement", "Aspect Enhancement", "Joint Enhancement Transformers"], "lowercase": true, "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["47f98c9d-038d-5a81-a0ac-e28c5c3bee9a"], "reference_pdf": []}
{"uuid": "085fa0be-3252-59dc-b265-959619c6aa8a", "question": "Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?", "reference_answer": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "089afa00-d83b-573e-9b8c-3ffdc81cee46", "question": "In CSCD-NS, what's the base model of the detection model for data selection? How many hours does it roughly take for the base model to reach at least 85 on GLUE?", "answer_format": "Your answer should be a Python list of 2 elements, the first is the name of the model, the second is the number of hours it takes.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["6b99b0a2-fb28-5266-8c2d-6581334ccbbf"], "reference_pdf": ["c4d02102-b1c7-5b72-a414-9c175a49be48"], "conference": [], "reasoning_steps": ["Read the anchor paper to find the base model in the corresponding section.", "Read the reference paper to find the table that discusses different models and setups."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ELECTRA", 96], "ignore_order": false, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "08a9f15f-cf93-57b2-8a07-072ca34906af", "question": "In the Sequence Mixer of the Monarch Mixer, what are the two types of convolution components?", "answer_format": "Your answer should be a python list of two strings", "tags": ["single", "objective", "image"], "anchor_pdf": ["f3c0827e-c512-50bc-91f0-6d5a9e1177b6"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate figure 3 and get the content of figure 3", "Second, from the content of figure 3, focus on the Sequence Mixer part to get the final answer"], "evaluator": {"eval_func": "eval_element_list_included", "eval_kwargs": {"gold": ["short conv", "Monarch long conv"], "element_type": "str", "ignore_blank": false, "threshold": -1, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "08bd3ae0-1594-510d-b67b-bfbd9fb73b56", "question": "What baselines do authors use? And according to Table 2, which baseline can reach the best performance?", "answer_format": "Your answer should a list with two items, the first item is a python list of the name of baselines, and the second item is the name of baseline reaching the best performance, e.g. [[baseline 1, baseline 2, ...], baseline 1].", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["First, get the content of Section 4.", "Second, get the content of Table 2, and get the final answer."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": ["Prompting", "Random", "BM25", "TopK", "TopK + MDL"], "ignore_order": true, "ignore_blank": true}, {"gold": "TopK + MDL", "ignore_blank": true}]}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["3d2fcb43-2cda-5645-99aa-da78c6cfd23f"], "reference_pdf": []}
{"uuid": "0903f58c-f1c2-5b77-8d14-b1ccef36d1a9", "question": "In the background section of the anchor PDF, under the topic Isotropy of PLMs, what is the main problem of PLMs? How is this problem defined in the paper introducing it?", "answer_format": "Your answer should be a python strings about the main problem of PLMs and how it is defined in the paper introducing it.", "tags": ["multiple", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the part about Isotropy of PLMs in the background section of the anchor PDF to identify the main problem of PLMs.", "Locate the reference paper introducing the problem.", "Find the definition of the problem in the reference paper."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The main problem of PLMs is the representation degeneration or anisotropy problem. The learned embeddings occupy a narrow cone in the vector space, which largely limits their expressiveness. Researches find that language models trained with tied input/output embeddings lead to anisotropic word embeddings, and the singular values of the word embedding matrix decay drastically. In other words, except for a few dominating singular values, all others are close to zero.", "question": "In the background section of the anchor PDF, under the topic Isotropy of PLMs, what is the main problem of PLMs? How is this problem defined in the paper introducing it?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["a6c04981-3a2d-5c78-acd2-66485765e32e"], "reference_pdf": ["ff0d0226-2dc4-5a18-9cc9-ec5826c16eb7"]}
{"uuid": "09c643c6-6a5f-5650-ae54-2bed43a55c17", "question": "What paper first adapted ControlNet to generate continuous videos in a training-free manner?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first adapted ControlNet to generate continuous videos in a training-free manner?", "reference_answer": "ControlVideo: Training-free Controllable Text-to-Video Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0a202041-de70-55b6-9aa0-6b6486166582", "question": "Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation?", "reference_answer": "Neural Machine Translation Methods for Translating Text to Sign Language Glosses"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0b0ef576-fe34-5e6a-bfd3-eafba60a82d5", "question": "What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?", "reference_answer": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0b1cad92-b6c6-51a8-b7f2-5c844e572024", "question": "On which language does LLaMA-2 13B with no removal reaches its second highest perplexity?", "answer_format": "Your answer should be a word DIRECTLY FROM THE PDF WITHOUT ANY EXPLANATION.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, locate the table that compares LLaMA-2 perplexity.", "Find the correct column and rows that correspond to the model LLaMA-2 13B with no removal.", "Finally, identify the language that has the second highest perplexity."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Italian", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["a20a1a7a-b335-54ef-82a5-b97be4405604"], "reference_pdf": []}
{"uuid": "0b1dbace-15fd-53b8-bf52-2bf158ceea33", "question": "Which paper first propose to mask positions to pre-train multi-modal document transformer？", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first propose to mask positions to pre-train multi-modal document transformer？", "reference_answer": "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "0d051ca5-f7ba-5ddf-b06a-9bfc12b97e0b", "question": "In Equation (1), what does the interval [:2] mean?", "answer_format": "Your answer should be a Python string, explaining with a brief sentence.", "tags": ["single", "text", "image", "formula", "subjective"], "anchor_pdf": ["3d698feb-d3ae-5d15-b469-580dfae393a3"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read Equation (1) in Section 2.3, find that the interval [:2] is related to the encoder.", "Check the model architecture in Section 2.1, 2.2 or Figure 1.", "Find out the specific meaning of the interval [:2]."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The interval [:2] means that the input to the TIRE module only passes through the first two blocks of the encoder.", "question": "In Equation (1), what does the interval [:2] mean?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "0d42a5b9-4dcb-5ac1-829f-e198d8f942c1", "question": "Which dataset is the downstream task with the largest training set in BigDocs-Bench curated from, and where do the samples originally come from?", "answer_format": "Your answer should be a Python list of two strings, the first is a single word, the name of the dataset, the second is a sentence or a phrase, the source of the samples.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["0ccddb86-7696-59b3-af01-b2df817a0714"], "reference_pdf": ["087bf567-2de0-5bc6-844d-24a7dc515ee6", "54f49a23-5197-5991-bb84-3a6e9a7eafc6"], "conference": [], "reasoning_steps": ["Locate the table that compares the size of downstream tasks.", "Identify the task with the largest training set.", "Read the corresponding section to find the dataset.", "Read the reference papers to find where the samples come from."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "SVG-Stack", "lowercase": true}, {"reference_answer": "These samples come from publicly available repositories on GitHub.", "question": "Where do the samples originally come from?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "0d69dd2c-5163-57c1-8bf0-c468e511724a", "question": "In the english dataset where HGALayoutLM reaches new SOTA, what's the evaluation metric for text recognition? Additionally, what's the formula for that metric?", "answer_format": "Your answer should be a Python list of two strings, the evaluation metric and the formula for that metric. Note that you should output the formula in the LaTex format.", "tags": ["multiple", "text", "table", "formula", "subjective"], "anchor_pdf": ["5d57350d-12be-5f72-a9ac-fff26f9a1a9b"], "reference_pdf": ["2996caf3-f7a5-515a-ba60-091b02f7c9e5", "ca763ccd-4ec8-5b90-9067-ada1af33f8be"], "conference": [], "reasoning_steps": ["Find the dataset where HGALayoutLM reaches new SOTA.", "Find the table that compares different datasets", "Identify the dataset that fulfills the coditions.", "Read the corresponding paper to find the metric and its formula."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_complex_math_formula_with_llm"], "eval_kwargs_list": [{"gold": "Levenshtein similarity", "ignore_blank": true, "lowercase": true}, {"formulas": "S(w_p, w_{gt}) = 1 - \\frac{L(w_p, w_{gt})}{\\max(|w_p|, |w_{gt}|)}", "question": "What's the formula for Levenshtein similarity?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "0d7b0180-0c7e-5eb6-a51f-6d7a473d33f2", "question": "Is there a paper that takes a mixed machine learning and solver based approach to code translation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that takes a mixed machine learning and solver based approach to code translation?", "reference_answer": "GUESS & SKETCH: LANGUAGE MODEL GUIDED TRANSPILATION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "106570b0-0e1a-5055-9e0d-fcc6eb3a1a1b", "question": "Which paper first combines different methods for uncertainty quantification in one?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first combines different methods for uncertainty quantification in one?", "reference_answer": "Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "11dbd6a6-2eb2-59a2-9ef9-4bdc723ba2c0", "question": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?", "reference_answer": "DNA-GPT: DIVERGENT N-GRAM ANALYSIS FOR TRAINING-FREE DETECTION OF GPT-GENERATED TEXT"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "11dbf1bb-b485-5aa7-8f6b-18b518bc6aec", "question": "What's the relationship between MINT and MINT-1T?", "answer_format": "Your answer should be a paragraph, illustrating the relationship between MINT and MINT-1T.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["1e8d36fb-5a9a-5db1-9903-84d146d46376", "09d48a2a-4ad0-5a7f-84ec-557ac57f5830"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read both papers to understand the purposes of MINT and MINT-1T."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["MINT and MINT-1T don't have direct relationship.", "MINT is short for \"multi-turn interactions\", while MINT in MINT-1T is short for \"Multimodal INTerleaved\"."], "question": "Can you explain the different types of machine learning?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "11e16d8d-e642-592e-a8cf-c38bb375630e", "question": "Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?", "reference_answer": "SHEARED LLAMA: ACCELERATING LANGUAGE MODEL PRE-TRAINING VIA STRUCTURED PRUNING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "12a70e18-aa46-5779-bd69-2f3620d7f484", "question": "Which downstream tasks does CLiCoTEA outperform other models in terms of zero-shot performance on the IGLUE benchmark?", "answer_format": "Your answer should be a Python list of string elements, every element is the abbreviation of a downstream task type mentioned in the paper.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, identify downstream tasks relevant to the question.", "Find tables zero-shot performance on the IGLUE benchmark.", "Finally, compare the performances on different tasks to identify where CLiCoTEA outperforms other models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VE", "VR"], "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0d6ea045-b831-520d-9b99-ba22a081a403"], "reference_pdf": []}
{"uuid": "139b4a99-bd26-5162-a087-d19ee079ebd2", "question": "For the four types of evaluation mentioned in the paper, provide their names and corresponding overall score range.", "answer_format": "Your answer should be a Python dictionary, where the keys are the names of the four types of evaluation and the values are the corresponding overall score range. e.g. {\"evaluation1\": [-1, 1], \"evaluation2\": [0, 0.5], ...} . YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["image", "objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section and figure that introduces the four types of evaluation.", "Identify the names of the four types of evaluation and their corresponding overall score range."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"SA-SAQs": [0, 1], "JWP-SAQs": [0.75, 2.5], "SA-MCQs": [0, 1], "JWP-MCQs": [0, 1]}, "ndigits": 2, "tolerance": 0.001, "ignore_order": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["a3a0b636-d0ab-5dab-a2c3-49f666755896"], "reference_pdf": []}
{"uuid": "14225073-8616-578e-bef7-5b63cfdaa994", "question": "In the prompt that Puzzler adopts, what's the exact context of Task Description for Event Detection?", "answer_format": "Your answer should be a raw text.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["4b4c72af-427d-54d6-be22-0e56ec92ba14"], "reference_pdf": ["73ad76d7-eb4b-59a0-ae8f-d5df7afbe505"], "conference": [], "reasoning_steps": ["Find the paper that introduces the prompt that Puzzler adopts.", "Find the full prompt in the appendix of that paper", "Identify the exact context of Task Description for Event Detection."], "evaluator": {"eval_func": "eval_string_fuzzy_match", "eval_kwargs": {"gold": "Task Description: Given an input list of words, identify all triggers in the list, and categorize each of them into the predefined set of event types. A trigger is the main word that most clearly expresses the occurrence of an event in the predefined set of event types.", "fuzz_method": "partial_ratio", "ignore_blank": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "146cb92e-45a7-5146-a88a-7492f9b12047", "question": "What paper proposes breaking down programming problems by predicting the objects that a solution would create?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper proposes breaking down programming problems by predicting the objects that a solution would create?", "reference_answer": "ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "14ae4304-6dbf-539e-8e9a-db0dde3b4959", "question": "How many websites are included in WebArena? Which are they?", "answer_format": "Your answer should be a Python list of 2 elements. The first one is an integer indicating the number of the websites included in WebArena. The second one is a string list storing the website names.", "tags": ["single", "text", "image", "objective"], "anchor_pdf": ["5a2b0d5c-6b51-5bbd-a001-a15f19f65a98"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find a figure showing the environment's architecture or find the section w.r.t. the environment details", "Locate in the figure of the section the parts describing the environment's websites", "Count and emunerate the websites' names in WebArena"], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_int_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": 4}, {"gold": ["e-commerce platform", "social forum platform", "collaborative development platform", "content management system"], "ignore_order": true, "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "158a0302-d656-5006-9ab8-421c8816faf6", "question": "Is there a paper that shows that language models' error distribution is different for unfamiliar entities that is not apparent when models are evaluated on familiar entities alone?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that shows that language models' error distribution is different for unfamiliar entities that is not apparent when models are evaluated on familiar entities alone?", "reference_answer": "Factual or Contextual? Disentangling Error Types in Entity Description Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "161f8248-8832-5bf9-85e7-7cbe5d89d69b", "question": "Is there any paper that uses prompt tuning in multi-answer QA?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that uses prompt tuning in multi-answer QA?", "reference_answer": "Answering Ambiguous Questions via Iterative Prompting"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "16a194ad-0f62-5048-ab0e-9afa26e75c66", "question": "Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?", "reference_answer": "REV: Information-Theoretic Evaluation of Free-Text Rationales"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "16e480a8-eb0f-5ead-9ca2-cd7b4103d6e4", "question": "Which stage of the two stage training mentioned in the paper is the labeled data used in?", "answer_format": "Your answer should be plain text.", "tags": ["single", "subjective", "text"], "anchor_pdf": ["0d99c5cf-d5f7-5fa6-9559-f88731d5fff1"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) or page(s) about the description of the two stage training.", "Then identify which stage the labeled data is used in."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The labeled data is used in the fine-tuning stage", "question": "Which stage of the two stage training mentioned in the paper is the labeled data used in?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "175c78ea-6395-5e79-9bb1-7211e16b8bd6", "question": "In the latest retrieval method that is applied in the experiment of the case study, and that doesn't require re-ranking model, which dataset is used as training data for tool learning?", "answer_format": "Your answer should be a string, the name of the dataset.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["5b327157-f0cb-568f-8181-397707615f40"], "reference_pdf": ["52845774-6019-555a-89ce-3677a2eaea06", "413e7de9-03c4-5c1f-9e42-cd48030c9369"], "conference": [], "reasoning_steps": ["Read the anchor PDF to find the retrieval methods that don't require re-ranking model.", "Identify the latest method.", "Read the corresponding paper to find the dataset used for tool learning."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "ToolLLM", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "1797020c-d6b0-5909-a17b-25688d7bc433", "question": "According to this paper, what type will the question \"How to keep strawberries fresh\" be classified into?", "answer_format": "Your answer should be a single string representing the question type.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Read section titles and locate the section that discusses the question type classification.", "Understand the definition of each question type.", "Identify the question type that the question falls into based on the definition."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "open", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["010e09cf-8f86-5759-8446-0f7e6558997c"], "reference_pdf": []}
{"uuid": "17c12592-2818-50f7-9f7c-e5247e778f58", "question": "Which newest paper about fine-tuning strategies does the anchor paper mention in the troduction? What is the originality of this paper(I mean the paper mentioned in the anchor paper) compared to previous works?", "answer_format": "Your answer should be a single python list, the first element is the string of the paper name, the second element is a string of the originality of this paper.", "tags": ["text", "multiple", "subjective"], "conference": [], "reasoning_steps": ["First, go to the introduction in the anchor paper to identify the newest paper about fine-tuning strategies.", "Second, find the full name of this paper and turn to it.", "Finally, read relevent section of the paper to find its originality."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": ["Delving Deeper into Cross-lingual Visual Question Answering"], "lowercase": true}, {"reference_answer": "This paper is the first to provide a comprehensive analysis of multilingual VQA, with a focus on cross-lingual transfer.", "question": "What is the originality of this paper compared to previous works?"}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["ce10ef41-538a-5958-b599-32431367af83"], "reference_pdf": ["2ba6d2a8-a65f-51d1-9ac2-df80a3e865de"]}
{"uuid": "17ed4a9d-9711-5799-b02a-6b2cdd366288", "question": "Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?", "reference_answer": "Unified Demonstration Retriever for In-Context Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "17fe77cb-688f-52d4-be70-a66eaadc17ff", "question": "What are the main works at the table-level when performing data augmentation on the MMTab dataset in this paper?", "answer_format": "Your answer should be a Python strings about the main works at the table-level when performing data augmentation on the MMTab dataset.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the corresponding sections about the data augmentation on the MMTab dataset.", "Retrieve for the texts discussing the main works of data augmentation at the table-level.", "If there is no clear expression, try to summarize it from the main text."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The main work is to separately design scripts to render table images with three different styles: Web-page (70.8%), Excel (19.4%) and Markdown (9.8%). Fine-grained adjustments such as font type and cell colors are also considered", "question": "What are the main works at the table-level when performing data augmentation on the MMTab dataset in this paper?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["a7f69ed5-2c8a-5892-bf66-c2599380f805"], "reference_pdf": []}
{"uuid": "18b13577-3570-5e5f-be1c-77606cce3cf4", "question": "When the authors use ChatGPT in generating data points, what is the templated prompt?", "answer_format": "Your answer should be raw text directly demonstrating the used prompt template without any other context.", "tags": ["single", "subjective", "table", "text"], "conference": [], "reasoning_steps": ["Locate the position about \"using ChatGPT in generating data points\".", "Find any text or table content that contains the templated prompt, and directly return the raw templated prompt."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "I am creating a dataset and need to generate data that is similar but not identical to the following examples. Here are 5 examples from my dataset:\n1. [Example 1]\n2. [Example 2]\n3. [Example 3]\n4. [Example 4]\n5. [Example 5]\nPlease generate [Specified Number] new data points that are similar in style and structure to these examples but are unique in content. Format the responses as a numbered list, starting from 6 onwards. Each data point should start on a new line and be prefixed with its corresponding number followed by a period and a space.\nFor example:\n6. [New Data Point 1]\n7. [New Data Point 2]\n...", "question": "When the authors use ChatGPT in generating data points, what is the templated prompt?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["09abed2f-a7af-56e5-8a34-3fc5e7130c6a"], "reference_pdf": []}
{"uuid": "192f5d76-b256-57c4-a3e7-1df0fffe30b4", "question": "Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?", "reference_answer": "Submodular Reinforcement Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "198666fc-a067-52c2-b80f-fb804bc80034", "question": "What is the ranking of the average performance of the models compared in the experiment across all languages where each model has a value in the all-language finetuning, from highest to lowest?", "answer_format": "Your answer should be a Python list of elements, each element is a model name string, e.g., [\"model_name 1\", \"model_name 2\", ...].", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the experiment section.", "Identify the finer requirements on all-language finetuning.", "Next, find the relevant table, figure or text that contains the average performance of the models across all languages.", "Finally, calculate or retrieve the exact value to figure out the ranking of the models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["mCLIP", "mCLIP+", "UC2", "M3P"]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["ff651d37-e725-5752-9c38-3361bc54723d"], "reference_pdf": []}
{"uuid": "1a8c2b00-29f9-5a58-83ab-6dbf7061a039", "question": "Which paper considers both weights and activations when pruning large language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper considers both weights and activations when pruning large language models?", "reference_answer": "A SIMPLE AND EFFECTIVE PRUNING APPROACH FOR LARGE LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "1aa41ae8-f867-5928-857a-22d7d028f976", "question": "According to Table 1, which setup (including its corresponding parameters) achieves the highest precision on the test set?", "answer_format": "Your answer should be a string, with the format of 'setup_name: parameter_value'.", "tags": ["single", "table", "objective"], "anchor_pdf": ["0f6af664-5af4-5274-bda9-66734c7fa9ef"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that discusses the ability of CoSA on Object Discovery and Composition", "Second, find all the benchmarks mentioned in this part of text, and count the total number."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "According to Table 1, which setup (including its corresponding parameters) achieves the highest precision on the test set?", "reference_answer": "Roberta: 1500"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "1bc803b2-807d-5218-891c-da60a470cd93", "question": "Which model achieves the highest accuracy of the classification when the training data consists of 512 pairs of FPQ and TPQs in this paper?", "answer_format": "Your answer should be a python string about the name of the best model. You\"d better use the names as they are referred to in the paper.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Usually, the experiment results between models are mentioned in the experiment or result section, especially in the form of tables or figures. Search the correpsonding parts.", "Find all the experiment results of models when the training data consists of 512 pairs of FPQ and TPQs.", "Finally, compare and rank to get the name of the model demonstrating the highest accuracy."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "MACAW-11B", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["529875d6-5189-5c4d-9076-1635a01a862d"], "reference_pdf": []}
{"uuid": "1c0a1908-daee-5d66-95ab-827900fa14c0", "question": "What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?", "reference_answer": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "1cc5d12c-31dd-5e52-92d5-9227e8cfbfcd", "question": "What are the main questions that this paper tries to resolve or answer?", "answer_format": "Your answer should be a Python list of text strings, with each element being one critical problem that this paper analyzes, e.g., [\"question 1\", \"question 2\", ...].", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Usually, the main questions that one work tries to resolve are mentioned in the end of the introduction section or at the beiginning of the experiment section. Search the correpsonding paragraphs of these two parts.", "If the questions are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core questions from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Is it possible to determine data contamination by solely analyzing the inputs and outputs of existing LLMs?", "Do recent GPTs excel in Text-to-SQL tasks in a zero-shot setting both on potentially leaked data and totally unseen one?", "Is data contamination affecting the accuracy and reliability of an existing GPT in Text-to-SQL tasks?"], "question": "What are the main questions that this paper tries to resolve or answer?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["effede2d-ed50-597e-b1fb-d5fc1b6dc554"], "reference_pdf": []}
{"uuid": "1ea15b22-ad6d-584d-80f0-3efa819fc91d", "question": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?", "reference_answer": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "1eab7a9f-66b4-5bd3-9870-e4df2e3192dc", "question": "Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?", "reference_answer": "On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "1fa16ed9-14d2-5abe-901b-3d34ddceceee", "question": "What research first proposed a new kind of cascaded diffusion of a Markov process?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What research first proposed a new kind of cascaded diffusion of a Markov process?", "reference_answer": "RELAY DIFFUSION: UNIFYING DIFFUSION PROCESS ACROSS RESOLUTIONS FOR IMAGE SYNTHESIS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "1faadd0a-1ee9-5541-b4ca-7a0bd3cacc0e", "question": "RRCP is a new pipeline proposed to recognize retrieval complexity, on which complex QA datasets does it significantly outperform the LLM baseline, with a clear improvement in Accuracy or F1 Score of at least 0.1?", "answer_format": "Your answer should be a list of elements, each element is the QA dataset name string, e.g., [\"QA dataset1\", \"QA dataset2\", ...].", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Usually, the comparison between new design and baseline are proposed in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Find the exact values of Accuracy and F1 Score on RRCP and baseline for differnt QA dataset.", "Finally, compare and calculate the values to find the datasets satisfying the requirements of outperforming."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["CWQ", "StrategyQA", "MuSiQue"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["7640a304-546b-5aa9-8111-8a56b9b06861"], "reference_pdf": []}
{"uuid": "2074d015-dc9a-5c20-aeba-2835003f4607", "question": "In the related work mentioned in the Table 1 of the paper Reflect-RL, that is categorized as RL Fine-tuning and that doesn't involve vision modal, what's the token-level probability of a_k?", "answer_format": "Your answer should be a formula in LaTeX format.", "tags": ["multiple", "text", "table", "formula", "subjective"], "anchor_pdf": ["5a11c640-e530-5c9e-b48c-d6130a4c4991"], "reference_pdf": ["f917a6ca-8134-57d1-9a6a-f28930a380d7", "83cda339-482e-5c4c-aeaa-eb7e51dba851"], "conference": [], "reasoning_steps": ["Lcoate table 1 and identify the papers that are categorized as RL Fine-tuning.", "Read the papers to identify the one without vision modal.", "Read the section that introduces the framework to find the corresponding formula."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": "P_{\\text{token}}(a_k \\mid s) = P(w_k^1, \\ldots, w_k^{N_k} \\mid s) = \\prod_{i=1}^{N_k} P(w_k^i \\mid s, w_k^1, \\ldots, w_k^{i-1})", "question": "What's the token-level probability of a_k?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2166da5e-be09-5f2b-a8e9-7fed58ede51d", "question": "According to Table 2, which models perform the highest on each of the 8 tasks of GLUE?", "answer_format": "Your answer should a python list of the name of models reaching highest performance on MNLI, QQP, QNLI, SST-2, STS-B, MRPC, RTE, and CoLA respectively. If two models get the same score, you can use \"and\" to connect their names, e.g. A and B.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["First, get the content of Table.", "Second, identify the models with the highest performance on eight tasks."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SCALEARN++", "SCALEARNUNIFORM and SCALEARNUNIFORM++", "SCALEARNUNIFORM and SCALEARNUNIFORM++", "SCALEARN++", "SCALEARN", "ADAPTERFUSION", "SCALEARN", "SCALEARN++"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["9b06b24b-0afc-5ccb-95fc-c662395d291d"], "reference_pdf": []}
{"uuid": "21ba07ba-2e6d-5200-9764-f40cc4aa3a6d", "question": "In the comparison of APoLLo with SOTA methods on a Base-to-Novel Class Generalization task, what is the title of the source paper of the dataset where APOLLO outperforms MaPLe(previous SOTA) the most?", "answer_format": "Your answer should be a single string.", "tags": ["text", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, find the relevant table about the comparison of APoLLo with SOTA methods on a Base-to-Novel Class Generalization task.", "Second, read the content of the table and identify the dataset where APOLLO outperforms MaPLe(previous SOTA) the most.", "Finally, go to the reference section(probably reference section) to find the title of the source paper of this dataset."], "evaluator": {"eval_func": "eval_string_fuzzy_match", "eval_kwargs": {"gold": "Describing textures in the wild", "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["1f118f96-bf9a-5dac-99ca-06c24f66d8cd"], "reference_pdf": []}
{"uuid": "220ea46c-5777-52dd-a581-54513207a179", "question": "How many thousand conversations are there in the datasets used to train CONVAUG in total?", "answer_format": "Your answer should be a python float with one decimal places.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["259a072c-f5f0-594d-8018-e6fc4d528d07"], "reference_pdf": ["8acb57ed-7324-5327-8d39-f2c041ec6f2d", "a9f80f03-a63b-564a-8789-70b7c7096819"], "conference": [], "reasoning_steps": ["First, find the datasets used to train CONVAUG.", "Then, Locate the relevant paper about the datasets.", "Finally, calculate the total number of conversations in the datasets."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 17.5, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2214bdec-6cf4-5cce-a5fb-b531bb41e777", "question": "Which paper first proposed shared adapter module across layers?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposed shared adapter module across layers?", "reference_answer": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "231728d1-f6b7-5cd5-862e-ee831b2c4ed4", "question": "In the paper that proposes the best-performing model on hMOF evaluated in LLM4Mat-Bench paper, which baseline that performs better than CGCNN both on validation and test sets is not evaluated in the LLM4Mat-Bench paper?", "answer_format": "Your answer should be a string, the name of the baseline.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["1e9a0edd-23ba-5ff3-89bb-4ae4350753be"], "reference_pdf": ["da1d6ccd-43f9-5f7d-888a-084994068ecb"], "conference": [], "reasoning_steps": ["Find the table that compares different models on LLM4Mat-Bench.", "Identify the best-performing model on hMOF.", "Read the corresponding paper to find the table that evaluates different baselines.", "Locate the baselines that perform better than CGCNN both on validation and test sets.", "Compare with the methods applied in LLM4Mat-Bench to find the baseline that is not evaluated."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "ALIGNN", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2351ad69-2ee2-5348-a305-1b7bc5a8fb3a", "question": "Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?", "reference_answer": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "235fcdd4-ea08-51ed-8a01-c9637eecfcab", "question": "Which three VQA benchmarks does the paper use for evaluation? Among the training datasets, which has the largest number of images?", "answer_format": "Your answer should be a list of four strings, the last element is the string of the name of the largest training dataset.", "tags": ["text", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, find the VQA benchmarks used for evaluation in the relevant section.", "Second, find the section or table that lists the training datasets used in the paper.", "Finally, identify the training dataset with the largest number of images."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["InfographicVQA", "ChartQA", "DocVQA", "WKVVQA"], "ignore_order": true, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["2c222763-f33b-5cfa-8897-5d217aaf9142"], "reference_pdf": []}
{"uuid": "23ca6f0a-69de-55f5-9489-c0d7ddd50b18", "question": "Which papers were among the first to explore the task of targeted training data extraction?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which papers were among the first to explore the task of targeted training data extraction?", "reference_answer": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "23cb6726-c7b6-56f0-86bc-4939eac49e1d", "question": "What is the innovation of the formula (6) in this paper?", "answer_format": "Your answer should be a Python strings of innovation of the formula.", "tags": ["formula", "single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the formula (6) in the Methodology section and the surrounding text to understand the meaning of the formula.", "Identify the innovation or just the difference of the formula based on the context and the formula itself."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The innovation of this loss function is that it consists of two parts: ground-truth loss and distillation loss. The ground-truth loss is to use one-hot labels to predict connectives, and LKD is the knowledge distillation loss utilizing the Kullback-Leibler divergence to quantify the difference of output distribution from student's soft predictions to teacher's soft labels, which means the student model S is required to match not only the groundtruth one-hot labels but also the probability outputs of the teacher model T.", "question": "What is the innovation of the formula (6) in this paper?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["7e6c6c6a-f0a6-59ee-8734-af8a912dcf09"], "reference_pdf": []}
{"uuid": "247b6978-be01-50c8-92fb-e27122c244f0", "question": "Is there any paper that explores using only an encoder-only masked language model for open-ended long text generation (such as story generation)?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that explores using only an encoder-only masked language model for open-ended long text generation (such as story generation)?", "reference_answer": "Open-ended Long Text Generation via Masked Language Modeling"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2536a846-15c8-5b2a-bedf-8b878bff149a", "question": "Which institution is the corresponding author of the paper \"DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering\" affiliated with?", "answer_format": "Your answer should be a string containing the exact full name of the institution without changing CAPITALIZATION.", "tags": ["single", "objective", "text", "metadata"], "anchor_pdf": ["92550830-406d-5c07-9eab-5e42dffbd632"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the corresponding author of the anchor paper.", "Find the institution the corresponding author is affiliated with."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "National Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "259a085d-f5b9-5b80-aa31-a9720bad7047", "question": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?", "reference_answer": "When can transformers reason with abstract symbols?"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "25c34c03-3d73-51df-bb4a-ba58f03bab41", "question": "On which benchmark does the author evaluate SnapKV against multiple baseline models? Additionally, what is the number of data for each type of task in the benchmark?", "answer_format": "Your answer should be a Python list containing two elements: the first element should be a string representing the benchmark name, and the second element should be a list of integers indicating the number of data points for each type of task in the benchmark. For example: [\"GLUE\", [100, 200, 300]].", "tags": ["multiple", "text", "image", "objective"], "anchor_pdf": ["7359e988-66ba-5081-9831-29196c891581"], "reference_pdf": ["d6b892b8-cf43-5b62-bde9-48c070c2e5dc"], "conference": [], "reasoning_steps": ["Identify the benchmark used in the paper and examine the baseline models evaluated for each benchmark. Determine which benchmark includes multiple baseline models.", "Locate the reference benchmark paper.", "Determine the number of data for each type of task within the benchmark."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "LongBench", "ignore_case": true}, {"gold": [750, 800, 600, 800, 600, 1000], "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "25c78c8f-a93c-547a-b06a-b46a60ecba87", "question": "Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?", "reference_answer": "Annealing Self-Distillation Rectification Improves Adversarial Training"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "25e00706-c80c-5169-a5e9-9256c5165a89", "question": "What is the method of prefix-tuning, mentioned as the PEFT module, in the method section of the anchor PDF?", "answer_format": "Your answer should be a python string about the method of prefix-tuning.", "tags": ["multiple", "text", "formula", "subjective"], "conference": [], "reasoning_steps": ["Find the part mentioned prefix-tuning in the method section of the anchor PDF.", "Locate the reference paper.", "Find the section of the method of prefix-tuning in the reference paper."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Prefix-tuning prepends a prefix for an autoregressive LM to obtain z = [PREFIX; x; y], or prepends prefixes for both encoder and encoder to obtain z = [PREFIX; x; PREFIX' ; y]. We follow the recurrence relation, except that the prefix are free parameters. Prefix-tuning initializes a trainable matrix Pθ (parametrized by θ) to store the prefix parameters. The language model parameters φ are fixed and the prefix parameters θ are the only trainable parameters. Here, hi (for all i) is a function of the trainable Pθ. When i ∈ Pidx, this is clear because hi copies directly from Pθ. When i \not ∈ Pidx, hi still depends on Pθ, because the prefix activations are always in the left context and will therefore affect any activations to its right.", "question": "What is the method of prefix-tuning, mentioned as the PEFT module, in the method section of the anchor PDF?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0905f55c-e8a3-5931-bd5a-dd9b69146ca1"], "reference_pdf": ["21df0715-990d-58d3-b218-280ac3a84c8f"]}
{"uuid": "25fd4dd0-a865-541f-bcdd-246a56ba36ed", "question": "Both the anchor_pdfs use the Model Performance on EditEval to test their models. What existing models' data do they use in common?", "answer_format": "Your answer should be a python list, each elemet is a string , which refers to a model name.", "tags": ["multiple", "objective", "text", "table"], "anchor_pdf": ["cabc7bed-6a8b-5030-a199-716eac881799", "f9c34aba-31a0-5b67-83a6-3cde37f2aecb"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find Model Performance on EditEval in the two papers.", "Then, compare the modal they used and get the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["T0++", "T0", "PEER-3", "PEER-11", "Tk", "PaLM 2"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "26030580-cffa-5664-bd4d-4f9eab957b98", "question": "In experiments with the similar two-stage framework as BalSum, are there any other available datasets besides the ones used in this paper?", "answer_format": "Your answer should be a python list of elements, each element is the experiment dataset name string, e.g., [\"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["0f17ce7f-7a37-56d7-bc18-a38d4649fef0"], "reference_pdf": ["1e1fc69b-da77-5cec-b254-7c60bf226a84", "ee3b5bf9-4c0f-5db3-ac89-6cde80f4789a", "f462838f-7bca-547d-b577-7c1dc8ff7f64"], "conference": [], "reasoning_steps": ["Find the section mentioned similar two-stage frameworks as BalSum, which is usually in experiment section.", "Locate the respective papers.", "Check these papers to find all the datasets used in the experiments.", "Identify the datasets that are not used in the BalSum paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Reddit TIFU", "NYT"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "26ec953d-2268-577d-a22e-7f8313b800d8", "question": "Which testing dataset in the paper has the largest size(determined by counts of instances)?I want to read its source paper, can you give me the title?", "answer_format": "Your answer should be a list of two strings, the first element is the name of the testing dataset, and the second element is the title of the source paper.", "tags": ["text", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, locate the table about the testing datasets.", "Second, determine the dataset with the most counts of instances and find some information about its source paper(often the author).", "Finally, turn to reference section to find the title of the source paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["GENIA", "The genia corpus: An annotated research abstract corpus in molecular biology domain"], "ignore_order": false, "lowercase": true, "threshold": 90}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["ceae4995-2be0-5b0e-8cd2-014bebec7870"], "reference_pdf": []}
{"uuid": "2719728b-95f0-5418-a64d-6f6a4b9d8e71", "question": "In the two phase pre-training of this paper, what is the phase after the regular pretraining? And for this phase how to obtain sparse contextualized representation?", "answer_format": "Your answer should be a list of two strings, the first element is the name(two words) of the phase, and the second element is the formula in latex format providing useful signal during the second phase of pre-training.", "tags": ["text", "formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, locate the relevant section to find the two phases of the pre-training.", "Second, find the formula in latex format providing useful signal during the second phase of pre-training."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_complex_math_formula_with_llm"], "eval_kwargs_list": [{"gold": ["Knowledge distillation"], "lowercase": true}, {"formulas": "\\min _{\boldsymbol{\\alpha} \\in \\mathbb{R}_{\\geq 0}^{k}} \frac{1}{2}\\left\\|\boldsymbol{h}^{(l)}-\boldsymbol{D} \boldsymbol{\\alpha}\right\\|_{2}^{2}+\\lambda\\|\boldsymbol{\\alpha}\\|_{1}", "question": "For this phase, how to obtain sparse contextualized representation?"}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["be9dbf2b-6279-53cb-a9df-cd5d983b7192"], "reference_pdf": []}
{"uuid": "27413ff9-4f7d-5885-a5ea-79e29a534fa9", "question": "Which paper first found that multilingual models can inference cross-lingual supervision in MLM training by themself?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first found that multilingual models can inference cross-lingual supervision in MLM training by themself?", "reference_answer": "On-the-fly Cross-lingual Masking for Multilingual Pre-training"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "27873950-73ca-554c-be4b-88fc723841e7", "question": "How to calculate $l^{\\prime}(c,c^{\\prime})$ in the equation under Section 3.2?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of the equation under 3.2.", "Second, get the content of Section 3.1 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$l^{\\prime}(c,c^{\\prime}) = \\mathbbm{1}(l(c) \\geq l(c^{\\prime}))$ is a pairwise comparator, which takes a pair of comparisons $(c,c^{\\prime})$ and determines the more consistent one. $l(c)$ evaluates the consistency of $c$, and a higher value of $l(c)$ indicates a greater degree of consistency.", "question": "How to calculate $l^{\\prime}(c,c^{\\prime})$ in the equation under Section 3.2?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["d23ce539-adb7-5709-b341-169c0dcd5871"], "reference_pdf": []}
{"uuid": "27bd3238-0bb7-540a-8e4f-5acc74fe7b92", "question": "In the paper that proposed two existing remote sensing vision-language datasets listed in the VRSBench paper, which method reaches the highest score on area comparison tasks?", "answer_format": "Your answer should be a single word, the name of the method.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["0ecb9fb6-f66a-519b-90de-10b955b8399d"], "reference_pdf": ["59bef33e-66e2-5e21-a026-d8e055da92f1", "3a7ec7eb-f552-5dfa-8801-5b03df2abc46"], "conference": [], "reasoning_steps": ["Find the table that lists all existing datasets.", "Identify the paper that proposed two datasets.", "Identify the corresponding method."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "InstructBLIP", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "27d44cad-3277-5e38-9d8a-87f953efe90f", "question": "Which datasets in the reading comprehension domain are used for instruction tuning datset curation in both FLAN and INTERS?", "answer_format": "Your answer should be a Python list of strings, the abbreviation of the datasets.", "tags": ["multiple", "text", "image", "objective"], "anchor_pdf": ["6cf825a3-6133-57ec-9a68-5789597b122e", "7908763f-3a9d-5ce5-af59-f68888750583"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the figures in each paper that list all the datasets.", "Find the datasets that are both used."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SQuAD", "BoolQ"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2819ea5c-0598-511e-a95f-ce3e567a1b10", "question": "Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?", "reference_answer": "Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "282710e9-2b2e-5e43-82c1-58505f4ee11f", "question": "Which pre-trained model does the Index Generation Framework of the anchor paper use as backbone? What's the architecture of this pre-trained model compared with GPT and BERT?", "answer_format": "Your answer should be a single python list, the first element is a string of the model name, the second element is s string about its special architecture.", "tags": ["text", "image", "multiple", "subjective"], "conference": [], "reasoning_steps": ["First, locate the section about the Index Generation Framework in the anchor paper.", "Second, find the pre-trained model used and its source paper.", "Finally, turn to the source paper and compare the architecture of this model with GPT and BERT."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": ["BART"], "lowercase": true}, {"reference_answer": "For BART, Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a document has been corrupted by replacing spans of text with mask symbols. The corrupted document is encoded with a bidirectional model, and then the likelihood of the original document is calculated with an autoregressive decoder. For fine-tuning, an uncorrupted document is input to both the encoder and decoder, and it uses representations from the final hidden state of the decoder.", "question": "What's the architecture of this pre-trained model compared with GPT and BERT?"}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["7d92e1d8-9216-529b-87bc-34f7508ed2b7"], "reference_pdf": ["f6e91a91-0b1e-5280-8522-a20492033f16"]}
{"uuid": "2a0aa66e-7f7a-5870-b5a3-935855255b31", "question": "Is there any paper that combines causal inference and finetuning for language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that combines causal inference and finetuning for language models?", "reference_answer": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2a25d73b-2f10-5623-8dc5-ff64901b0c82", "question": "Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?", "reference_answer": "SEEKING NEURAL NUGGETS: KNOWLEDGE TRANSFER IN LARGE LANGUAGE MODELS FROM A PARAMETRIC PERSPECTIVE"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2a448d7b-073e-5d05-b1ed-4368558ab1d5", "question": "Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?", "reference_answer": "Adaptive Chameleon or Stubborn Sloth: REVEALING THE BEHAVIOR OF LARGE LANGUAGE MODELS IN KNOWLEDGE CONFLICTS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2abed84f-df45-53f5-8761-12df8c5f8185", "question": "What is BLIP-2' s Trainable Params? Which function will use the BLIP-2 related model in VisualWebArena paper?", "answer_format": "Your answer should be a python list of two strings, the first strings is Trainable Params and the second is a function name.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["fec0ffc9-d5d1-5cb3-ad3a-b39bb7017689", "46cca6ed-363d-5bcf-8b04-6e8f56b1debb"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the paper about BLIP-2 and find its Trainable Params.", "Then, retreive the function in VisualWebArena paper and find the function that uses BLIP-2 related model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["188M", "eval_vqa"]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2baffe53-b50a-51a0-b88d-bf0bc18e1b00", "question": "Accoring to the course plan in this paper, what percentage of the total course time do students spend in lectures?", "answer_format": "Your answer should be a a Python float rounded to two decimal places ranged from 0 to 1", "tags": ["single", "objective", "text"], "anchor_pdf": ["0c14c555-b484-5afa-82df-d48084952085"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the course plan in this paper.", "Second, locate the total course time and the lecture time in the course plan.", "Finally,devide the lecture time by the total course time and round it to two decimal places."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.21, "ndigits": 2, "tolerance": 1e-06}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2cd0cc5e-defb-51aa-b04d-1cfead682bda", "question": "For handling hallucinations with auxiliary models, what is the model they use, and what are the metrics or measures to evaluate semantic similarity of two sentences?", "answer_format": "Your answer should be a Python list of two elements, the first element is the model name string, and the second element is a list of metric names, e.g., [\"model_name\", [\"metric1\", \"metric2\", ...]].", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the section that discusses hallucinations.", "Identify the finer requirements on external or auxiliary models for handling hallucinations.", "Finally, retrieve the context for this subsection to find the model name and the concrete metrics."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["COMET-QE", ["LASER", "LaBSE", "XNLI"]]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["b5062515-e162-5a98-a421-ab84dfe1d930"], "reference_pdf": []}
{"uuid": "2e8bd79d-01b0-5ee1-accf-eed43dc316da", "question": "Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?", "reference_answer": "OMNICONTROL: CONTROL ANY JOINT AT ANY TIME FOR HUMAN MOTION GENERATION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2ee66dfa-7715-5103-8a58-1b372665df07", "question": "Is there any generalizable NeRF paper that disentangles texture and shape?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any generalizable NeRF paper that disentangles texture and shape?", "reference_answer": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "2f769184-b5d0-5b61-952d-3ac813a55275", "question": "What assumption does Deja Vu make to accelerate LLM inference? According to the source paper of the subsequent work PowerInfer, what is the key challenge of Deja Vu?", "answer_format": "Your answer should be a python list of two strings", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["ef22ae5d-6800-5707-b8af-6c8984e27c8a", "deb2d033-fbed-5014-93b1-528996e5c9cc"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Introduction in the source paper of Deja Vu", "Find the assumption made to accelerate LLM inference", "Next, turn to the source paper of PowerInfer", "Locate Section 2.2", "Find the key challenge mentioned about Deja Vu."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_reference_answer_with_llm", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"reference_answer": "Contextual sparsity exists given any input.", "question": "What assumption does Deja Vu make to accelerate LLM inference?"}, {"reference_answer": "The key challenge with DejaVu in such contexts stems from the need to frequently transfer activated neurons from the CPU to the GPU during runtime.", "question": "According to the source paper of the subsequent work PowerInfer, what is the key challenge of Deja Vu?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2f7da671-2337-5c7b-9a25-35c1b996fe80", "question": "In Figure 1 of the anchor PDF, which model has the largest difference in ranking between Fixed Answer and Cloze Prompt? For the dataset that contains the original question, what's the estimated expert-level accuracy?", "answer_format": "Your answer should be a Python list of two strings, the first is the name of the model, as proposed in the figure, the second is the estimated expert-level accuracy, rounding to 1 decimal place, like \"12.3%\".", "tags": ["multiple", "text", "image", "objective"], "anchor_pdf": ["5fe57755-14f1-5ee7-a4b4-ecaba6827045"], "reference_pdf": ["c2c5bf1a-3d4a-508e-a217-b3e4b78ce7f7", "be088b19-03fb-584b-a62d-2ab4b5d7fdd8"], "conference": [], "reasoning_steps": ["Watch figure 1 to answer the first question", "Read the caption of the figure or the other section to find the dataset.", "Read the corresponding paper to find the estimated expert-level accuracy."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Llama2-7b-chat", "89.8%"], "ignore_order": false, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "2fee39d3-e6a7-50d1-918c-3f8a140a47bb", "question": "In Figure 1, the presence of what operation divides the discretization process of continuous speech into two categories?", "answer_format": "Your answer should be a python string.", "tags": ["single", "image", "subjective"], "anchor_pdf": ["4df38ee7-15d9-5448-95fd-c7b37c9d261d"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate Figure 1 in the paper.", "Find out where the pipeline branches and compare the two branches."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "K-means clustering.", "question": "In Figure 1, the presence of what operation divides the discretization process of continuous speech into two categories?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "302c67ba-c324-5ae2-9757-0e05956f17cc", "question": "Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?", "reference_answer": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "30335449-a618-5e66-8c7e-dc0eb81bfaae", "question": "Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?", "reference_answer": "LEGO-PROVER: NEURAL THEOREM PROVING WITH GROWING LIBRARIES"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "31c0e826-57b0-5445-a16d-0e3d4adc46ab", "question": "Of the following three combinations, which reaches the highest pass@1 accuracy on HumanEval and what's the exact accuracy value: Codestral+MGDebugger, Reflexion+LDB(GPT-4), MetaGPT.", "answer_format": "Your answer should be a Python List of 2 elements, the first is the combination and the second is the exact accuracy value, rounded to one decimal places. Note that you should use the same names as in the question.", "tags": ["multiple", "text", "table", "image", "objective"], "anchor_pdf": [], "reference_pdf": ["bafa4ba3-f7e9-5bf2-960d-cb11f11ec138", "460c65d7-a298-5bd3-baa2-dd8683885308", "80a14542-96a0-5a15-a501-959b9007a8b6"], "conference": [], "reasoning_steps": ["Read the tables, images to identify the pass@1 accuracy of the three combinations.", "Notice the specific conditions for different combinations."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Reflexion+LDB(GPT-4)", 96.9], "lowercase": true, "ignore_order": false, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3272942c-5db5-5122-b612-f09332a27a5a", "question": "Is the data in table 5 in the anchor_pdf obtained using the valid dataset or the test dataset?", "answer_format": "Your answer should be a python strings of \"valid\" or \"test\".", "tags": ["multiple", "table", "text", "objective"], "anchor_pdf": ["c43c8e72-1389-54f0-b36e-ab07d569f3e0"], "reference_pdf": ["fe31ca58-7239-5d36-b623-2540d7b9b01a"], "conference": [], "reasoning_steps": ["Locate table 5 in the anchor_pdf.", "Retreive the reference_pdf and find the corresponding table in the reference_pdf.", "Compare the two tables and find the answer."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "test"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "330ec130-4467-529e-a5e3-83d9391863e7", "question": "What dataset does the anchor paper use for Anomaly Detection?How many datasets is this dataset originally consist of?", "answer_format": "Your answer should be a single python list, the first element is a string of the dataset name, the second element is an integer number.", "tags": ["text", "table", "multiple", "objective"], "conference": [], "reasoning_steps": ["First, locate the table about the datasets used.", "Second, find the dataset used for Anomaly Detection and its source paper.", "Finally, find the number of datasets it originally consists of in the source paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["LogHub", 19], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["f8a9f4e4-773f-5a07-8904-d6a265832d5e"], "reference_pdf": ["95266489-4f59-58e1-badf-cbf53131c665"]}
{"uuid": "3337061c-d350-5522-9c68-f810e017a567", "question": "Can we reduce visual tokens in vision transformers right from the beginning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Can we reduce visual tokens in vision transformers right from the beginning?", "reference_answer": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "333e0fbf-b322-5998-939c-cada7786f47a", "question": "Which dataset supports narration generation and temporal localization tasks in Chinese movies?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which dataset supports narration generation and temporal localization tasks in Chinese movies?", "reference_answer": "Movie101: A New Movie Understanding Benchmark"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "33f77112-8775-5066-8bb6-e74f93379410", "question": "In the anchor paper named\"MBIAS: Mitigating Bias in Large Language Models While Retaining Context\", to develop MBIAS, which PEFT(Parameter Efficient Fine-tuning) technique is used to finetune the model?In the paper where this technique is proposed, what's the innovations introduced to save memory?", "answer_format": "Your answer should be a single python list, containing two strings. The first string is the name(abbrievation) of the PEFT technique. The second string is the innovations introduced to save memory in the relevant paper.", "tags": ["subjective", "multiple", "text"], "anchor_pdf": ["e49150c3-c433-515e-be7d-d3dc87048029"], "reference_pdf": ["c7b3bb82-aadc-5ff3-80e5-6a87188c8e20"], "conference": [], "reasoning_steps": ["Firstly, locate and identify the PEFT technique used to finetune the model in the anchor paper.", "Then turn to the source paper of this technique.", "Finally, find the innovations introduced to save memory in the relevant paper(usually in abstract, introduction or conclusion)."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "QLoRA", "lowercase": true}, {"reference_answer": "QLORA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes.", "question": "In the paper where this technique is proposed, what's the innovations introduced to save memory?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "34031849-a464-5cf5-a3f4-c70b6dfb37e8", "question": "Among the papers that proposed PopQA, KBP and ASQA, which one evaluates the most language models? What question does it want to answer by evaluating so many models?", "answer_format": "Your answer should be a Python list of two strings, the first string is the name of the dataset, that evaluates the most models in its paper, and the second string is the question that it wants to answer.", "tags": ["text", "multiple", "subjective"], "anchor_pdf": ["08f0b49d-a02d-5eba-8fa3-51284c90822b", "8ec52878-4dbf-52f2-9062-1225adff8e7b", "ca66eda5-e6f7-5d97-b474-6f515c7754eb"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read the experiment sections of the papers to locate the numbers of models evaluated.", "Read the same section of the paper that evaluates the most models to find the question."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "PopQA", "lowercase": true}, {"reference_answer": "How much factual knowledge is memorized by LMs and what factors affect the memorization?", "question": "What research question does the paper want to answer?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "343baca6-bb8b-55a6-8bb4-8aaa548dc66d", "question": "In the reference paper that proposes the second method to verify the LLMs' outputs introduced in the anchor paper, the method was mainly evaluated on which dataset?", "answer_format": "Your answer should be a string, the name of the main dataset.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["6b5eb663-a966-5a8a-9f29-81c24781e559"], "reference_pdf": ["6bce5c12-7e36-504e-b30f-b5f67d27b0b0", "aadfb703-a64a-56a1-b1b9-87a74f9b19a3"], "conference": [], "reasoning_steps": ["Find the second method in the section that discusses the works about self-reference.", "Read the corresponding paper to find the main dataset."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "MATH", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "34fe12fd-640c-506e-86a2-5ab70a15c11a", "question": "Is there any paper that leverages graph neural network by integrating label information for multi-label low-resource intent classification?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that leverages graph neural network by integrating label information for multi-label low-resource intent classification?", "reference_answer": "Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "34fed469-2cc2-531c-8d93-4e318d5de7c0", "question": "Which datasets are used for Multi-Document QA in this paper?", "answer_format": "Your answer should be python list, each element of the list is a string of the name of a dataset.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, find the section(s) about dataset, especially about Multi-Document QA", "Then identify the Multi-Document QA datasets used in this paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["HotpotQA", "2WikiMultihopQA", "MuSiQue", "DuReader"], "ignore_order": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["d6b892b8-cf43-5b62-bde9-48c070c2e5dc"], "reference_pdf": []}
{"uuid": "354583f4-8367-5e41-b3a6-9b63d9e05e69", "question": "I want to download this paper from the internet.Can you give me a link?", "answer_format": "Your answer should be single string of the link.", "tags": ["metadata", "objective", "single"], "conference": [], "reasoning_steps": ["Find the pdf_url of the specified pdf."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "https://aclanthology.org/2024.acl-long.182.pdf"}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3097e545-7cad-5021-9d16-57938472fc77"], "reference_pdf": []}
{"uuid": "35843fb0-6f14-51a5-a205-2acf0faa83a5", "question": "Which tool is used as the basic verification tool in the anchor_pdf? Can the tool call executable functions in proof mode?", "answer_format": "Your answer should be a python list of two strings. The first string is the name of the tool, and the second string is \"yes\" or \"no\".", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["4fbb082f-f76e-50d3-9155-2810ab4dbfd5"], "reference_pdf": ["9f22287f-4186-5f55-8b2c-620b02f89b82"], "conference": [], "reasoning_steps": ["Read the anchor_pdf and find the basic verification tool.", "Locate the paper about the tool", "Read the pdf and find  the tool' s modes and their properties to get the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Verus", "no"]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "35b4110d-486c-562f-b488-c8a8b417ef82", "question": "Which paper first apply mixture of experts idea to large language models for domain adaptation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first apply mixture of experts idea to large language models for domain adaptation?", "reference_answer": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "37877f34-e27f-5de2-a0ee-ffa5a543a374", "question": "Is there a paper that supports the use of automated coherence metrics in topic model evaluations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that supports the use of automated coherence metrics in topic model evaluations?", "reference_answer": "Large-Scale Correlation Analysis of Automated Metrics for Topic Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "37e98c25-68ba-54c4-9068-596ed64b546d", "question": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?", "reference_answer": "ALIGNSCORE: Evaluating Factual Consistency with A Unified Alignment Function"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "383909ad-dc1d-5f60-ade6-46bea6e7c62b", "question": "In the paper \"Mastering Task Arithmetic: $\\tau$Jp as a Key Indicator for Weight Disentanglement\", what are the names of the datasets used for task addition on vision tasks? Did the paper which proposed baseline \"Linear FT\" use the same datasets for task addition on vision tasks?", "answer_format": "Your answer should be a Python dictionary, containing the names of datasets for the first question and a boolean value for the second question, e.g., {\"datasets\": [\"dataset 1\", \"dataset 2\", ...], \"same_datasets\": true}. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["c2da75c1-ee57-5460-babc-fdf4b7f04009"], "reference_pdf": ["379352ef-2540-5680-821a-2a9ef5ef979f", "7efe0293-9ecd-5386-b1c5-a851c7a0fdf1", "153d1505-a286-5ceb-9858-c272e31a7d7e"], "conference": [], "reasoning_steps": ["Find the section discussing task addition on vision tasks.", "Identify the datasets used for task addition on vision tasks.", "Find the section mentioned the baseline \"Linear FT\".", "Locate the paper which proposed the baseline \"Linear FT\".", "Find the section discussing task addition on vision tasks in that paper and identify the datasets.", "Check if the datasets used for task addition on vision tasks are the same in both papers."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"datasets": ["Cars", "DTD", "SUN397", "EuroSAT", "GTSRB", "MNIST", "SVHN", "RESISC45"], "same_datasets": true}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "38965ab2-4bc0-562a-98bf-805f7a9fc3ee", "question": "Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?", "reference_answer": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "38a692e8-8566-539a-aecd-b3e7df04dbcf", "question": "According to the methods proposed by this paper,how to calculate the bias Scores when aggregating attributions for tokens, instances and instructions respectively?", "answer_format": "Your answer should be a python list of three elements, every element is a formula string in latex format.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["Locate the sections about the aggregation of bias scores.", "Extract formulas that correspond to aggregating tokens, instances and instructions respectively."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["$$\\nB_{i}^{(\\iota,x_{j})}(h)=\\operatorname*{max}_{k}B_{i}^{(\\iota,x_{j},t_{k})}(h)\n$$", "$$\\n\\begin{array}{c}{{B_{i}^{(\\iota,\\mathcal{D})}(h)=\\displaystyle\\sum_{j}^{N}\\alpha^{(\\iota,x_{j})}B_{i}^{(\\iota,x_{j})}(h)}}\\\\ {{\\alpha^{(\\iota,x_{j})}=\\mathcal{P}(\\hat{y_{j}}|\\iota,x_{j})}}\\end{array}\\n$$", "$$\\nB_{i}^{(\\mathbb{Z},\\mathcal{D})}(h)=\\frac{1}{M}\\sum_{\\iota}^{\\mathcal{Z}}B_{i}^{(\\iota,\\mathcal{D})}(h)\\n$$"], "question": "According to the methods proposed by this paper,how to calculate the bias Scores when aggregating attributions for tokens, instances and instructions respectively?", "ignore_order": false}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["2b003f7e-a995-57d1-af78-78432ed96561"], "reference_pdf": []}
{"uuid": "396c566e-ead8-50a4-b00a-d5ca4c432275", "question": "What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?", "reference_answer": "GTA: A GEOMETRY-AWARE ATTENTION MECHANISM FOR MULTI-VIEW TRANSFORMERS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "398ee3a7-26c8-5967-8b5b-196b5d7641b3", "question": "According to Figure 1 in the \"Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research\" paper, for TACL papers based on Spanish, where does their LMs mainly come from?", "answer_format": "Your answer should be a phrase indicating the category DIRECTLY FROM THE PDF WITHOUT ANY MODIFICATION OR EXPLANATION.", "tags": ["image", "multiple", "objective", "table", "text"], "anchor_pdf": ["5e692b45-81a3-5edc-a464-5025866db42a"], "reference_pdf": ["4c29dcbb-73f5-575d-a928-f1029e56d7e5"], "conference": [], "reasoning_steps": ["Firstly, locate and understand Figure 1.", "Locate the Language Class and Main Conference/Journal information in the appendix.", "Decide TACL belongs to which kind of Conference/Journal.", "Try to decide Spanish belongs to which Language Class.", "Locate the examples of Language Class in the \"The State and Fate of Linguistic Diversity and Inclusion in the NLP World\" paper.", "Decide Spanish belongs to which Language Class.", "Return to the \"Shoulders of Giants: A Look at the Degree and Utility of Openness in NLP Research\" paper Figure 1 and locate the corresponding sub-graph and column.", "Finally, answer the question based on the label of the figure."], "evaluator": {"eval_func": "eval_string_fuzzy_match", "eval_kwargs": {"gold": "Used others' artefact + added their own part", "threshold": 95, "ignore_blank": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "39fb54be-7c67-59c2-9179-8cd66ce19bc2", "question": "Considering the performance of ChatDev agent on DSEval-LeetCode benchmark, what is the most common cause of the errors?", "answer_format": "Your answer should be a python list of elements, the first element is the string of the main verdict, the second element is the string of the sub-verdict, e.g., [\"verdict_name\", \"sub-verdict_name\"].", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Usually, the performance results of agents on benchmarks are mentioned in the experiment or result section, especially in the form of tables or images. Search the correpsonding parts.", "Find the error analysis about the performance of ChatDev agent on DSEval-LeetCode benchmark.", "You can also retrieve the reference section for any additional information if you can't find in the main text.", "Finally, find the most common cause of the errors based on the information from the corresponding charts and legends."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Presentation Error", "Index Mismatch"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0fe6d2d4-00e7-596b-a80c-ffe5a6d88b97"], "reference_pdf": []}
{"uuid": "3a357488-48e9-58d5-ab3f-fdb931ab1db1", "question": "What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?", "reference_answer": "VIDEO LANGUAGE PLANNING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3ae37796-7491-5c6f-9d5c-c6f3e358a888", "question": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?", "reference_answer": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3b007244-9a68-5972-b0a9-04691a2dd6d2", "question": "Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?", "reference_answer": "Lifting the Curse of Capacity Gap in Distilling Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3b42e1f2-e150-5216-aeab-44e976e28900", "question": "Which operation on task vectors is employed in the anchor paper named \"Towards Safer Large Language Models through Machine Unlearning\"? Then what's other operations that can be performed on task vectors and their functions according to the paper where this technique is proposed?", "answer_format": "Your answer should be a single string about the operations used in the two papers.", "tags": ["subjective", "multiple", "text"], "anchor_pdf": ["e1e05ea5-d527-5a3c-9e2b-e100a396b97f"], "reference_pdf": ["7811d1d6-a569-5a9e-bf51-2971e3fdc7f8"], "conference": [], "reasoning_steps": ["Firstly, locate the relevant section in the anchor paper to identify the operation employed on task vectors.", "Then turn to the source paper to find the section about the operations that can be performed on task vectors and their functions.(maybe a section like abstract, introduction or conclusion)"], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The anchor paper employs the negation of task vectors to effectively erase the harmful knowledge (to remove undesirable behaviors or unlearn tasks)", "Adding task vectors can also be performed, which leads to better multi-task models,or even improves performance on a single task", "When tasks form an analogy relationship, task vectors can be combined to improve performance on tasks where data is scarce."], "question": "Which operation on task vectors is employed in the anchor paper? Then what's other operations that can be performed on task vectors and their functions according to the paper where this technique is proposed?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3b83e010-75b3-5fa9-a5c1-7f786db8d957", "question": "Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?", "reference_answer": "GROUP PREFERENCE OPTIMIZATION: FEW-SHOT ALIGNMENT OF LARGE LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3bec1f83-7dfa-5650-81b5-f70d0aaf5232", "question": "Among AlpacaEval, MT-Bench and MMLU, which ones collect open-ended questions accross different domains without providing concrete reference answers?", "answer_format": "Your answer should be a python list of 1-3 strings, and the strings should be AlpacaEval, MT-Bench or MMLU.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["9156c181-6be2-5e8a-ba2e-4658dce594e7", "9156c181-6be2-5e8a-ba2e-4658dce594e7", "c2c5bf1a-3d4a-508e-a217-b3e4b78ce7f7"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the paper about AlpacaEval, MT-Bench and MMLU.", "Read the paper to find the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["AlpacaEval", "MT-Bench"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3c712282-2534-5627-84f0-ce1e39212d20", "question": "Is there a paper that uses evolutionary algorithms and neural MT metrics to produce translations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that uses evolutionary algorithms and neural MT metrics to produce translations?", "reference_answer": "Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3c770698-2830-5eea-9b03-3984091527a3", "question": "How many more LLMs are evaluated in ConvBench paper than in MINT paper?", "answer_format": "Your answer should be an integer.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["1cf7ea57-d128-5619-9361-6b35db040c25", "09d48a2a-4ad0-5a7f-84ec-557ac57f5830"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read the experiment section of each paper to calculate the difference."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 0}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3cc9e70a-bd6b-525c-af61-4b66f9ef8a77", "question": "Who is the corresponding author of this paper?", "answer_format": "Your answer should be a python string about the name of the corresponding author.", "tags": ["single", "metadata", "objective"], "conference": [], "reasoning_steps": ["To find the corresponding authors, we need to look at the first page of the paper.", "Pay attention to the superscript symbols next to the author names, which usually indicate the author's affiliation and contribution (e.g., co-first or corresponding authors).", "Find the footnote of the first page to see if there is any explanation of the superscript symbols, such that we can determine the corresponding author."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Dongyan Zhao", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["122bad91-1e5a-554e-bf1b-7f1e375aaf71"], "reference_pdf": []}
{"uuid": "3cdb56d6-cdfc-57d0-8b3d-736aee6fa4c7", "question": "How to initialize $h_{i,l-1}^S$ in Equation (11)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of the Equation (11).", "Second, get the content of section 3.3, and find the definition of $h_i^S$.", "Finally, get the content of Section 3.2, and find the definition of $\\hat{h_i}$."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "To initialize $h_{i,0}^S$ in Equation (11), we start with the output from Equation (8): $h_i^S = \\hat{h}_i + s_i$.\n\nHere, $\\hat{h}_i$ is the output from the Aspect-Aware Attention Module (A3M), and $s_i$ is the sentiment feature obtained by projecting the affective score of word $w_i$ from SenticNet into the same dimensional space as $\\hat{h}_i$. Specifically:\n\n1. For each word $w_i$ in the sentence, obtain its affective score $w_i^S$ from SenticNet.\n2. Project this affective score into the same dimensional space as $\\hat{h}_i$: $s_i = W_S w_i^S + b_S$, where $W_S$ and $b_S$ are learned parameters.\n3. Add the sentiment feature $s_i$ to $\\hat{h}_i$: $h_i^S = \\hat{h}_i + s_i$\n\nThis $h_i^S$ serves as the initial node representation $h_{i,0}^S$ for the Aspect-Guided Graph Convolutional Network (AG-GCN). Therefore, for the first layer $l=0$: $h_{i,0}^S = h_i^S$.", "question": "How to initialize $h_{i,l-1}^S$ in Equation (11)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["6a24d7f4-430d-5c92-b259-f62f76490147"], "reference_pdf": []}
{"uuid": "3d3d6314-7069-5382-b942-830f22b0b94c", "question": "Which conference was the paper 'Fact-Checking Complex Claims with Program-Guided Reasoning' published in? Is it a long paper, a short paper or findings?", "answer_format": "Your answer should be a Python list of two elements, the first element is the abbreviation of the conference name (including the year), e.g. EMNLP 2022, and the second element is the type of this paper, i.e. long paper, short paper or findings.", "tags": ["metadata", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the metadata of this paper.", "Second, according to key 'conference', 'year' and 'volume', get the answer"], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ACL 2023", "long paper"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["ed5d7873-1891-50ad-a358-d976054e12f7"], "reference_pdf": []}
{"uuid": "3dc8318e-3f56-5ba9-8542-f845aad5e8a8", "question": "What are some methods for solving the class-incremetal continual learning problems?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What are some methods for solving the class-incremetal continual learning problems?", "reference_answer": "Rehearsal-free Continual Language Learning via Efficient Parameter Isolation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "3e1ea082-261d-549f-88fe-2bfe6e9d7b4c", "question": "Which meta learning-based baseline is used in the paper named \"Can We Continually Edit Language Models?On the Knowledge Attenuation in Sequential Model Editing\"? What's the full name of this baseline according to the paper where it's proposed?", "answer_format": "Your answer should be a single python list containing two strings, the first element of the list is the abbreviation of the baseline, the second element of the list is the full name of this baseline, e.g.[\"MAML\",\"Model-Agnostic Meta-Learning\"].", "tags": ["objective", "multiple", "text"], "anchor_pdf": ["e951485d-ebe5-5d66-bc8b-4f8baba6caef"], "reference_pdf": ["44c58240-57f2-5f7c-b511-e44337f6a5af"], "conference": [], "reasoning_steps": ["Firstly, locate and identify the meta learning-based baseline used in the anchor paper.", "Then turn to the source paper of this baseline.", "Finally, locate the relevant section in the source paper to extract the information about the full name of this baseline."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["MEND", "Model Editor Networks with Gradient Decomposition"], "lowercase": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3f69a7de-fe99-531a-8399-d4cbbb1b8da0", "question": "In the paper \"Puzzle Solving using Reasoning of Large Language Models: A Survey\", which methods mentioned in the paper could be used to help solve the puzzle in figure 1?", "answer_format": "Your answer should be a python list containing names of several methods mentioned in the paper. Each element in the list should only contain the name of ONE method.", "tags": ["single", "objective", "image", "text", "table"], "anchor_pdf": ["8689853d-6fd5-5394-86e4-f836490e237c"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Identify the category of the puzzle in figure 1.", "Find the table discussing which methods could solve which kind of puzzles.", "Locate the methods in the table that could be used to help solve the puzzle in figure 1."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Few-shot", "Chain-of-Thought", "Inferential Exclusion Prompting", "Hints", "Introduction", "Summarization", "Fine-Tuning"], "ignore_order": true, "lowercase": true, "threshold": 80, "fuzz_method": "token_sort_ratio"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "3fd4d805-ae6c-527d-b6d2-30a18fb0ab12", "question": "On the dataset proposed by this work, how much does the GPT-3.5-turbo model improve its GPT4score after using Graph-CoT?", "answer_format": "Your answer should be a single float number ranging from 0 to 100, representing the subtraction result.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Find the name of the dataset proposed by this work.", "Locate the experiment table related to the proposed dataset that demonstrates the GPT4score of the GPT-3.5-turbo model before and after using Graph-CoT.", "Record the two GPT4scores respectively.", "Calculate the subtraction result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 16.81, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "reference_pdf": []}
{"uuid": "3fe5526c-b647-51b0-9abb-6edd43c20f79", "question": "Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?", "reference_answer": "SAFE RLHF: SAFE REINFORCEMENT LEARNING FROM HUMAN FEEDBACK"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4098e496-9c0b-53b7-acf1-5cde707b8f91", "question": "Which paper proposed decomposing the logit update of each of the attention blocks’ inputs to analyze how the context influences the prediction?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposed decomposing the logit update of each of the attention blocks’ inputs to analyze how the context influences the prediction?", "reference_answer": "Explaining How Transformers Use Context to Build Predictions"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "40d036d6-78d8-5a2d-b692-9cb3fb24b3a6", "question": "In Table 2 of the paper \"Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning\", which method performs better between TAALM and Rho-1? In the original paper of Rho-1, what kind of tasks were mainly used to evaluate the Rho-1 method?", "answer_format": "Your answer should be a single python list of two strings, the first element is the name of the method, the second element is the type of tasks", "tags": ["multiple", "table", "text", "objective"], "anchor_pdf": ["8c6e03c9-4862-5560-b472-d9ca689cb0ba", "22a670fd-c1d3-50d9-9c10-7ef49a3a2c24"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Table 2 in the paper \"Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning\" and compare the experimental results", "Second, turn to the source paper of Rho-1", "Finally, locate the abstract and extract the main types of evaluation tasks."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_string_fuzzy_match"], "eval_kwargs_list": [{"gold": "TAALM", "lowercase": false}, {"gold": "Math", "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "412f7530-b194-5aca-8508-22318575e1b2", "question": "According to the expression and physical meaning of formula (2), if I want the weight to be 0.5 right at the middle of the training process, what is the value of parameter s?", "answer_format": "Your answer should be a python float of the exact value of parameter s.", "tags": ["formula", "objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, find the formula (2) in the paper and retrieve the context to understand the physical meaning.", "Then, find the parameters and retrieve the context to understand the effect of parameters on the weight, e.g., the relationship between training process and the parameter t implies t = 0.5.", "Finally, calculate the value of s when the weight is 0.5 at the middle of the training process."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.5, "ndigits": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["996afc36-70e9-5f75-8e9f-6f0e5587c451"], "reference_pdf": []}
{"uuid": "416e608a-d105-5d0f-ad19-c046bc2e8a12", "question": "The paper \"Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic\" proposed a new safety evaluation benchmark. It also mentioned 3 existing safety evaluation benchmarks with papers. In the paper which was preprinted earliest on ArXiv among these 3 papers, which dataset did it construct and how was it constructed?", "answer_format": "Your answer should be brief text giving the dataset's name in the paper and how it was constructed.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["1f33c39c-ea03-5618-935e-206af0fd5f14"], "reference_pdf": ["85b3d5bd-0bbc-5f40-a1c7-6b8fd73e6dca", "df3936bb-33de-54f1-890e-4c08d4b00cc8", "20d260fa-1e33-5f05-b84a-132458d61695"], "conference": [], "reasoning_steps": ["Find the 3 papers referenced in the anchor paper with safety evaluation benchmarks.", "Check \"References\" section to identify the paper which was preprinted earliest on ArXiv among these 3 papers.", "Locate the section discussing the dataset constructed in that paper and how it was constructed."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The dataset constructed is HarmfulQ. It was constructed by recursively prompting LLM to generate harmful questions based on examples, including questions earlier generated, and manually filtering out similar generations.", "question": "The paper \"Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic\" proposed a new safety evaluation benchmark. It also mentioned 3 existing safety evaluation benchmarks with papers. In the paper which was preprinted earliest on ArXiv among these 3 papers, which dataset did it construct and how was it constructed?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "423fc0b3-7a09-5941-a3fb-220ae1d220ff", "question": "In terms of joint models for Hebrew parsing, compared to the new 'flipped pipeline' where decisions are made directly on the whole-token units by expert classifiers, what drawbacks does the model in the anchor paper named \"A truly joint neural architecture for segmentation and parsing\" have?", "answer_format": "Your answer should be a single python string about the drawbacks of the model.", "tags": ["subjective", "multiple", "text"], "anchor_pdf": ["6873b347-ad4a-544e-b24f-cce1668924b4"], "reference_pdf": ["0bc1963c-47f0-5407-848e-223c1da2c0a5"], "conference": [], "reasoning_steps": ["Firstly, locate the two papers the models of which are mentioned in the question.", "Then understand the architecture design of two models respectively", "Compare and analyse the drawbacks of the previous model.", "If possible, find relevant section in each paper mentioning other models to extract useful information for comparison."], "evaluator": {"eval_func": "eval_partial_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The model relies on an external lexicon which dictates the range of linguistic realizations for each word in the language. This creates complications for practical integration of the systems.", "The model parses the text with a single joint morphosyntactic model which suffers in performance, because it entails computation of comprehensive lattices detailing all permutations of all segmentation, morphological, and syntactic possibilities across the whole sentence.", "The joint prediction architecture of the model comes at a high latency cost, because it requires processing so many different permutations at once via an all-encompassing lattice."], "question": "In terms of joint models for Hebrew parsing, compared to the new 'flipped pipeline' where decisions are made directly on the whole-token units by expert classifiers, what drawbacks does the model in the anchor paper have?", "count": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "424196cb-a6e0-5e75-8b3a-379e266bbcfb", "question": "In terms of multilingual lexical specialization for XLM-R, on which task(s) does Babel-FT get the highest score among the three tasks? Please give the full name of the task, not the abbreviation.", "answer_format": "Your answer should be a single python list, every element of the list is a string of the full name of the task.", "tags": ["text", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, go to relevant table about the main results of the experiment.", "Second, read the content of the table to find the task name on which Babel-FT gets the highest score for XLM-R.", "Finally, find the full name of this task if the name in the table is an abbreviation."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "bilingual lexicon induction", "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["c9e569a7-d140-5c56-9051-0ec058334907"], "reference_pdf": []}
{"uuid": "432471a3-12dc-5238-99c0-67b83fe63ce9", "question": "In the LMRL-Gym domain, besides the task mentioned in anchor PDF, what other interactive dialogue tasks are proposed in the paper?", "answer_format": "Your answer should be a python list, each element is the name of the task, e.g.,[\"task1\", \"task2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION AND INCLUDE THE FULL NAMES OF THE TASKS.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["e5838af7-f285-5a91-8e5a-d9d1370f97ab"], "reference_pdf": ["148a4beb-4abf-5a7b-bc09-32f8519520da"], "conference": [], "reasoning_steps": ["Find the part mentioned the LMRL-Gym domain, which is in the Experimental Setup section.", "Retrieve the name of the task mentioned in the anchor PDF.", "Locate the respective paper proposing this LMRL-Gym domain.", "Find the other interactive dialogue tasks proposed in the paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Twenty Questions", "Guess My City"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "43938b52-8259-5777-a088-4faa891a1ba6", "question": "How does the anchor paper formulate the objective of OOD detection?If I want to contact the author(s) of the source paper of this formula, what is the email address I can refer to?", "answer_format": "Your answer should be a single python list, the first element is a formula in latex format, the second element is a string of the email address.Note that there might be multiple possible email addresses, you can choose any one of them.", "tags": ["text", "formula", "multiple", "subjective"], "conference": [], "reasoning_steps": ["First, locate the section about the objective of OOD detection in the anchor paper.", "Second, find the formula used to formulate the objective.", "Finally, turn to the source paper to find the email address of the author(s)."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_complex_math_formula_with_llm", "eval_element_included"], "eval_kwargs_list": [{"formulas": "\\mathcal{H}_{0}: \boldsymbol{x} \\sim \\mathcal{P}_{\text {out }} \\quad \text { v.s. } \\quad \\mathcal{H}_{1}: \boldsymbol{x} \\sim \\mathcal{P}_{\text {in }}", "question": "How does the anchor paper formulate the objective of OOD detection?"}, {"gold": ["az381@cam.ac.uk", "djw1005@cam.ac.uk"]}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["e388e95f-db5f-541a-9a5b-2f2109375f61"], "reference_pdf": ["9523ad64-fad9-5352-bc32-ceb1a8f5adbc"]}
{"uuid": "43ab49eb-020d-5c64-a210-7f0931d39224", "question": "How can I get $h_i$ or $h_j$ in Equation (1)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 3.1, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In Equation (1), $h_i$ or $h_j$ represents the feature representation of the i-th or j-th utterance, respectively. According to the methodology described in the paper, these feature representations are obtained through the following process: The authors use RoBERTa Large as an utterance encoder to extract features from each utterance. For each utterance $u_i$, a special token “[CLS]” is prepended to its tokens, forming an input like $\\{[CLS], w_1,\\dots,w_{ni}\\}$, where $ni$ is the number of tokens in $u_i$. After passing this input through RoBERTa, the output activations corresponding to the “[CLS]” token from the last layer of RoBERTa are extracted. These activations serve as the feature representation $h_i \\in \\mathbb{R}^{d_u}$ of the utterance $u_i$, where $d_u$ is the dimension of the feature representation.", "question": "How can I get $h_i$ or $h_j$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["32d1e04a-e87c-5179-8b13-4ad86585c55f"], "reference_pdf": []}
{"uuid": "44db1f84-1791-509e-91ae-79b2856153ee", "question": "What are the datasets and their metrics used in this paper according to the tables?", "answer_format": "Your answer should be a Python dictionary, e.g., {\"dataset1\": \"metric1\", \"dataset2\": \"metric2\", ...}. YOU MUST USE THE EXACT TEXT AND FULL DATASET NAME FROM THE PAPER WIHOUT CHANGING CAPITALIZATION.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, find all tables in the paper that include dataset and metric.", "Identify the full name of the datasets and metrics from the tables.", "Finally, list the dataset-metric pairs."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Cornell Movie": "Rouge-1", "DailyDialog": "BLEU-1", "CMU_DoG": "Rouge-1", "LIGHT": "unigram-F1", "EmpathicDialogue": "Rouge-1", "ConvAI2": "unigram-F1", "Wizard of Wikipedia": "unigram-F1", "Mutual": "Rouge-L", "CommonsenseDialog": "Rouge-1"}}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "reference_pdf": []}
{"uuid": "4684de7e-9fc6-5bfe-acd6-0b8d0fc97647", "question": "Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?", "reference_answer": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "4697c604-fb77-54a5-9a22-f1e8cf32351e", "question": "According to the anchor paper, what's the best defense for PAIR attack? Additionally, what's the system prompt for the pre-trained language model safety filter?", "answer_format": "Your answer should be a Python list of 2 elements, the first is a string, the name of the defense, the second", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["1affe502-59fa-5851-8b8f-759ac1937c95"], "reference_pdf": ["00c1d25f-b86c-5903-862c-184c70969e85", "f9b87e93-f037-557b-823b-36f9c91d2065", "e59dad76-a2c8-56a7-8a17-8e60ea99f4b0"], "conference": [], "reasoning_steps": ["Locate the table that compares different attacks and defenses.", "Identify the best defense for PAIR attack.", "Read the corresponding paper to find the system prompt."], "evaluator": {"eval_func": "eval_string_fuzzy_match", "eval_kwargs": {"gold": "Your objective is to determine if the user’s request or question is harmful or not. Just answer “harmful” or “not harmful” to each user message. If you cannot provide a definitive answer or the request is unclear, just answer “not harmful”. If the request is too short and incomplete, just answer “not harmful”. Do not write anything else.", "fuzz_method": "partial_ratio", "ignore_blank": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "46be0f62-9897-5042-b3c4-f67bdd0bed89", "question": "Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?", "reference_answer": "ALT-TEXT WITH CONTEXT: IMPROVING ACCESSIBILITY FOR IMAGES ON TWITTER"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "46d8670b-3464-5526-b9f9-d5d48dd5bfa1", "question": "Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?", "reference_answer": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "46ea5bb8-9895-5439-8f45-8e1792b1ec8b", "question": "On the ALFWorld dataset experiments, how much did the success rate improve when the authors used their method compared to the original baseline model?", "answer_format": "Your answer should be a floating-point number with one decimal place.", "tags": ["single", "table", "objective"], "anchor_pdf": ["3ca4cb71-29ee-509e-abfb-cbd14fd93a8e"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the information about the ALFWorld dataset in the experimental section of the paper.", "Then, locate the accuracy rates of the method used by the authors and the original baseline model.", "Finally, calculate the difference to obtain the final result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 3.5}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "47389b0a-23c2-5a87-9ee6-cabde545a2ef", "question": "What're the three types of agents in IBSEN and which agent involves the usage of database?", "answer_format": "Your answer should be a Python list of 2 elements. The first element is a Python list of 3 elements, containing the names of the three types of agents in IBSEN. The second element is a string, indicating the name of the agent that involves the usage of database. e.g. [[\"agent1\", \"agent2\", \"agent3\"], \"agent\"].", "tags": ["objective", "single", "text"], "anchor_pdf": ["00cd0971-99bd-5174-b566-861d6df264e0"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the section that introduces the three types of agents in IBSEN.", "Find the names of the three types of agents.", "Identify the agent that involves the usage of database."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": ["director", "actor", "player"], "ignore_order": true, "lowercase": true}, {"gold": "actor", "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "48471601-0130-52f7-8580-d15b057e1bbf", "question": "Is there any paper that constructs augmented training data based on the entity-to-entity correlations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that constructs augmented training data based on the entity-to-entity correlations?", "reference_answer": "PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks *"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4a2b4ab6-a332-5d58-b58b-b2e8405edf77", "question": "What does formula (3) in this paper mean?", "answer_format": "Your answer should be a Python strings of the detailed explanation of the formula.", "tags": ["formula", "single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the formula (3) in the paper and the surrounding text to understand the context of the formula.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "This formula is to compute dynamic parameters and transform the less important dynamic parameters into input-agnostic static parameters. Specifically, a mask M_i is utilized to indicate whether the i-th element of ˆΘ is dynamic or static. Mi = 1 means the i-th element of ˆΘ is dynamic so we update it through the dynamic function W with input x and dynamic factors Θ. Otherwise, ˆΘ remains the same.", "question": "What does formula (3) in this paper mean?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["f0aab1fb-be5b-5b84-aa0e-a13aa814c7b0"], "reference_pdf": []}
{"uuid": "4a99d14d-69e7-55d7-b6fa-2878ad1a8e50", "question": "Which paper did a comprehensive survey of the code large language model (code LLMs)?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper did a comprehensive survey of the code large language model (code LLMs)?", "reference_answer": "Large Language Models Meet NL2Code: A Survey"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4ab4e4dc-fc8a-5749-a2cf-171f0a0bc4e3", "question": "What are the meanings of $h_i, r_i, t_i$ in Equation (1)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 3.2 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$h_i, r_i, t_i$ represent the $i$-th dimension in the head, relation, and tail representations respectively.", "question": "What are the meanings of $h_i, r_i, t_i$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["a3e3cee1-d140-5dc1-9608-2f1a1d924229"], "reference_pdf": []}
{"uuid": "4b4877cd-4cdc-5d52-ac20-edfaa6dd7e32", "question": "Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?", "reference_answer": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4c29808a-cdfa-5e4b-90ee-318b30636e7c", "question": "Which paper studies how current retrieval systems handle queries which contain multiple constraints?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper studies how current retrieval systems handle queries which contain multiple constraints?", "reference_answer": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4c9a32c4-52df-56cf-bbcf-0a10a18d594f", "question": "According to Table 1, how many times is the average number of tokens for the dataset with the highest average number of tokens versus the one with the least average number of tokens?", "answer_format": "Your answer should be a floating-point number with two decimal places.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["First, get the content of Table 1.", "Second, identify the dataset with the highest average number of tokens and the dataset with the least average number of tokens.", "Finally, calculate the result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 1210.75, "ndigits": 2, "tolerance": 1e-06}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["9c96433d-817e-5dad-a394-d7d80a428ed0"], "reference_pdf": []}
{"uuid": "4ca40740-fa6c-50f2-b417-7a2ebfd0cc22", "question": "I would like to utilize the datasets introduced in the anchor PDF. Could you tell me in which format were the papers in the datasets retrieved from each data source?", "answer_format": "Your answer should be a string, the name of the format, e.g. JSON, HTML, MARKDOWN.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["3e8c5246-a3d8-5d5e-9afc-98df4043e2ae"], "reference_pdf": ["81186251-6f85-50cb-a348-fc859162ba8a"], "conference": [], "reasoning_steps": ["Read the section that introduces the datasets to identify the datasets used.", "Find the paper that first proposed the datasets.", "Identify the format of the papers retrieved from each data source."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "XML", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "4cd78a42-44db-51a7-b895-a2481a526788", "question": "Which acl papers mention 'transfer learning' in their abstracts?", "answer_format": "Your answer should be a Python list, every element of the list is the string of a paper_uuid", "tags": ["retrieval", "metadata", "objective"], "conference": ["acl2023", "acl2024"], "reasoning_steps": ["Find papers with 'transfer learning' in the abstract."], "evaluator": {"eval_func": "eval_element_list_included", "eval_kwargs": {"gold": ["9b06b24b-0afc-5ccb-95fc-c662395d291d", "ef699d3b-ffef-5b18-8527-826110f880fd", "0d6ea045-b831-520d-9b99-ba22a081a403", "03166771-5ae8-57b9-9c10-3120423adc5c", "7e6c6c6a-f0a6-59ee-8734-af8a912dcf09"], "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": [], "reference_pdf": []}
{"uuid": "4da68474-8cf2-5077-aa1d-3b7ae74cc70e", "question": "Is there a paper that applies large language models to visual Raven’s Progressive Matrices?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that applies large language models to visual Raven’s Progressive Matrices?", "reference_answer": "In-Context Analogical Reasoning with Pre-Trained Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4dbe770d-1734-5c99-b16d-af3242b8c0ee", "question": "Give me a paper proposing to circumvent a single-truth target in training generative language models.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Give me a paper proposing to circumvent a single-truth target in training generative language models.", "reference_answer": "Soft Alignment Objectives for Robust Adaptation of Language Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4de3ce4f-4b12-59ea-9141-fe765b6e94b3", "question": "Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?", "reference_answer": "LEARNING TO RELAX: SETTING SOLVER PARAMETERS ACROSS A SEQUENCE OF LINEAR SYSTEM INSTANCES"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4ea66ea8-4a7e-52a2-9c97-c900c9e55da6", "question": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?", "reference_answer": "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4f284188-a3d4-5a9a-a723-4f589f221cdd", "question": "Which paper systematically examed the input mismatch between training and sampling in diffusion models", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper systematically examed the input mismatch between training and sampling in diffusion models", "reference_answer": "ELUCIDATING THE EXPOSURE BIAS IN DIFFUSION MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "4f7ee674-3282-554e-a59c-2f911bd5d9e0", "question": "How many datasets are generated in the source paper of the dataset mainly used in the anchor paper named \"Steering Llama 2 via Contrastive Activation Addition\"?", "answer_format": "Your answer should be a single integer.", "tags": ["objective", "multiple", "text"], "anchor_pdf": ["ca818639-56e7-5e2e-84d6-7cdeddf9dcbc"], "reference_pdf": ["a701b27c-1649-5b32-b066-5ddc1b4e7c07"], "conference": [], "reasoning_steps": ["Firstly, locate and identify the dataset mainly used in the anchor paper.", "Then find the most relevant reference paper about this dataset and locate the section mentioning the number of datasets generated."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 154}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "4fe2e01e-83c6-5121-80fc-7c937e0d73ae", "question": "What paper first uses decoupled workers in distributed RL system?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first uses decoupled workers in distributed RL system?", "reference_answer": "SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "50f6e66a-aa2a-56ee-bb54-d2ade82a95ad", "question": "What success rate does MapGPT(with GPT-4V) achieve on the validation unseen set of the R2R dataset?", "answer_format": "Your answer should be a single float.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) or page(s) about the experiment results.", "If necessary, find the table concerning results on the validation unseen set of R2R dataset.", "Finally, identify the success rate  MapGPT(with GPT-4V) achieves on the specified dataset."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 43.7}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0a412245-e6a6-5d0d-a25d-5b79b8c4faaf"], "reference_pdf": []}
{"uuid": "510b8067-46e8-5783-a8a6-e752132a8a7a", "question": "In the proxy task of exploring lexical semantics in anchor_pdf, how many instances are there in total?", "answer_format": "Your answer should be a python int.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["f0d961d2-d190-5de7-874b-91a05aa91921"], "reference_pdf": ["6cb48d9e-f803-5274-8b12-b6ca17473e50"], "conference": [], "reasoning_steps": ["Find the dataset used to construct the proxy task in the anchor_pdf.", "locate the paper about the dataset.", "Retreive the paper and compute the number of instances in the dataset."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 7466}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "512fd6fd-1c6a-54a9-addf-51e622e99dfe", "question": "In terms of WER values with ASR across the six different methods tested in the paper, how much higher is DD2 compared to NV1?", "answer_format": "Your answer should be a single float number.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the section or figure relevant to WER values with ASR across the six different methods.", "Find WER values for DD2 and NV1.If necessary, view and analyze the figure.", "Finally, calculate how much higher DD2 is than NV1."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.598}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["81b6a6b0-a195-5cae-9e30-137150b64352"], "reference_pdf": []}
{"uuid": "51690cda-38bb-51a8-8c7d-59e8a7f732eb", "question": "Which paper first conducted the positioned error test for the MAUVE metric?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first conducted the positioned error test for the MAUVE metric?", "reference_answer": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "52bc8a41-b87d-56ad-b253-83e0fd05e698", "question": "Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?", "reference_answer": "Model-Based Simulation for Optimising Smart Reply"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "536d890f-e245-5bc4-9265-820664e843d6", "question": "According to Figure 4, when generating Token 11, which tokens will the cache preserve, and what positions will be assigned to them?", "answer_format": "Your answer should be a Python list containing two sublists. The first sublist should list the tokens that the cache preserves. The second sublist should contain the positions assigned to each corresponding token. Example format: [[0, 1, 2], [0, 1, 2]].", "tags": ["single", "image", "text", "objective"], "anchor_pdf": ["2d831b51-a802-51f4-9b55-39ab8c0ade5a"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the corresponding section mentioned in Figure 4.", "Examine Figure 4 for the examples provided, and determine which tokens the cache will preserve during the generation of Token 11.", "Find the section explaining how to determine the positions of the tokens in the cache.", "Identify and record the positions assigned to each of the preserved tokens."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": [0, 1, 2, 3, 8, 9, 10], "ignore_order": true}, {"gold": [0, 1, 2, 3, 4, 5, 6, 7], "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "539593f7-e17a-57d2-9030-b8e6690c27e3", "question": "Among the dataset cited in the anchor_pdf, how many were proposed in 2022?", "answer_format": "Your answer should be a python int.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["79530cb7-2b29-5a81-8604-cba3eac79146", "0edab682-4b9c-5017-a78a-65fa427ee35a"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the datasets used in the anchor_pdf.", "Then, determine which datasets are proposed in 2022."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "541382e2-2866-5c2c-9a53-36c96868b9f1", "question": "Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?", "reference_answer": "Songs Across Borders: Singable and Controllable Neural Lyric Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "541435a6-878e-540f-8b6a-86bf7920dc82", "question": "What is the main design of Auto-GUI framework from the aspects of the encoder, interaction, and decoder?", "answer_format": "Your answer should be a Python list of text strings, with each element being one core stage of this framework, you\"d better use the origin text, e.g., [\"stage 1\", \"stage 2\", ...].", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Usually, the framework of a model is mentioned in the methodology or related work section. Search the corresponding paragraphs of these two parts.", "If the core stages are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core stages from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Encoding: Acquire encoded features from both vision and language inputs. The vision input is encoded by a frozen vision encoder, the language input is encoded by a language encoder.", "Interaction: The encoded vision and language representations are integrated by a single-head self-attention network and a gated fusion", "Decoding: The fused representation is fed to the decoder to generate a chain of future action plans. The target predictions consist of a chain of future action plans and the current action prediction separated by specific prompts"], "question": "What is the main design of Auto-GUI framework?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["648e3d50-375b-5189-b6b0-e0520626716e"], "reference_pdf": []}
{"uuid": "546b830f-aca5-56e1-8ebc-cffda2bd6ad6", "question": "Except WKM, which method performs the best on WebShop? Whether the two methods' papers use the same evaluation datasets or not?", "answer_format": "Your answer should be a Python list of two strings, the first is the abbreviation of the method, the second is either `true` or `false`.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["0d7108da-de5c-5e4c-9865-7c4141672767"], "reference_pdf": ["722fbe98-4e3d-5d07-aea6-e4261418a8c8"], "conference": [], "reasoning_steps": ["Find the table that compares different methods and models.", "Identify the second best method on WebShop.", "Read the corresponding papers to find the evaluation datasets."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ETO", "true"], "ignore_order": false, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "546c4f0c-bbb9-5de6-913b-c1685321039c", "question": "In the anchor PDF, which model-free algorithm applies weakly communicating MDP assumption? What's the algorithm's main contribution in the online setting, regarding the assumption?", "answer_format": "Your answer should be a Python list of two strings, the first string is the name of the algorithm, and the second string is its main contribution.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["0a38545d-1e82-5713-ae3f-157bb8623bc0"], "reference_pdf": ["44ea9e05-6b8f-555d-b68d-ccc7fac68de7", "4f363689-4ccd-5ee7-b03b-ef64fcf1544a"], "conference": [], "reasoning_steps": ["Locate the table that lists the details of the algorithms.", "Identify the algorithm", "Find the corresponding paper", "Read the introduction section to find the contribution."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "UCB-AVG", "lowercase": true, "ignore_blank": true}, {"reference_answer": "Our algorithm is the first computationally efficient model-free method with \\tilde{O}(\\sqrt{T}) regret for weakly communicating MDPs.", "question": "What's the algorithm's main contribution in the online setting, regarding the assumption?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "55534d55-ed7c-5240-96a5-cde7fd739de8", "question": "What're the related domains of this paper according to related works?", "answer_format": "Your answer should be a Python list of strings where each string is a related domain. e.g. [\"domain1\", \"domain2\", ...]", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Locate the related works section of the paper.", "Identify the subsections in the related works section.", "Use the subsections titles as the related domains."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Prompt-based Learning", "Lexical Relation Classification"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["9cf8bd4d-0120-5a3b-b926-1a2d7d7b4f0a"], "reference_pdf": []}
{"uuid": "55c4fae8-375a-53eb-819d-e6d81a7c62ea", "question": "In terms of experimental results when unigrams are used for evaluation, which model gets the highest F1-score among Mbase, Mclf, Mcxt and Mclfcxt? What's its added module compared with Mbase according to figure 2?", "answer_format": "Your answer should be a list of two strings, the first element is the name of the model(chosen from Mbase, Mclf, Mcxt and Mclfcxt), and the second element is the name of the added module presented in figure 2.", "tags": ["figure", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, locate the table about the results when unigrams are used for evaluation.", "Second, find the model getting the highest F1-score among Mbase, Mclf, Mcxt and Mclfcxt.", "Finally, turn to figure 2 to identify the added module of this model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Mcxt", "Preceding Updates"], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["d99f2324-cddc-5bfe-adf4-10c6a05dbeb2"], "reference_pdf": []}
{"uuid": "560cb9c7-cd1b-5574-947b-8a3da732d2e3", "question": "Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training?", "reference_answer": "UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "561f7371-37d2-5940-9171-73472e33cded", "question": "Which core NLP problem is mentioned in the anchor_pdf, and how many class classification problems is it usually structured as?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["283d3b36-27d2-5459-b625-f2496fa4c35f"], "reference_pdf": ["02048feb-ade8-5efc-9465-547e1969410d"], "conference": [], "reasoning_steps": ["Find the core NLP problem mentioned in the anchor_pdf.", "Then search the paper about the core NLP problem.", "Finally, find its structure."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The core NLP problem is Natural Language Inference (NLI), and it  is usually structured as a two or three class classification problem.", "question": "Which core NLP problem is mentioned in the anchor_pdf, and how many class classification problems is it usually structured as?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "56b65fba-a965-5e63-a409-4d834fe2926f", "question": "Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?", "reference_answer": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "56d50d2a-9ade-583d-a3e9-277363538066", "question": "Which paper shows assessment of training instabilities at different levels for language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper shows assessment of training instabilities at different levels for language models?", "reference_answer": "Measuring the Instability of Fine-Tuning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "56f3ff15-de1c-5769-ac00-6218e9d9a0a6", "question": "Among the previous methods applied in the FunCoder's experiments on open-source models, which one was proposed later? Additionally, which datasets was applied in the evaluation of that method, but not in FunCoder?", "answer_format": "Your answer should be a Python list of 2 elements, the first is the name of the method, the second is a python list of strings, the name of the datasets.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["0e060d12-30d9-5e34-b7e3-4874dd94be7b"], "reference_pdf": ["98350979-1991-571f-bd8a-5f8624b832f3", "813a0918-58f6-57fa-aa79-e3065a5ff88a", "7b9d2080-37d8-593b-93e9-abfcb2aead4a", "bafa4ba3-f7e9-5bf2-960d-cb11f11ec138", "460c65d7-a298-5bd3-baa2-dd8683885308"], "conference": [], "reasoning_steps": ["Find the table that compares different methods, notice the limitations.", "Identify the method.", "Read the corresponding paper to find the evaluation datasets.", "Compare the datasets with the ones used in FunCoder."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "CodeT", "lowercase": true}, {"gold": ["APPS", "CodeContests"], "lowercase": true, "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "57082e3d-1fcd-54f5-8985-370723fcc4c2", "question": "Which domain in the GRBench dataset has the most number of questions?", "answer_format": "Your answer should be a single string representing the domain name.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Locate the statistics table for the GRBench dataset.", "Record or calculate the total number of questions in each domain.", "Identify the domain with the highest number of questions."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "academic", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "reference_pdf": []}
{"uuid": "5741c36f-3c84-51e1-80ac-960026dfba12", "question": "According to the results, in which interval of attack budget does the ASR of SCTS saturate? Note that the interval has been indicated directly in the text.", "answer_format": "Your answer should be a python list of two float, e.g [0.35,0.4]", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) about experiment result, especially about attack budget.", "Then identify the interval where the ASR of SCTS saturates."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [0.35, 0.45], "ignore_order": false}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["d0d476c5-880d-5b71-ae01-adc1111550a1"], "reference_pdf": []}
{"uuid": "5752ba6d-a2f0-5672-90c8-919979dd4edf", "question": "Are there any papers that construct convolutional networks which are equivariant with respect to non-compact/non-abelian Lie groups?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any papers that construct convolutional networks which are equivariant with respect to non-compact/non-abelian Lie groups?", "reference_answer": "LIE GROUP DECOMPOSITIONS FOR EQUIVARIANT NEURAL NETWORKS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "58bdb8e3-b1ad-5e0a-9c11-ac8b7bf63570", "question": "On average, how many steps does a solution have in the training set of PRM800K, and how many solutions are provided per question?", "answer_format": "Your answer should be a Python list containing two floating-point numbers, each rounded to two decimal places. The first number represents the steps per solution, and the second number represents the solutions per question. Example format: [1.23, 4.56].", "tags": ["single", "text", "objective"], "anchor_pdf": ["bb9a1e79-91a9-5854-a44f-288b212264d7"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the section that introduces the PRM800K training set.", "Identify the number of step-level labels, solutions, and questions in the training set.", "Calculate the average number of steps per solution and the average number of solutions per question."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [10.67, 6.25]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "59369806-b544-5f82-b668-1bd4b943e892", "question": "What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities?", "reference_answer": "Knowledge Graph-augmented Language Models for Complex Question Answering"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "5960606a-4a02-5726-8048-bc2c52ad726b", "question": "Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?", "reference_answer": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "5be96361-68a3-5b32-8d15-668f306d33e7", "question": "According to the tables about Zero-shot performance, what is the range of accuracy of OPT-125M in different task tests (considering the data tested in all papers)?", "answer_format": "Your answer should be a python list of two floats, rounding to one decimal place, e.g. [12.1, 23.4].", "tags": ["multiple", "objective", "text", "table"], "anchor_pdf": ["b9eb94b0-2d7a-5101-a3b4-8d0ee70b77ca", "3db38bf7-3168-5855-958a-c2fa458d33d8"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the table about Zero-shot performance in the papers.", "Second, find the maximum and minimum values of the data.", "Finally, subtract the two datas."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [25.2, 80.3], "ignore_order": false, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "5c49a736-420a-52b4-8188-ad80f375e948", "question": "From which subset of ExHVV was MemeMQACorpus chosen, and why? How many questions were selected? Also, provide the changes in each role-label for the chosen subset.", "answer_format": "Your answer should be a Python list of 4 elements. The first element is the subset's name. The second element is the reason why the author chose this subset. The third element is an integer, denoting the number of questions chosen. The fourth element is a Python dict, containing role-labels and their corresponding changes, where each change is calculated as the new count minus the old count. e.g. [\"answer1\", \"answer2\", 3, {\"role1\": -2, \"role2\": 3, ...}]", "tags": ["multiple", "subjective", "table", "text"], "anchor_pdf": ["0b35eefa-16ce-5586-9a5b-5ce712108204", "0514e9c9-a396-56cc-be31-2045b166c85d"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate the section that mentioned dataset MemeMQACorpus.", "Find the answer to the first three sub-questions.", "Locate the section that mentioned dataset ExHvv.", "Find the table that lists the distributions of role lables on given subset for both MemeMQACorpus and ExHvv.", "Finally, calculate the differences and form final answer."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm", "eval_int_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "US Politics", "ignore_blank": true, "ignore_order": true, "lowercase": true}, {"reference_answer": "This domain choice is based on diversity in the entity distribution across different roles compared to the other subset (on Covid-19) of ExHVV dataset.", "question": "Why was MemeMQACorpus chosen from the US Politics subset of ExHVV?"}, {"gold": 1880}, {"gold": {"hero": -89, "villain": -628, "victim": -241}}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "5c4be3c8-e4ad-5154-83af-3e2ff896c210", "question": "How many words are in the train splits of the oldest dataset used by GeNE to evaluate language modeling?", "answer_format": "Your answer should be a Python integer.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["2d0a9f7f-6c7d-571d-90f4-8bafdbb97ce3"], "reference_pdf": ["d6b892b8-cf43-5b62-bde9-48c070c2e5dc", "866c3296-5bb8-5010-89e5-89a849f6dda9", "14a49a53-f223-549d-a025-d745f23f1adf"], "conference": [], "reasoning_steps": ["Read the anchor PDF to find the datasets used.", "Locate the oldest dataset for language modeling among them.", "Find the number of words in the train splits of the dataset."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 1973136207}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "5c967488-f464-5ab5-aa13-d1dc6be7e4e2", "question": "Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?", "reference_answer": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "5c98eeb0-ae95-530c-85b4-6e7dc3c12ecf", "question": "For Llama2 on DialogSum, which newly proposed module contributes more to the improvement of performance? How do the authors further explain why that module works?", "answer_format": "Your answer should be a Python list of 2 elements, where the first element is the FULL NAME of the module that contributes more to the improvement of performance, and the second element is a string, explaining why that module works. e.g. [\"module\", \"explanation\"].", "tags": ["single", "subjective", "table", "text"], "anchor_pdf": ["0a2c3d8b-dc16-570c-b354-11797aebe290"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the table that lists the performance of Llama2 on DialogSum with different modules utilized.", "Find the module that contributes more to the improvement of performance.", "Locate the section that explains why that module works.", "Understand the explanation and form the final answer."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "fusion generation", "lowercase": true}, {"reference_answer": "In our main experiments, we use dialogue as the condition for generating the final summary in the FG. To explore the effect of using such condition, we run experiments without using it, where the results are ported in Table 10. It clearly shows that, compared with the models without using the entire dialogue, our approach is able to generate better summaries, which emphasizes the contribution of the entire dialogue, for the reason that it provides global or environmental information to guide FG identifying useful content produced by the experts.", "question": "How do the authors further explain why Fusion Generation (FG) works?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "5cae6dda-4a2d-52ec-b511-953f476c3600", "question": "Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?", "reference_answer": "Revealing Single Frame Bias for Video-and-Language Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "5f2de2c6-fbcd-561a-b7a4-be129671f5db", "question": "On which labeled dataset did the metric AMR not reduce to Acc? On that dataset, which model performs best on the metric AMR?", "answer_format": "Your answer should be a Python list of three elements, the first element is the name of the labeled dataset, the second and third element is the model family and the variant of the model. e.g. [\"answer1\", \"answer2\", \"answer3\"].", "tags": ["objective", "single", "table", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section that includes the relevant metrics.", "Find the labled dataset's name in the section text.", "Locate the table that compares different models.", "Finally, analyse the columns and fetch the best model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VITC-L", "GPT-3.5", "0301"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0da230cb-d487-56fa-9a85-4648f3f1e6c5"], "reference_pdf": []}
{"uuid": "6098fb2b-f951-52c7-8cf9-e17aa7124833", "question": "What is the difference between Equation (1) and Equation (2)?", "answer_format": "Your answer should be text describing the difference.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation 1 and its corresponding paragraphs.", "Second, get the content of Equation 2 and its corresponding paragraphs.", "Finally, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Equation (1) is used in single-vector retrieval models. It calculates the similarity score $s(q, d)$ as the dot product between the encoded vector of the query $v_q = \\eta_Q(q)$ and the encoded vector of the document $v_d = \\eta_D(d)$. In this method, the entire query and the entire document are each represented by a single vector, and their similarity is determined by the cosine of the angle between these two vectors. This approach does not consider token-level interactions, as all token embeddings are pooled into a single vector before the similarity score is computed.", "Equation (2) is used in multi-vector retrieval models, specifically in ColBERT. It calculates the similarity score by considering the interaction between each token in the query and each token in the document. The similarity score $s(q, d)$ is defined as $s(q, d) = \\sum_{i=1}^{N} \\max_{j} v_{qi}^T v_{dj}$, where $v_{qi}$ and $v_{dj}$ are the last-layer contextualized token embeddings of BERT for the i-th token in the query and the j-th token in the document, respectively. This operation, known as MaxSim, exhaustively compares each query token to all document tokens, effectively capturing the most relevant token-level interactions."], "question": "What is the difference between Equation (1) and Equation (2)?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["bbe1cd56-d6c0-5ab7-8f3e-a54ff7489d0b"], "reference_pdf": []}
{"uuid": "60bf1f10-7280-54b7-b364-b7c322b69d51", "question": "Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?", "reference_answer": "Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "60ef8ee5-97de-59a7-8c22-9fa45df8d152", "question": "Which subtask in NADI 2024 was the anchor_pdf related?", "answer_format": "Your answer should be a python string. The string should be \"Subtask 1\", \"Subtask 2\" and so on.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["fcbfe144-254e-5c9f-942c-6f154c8363e0"], "reference_pdf": ["56abb8d5-4bea-5698-b662-a8668eb8abe7"], "conference": [], "reasoning_steps": ["Locate the paper about NADI 2024.", "Find the Subtasks in the paper.", "Read the anchor_pdf and find the corresponding subtask."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Subtask 3"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6167db60-98c3-5ad6-b051-9d79f76e065c", "question": "In the experiment section of the anchor PDF, it is proposed that research shows one evaluation method is better. What desired criteria are these conclusions based on?", "answer_format": "Your answer should be a python list about the criteria, e.g. [\"criterion1\", \"criterion2\"]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "objective", "text"], "conference": [], "reasoning_steps": ["Find the part about the best evaluation method in the experiment section of the anchor PDF, which is usually in the evaluation or metric part.", "Locate the relevant paper about the metric and the conclution.", "Identify the desired criteria that the conclusions are based on, which are usually the subsections."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Metric Monotonicity", "Metric Separability", "Metric Linearity", "Metric Time Efficiency", "Metric Accuracy", "Size Robustness", "Imbalance Robustness"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["b6867c59-3b76-5b78-a1c0-2001c2033f3b"], "reference_pdf": ["739485f1-c217-5e99-86d9-c6d11c570228"]}
{"uuid": "61e20be1-7b19-580f-a86e-2132be450bc3", "question": "Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?", "reference_answer": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "63155a14-fe2e-5eb3-aacf-3a7e97368faf", "question": "Among the tested models, which model performs best on code problems?", "answer_format": "Your answer should be a python string of the name of the model.", "tags": ["objective", "single", "table"], "anchor_pdf": ["1f00c9dd-39b4-5302-8fd4-49f0c3a3d857"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate the table that contains the results of the tested models.", "Then, find the model that performs best on code problems."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Llama3-70B"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "639f4526-9d30-5840-977f-900496bc4b09", "question": "How many datasets are evaluated in the work that the anchor PDF follows to generate each step separately?", "answer_format": "Your answer should be a integer.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["3cd97002-d41b-51e2-921e-aaeb6c037a00"], "reference_pdf": ["2f2e4311-fc9b-5e36-bb18-7c3fee141713"], "conference": [], "reasoning_steps": ["Locate the section that discusses the bench Mistake.", "Find the paper it cites.", "Read the reference paper to identify the number of datasets evaluated."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "63dc113b-0220-5cb4-9bd3-17ba26c310b0", "question": "Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.", "reference_answer": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "646bc801-d082-54bf-b3f0-5437c6fad2be", "question": "On which downstream tasks did the authors experiment with their method, and by how much did it improve compared to the best existing methods?", "answer_format": "Your answer should be a Python dictionary, where the keys represent the downstream tasks on which the authors conducted experiments, and value is the numerical part of a percentage, indicating the improvement compared to the best existing method.. e.g. {\\\"task1\\\": 1.9%, \\\"task2\\\": 3.5%, ...} .", "tags": ["single", "text", "objective"], "anchor_pdf": ["d7638fd4-69e4-5959-b0f5-84a4d53b1e3a"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the authors’ description of their contributions.", "Then, identify the downstream tasks they experimented on and the improvement of their proposed method compared to the best existing methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"question answering": 10.6, "autofill forms": 9.5, "user services": 9.7}, "lowercase": true, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "655b8b31-8ecd-5b34-9bc4-e9816b314c27", "question": "Could you recommend research that assesses how well language learning models, such as ChatGPT, perform in creating reading comprehension tasks for educational software?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Could you recommend research that assesses how well language learning models, such as ChatGPT, perform in creating reading comprehension tasks for educational software?", "reference_answer": "Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "65a648a6-9bea-5467-84fd-2ca01dc52084", "question": "Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?", "reference_answer": "Efficient Planning with Latent Diffusion"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "65c25042-0b5e-5677-8c23-2374a72947c0", "question": "In the existing deblurring dataset compared in GS-Blur paper, that contains both real and synthetic data, what's the average noise level estimated?", "answer_format": "Your answer should be a float, rounding to 4 decimal places.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["1e8b1748-cb55-52a7-b27a-e38ccbee18a7"], "reference_pdf": ["dd0147e0-290b-59ea-8742-0be38bb795c7"], "conference": [], "reasoning_steps": ["Find the table that compares different datasets.", "Identify the dataset that contains both real and synthetic data.", "Read the corresponding paper to find the average noise level estimated."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.7736, "ndigits": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "65c9fe88-46f0-579d-ba7b-ca58ee7c55f2", "question": "Which paper introduces the R-GCN technique into document-level joint entity and relation extraction?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper introduces the R-GCN technique into document-level joint entity and relation extraction?", "reference_answer": "A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "65d3fbf5-5319-5490-9686-537924c3c4ee", "question": "I want to replicate the experiment in this paper. Please list all the datasets and baselines that I should prepare.", "answer_format": "Your answer should be plain text", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the experiment section.", "Identify the datasets and baselines used in the experiment.", "Finally, answer with the list of datasets and baselines."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "1. For datasets, this paper select five challenging logical reasoning benchmarks: (1)LogiQA (2)ProofWriter (3)FOLIO (4)PrOntoQA (5)LogicalDeduction(LD). 2. For baselines, (1)Standard prompting (2)Chain-of-Thought(CoT) (3)Chain-of-Thought with Self-Consistency(CoT-SC) (4)Selection-Inference(SI) (5)LAMBADA (6)Tree-of-Thought(ToT) (7)Cumulative Reasoning(CR).", "question": "I want to replicate the experiment in this paper. Please list all the datasets and baselines that I should prepare."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["4f88e7de-b217-5d6b-a315-a872e927bdfe"], "reference_pdf": []}
{"uuid": "66c5fd15-e82b-5a02-bce6-bb0aab05184f", "question": "Among the datasets of the benchmark that collects CHIP-CDN, what are the evaluation metrics they applied?", "answer_format": "Your answer should be a Python list of strings, containing the names of the evaluation metrics.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["3eb8365d-eb44-5413-adeb-7380c9824e3d"], "reference_pdf": ["09811e9e-5c35-5695-a106-df02aaff357c"], "conference": [], "reasoning_steps": ["Read the anchor PDF to identify the benchmark that collect CHIP-CDN.", "Read the PDF that introduces the benchmark to identify the evaluation metrics applied."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Micro F1", "Macro F1", "Accuracy"], "ignore_order": true, "ignore_blank": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "66e99a9b-0660-5574-b8b6-1a05b76c7396", "question": "What are the two loss functions in Equation (8) means?", "answer_format": "Your answer should a list with two items, representing the meaning of the first and the second loss function respectively.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (8).", "Second, get the content of section 3.3.1.", "Third, get the content of section 2, and find what S2T and S2U mean.", "Finally, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["$\\mathcal{L}_{y}(\\theta)$: This term refers to the loss associated with the text prediction task. It is part of the multi-task learning strategy, where the model is trained to predict the textual output, such as the transcription or translation of the input speech.", "$\\mathcal{L}_{z}(\\theta)$: This term corresponds to the loss for the acoustic unit prediction task. Acoustic units are discrete representations of the speech signal, and this loss helps the model learn to generate these units, which are then used to synthesize the translated speech."], "question": "What are the two loss functions in Equation (8) means?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["85e8f01d-9273-5e20-a37a-fb9e82cf2984"], "reference_pdf": []}
{"uuid": "6746c386-b889-59ad-abed-144cd56101d3", "question": "What's the baseline used in the experiment?", "answer_format": "Your answer should be a plein text DIRECTLY FROM THE PDF.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Locate the experiment section.", "Identify the baseline."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "We employ the released PLATO-v1 model, a pre-trained dialogue generation model based on UniLM, for our experiment.", "question": "What's the baseline used in the experiment?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["613d5129-98bd-5f6b-95d3-22b0a9966455"], "reference_pdf": []}
{"uuid": "678cd86e-af27-585d-b938-e49a7c18e229", "question": "What are all the papers that Aviv Slobodkin has participated in acl2024?", "answer_format": "Your answer should be a Python list, every element of the list is the string of a paper_uuid", "tags": ["retrieval", "metadata", "objective"], "conference": ["acl2024"], "reasoning_steps": ["Find papers with Aviv Slobodkin as one of the authors from the database."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["3097e545-7cad-5021-9d16-57938472fc77", "7eb41d67-59e3-542a-8f03-93ab8f53216a"], "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": [], "reference_pdf": []}
{"uuid": "67f33e3f-646d-5bac-8a18-5080e6a2563e", "question": "How to calculate final loss function(Loss) in this paper?", "answer_format": "Your answer should be a python string, which is a formula in latex format to calculate a parameter.", "tags": ["single", "text", "formula", "subjective"], "anchor_pdf": ["2a5f542a-262f-5b41-80c5-429b7f88e312"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the part mentioned the loss function in the paper.", "Extract formulas related to final loss function from the paper."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["\\textit{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( w_p y_i \\log(\\hat{y}_i) + w_n (1 - y_i) \\log(1 - \\hat{y}_i) \\right) -\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\frac{\\exp(s(y_i)/\\tau)}{\\sum_j \\exp(s(y_j)/\\tau)} \\right)"], "question": "How to calculate final loss function(Loss) in this paper?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "681c2bb8-b4cf-5f2a-bd56-ae26e0bb51b6", "question": "I would like to reproduce the experiments of KnowGPT, could you please provide me with the websites of the datasets applied in the experiment?", "answer_format": "Your answer should be a Python list of 3 strings, the websites. Note that you should provide the original URL as given in the papers that proposed the datasets.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["1ea6baba-2167-5d05-9a40-d142ad39358f"], "reference_pdf": ["1d779e37-9a20-5a90-80c9-a7aaf2b6cfe5", "a87a7490-623a-54af-bad6-ef68b0757499", "6ee7bf32-7948-5ba3-a9b2-571dafb53a37"], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["data.allenai.org/OpenBookQA", "www.tau-nlp.org/commonsenseqa", "github.com/jind11/MedQA"], "threshold": 90, "fuzz_method": "partial_ratio", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "68855b4d-dd5d-5b33-8ddd-61b13b1b6c51", "question": "On cmudog, which one among the linguistics operators appears the most frequently? What's its distribution?", "answer_format": "Your answer should be a Python list of 2 elements. The first element is the linguistic operator\"s name, and the second element is its disrtibution in percent in string format. e.g. [\"answer\", \"5%\"].", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the image that contains the distribution of linguistic operators on cmudog.", "Finally, output the most frequent linguistic operator and its distribution."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VERB_MODIFY", "36%"]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "reference_pdf": []}
{"uuid": "68baa0b9-8e5e-5436-94d5-6dd0b3bbfff0", "question": "On which datasets this study surpassed the SOTA?", "answer_format": "Your answer should be a Python list of dataset, e.g., [\"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT TEXT FROM THE PAPER.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, find all sections in the paper that mentions SOTA.", "Identify the part that compare the performance with SOTA.", "Finally, list all the datasets that surpasse SOTA."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["DailyDialog", "CMU_DoG", "LIGHT", "EmpathicDialogue", "Wizard of Wikipedia", "CommonsenseDialog"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "reference_pdf": []}
{"uuid": "69fb7412-8288-562a-8de7-d7727f689fcf", "question": "What manipulation operations does the manipulation network in the anchor paper allow? For the add network, how is the training loss of relation defined?", "answer_format": "Your answer should be a single python list, the first element of the list is a list of the strings of manipulation operations, the second element of the list is a string of the formula in latex format, e.g. [[\"operation1\", \"operation2\"], \"l_{\\text {relation }}=...\"]", "tags": ["text", "formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, go to relevant section about the modules.", "Second, read the content of manipulation network to find the operations it allows.", "Finally, read the content of add network to find the definition of training loss of relation."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_complex_math_formula_with_llm"], "eval_kwargs_list": [{"gold": ["add", "remove", "change"], "lowercase": true, "ignore_order": true}, {"formulas": "\\ell_{\text {relation}}=-\\log \\left(p\\left(h_{\\mathrm{r}}\\left(\\widetilde{o}_{\text {new }}, o_{i}\right)=r\right)\right)", "question": "For the add network, how is the training loss of relation defined?"}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["0cc6d173-ff62-5ae6-80c5-56bd73ee82a2"], "reference_pdf": []}
{"uuid": "6a72002b-7dcf-55df-8a3c-3cc49ee326a3", "question": "What dataset does the anchor paper use for training? How many event coreference chains does this dataset contain?", "answer_format": "Your answer should be a python list of two strings, the first element is the dataset name(abbrievation), and the second element is an integer number.", "tags": ["text", "multiple", "objective"], "conference": [], "reasoning_steps": ["First, locate the section about the training in the anchor paper.", "Second, find the dataset used for training and its source paper.", "Finally, find the number of event coreference chains it contains in the source paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["MAVEN-ERE", 103193], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["0c4fa692-c546-5576-8020-4a2a41c3c1f7"], "reference_pdf": ["dee7640b-bff4-5af0-a13f-7270194ac651"]}
{"uuid": "6aefdbec-8411-50e8-a9f3-b26afe188083", "question": "In the paper that proposes the only comparable interactive theorem prover applied as a baseline by AIPS, where are the evaluation samples chosen from?", "answer_format": "Your answer should be the a raw text from the papers.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["0e91df50-6832-5615-baf8-af56e93ea272"], "reference_pdf": ["f1c80ac8-4588-586b-bf00-1151edd91acd", "67f92ccd-ca66-5cd5-b6ac-3852a53255e2"], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "We perform experiments on theorems from \"Mathematics in Lean\" : a book for beginners to formalize and prove mathematical theorems in Lean. It has 233 theorem proving exercises, covering topics from sets and functions to topology, calculus, and measure theory. For evaluation, we randomly selected 50 theorems, and their proofs have 5.52 tactics on average.", "question": "In the paper that proposes LeanCopilot, where are the evaluation samples chosen from?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6af99fe6-5e33-5632-8553-aa9d1daaad86", "question": "Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.", "reference_answer": "DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "6b660c4a-c2a0-538f-b42a-bfe6337add99", "question": "Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?", "reference_answer": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "6bb32702-f9f0-53a5-a534-be38bfc75b3f", "question": "In Figure 3, what can we infer from comparing the performance with training data generated by self-training (ST) versus without it?", "answer_format": "Your answer should be a Python strings about the conclusion from comparing the performance with training data generated by self-training (ST) versus without it.", "tags": ["image", "single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the corresponding sections mentioned Figure 3.", "Retrieve for the texts discussing the performance with training data generated by self-training (ST) versus without it.", "If there is no clear expression, try to summarize it from the main text."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Figure 3 shows that self-training is beneficial for each of the languages. The improvement is particularly strong when the teacher model was based on a very small amount of data.", "question": "In Figure 3, what can we infer from comparing the performance with training data generated by self-training (ST) versus without it?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["c1f72e20-0020-59c5-85c5-b1ab703b22b7"], "reference_pdf": []}
{"uuid": "6bdd99b0-3976-5029-9b64-d82b7bfb4276", "question": "What does formula (1) mean in the Methodology section?", "answer_format": "Your answer should be a python strings of the detailed explanation of the formula.", "tags": ["formula", "single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the formula (1) in the Methodology section and the surrounding text to understand the meaning of the formula.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The idea of this formula is decomposing token representations into their constituent vectors based on vector-based approaches. Decompose the i-th token representation in layer l into elemental vectors attributable to each of the N input tokens. So we can compute the norm of the attribution vector of the k-th input to quantify its total attribution to xi.", "question": "What does formula (1) mean in the Methodology section?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["cd91a901-384e-597b-bb20-2d4f9fa0c4f9"], "reference_pdf": []}
{"uuid": "6beb7fc3-96ff-587f-8362-bcd0f709a2e9", "question": "How does the system efficiently adapt to completely unfamiliar opponent policies during deployment, while still maintaining performance with known policies?", "answer_format": "Your answer should be a string.", "tags": ["single", "text", "subjective"], "anchor_pdf": ["8a1e3915-e42d-581e-aa46-9b520f4b03ec"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the part mentioning deployment or adaptation", "Second, find the part mentioning unfamiliar opponent policies and known policies"], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "How does the system efficiently adapt to completely unfamiliar opponent policies during deployment, while still maintaining performance with known policies?", "reference_answer": "The system adapts to completely unfamiliar opponent policies by collecting and accumulating opponent trajectory data in an Opponent-Collecting Window (OCW), which is then sampled and stitched together using the GetOnD function for in-context learning. For known policies, the system quickly re-engages suitable responses by leveraging previously accumulated trajectories, ensuring both fast adaptation to familiar policies and effective extrapolation for unfamiliar ones."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6dc50c47-0782-5277-b10a-e5e427a10223", "question": "What is the first paper that theoretically studies training neural networks under small initialization?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What is the first paper that theoretically studies training neural networks under small initialization?", "reference_answer": "Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "6de72b3a-ac37-5d2d-b870-a61dac353bdb", "question": "Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data?", "reference_answer": "Evaluate AMR Graph Similarity via Self-supervised Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "6df645eb-d78e-56b7-be7a-8712b3ed7a75", "question": "In which four directions can the author's model be trained?", "answer_format": "Your answer should be python list, each element of the list is a string like 'A-to-A', 'A-to-B'.", "tags": ["objective", "single", "text"], "anchor_pdf": ["1dc45e9f-f844-5e78-b762-7784d6e52eb4"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Retrieve the paper and find detailed illustration of the model.", "Find the four directions and get the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["word-to-word", "word-to-definition", "definition-to-definition", "definition-to-word"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6ec30967-a7aa-5ecb-8819-15e738ad4b50", "question": "In the two papers that updated ToMi according to the SimTom paper, what are the other datasets used to evaluate the models, besides ToMi?", "answer_format": "Your answer should be a Python list of strings, containing the name of the datasets.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["2f861e4d-1888-5355-a9c2-26411f14efe4"], "reference_pdf": ["56bb5074-0a00-578b-ad44-e24096458b1e", "1ff930b7-3ecb-50ee-bfb4-777d7d8636ad"], "conference": [], "reasoning_steps": ["Read the anchor PDF to find the two papers that udpateed ToMi.", "Locate the section that describes the datasets used to evaluate the models in the papers.", "Identify the datasets besides ToMi that are used to evaluate the models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ToM", "SocialIQA"], "lowercase": true, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6ee75006-72d3-5d81-b85d-ec25b99ed502", "question": "Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)", "reference_answer": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "6f0ece87-9055-5ad9-9b89-f88c7a19d08f", "question": "For the strongest baseline mentioned in \"TIES-Merging: Resolving Interference When Merging Models\", which benchmark and what tasks were used for NLP in the paper which proposed it?", "answer_format": "Your answer should be a python dictionary with the keys \"benchmark\" and \"tasks\". The value for \"benchmark\" should be a string and the value for \"tasks\" should be a list of strings.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["153d1505-a286-5ceb-9858-c272e31a7d7e"], "reference_pdf": ["7efe0293-9ecd-5386-b1c5-a851c7a0fdf1"], "conference": [], "reasoning_steps": ["Find out which baseline is the strongest baseline in the anchor pdf.", "Locate the paper which proposed the strongest baseline.", "Identify the benchmark and tasks used for NLP in that paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"benchmark": "GLUE", "tasks": ["CoLA", "SST-2", "MRPC", "RTE"]}, "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "6f68d5a6-8a34-55e9-8212-64c01d072d68", "question": "What are the datasets used in the experiments and what are their respective durations? Where can I get these datasets?", "answer_format": "Your answer should be a Python list of 2 elements. The first element is a Python dictionary containing dataset names and respective durations (in hours), and the second element is a Python string containing the answer to the last question, e.g. [{\"dataset1\": 2.1, \"dataset2\": 10.8, ...}, \"answer\"]", "tags": ["single", "text", "subjective"], "anchor_pdf": ["7f3d16e8-d399-5ffd-bae5-7aad916c36eb"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate section 3.1 Experimental Setup.", "Find the datasets used and their respective durations.", "Notice that the beginning of this section says they are internel datasets."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": {"M30S3": 18.5, "M3E6": 21.1, "M30U": 18.2}}, {"reference_answer": "The article indicates that these are internal datasets, thus they may not be currently accessible.", "question": "Where can I get these datasets?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7126cbfb-136e-5a9a-950f-8fc57feda734", "question": "How does the latest Wellness Descriptions dataset used in the anchor pdf address the ambiguity issue in the task of annotating text for wellness dimensions?", "answer_format": "Your answer should be a python strings concisely summarizing the method.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["5b6dd112-3ac4-58e4-8855-39c6b03a3553"], "reference_pdf": ["fd26f57c-13f5-5248-a4d3-0c5a42fe04ac"], "conference": [], "reasoning_steps": ["Find the latest Wellness Descriptions dataset in the anchor PDF.", "Locate the relevant paper in the reference PDF.", "Identify how the dataset addresses the ambiguity issue in the task of annotating text for wellness dimensions, which is mentioned in the Corpus Construction section."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "They address the ambiguity issue by providing a set of perplexity guidelines. The major perplexity guidelines are as follows. 1. Presence of Multiple Aspects: Social media posts often express feelings due to various reasons. To address this, specific text spans contributing to the more focused health consequences can be identified and associated with corresponding wellness dimensions. 2. Annotation Ambiguity: Although there may be multiple aspects in the post, we must consider the holistic aspect as per the experts' opinion. 3. Reading between the lines: The text may contain implicit or subtle hints that suggest a particular wellness dimension. But clear and meaningful words that suggest one of the four wellness dimensions should be annotated accordingly.", "question": "How does the latest Wellness Descriptions dataset used in the anchor pdf address the ambiguity issue in the task of annotating text for wellness dimensions?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7156d9cc-5b02-50d7-bb20-bdcc414b76e4", "question": "Among the diverse interactive domains used to test SOFT-SC, which one is the first parallel interactive text-based and embodied environment?", "answer_format": "Your answer should be a python string, and it should be a diverse interactive domain name.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["feee5f2e-295c-5232-9c77-cb3a13763f65"], "reference_pdf": ["db340dc7-ff3c-591a-81a6-88243bf559df", "75494679-b547-5df0-a83a-75410da0f379", "25ffe76f-9274-56ea-b07f-4086da57bb65"], "conference": [], "reasoning_steps": ["Find the diverse interactive domains", "Find the papers about the diverse interactive domains.", "Retrieve the papers about the diverse interactive domains to find the answer."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "ALFWorld", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7175414d-1ddc-5d5a-b4a6-8a25ba6f2078", "question": "Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?", "reference_answer": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "71fd543c-b0f5-5631-b97d-0c9f7a996a86", "question": "What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?", "reference_answer": "AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7231e809-3ffe-5fb5-84b6-633ba6c788f5", "question": "Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?", "reference_answer": "TALK LIKE A GRAPH: ENCODING GRAPHS FOR LARGE LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7236429c-2845-556e-98c9-886d6a05c384", "question": "How much higher ASRs do user cases with high content freedom yield, compared to those with low content freedom?", "answer_format": "Your answer should be a floating point numbers with one decimal places.", "tags": ["single", "image", "objective"], "anchor_pdf": ["ab9342dc-a363-5a2a-b678-043637e4dd05"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that comparing the effect of high content freedom or low content freedom.", "Second, locate the figure mentioned in the text.", "Finally, read the ASR numbers from the figure and calculate the difference between the two settings."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 18.0, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "72c5b793-458d-5af4-86eb-542f839c023a", "question": "What research has been conducted on incorporating visual data into the text summarization process?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What research has been conducted on incorporating visual data into the text summarization process?", "reference_answer": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "7350bb9b-a510-5614-a994-1d99a6368e57", "question": "In the paper that DRAGIN follows the most in term of template prompt, which models are utilized as the CoT generator of the proposed retriever?", "answer_format": "Your answer should be a string, giving the detailed names of the models, as proposed in the reference paper.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["4c90f20c-5542-5573-96d2-62745066c3ca"], "reference_pdf": ["6d0ea9bc-a7ee-5598-896f-02c44aa42194", "89b83f29-085e-5da6-a5b4-7cba324a4052", "f398b6e5-0ff2-59e4-9f26-eba5cea5b48c"], "conference": [], "reasoning_steps": ["Read the section that discusses the settings of datasets.", "Find the paper that DRAGIN follows the most in term of template prompt.", "Read the section about experiment in that paper to identify specific configuration."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "We experiment with OpenAI GPT3 (code-davinci-002) and Flan-T5 of different sizes as its CoT generator.", "question": "In the paper that DRAGIN follows the most in term of template prompt, which models are utilized as the CoT generator of the proposed retriever?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7369f690-c9b9-52d7-8698-3b38d8c2baf1", "question": "For dataset MultiDialog, what's the number of dialogues,the number of turns, total length in hours, number of speakers, and the name of source dataset of dialogue scripts?Please adopt the statistics the most accurate you can find in the paper.", "answer_format": "Your answer should be a python list,of which the elements are in the following order: number of dialogues(int), number of turns(int), total length in hours(float), number of speakers(int), and the name of source dataset of dialogue scripts(str), every element of the list is an int or float or string representing the relevant dataset information.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Locate the sections or tables in the specified PDF relevant to the basic information of MultiDialog.", "Extract the exact number of dialogues, turns, total length in hours, number of speakers, and source dataset name.", "Summarize the extracted information in the specified order."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [8733, 187859, 339.71, 12, "TopicalChat"], "ignore_order": false, "threshold": 95}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["4beb073a-74b4-563a-82a7-da7513acaff0"], "reference_pdf": []}
{"uuid": "748c93fa-539a-5f0f-887d-746da0323e23", "question": "Which paper first proposed to only update some original weights of self-attention layers in parameter-efficient fine-tuning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposed to only update some original weights of self-attention layers in parameter-efficient fine-tuning?", "reference_answer": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "74d535a9-5796-57d9-81c8-0c68e3c7188d", "question": "How many programming languages in The Stack are selected in the code dataset used for hypernetwork training in the Anchor PDF?", "answer_format": "Your answer should be a single integer.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["e3687846-983e-5174-b918-cf7abd297030"], "reference_pdf": ["6d6f8a4b-0f39-5f6f-9513-678e6f490f84"], "conference": [], "reasoning_steps": ["Find the code dataset used for hypernetwork training in the Anchor PDF, which is usually in the experimental section.", "Locate the relevant reference paper.", "Retrieve the number of programming languages in The Stack that are selected in this dataset, which is usually mentioned in the dataset description."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 86}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "75b9dc7d-abbe-5627-ac3d-649055da6df9", "question": "Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?", "reference_answer": "Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "75c1fd66-8271-5ae8-b45f-c188ae9ccf84", "question": "Which evaluation metric demonstrates the greatest improvement in the finetuned model proposed in this paper compared to GPT baseline?", "answer_format": "Your answer should be a Python string, which is the name of the evaluation metric DIRECTLY FROM THE PDF.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Usually, the comparison results between new models and baseline are mentioned in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Find the exact values of the performance of the new model and the baseline on all evaluation metrics.", "Finally, calculate or compare to get the evaluation metric demonstrates the greatest improvement in the finetuned model compared to GPT baseline."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "sBLEU", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["481d851e-214b-5d6b-af6c-880a1be8f3b9"], "reference_pdf": []}
{"uuid": "75cd4886-f858-506d-ad37-85cc7c605b3f", "question": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?", "reference_answer": "TOME: A Two-stage Approach for Model-based Retrieval"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "765fc890-3100-5b7f-9068-9460147a99cd", "question": "Which article first proposed shuffled-group-whitening to solve the problem of sentence representation learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which article first proposed shuffled-group-whitening to solve the problem of sentence representation learning?", "reference_answer": "WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7696934c-fc83-504d-83d9-3716e13dfd89", "question": "How much does the average performance of the model improve on WMT'19 test sets by replacing one of example-specific prompts with a task-level prompt?", "answer_format": "Your answer should be a single float number ranging from 0 to 100, representing the subtraction result.", "tags": ["objective", "single", "table", "text"], "conference": [], "reasoning_steps": ["Locate the experiment table that shows the model performances on WMT'19 test sets with different distributions of prompts.", "Find out the mathematical symbols indicating the numbers of two types of prompts.", "Locate the two rows in the table related to the question.", "Record the two average performance scores.", "Calculate the subtraction result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.34, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["5a2b95c1-12d6-5b77-82a1-ee24180d27ae"], "reference_pdf": []}
{"uuid": "76aee9c9-711d-5c33-9edd-68f80d3dc1ca", "question": "Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?", "reference_answer": "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "76dc78aa-daa0-5e3a-8377-96072b98e408", "question": "Which PLM method achieve the best bias score in the experiment?", "answer_format": "Your answer should be a single string representing the PLM method.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Locate the main experiment table that compares the bias scores of different PLM methods.", "Determine whether this metric is better when higher or lower.", "Select the PLM method with the best bias score in the experiment."], "evaluator": {"eval_func": "eval_element_included", "eval_kwargs": {"gold": ["PICARD(T5)", "T5(PICARD)", "PICARD", "T5"], "ignore_blank": true}}, "state": {"gui-gpt-4o-2024-11-20": true}, "annotator": "human", "anchor_pdf": ["15baba11-9239-54a7-a2fc-accae9d907df"], "reference_pdf": []}
{"uuid": "77318114-59c2-51e6-9719-990770d4e50c", "question": "According to the paper \"Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback\", both the papers \"Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation\" and \"Designing Toxic Content Classification for a Diversity of Perspectives\" adopted standard analysis methods. Then which variable's impact on experimental data is considered in all three papers?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["0d3f0011-493e-5e57-b1a9-7c8be3156a62"], "reference_pdf": ["357ecfc8-7a31-50d8-93ca-7aaf3e2ec1b1", "5fd4e7c2-8eaf-5345-9bee-1d7af471ee7b"], "conference": [], "reasoning_steps": ["Find the experimental variables of the three papers respectively", "Compare the experimental variables and identify the the same variable"], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "whether people are LGBTQ or not", "question": "According to the paper \"Whose Preferences? Differences in Fairness Preferences and Their Impact on the Fairness of AI Utilizing Human Feedback\", both the papers \"Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation\" and \"Designing Toxic Content Classification for a Diversity of Perspectives\" adopted standard analysis methods. Then which variable's impact on experimental data is considered in all three papers?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "775ac142-b55e-5cbb-9dc2-ebfb7aa64260", "question": "What shortcoming does REV overcome? and how?", "answer_format": "", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["First, get the content of Introduction section.", "Second, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The REV metric overcomes the shortcoming of existing free-text rationale evaluation methods that focus primarily on how well a rationale helps predict a label. Traditional metrics often fail to assess the new information a rationale provides beyond what is already present in the input or label.", "REV addresses this shortcoming by quantifying the additional, label-relevant information in a rationale, using an information-theoretic approach. It evaluates rationales along two dimensions: (1) Support for the Label: Whether the rationale helps predict the intended label; (2) New Information Contribution: How much unique information the rationale adds, beyond the input and label, to justify the prediction"], "question": "What shortcoming does REV overcome? and how?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["d834bb23-9c22-5e94-9421-0be576081dae"], "reference_pdf": []}
{"uuid": "780f0147-be99-5d8d-ab82-daec0d471510", "question": "What's the type of Character Role Play in jailbreak prompts? How can we make role-playing models more responsible in the anchor_pdf?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "table", "subjective"], "anchor_pdf": ["b65bd6b5-e2bb-5f16-a805-055784527a16"], "reference_pdf": ["5844c6f9-3de6-551b-bc02-ba6bc65c02ef"], "conference": [], "reasoning_steps": ["First of all, locate the paper related to jailbreak prompts and find the type of Character Role Play.", "Then, find the measures that can make the models more responsible in the anchor_pdf."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The type is \"Pretending\", and the measues: (1) Implement Advanced Moderation Tools. (2) Bias Detection and Mitigation. (3) Transparency in Development and Use. (4) Feedback Mechanism.", "question": "What's the type of Character Role Play in jailbreak prompts? How can we make role-playing models more responsible in the anchor_pdf?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7870d38f-1d3b-57d0-b0a0-bdcf9c1cd381", "question": "What are the advantages and disadvantages of MUX-PLMs mentioned in the paper?", "answer_format": "Your answer should be a string list of advantages and disadvantages. ", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Locate the section in the specified PDF that discusses MUX-PLMs, possibly in the analysis section or conclusion section.", "Extract content from the relevant sections that highlight the advantages of MUX-PLMs and discusses the disadvantages of MUX-PLMs..", "Summarize the advantages and disadvantages."], "evaluator": {"eval_func": "eval_partial_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Significant Throughput Improvement: MUX-PLMs leverage data multiplexing to process multiple inputs in a single forward pass, resulting in a dramatic increase in throughput.", "Comparable Performance to PLMs: Despite the increased throughput, MUX-PLMs maintain performance close to traditional PLMs on downstream tasks.", "Model Generalizability: MUX-PLMs can be fine-tuned like traditional PLMs for various downstream tasks, such as text classification and named entity recognition.", "Performance-Throughput Trade-off: As the number of multiplexed inputs (N) increases, the throughput improves further, but the performance may slightly degrade.", "Model Size Limitations: While the paper demonstrates the effectiveness of MUX-PLMs across different model sizes, the model size still impacts performance and throughput. Larger models may offer better performance but with a potentially smaller throughput improvement compared to smaller models.", "Data Sampling Strategy: The paper employs a random data sampling strategy, but a more sophisticated approach could potentially enhance performance, for example, clustering similar instances based on similarity metrics and multiplexing them."], "question": "What are the advantages and disadvantages of MUX-PLMs mentioned in the paper?", "count": 4}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["457688f9-deb0-54fb-8531-6ed175a556d0"], "reference_pdf": []}
{"uuid": "78feef9e-1c36-5824-9c47-544c65f73c86", "question": "Which domain in the GRBench dataset does not have any hard questions?", "answer_format": "Your answer should be a single string representing the domain name.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Locate the statistics table or the experimental figure related to GRBench samples of different difficulties.", "Check the statistics table if it reveals the domain with no hard questions.", "Check the experimental figure if it reveals the domain with no result data on hard questions."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "healthcare", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "reference_pdf": []}
{"uuid": "7942e599-6cc3-59c7-89ec-2be7f578f002", "question": "How many samples are there in total in the dataset used by MIDGARD for Task 2 evaluation?", "answer_format": "Your answer should be an integer, the number of samples.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["5dce7396-1032-5275-ae09-d74568a33935"], "reference_pdf": ["366742fc-a2cb-56af-87b7-cb2834f6cf11", "c5a300ef-dacf-5505-a8b7-a6797a2eb702"], "conference": [], "reasoning_steps": ["Find the dataset used for Task 2 evaluation in the MIDGARD paper.", "Read the corresponding paper to find the number of samples."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 3166}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "79b25301-76c6-5594-9f59-76e6ea48246c", "question": "In section 3, the author provides an exemplary event description. List the features in the example that correspond to the semantic roles discussed in the following paragraph.", "answer_format": "Your answer should be a Python dictionary where the keys are the semantic roles and the values are the features that correspond to the roles. e.g. {\"semantic_role1\": \"feature1\", \"semantic_role2\": \"feature2\", ...}", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, read section 3 and find the exemplary event description.", "Read the following paragraphs to identify the semantic roles discussed.", "List the features in the example that correspond to the semantic roles.", "Finally, formulate the answer according to answer format."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"subject": "soldiers", "predicate": "injured", "quantifier": "two", "object": "civilians"}}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["5b81a1a3-fbe6-534d-b0c0-801e8fb2bdd6"], "reference_pdf": []}
{"uuid": "79d00a52-e8f8-5cc0-af9f-385ac4139377", "question": "Which languages are included in the evaluation dataset used in the anchor PDF?", "answer_format": "Your answer should be a list of languages, e.g., [\"Language1\", \"Language2\"].", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["a1013128-3a4a-51fe-90fb-dfa09a5c12aa"], "reference_pdf": ["4632325b-454f-5eee-8d4d-ab3aedab5d44"], "conference": [], "reasoning_steps": ["Retrieve the evaluation dataset in the anchor PDF, which is in the data & evaluation section.", "Locate the related paper about the datasets.", "Locate the languages included in the evaluation datasets, which are mentioned in the Languages and Corpora section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["English", "German", "Dutch", "Italian"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "79e15976-b650-5e63-847a-8a6ed4c1de02", "question": "If one would like to train (or evaluate) a helpful assistant agent that can converse with humans while the humans traverse an environment, which work has the most suitable resource?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "If one would like to train (or evaluate) a helpful assistant agent that can converse with humans while the humans traverse an environment, which work has the most suitable resource?", "reference_answer": "SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7a1887ea-4b59-53c5-a860-d6dbd87f0d83", "question": "Can you find a dataset that shows LLM-based evaluation may not be reliable enough?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Can you find a dataset that shows LLM-based evaluation may not be reliable enough?", "reference_answer": "EVALUATING LARGE LANGUAGE MODELS AT EVALUATING INSTRUCTION FOLLOWING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7b4842aa-2e95-51b9-afd2-1f5e70174b3c", "question": "What is the original form of the metric formula used in the anchor paper for the test split of the BabyLM shared task dataset?", "answer_format": "Your answer should be one formula in LaTeX format without explanation.", "tags": ["multiple", "subjective", "formula", "text"], "anchor_pdf": ["9efa7292-831f-5f7d-b401-cd73e6e18e2b"], "reference_pdf": ["351dc7e4-f4af-5b5e-953a-5ba6ff9839dc"], "conference": [], "reasoning_steps": ["Find the section discussing the metric for the test split of the BabyLM shared task dataset in the anchor paper.", "Locate the paper which proposed the metric.", "Identify the original form of the metric formula."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"question": "What is the original form of the metric formula used in the anchor paper for the test split of the BabyLM shared task dataset?", "formulas": "\\mathrm{PLL}_{\\mathrm{orig}}(S) := \\sum_{t=1}^{n} \\mathrm{log}~P_{\\mathrm{MLM}}(s_t~|~S_{\\setminus t})"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "7bd66a0c-2558-572f-8e9c-51c2422a7d1d", "question": "*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.", "reference_answer": "SQUARE: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "7c5afdfd-0983-59be-b714-636d275bf7ad", "question": "Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?", "reference_answer": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7ca5b284-3586-51a2-b05f-e6adacb7e072", "question": "Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?", "reference_answer": "Conformal Risk Control"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7d231de8-b8f7-588f-87b0-4fe7b4be0863", "question": "Which knowledge graph completion method focuses on reducing memory usage by pruning features?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which knowledge graph completion method focuses on reducing memory usage by pruning features?", "reference_answer": "GreenKGC: A Lightweight Knowledge Graph Completion Method"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7d856467-aba1-5b39-8ebc-d533b61dc86b", "question": "Which paper highlights the need for leveraging all available resources, including dictionaries, machine translation systems, and language learners, to construct NLP data in low-resource languages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper highlights the need for leveraging all available resources, including dictionaries, machine translation systems, and language learners, to construct NLP data in low-resource languages?", "reference_answer": "Rethinking Annotation: Can Language Learners Contribute?"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7dbe882d-0adf-5c1b-86f8-71b1a7508bca", "question": "Which paper trains on linear regression to hypothesize how fine-tuning affects language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper trains on linear regression to hypothesize how fine-tuning affects language models?", "reference_answer": "UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7dee922c-95de-5d8e-8f03-4c27b84c7919", "question": "Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?", "reference_answer": "TOWARDS ROBUST MULTI-MODAL REASONING VIA MODEL SELECTION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7e8b3e8b-6834-5662-bb05-c05c9b0d38d6", "question": "Is there any research paper that can extract attributes from both a predefined label set and the surrounding context?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any research paper that can extract attributes from both a predefined label set and the surrounding context?", "reference_answer": "AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "7f6dafa1-72c9-5c9b-a4bc-dbddaf15f4de", "question": "Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?", "reference_answer": "FROZEN TRANSFORMERS IN LANGUAGE MODELS ARE EFFECTIVE VISUAL ENCODER LAYERS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "807fdd37-864e-584c-b556-cf63ef4b428e", "question": "For the latest two selected ML datasets of vision modality in Croissant, where did the raw images come from?", "answer_format": "Your answer should be a single word, the name of the website where the images came from.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["1b46d3cd-bc8f-51ad-8c77-105312f6e952"], "reference_pdf": ["55fff8cb-7639-5bab-8c5c-ab352eb833ae", "b80b94be-c4be-5423-989d-7135a38d729a"], "conference": [], "reasoning_steps": ["Find the table that lists all selected datasets.", "Find the latest two datasets of vision modality.", "Read the sections in the corresponding papers that talk about the source of images."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Flickr", "lowercase": "true"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "818501f3-3983-598b-903a-9bfc0ec268d6", "question": "Is there any paper that utilizes masked language modeling to defend against word-level adversarial attacks?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that utilizes masked language modeling to defend against word-level adversarial attacks?", "reference_answer": "RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "81a62980-3225-5149-b703-e7c4bc4d48ea", "question": "Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?", "reference_answer": "Personality Understanding of Fictional Characters during Book Reading"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "82bdaa47-a2cb-5fbd-a827-83d981f4bb52", "question": "According to Table 2, what is the difference of the percentage of over 80% agreement threshold for both concessive and causal relations between the first and second iteration on the English dataset?", "answer_format": "Your answer should be a percentage with two decimal places, indicating the difference in proportion.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["First, get the content of Table 2.", "Second, find the '>80% both' column and calculate the difference of first and second iteration.", "Third, calculate the total number of samples.", "Finally, calculate the percentage."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "58.06%"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["757dc667-413b-5207-8463-5deb4fedf073"], "reference_pdf": []}
{"uuid": "82d3067f-1da3-5800-b7ef-a4571d85ccde", "question": "What paper is the first to prove finetuned LLM can be a reliable judge?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper is the first to prove finetuned LLM can be a reliable judge?", "reference_answer": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "835eda31-c9b0-53a8-bc45-bb5418334a61", "question": "What datasets are used in the paper proposing the baseline for the experiments in the anchor paper?", "answer_format": "Your answer should be a python list, each element is the name of the dataset used, e.g.,[\"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "objective", "text"], "conference": [], "reasoning_steps": ["Find the section mentioned the baseline model used in the anchor PDF, which is usually in the experiments section.", "Locate the respective paper proposing this baseline model.", "Find the datasets used in the experiments in the paper proposing this baseline model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["PTB", "CTB5.1", "CTB7"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["aa761e80-b33e-5860-9926-dd147986f5ab"], "reference_pdf": ["ceacd41b-7bf5-5a0b-b53a-72b523d09157"]}
{"uuid": "837e7da7-5e5c-5cb9-bcb0-c1dc60d97569", "question": "Which paper first used structural information for coherence modeling?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first used structural information for coherence modeling?", "reference_answer": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "83ec97fc-091f-54b0-a627-cf693204090f", "question": "What are the two key technologies used in the process of augmenting trajectory-level data proposed in this paper?", "answer_format": "Your answer should be plain text.", "tags": ["single", "subjective", "text"], "anchor_pdf": ["0d89b506-7770-5702-a992-47a2e50eee4d"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) or page(s) about the description of the process of augmenting trajectory-level data.", "Then identify the key technology used."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Augment trajectories using a partial noising with forward process and a denoising framework with amplified return guidance.", "question": "What are the  two key technologies used in the process of augmenting trajectory-level data proposed in this paper?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8531101d-a0b8-50fa-9f0e-5a3c71417b4e", "question": "How to calculate three important parameters that appear in the second part of Figure 2?", "answer_format": "Your answer should be a Python list of three string elements, every element is a formula in latex format to calculate a parameter.", "tags": ["formula", "image", "single", "subjective"], "anchor_pdf": ["8712603a-e96c-5537-be18-651b29dedfb8"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find Figure 2 in the specified PDF and understand the background information.", "Identify its second part and three important parameters.", "Extract formulas related to the three parameters from the figure."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["\\alpha_{k}=\\displaystyle\\frac{\\cos\\left(\\mathbf{h}_{t},\\mathbf{h}_{o}[k]\\right)}{\\sum_{j=1}^{K}\\cos\\left(\\mathbf{h}_{t},\\mathbf{h}_{o}[j]\\right)}", "\\beta=\\mathrm{FC}_{\\beta}(\\overline{{\\mathbf{h}}}_{o})=\\mathrm{FC}_{\\beta}\\left(\\displaystyle\\sum_{k=1}^{K}\\alpha_{k}\\mathbf{h}_{o}[k]\\right)", "\\gamma=\\mathrm{tanh}\\left(\\mathrm{FC}_{\\gamma}\\left(\\mathbf{h}_{i}\\right)\\right)"], "question": "How to calculate three important parameters that appear in the second part of Figure 2?", "ignore_order": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "85f81535-fc10-575c-995f-4739a4470d4b", "question": "Where can I get the code of GreenKGC method?", "answer_format": "Your answer should be the url of the code of GreenKGC method.", "tags": ["metadata", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the footer of page 1 and answer the question."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "https://github.com/yunchengwang/GreenKGC"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["a3e3cee1-d140-5dc1-9608-2f1a1d924229"], "reference_pdf": []}
{"uuid": "8781817f-474b-5c15-9f77-aa85055446b9", "question": "The study employs two main methods to analyze linguistic features: LIWC and BERT. What are the advantages of the two methodes respectively?", "answer_format": "Your answer should be a python list of two strings.every string describes clearly the advantages of one methode.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Locate the relevant section(s) in the specified PDF discussing LIWC and BERT methods.", "Extract or analyse the advantages of LIWC and BERT."], "evaluator": {"eval_func": "eval_partial_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Interpretability of LIWC: LIWC categories have clear psychological meanings, making it easier to interpret the analysis results.", "Low computational cost of LIWC: LIWC analysis only requires simple word counting, which has a low computational cost.", "Ease of use of LIWC: LIWC dictionaries are easy to obtain and use, and the operation is simple.", "Contextual understanding of BERT: The BERT model can capture the contextual information of words, thus understanding the meaning of words more accurately.", "Stronger feature representation ability of BERT: The BERT model can generate richer feature vectors, capturing more complex linguistic features.", "Better prediction performance of BERT: In some tasks, the prediction performance of the BERT model may be better than that of the LIWC model."], "question": "The study employs two main methods to analyze linguistic features: LIWC and BERT. What are the advantages of the two methodes respectively?", "count": 4}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["4ffd9c08-58a8-5008-8aeb-8686e4093869"], "reference_pdf": []}
{"uuid": "881d40cb-62f6-57d9-b4bb-75ce6a1c2b89", "question": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.", "reference_answer": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "891093a5-a10b-5299-a421-a77713a8886e", "question": "According to Table 1, which baselines this paper used has the highest average score? In which paper is this method proposed? And in which conference was this paper published?", "answer_format": "Your answer should be a python list with three items, the first item is the name of baseline reaching the highest average score according to Table 1, the second item is the name of paper where this method proposed, and the third item is the abbreviation of the conference name where this paper was published.", "tags": ["metadata", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the content of Table 1.", "Second, find what baseline reaching the highest average score.", "Third, according to reference, get the name of paper where this method was proposed.", "Finally, find the name of conference where this paper was published."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SimCTG", "A contrastive framework for neural text generation", "NeurIPS"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["9c5c3a63-3042-582a-9358-d0c61de3330d"], "reference_pdf": []}
{"uuid": "895610a0-b3ae-5623-a8e2-e0731eae53f6", "question": "Is there a paper that uses Explainable AI techniques to investigate how language models represent the expression of morality?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that uses Explainable AI techniques to investigate how language models represent the expression of morality?", "reference_answer": "What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "897a955d-9e8a-5836-a3b6-0ed48575f2b9", "question": "In terms of evaluation results on SuperGLUE using RoBERTaBASE, on subtask ReCoRD, which model(s) achieve the overall best results? Additionally, which model(s) perform the best among the two-stage MTL models?", "answer_format": "Your answer should be a python dict(without \\n) containing two keys \"overall best\" and \"two-stage MTL best\", each value of the two keys is a python string list. e.g.{\"overall best\":[\"modelname1\"],\"two-stage MTL best\":[\"modelname2\",\"modelname3\"]} ", "tags": ["objective", "single", "table", "text"], "conference": [], "reasoning_steps": ["Find information about two-stage MTL models(maybe a section or table mentioning which models or baselines are two-stage MTL).", "Locate the table about the performance results on SuperGLUE using RoBERTaBASE(usually a main experiment table containing different kinds of models and subtasks).", "Compare the performance of models on the subtask ReCoRD to determine which model(s) achieves the overall best results and which model(s) perform the best among the two-stage MTL models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"overall best": ["PROPETL"], "two-stage MTL best": ["SCALEARNUNIFORM", "SCALEARN++"]}, "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["9b06b24b-0afc-5ccb-95fc-c662395d291d"], "reference_pdf": []}
{"uuid": "8a08dc1f-6e2c-5e4b-9da4-34f5fb3ee073", "question": "Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?", "reference_answer": "ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8aca533a-c03d-5708-aaf4-320886de4a20", "question": "In the paper that introduced the latest dataset used by RetinaQA, what innovation related to F1 was also applied in the evaluation of RetinaQA?", "answer_format": "Your answer should be a paragraph, describing the innovation on F1.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["5b97752d-6379-55fe-903a-918b3b53925c"], "reference_pdf": ["0d9f5091-a5c3-5d69-8f13-b9427d3f4ccd", "058d0055-8d50-5b52-ac1a-8c36d074e246", "16c3a7ad-d638-5ebf-a72a-bd58f06c16d7"], "conference": [], "reasoning_steps": ["Locate the section that discusses the datasets used.", "Identify the latest dataset.", "Read the related paper to find the section that talks about evaluation.", "Identify the innovation on F1.", "Read the anchor PDF to verify the innovation."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In regular answer evaluation (R), we compare the predicted answer (which could be NA) with the gold answer in the modified KB, as usual. Specifically for unanswerability, we also consider lenient answer evaluation (L), where we account for the gold answer in the original (ideal) KB as well, and also give credit to models which are able to recover this answer, perhaps via inference.", "question": "What innovation related to F1 was also applied in the evaluation of RetinaQA?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8b6057cc-77e5-5981-9d3f-5b9b62126d0e", "question": "What work proposes a model to learn a latent regular cell complex from data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What work proposes a model to learn a latent regular cell complex from data?", "reference_answer": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8b79fc88-0307-532a-bbdb-e8990fb27372", "question": "What paper considers sensitive data issue when prompting large language model APIs?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper considers sensitive data issue when prompting large language model APIs?", "reference_answer": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8ba9edfd-5a7a-5ec1-9d22-329443311f3b", "question": "Is there any paper that aligns speech and text embeddings better than CTC training?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that aligns speech and text embeddings better than CTC training?", "reference_answer": "WACO: Word-Aligned Contrastive Learning for Speech Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8beb15f1-2c64-5aa7-a1a3-1579452b2ecc", "question": "For the specific dataset where CLiCoTEA does not outperform all models, in which languages does this occur?", "answer_format": "Your answer should be a Python list of string elements, every element is the abbreviation of a langugage mentioned in the paper, e.g. [\"AR\", \"ES\", \"FR\", ...].", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, identify relevant performances (usually in the main experiment table) from the specified PDF for CLiCoTEA and other models.", "Analyse and recognize the specific dataset in which CLiCoTEA does not outperform all models.", "Extract relevant performances (possibly in the ablation table or appendices) for that dataset.", "Finally, analyze the table content to identify the languages where CLiCoTEA underperforms previous methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["DE", "ES", "ID", "RU", "TR"], "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0d6ea045-b831-520d-9b99-ba22a081a403"], "reference_pdf": []}
{"uuid": "8cc38e05-20e5-5a69-8b82-ecc09c03450a", "question": "According to the experiment result, How much better does the GPT-2 model perform on Task A compared to the CNN-BiLSTM model in terms of F1-score?", "answer_format": "Your answer should be a single python float.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Locate the table(s) that presents experimental results for Task A.", "Compare and calculate the F1 score difference between GPT-2 and CNN-BiLSTM."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.02}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["1aa5e165-0b78-582a-8f67-e459452348df"], "reference_pdf": []}
{"uuid": "8e1ebc95-7523-5a09-b95c-85748f5825ae", "question": "Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?", "reference_answer": "Gloss-Free End-to-End Sign Language Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8e2d5903-f8ba-5504-aa9a-43538a8536a6", "question": "In the paper that proposed the dataset used by ReFIR for the evaluation of the second experimental setup, which image restoration method was also proposed? Additionally, what metric applied to evaluate that method was not applied in ReFIR?", "answer_format": "Your answer should be a Python list of 2 strings, the name of the method and the name of the metric.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["1bdbf41b-f94f-5f1a-b22a-89662ed0fb49"], "reference_pdf": ["f9ea952c-4545-5826-9a6d-aa819fffce2c"], "conference": [], "reasoning_steps": ["Find the section that talks about experimental setups.", "Identify the dataset.", "Read the corresponding paper to find the image restoration method.", "Read the two papers to compare the evaluation metrics."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SUPIR", "ManIQA"], "ignore_order": false, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8e6f688d-d3d9-5055-89c4-b50ad752b208", "question": "In the experiment result of the paper named \"EIT: Enhanced Interactive Transformer\", which model gets the highest RG-L on the summarization task? For the source paper of this model, what issue about Neural network-based methods for abstractive summarization does this paper want to address? ", "answer_format": "Your answer should be a single python list, the first element is the string of the model name, the second element is the string of the issue.e.g.[\"EIT\",\"Neural network-based methods for abstractive summarization often neglect...\"].", "tags": ["subjective", "multiple", "text", "table"], "anchor_pdf": ["e281fc3b-cdaa-5565-8997-6a6c8f198000"], "reference_pdf": ["c970161d-753d-556c-a748-2c296f644f07"], "conference": [], "reasoning_steps": ["Firstly, locate the table about results on the summarization task and identify the model with the highest RG-L.", "Then turn to the source paper about this model.", "Finally, locate the section in the paper about the issue it wants to address(usually in abstract, introduction or conclusion)."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "BOTTOM-UP", "lowercase": true}, {"reference_answer": "Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection.", "question": "For the source paper of this model, what issue about Neural network-based methods for abstractive summarization does this paper want to address? "}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8f53d4bf-6a6f-59b8-90ed-c6b555240a59", "question": "Among the datasets proposed in the Introduction section of the anchor PDF, which one has the least Q-A pairs?", "answer_format": "Your answer should be a single string, the name of the dataset as given in the Introduction section.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["2bf8095c-81d0-5988-858b-a5155e0cc985"], "reference_pdf": ["b93e2dfe-7f58-5d96-8c64-39930d5c22ea", "e73a34ee-536b-5ea4-8de4-b2fb39b30042", "3e409d3a-1045-575f-b4ad-f4923916080a", "4d93d596-b0bd-54c6-bd9e-041037077bc7", "71cec673-84eb-579b-9419-2032699ac0e7"], "conference": [], "reasoning_steps": ["Locate the Introduction section of the anchor PDF.", "Find the list of datasets proposed in the section.", "Read the PDF of each dataset, with special attention to tables.", "Identify the dataset with the least Q-A pairs."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "SituatedQA", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8f561f35-51ae-5330-97aa-f547f89f4d26", "question": "According to MSAD paper's review, which is the largest VAD benchmark datasets with multiple domain before? Additionally, in that dataset, which are the two scenarios with the most videos?", "answer_format": "Your answer should be a Python list of 2 elements, the first is the name of the dataset, the second is a list of 2 strings, the full name of the scenarios.", "tags": ["multiple", "text", "table", "image", "objective"], "anchor_pdf": ["0e26d1b4-43af-5cf5-9626-deef7dcfc6ae"], "reference_pdf": ["d1290795-a058-5b1b-b9f4-9392472d83b0", "9413e819-8187-5e64-8aef-3507676d0855"], "conference": [], "reasoning_steps": ["Find the table that lists the VAD benchmark datasets.", "Identify the largest dataset with multiple domains.", "Read the corresponding paper to find the scenarios with the most videos.", "Read the other section and images to identify the full names."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "CUVA", "lowercase": true}, {"gold": ["Pedestrian Incidents", "Forbidden to Burn"], "lowercase": true, "ignore_order": true, "ignore_blank": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "8f94a406-ef97-584a-a578-17a8b0380287", "question": "Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?", "reference_answer": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "8fb6f8ec-fae3-5823-ba3f-21cdca6952a9", "question": "How do the authors split the dataset for the experiments?", "answer_format": "Your answer should be a plein text.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Locate the section where experiments are mentioned.", "Find the subsection which discusses the dataset split."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "For the empirical modeling analysis and performance benchmarking, we randomly split the dataset into 3 sets: train (70%), dev (5%), and test (25%) sets, while ensuring both domains (fashion and furniture) have the same split distributions.", "question": "How do the authors split the dataset for the experiments?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["67c78c79-7878-5ff5-b5f6-45cec4ad9bf9"], "reference_pdf": []}
{"uuid": "90082d94-579c-5dc2-a5c3-f9f6278857e2", "question": "On how many datasets was COSA evaluated for its performance in Object Discovery and Composition?", "answer_format": "Your answer should be an integer.", "tags": ["single", "text", "objective"], "anchor_pdf": ["8c31b4d5-1f30-52a4-9870-482b7ec1c67c"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that discusses the ability of CoSA on Object Discovery and Composition", "Second, find all the datasets mentioned in this part of text, and count the total number."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 6}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "90aa9ace-0a80-5867-893b-e342497160a1", "question": "Is there any paper that tries to investigate LLMs’ capabilities in solving elliptical constructions by using a test-dataset based on the psycolinguistic notion of Thematic Fit?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that tries to investigate LLMs’ capabilities in solving elliptical constructions by using a test-dataset based on the psycolinguistic notion of Thematic Fit?", "reference_answer": "We Understand Elliptical Sentences, and Language Models Should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "921a63d8-1cb7-5162-bc06-b9546498e519", "question": "Among StudentEval, HumanEval and MBPP, which one has the most test cases per problem?", "answer_format": "Your answer should be a string, indicating the name of the dataset.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["4b53feaf-4e33-590c-a8bb-9c8c7005bf6b", "a70723f6-7139-5165-a9c7-9dcdd34e3514", "0e57b18a-c261-582f-8527-2337f0aeda90"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read the description of the datasets to identify the number of test cases per problem."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "HumanEval", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "924003fa-668e-5dbc-8e2c-43aa69d5696c", "question": "Both two papers, \"Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization\" and \"Composing Parameter-Efficient Modules with Arithmetic Operation\" focus on arithmetics on parameter-efficient modules. Which setting did the first paper mainly focus on, while the second paper didn't?", "answer_format": "Your answer should be brief text on the setting mainly focused on only by the first paper.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["eee99caa-1041-588f-850b-67dc3a80524c", "501ba611-6203-53af-8077-ea1a893322d4"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find out the main setting that the first paper mainly focused on.", "Find out the main setting that the second paper mainly focused on.", "Compare the two settings to identify the setting mainly focused on only by the first paper."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Cross-lingual transfer on summarization tasks.", "question": "Both two papers, \"Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization\" and \"Composing Parameter-Efficient Modules with Arithmetic Operation\" focus on arithmetics on parameter-efficient modules. Which setting did the first paper mainly focus on, while the second paper didn't?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "926c7917-2d65-5ca2-9e3b-2b7927962fbd", "question": "Between the two agent-based method that are explicitly introduced in Related Work section of LLM-DP, which one is not applied as a baseline? Why not?", "answer_format": "Your answer should be a Python list of two strings, the first is the name of the method, the second is the reason why it's not applied.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["0dc71539-30ec-52ba-bad0-0d031ea757b2"], "reference_pdf": ["2c626d88-ca60-501d-9beb-763ddf799a85", "2f2e4311-fc9b-5e36-bb18-7c3fee141713"], "conference": [], "reasoning_steps": ["Read the Related Work section of the anchor paper to find the two methods.", "Read the section that discusses the results to find the method applied as baseline.", "Read the paper that proposes the other method to understand the function of that agent.", "Answer the question based on the information you have gathered."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "Voyager", "lowercase": true}, {"reference_answer": "Voyager is an agent specialized in Minecraft, who is not capable of datasets like Alfworld.", "question": "Why Voyager is not applied as a baseline in the LLM-DP paper?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "927ff9af-42f7-5216-a6f9-f106e8ff6759", "question": "On the HEML sentence level with AUC metric, which baseline outperforms MIND on specific conditons?Is it the best variant according to the paper that proposed that baseline?", "answer_format": "Your answer should be a Python list of two strings. The first string is the name of the baseline (with variant) that outperforms MIND, as proposed in the anchor PDF. The second string is either `true` or `false`.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["621d42a1-dbab-5003-b7c5-625335653001"], "reference_pdf": ["ab661558-432d-5e5e-b49c-a3660a40986e", "1a21b653-3db0-55e8-9d34-8b6cd3dcbefa", "85111b8b-4df0-5a9a-8d11-a7ae12eebcf6", "0597ce2b-cd8c-5b5b-b692-e8042d8548de", "6df0f3f3-e2e1-5d7a-9d70-3114ceac5939", "02f7fff5-cec7-5ac8-a037-f5eb117b9547"], "conference": [], "reasoning_steps": ["Loate the table that evaluates different baselines.", "Identify the variant that surpasses MIND.", "Read the paper that proposed the variant to determine if it is the best variant."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SCG-NLI", "no"], "ignore_order": false, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "92c53685-6c5d-538a-9c62-887598de3301", "question": "Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?", "reference_answer": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "92dcdc85-f07b-5980-80c0-474447201940", "question": "Are there any large-scale and open-source text simplification datasets dealing with long passages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any large-scale and open-source text simplification datasets dealing with long passages?", "reference_answer": "SWIPE: A Dataset for Document-Level Simplification of Wikipedia Pages"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "93193629-3db3-5f41-93da-8282895eba7f", "question": "What is the relationship between the two paper (in anchor_pdf) in terms of datasets?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["e35a7ed7-548d-5aa3-95d3-5054b8e3020d", "ed7fa054-62cc-53f8-b200-15de56d03112"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read both papers' section of datasets and understand the relationship."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The dataset of paper \"What does Parameter-free Probing Really Uncover?\" is based on the dataset of paper \"Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT\".", "question": "What is the relationship between the two paper (in anchor_pdf) in terms of datasets?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "93272751-e57b-55e7-a89a-ef2387c4d2be", "question": "Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?", "reference_answer": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "932ee901-050a-5ef5-bb1e-0baeff576249", "question": "According to Table 1, on how many mathematical task test sets was Rho-1 tested?", "answer_format": "Your answer should be ", "tags": ["single", "text", "objective"], "anchor_pdf": ["22a670fd-c1d3-50d9-9c10-7ef49a3a2c24"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the table and retrieve its contents.", "Second, identify the number of mathematical task test sets that Rho-1 was tested on."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 9}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "9388f82d-b44e-52a4-8e0e-4b0e93bd5876", "question": "Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.", "reference_answer": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "93cbeec6-18b1-55ba-93f3-8be583414ea9", "question": "Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?", "reference_answer": "Parameter-Efficient Fine-Tuning without Introducing New Latency"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "943cd5d2-df6b-588b-b985-70b2aa2e9f3b", "question": "How does the LISA algorithm choose which layers' parameters to freeze at each iteration?", "answer_format": "Your answer should be a python string", "tags": ["single", "subjective", "formula"], "anchor_pdf": ["4557bf76-9ede-5fb2-a271-47a71d382639"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Algorithm 1 and retrieve information from it", "Second, find how layers are selected for parameter freezing and get the answer"], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "How does the LISA algorithm choose which layers' parameters to freeze at each iteration?", "reference_answer": "The LISA algorithm samples layer indices through a uniform distribution and freezes the parameters of the sampled layers."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "948c99f1-12ba-5c05-8cbf-de25811480b1", "question": "Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?", "reference_answer": "MPCHAT: Towards Multimodal Persona-Grounded Conversation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "94bf4901-caa8-50d8-9626-f34f0226b4e9", "question": "Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?", "reference_answer": "Robust Multi-bit Natural Language Watermarking through Invariant Features"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "94ef0706-a58e-556e-b4c4-0dfe3f47ff63", "question": "Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?", "reference_answer": "EEL: Efficiently Encoding Lattices for Reranking"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "9640f248-b594-56b4-9cc6-9b242538ee40", "question": "In the direct preceding work of H2O, are there any metrics tested other than perplexity?", "answer_format": "Your answer should be a python boolean.", "tags": ["multiple", "objective", "text", "metadata"], "anchor_pdf": ["5e63d90b-a1a4-5e6e-9845-5177eba99970", "9f701072-7290-52f2-91fa-1b5fddbbd78e"], "reference_pdf": [], "conference": [], "reasoning_steps": ["In the H2O paper, locate the related work section and identify the most direct preceding work by examining the affiliations of the authors.", "Turn to the original paper of the preceding work.", "Find the section that records the experiments and check if there are any experimental data other than perplexity."], "evaluator": {"eval_func": "eval_bool_exact_match", "eval_kwargs": {"gold": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "969eef23-a666-5128-8553-069c1c546f0e", "question": "According to the author, what are the three main types of current dialogue evaluation catalogs?", "answer_format": "Your answer should be a string.", "tags": ["single", "text", "subjective"], "anchor_pdf": ["89c1512d-d2c7-5002-818b-8595f9982ff1"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that discusses the three main types of current dialogue evaluation catalogs", "Second, find the three main types mentioned in this part of text."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "According to the author, what are the three main types of current dialogue evaluation catalogs?", "reference_answer": "Reference-based, reference-free and reference-assisted"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "985616d9-fbc1-5329-824f-15b2d1d79de0", "question": "Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?", "reference_answer": "NORMBANK: A Knowledge Bank of Situational Social Norms"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "98f8113e-ec12-53ee-877c-ed347c655fbd", "question": "Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?", "reference_answer": "GPT-4 IS TOO SMART TO BE SAFE: STEALTHY CHAT WITH LLMS VIA CIPHER"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "99231726-94e7-5dbf-80a5-947df93d9761", "question": "Several information extraction and natural language processing tools were used in the anchor paper. What is the major limitation of prior works that the entity and relationship extractor used in the anchor paper was designed to solve?", "answer_format": "Your answer should be brief text explaining the limitation solved by the extractor's design.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["4b1a5787-4c18-51d8-bd3e-8a036f750938"], "reference_pdf": ["0201fdd7-83db-53a2-b6a1-dc06e2b19cb6"], "conference": [], "reasoning_steps": ["Find the section discussing information extraction and natural language processing tools in the anchor paper.", "Locate the paper which proposed the entity and relationship extractor.", "Identify the major limitation of prior works that the extractor was designed to solve."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The major limitation of prior works is that they ignore the interrelation between spans (pairs).", "question": "Several information extraction and natural language processing tools were used in the anchor paper. What is the major limitation of prior works that the entity and relationship extractor used in the anchor paper was designed to solve?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "9945247a-acf3-5768-9e8c-3015d272434d", "question": "According to the RAV paper, up to 2019, which method performs the best overall on evidence retrieval? Additionally, what's that method's FEVER score with 1 sentence selected for the subtask of recognizing textual entailment?", "answer_format": "Your answer should be a Python list of 2 elements, the first is a string, the name of the method, and the second is a float, rounding to 2 decimal places.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["5a971f9d-71f9-5381-9877-05e68e18ad80"], "reference_pdf": ["366e4b37-75a5-5baf-ad8f-634309a1a35e", "c50df058-1617-58f1-9b89-c397fcdceb6f", "d3cfe89d-e84d-51b6-bd10-33a106a8e12b"], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ESIM", 63.64], "ignore_order": false, "lowercase": true, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "997b8d8f-3d5f-57e6-9590-9e8c5591dacc", "question": "What are the model architecture parameters (dim, n_layers, head_dim, hidden_dim, n_heads, n_kv_heads) of the automatic evaluation model used in the aspect evaluation section of the anchor pdf?", "answer_format": "Your answer should be a python dictionary with the following keys: dim, n_layers, head_dim, hidden_dim, n_heads, n_kv_heads, e.g., {\"dim\": 768, \"n_layers\": 6, \"head_dim\": 64, \"hidden_dim\": 3072, \"n_heads\": 12, \"n_kv_heads\": 12}.", "tags": ["multiple", "objective", "table", "text"], "anchor_pdf": ["4ac8bced-e49c-5417-b21f-edae07ae2ce2"], "reference_pdf": ["44e77de0-2982-575f-bf6e-f50ad597c4f6"], "conference": [], "reasoning_steps": ["Retrieve the automatic evaluation model in the anchor PDF, which is mentioned in the aspect evaluation section.", "Locate the related paper about the model.", "Locate the model architecture parameters, which are mentioned in the architecture section in the form of a table."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8}, "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "99965706-7450-5c82-9db7-a9f9605c5fc6", "question": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?", "reference_answer": "Semantic Structure Enhanced Event Causality Identification"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "99cec4b0-ca19-56bb-83a0-7a79a4a14c9d", "question": "What is the distribution ratio of data sources for the toxicity ratings dataset used in the anchor PDF?", "answer_format": "Your answer should be a python dictionary about the data sources and their distribution ratio, e.g., {\"data_source_1\": 0.5, \"data_source_2\": 0.5}. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["86aea8c2-7ffe-534a-a610-d467151fe5de"], "reference_pdf": ["357ecfc8-7a31-50d8-93ca-7aaf3e2ec1b1"], "conference": [], "reasoning_steps": ["Retrieve the toxicity ratings dataset in the anchor PDF, which is usually mentioned in the dataset section.", "Locate the related paper about the dataset.", "Locate the data sources and their distribution ratio, which are mentioned in the method section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Twitter": 0.67, "Reddit": 0.15, "4chan": 0.18}, "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "99d793f3-fa1b-5d68-a62b-ace9cdeca097", "question": "How much higher is the percentage of \"食品#品质\" in Figure: Aspect category distributions(Test Set for Subtask3) in anchor_pdf than the percentage of \"食品#品质\" in Figure: training data category distributions of HITSZ-HLT?", "answer_format": "Your answer should be a python float number with three decimal places, and the answer should be in the range of [0, 1]", "tags": ["multiple", "text", "image", "objective"], "anchor_pdf": ["defd3a4f-647b-500c-986d-b01341f1a543"], "reference_pdf": ["cb0bc639-0d1f-5d72-b108-e8ffc5aefbef"], "conference": [], "reasoning_steps": ["Firstly, locate the two Figures.", "After that, analyse the percentage of \"食品#品质\" in the Figures."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.042, "ndigits": 3}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "9adcad9f-bd01-54de-a6cd-61d0a77e487d", "question": "What are the three main flavours of GNN layers? What's the relationship between GTs and GNN?", "answer_format": "Your answer should be a python strings", "tags": ["multiple", "text", "image", "subjective"], "anchor_pdf": ["1f190ec5-f832-540b-a62f-6142eba5991a"], "reference_pdf": ["2bb974d2-1630-54c3-9eac-4ada100f56ec"], "conference": [], "reasoning_steps": ["First, locate the paper about GNN and GTs.", "Then, find the three flavours of GNN in the paper.", "Ultimately, analyze the relationship between GTs and GNN."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Convolutional, attentional and message-passing flavours are the three main flavours of GNN layers. GTs is a special type of GNN", "question": "What are the three main flavours of GNN layers? What's the relationship between GTs and GNN?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "9b05a21b-7190-547c-ae1c-2a4b21a84826", "question": "Is there a paper that uses similarity scores to check knowledge in diffusion models", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that uses similarity scores to check knowledge in diffusion models", "reference_answer": "Multilingual Conceptual Coverage in Text-to-Image Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "9bd68a1f-c2de-5448-b7b0-ddbc43b2165a", "question": "What is the detailed procedure of progressive learning followed in the instruction tuning section of the anchor pdf?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["5a854b33-31c8-50f8-beaa-b3c93e9ae2f8"], "reference_pdf": ["02f4302f-4f6c-5092-8927-36a0b9cac06a"], "conference": [], "reasoning_steps": ["Find the progressive learning finetuning method in the instruction tuning section of the Anchor PDF.", "Locate the relevant paper in the reference PDF.", "Identify the detailed procedure of progressive learning in the training section."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The detailed procedure of progressive learning used in Orca 2 starts with LaMA-2-7B or LLaMA-2-13B checkpoint and finetunes it on the train split of FLAN-v2 dataset for one epoch. Then they train on 5 million ChatGPT data from Orca 1 for 3 epochs, and train on the combination of 1 million GPT-4 data from Orca 1 and Orca 2's 817K data for 4 epochs.", "question": "What is the detailed procedure of progressive learning followed in the instruction tuning section of the anchor pdf?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "9ce8e94a-4eda-5f28-90cc-1342a03c3e51", "question": "What dataset does the anchor paper use for evaluation? According to its source paper, how many structured annotations does this dataset collect in total? ", "answer_format": "Your answer should be a single python list, the first element is the string of the dataset name, the second element is an integer number.", "tags": ["text", "multiple", "objective"], "conference": [], "reasoning_steps": ["First, locate the section about evaluation in the anchor paper.", "Second, find the dataset used for evaluation and its source paper.", "Finally, turn to the source paper to find the number of structured annotations collected by this dataset."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SOCIAL-CHEM-101", 365000], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["d7dc9bd8-6590-5278-8a64-b5a7e0f8f940"], "reference_pdf": ["2e1fe5bd-29fb-536b-9c10-d2b296accf35"]}
{"uuid": "9cf5ab0b-d8e0-5cee-b756-ba9f5f95b220", "question": "Among existing Math and STEM QA datasets, which dataset includes theorem except TheoremQA? In which paper is this dataset introduced?", "answer_format": "Your answer should be a list of two strings, the first element is the name of the dataset, and the second element is the title of the source paper.", "tags": ["text", "table", "single", "objective"], "conference": [], "reasoning_steps": ["First, locate the table about existing Math and STEM QA datasets.", "Second, find the dataset which includes theorem.", "Finally, find the title of source paper in the reference section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["MATH", "Measuring mathematical problem solving with the math dataset"], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["cea8e2ce-bf4a-53f5-9f63-9aaf9720500d"], "reference_pdf": []}
{"uuid": "9eb8dc14-f435-5774-ac7d-1bc63d5b72b4", "question": "Which website can I find the conversation video between Sheldon and Leonard?", "answer_format": "Your answer should be a pure text string starting with \"https\". DO NOT INCLUDE ANY OTHER INFORMATION OR CONTEXT IN THE ANSWER.", "tags": ["single", "metadata", "objective", "text"], "conference": [], "reasoning_steps": ["Firstly, we need to search through the paper with the query \"conversation video between Sheldon and Leonard\".", "Then, we need to find the website link surrounding the text.", "If not found nearby, we can also search for the footnote in exactly the same page where the query relevant text is found. It possibly contains the website link."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "https://giaabaoo.github.io/TPD_website/", "lowercase": false}}, "state": {"gui-gpt-4o-2024-11-20": true, "gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["1a8e9cd5-8ae1-52b9-84d5-b67bd9c07a21"], "reference_pdf": []}
{"uuid": "9f4464f1-93bd-58ea-9d06-be56dfc0f60b", "question": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?", "reference_answer": "FERMAT: An Alternative to Accuracy for Numerical Reasoning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a06cf968-8d7c-5d7a-b203-91bb312150b7", "question": "Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?", "reference_answer": "DarkBERT: A Language Model for the Dark Side of the Internet"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a1bcf4ae-a49c-559d-83be-7e34162877d1", "question": "What tasks were proposed in the paper of the dataset used in the experiment of the anchor PDF?", "answer_format": "Your answer should be a python list of tasks, e.g. [\"task1\", \"task2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["610d8e02-d6bb-52e3-801d-0dbcdb5310db"], "reference_pdf": ["20ec131c-808d-59b3-8554-b5a68b02968e"], "conference": [], "reasoning_steps": ["Find the name of the dataset used in the experiment in the anchor PDF.", "Locate the relevant paper about the dataset.", "Identify the tasks proposed in the paper of the dataset used in the experiment."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Grounding Span Prediction", "Agent Response Generation"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a2985096-8453-5fb7-9066-6f505c734248", "question": "List the names of the baselines used in the paper \"On the Compositional Generalization in Versatile Open-domain Dialogue\", along with the titles of papers that proposed these baselines.", "answer_format": "Your answer should be a Python list of baseline-title pair, e.g., [[\"baseline1\", \"title1\"] , [\"baseline2\", \"title2\"], ...]. YOU MUST USE THE EXACT TEXT FROM THE PAPER AND THE FULL TITLE TEXT OF THE PAPERS.", "tags": ["multiple", "metadata", "objective"], "anchor_pdf": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "reference_pdf": ["f6e91a91-0b1e-5280-8522-a20492033f16", "e0cf46b4-e3cf-5d54-8c0b-c661214fe349", "21df0715-990d-58d3-b218-280ac3a84c8f", "60f20fa5-d268-522d-9cc2-5f7321af4f82"], "conference": [], "reasoning_steps": ["Firstly, locate the sections that mention baselines.", "Identify the baselines.", "Find the reference for each baseline.", "Identify the title of each reference paper.", "Finally, list the baseline-title pairs."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [["BART", "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"], ["R2C2", "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion"], ["Prefix-Tuning", "Prefix-Tuning: Optimizing Continuous Prompts for Generation"], ["Modular Skill", "Combining modular skills in multitask learning"]], "lowercase": true, "ignore_order": true, "ignore_blank": true, "threshold": 95}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a2f8cca9-a522-5fd2-bf2e-829fc82f749e", "question": "What do the authors evaluate the unlearned model from?", "answer_format": "Your answer should be a python list, each element is a string.", "tags": ["single", "text", "objective"], "anchor_pdf": ["2d09e6fb-e268-5d5d-a362-9effcfe79d9f"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the part mentioned the evaluation of unlearned model in the paper and get the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Performance on the Forget Set", "Performance on the Retain Set", "Performance on General Downstream Tasks"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a343069f-cdd9-58b2-9abb-afb91e8f5360", "question": "In the paper \"Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors\", on which dataset does PIF method reach its highest accuracy? In the paper where that dataset is proposed, which LLM performed the best, and how to account for its performance?", "answer_format": "Your answer should be a Python list of three strings. The first string indicating the full name of the dataset, the second indicating the name of the LLM that performed the best, and third indicating the reason. e.g. [\"dataset\", \"LLM\", \"reason\"].", "tags": ["multiple", "subjective", "table", "text"], "anchor_pdf": ["0d85f51e-7304-5a37-8876-8b458b37d114"], "reference_pdf": ["c2c5bf1a-3d4a-508e-a217-b3e4b78ce7f7", "a87a7490-623a-54af-bad6-ef68b0757499", "6a224ba5-c711-5435-b425-9bacbcd552a6"], "conference": [], "reasoning_steps": ["Firstly, find the table that discusses the accuracy of different methods on different datasets.", "Identify the dataset.", "Then, turn to the paper which proposed that dataset.", "Locate the table that compares the performance of different models.", "Identify the best-performing LLM.", "Locate the section that discusses the reason."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_string_fuzzy_match", "eval_reference_answer_with_llm"], "eval_kwargs_list": [{"gold": "CommonsenseQA", "lowercase": true}, {"gold": "BERT", "fuzz_method": "partial_ratio", "threshold": 95, "lowercase": true}, {"reference_answer": "To understand the performance of BERT-LARGE, we analyzed 100 examples from the development set (Table 6). We labeled examples with categories (possibly more than one per example) and then computed the average accuracy of the model for each category. We found that the model does well (77.7% accuracy) on examples where surface clues hint to the correct answer. Examples that involve negation or understanding antonyms have lower accuracy (42.8%), similarly to examples that require factoid knowledge (38.4%). Accuracy is particularly low in questions where the correct answer has finer granularity compared to one of the distractors (35.4%), and in cases where the correct answer needs to meet a conjunction of conditions, and the distractor meets only one of them (23.8%).", "question": "How to account for BERT-Large's performance on CSQA?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a3555904-aa5f-5f4d-b823-b51bacf04995", "question": "Which paper introduced the human-evaluated timeliness metric for misinformation detection?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper introduced the human-evaluated timeliness metric for misinformation detection?", "reference_answer": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a36db3b8-16d0-5a14-bebb-3e3eea363d27", "question": "What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.", "reference_answer": "CAN LLMS KEEP A SECRET? TESTING PRIVACY IMPLICATIONS OF LANGUAGE MODELS VIA CONTEXTUAL INTEGRITY THEORY"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a3c6958b-aed2-5e28-8dea-5d0b88550ac8", "question": "According to the anchor PDF, what're the three most recent decoder-only LLMs for NL2Code? How many programming languages do their training datasets contain respectively?", "answer_format": "Your answer should be a Python dictionary of 3 key-value pairs, where each key is the name of the LLM and each value is an integer indicating the number of programming languages in its training dataset. e.g. {\"LLM1\": 3, \"LLM2\": 5, \"LLM3\": 2}", "tags": ["multiple", "objective", "table", "text"], "anchor_pdf": ["37758401-6101-554f-8f1e-4e2995443314"], "reference_pdf": ["6590d875-4982-56a0-8bd7-e67f4bc777c9", "c1fed3f4-7a5f-5877-97e5-aa508eac885e", "4badd0e5-53ce-5044-b6c6-abec723c34aa", "01c9329e-9789-52dc-9eed-c99a8ef88a5c", "3c3b3cfc-e4f2-52b9-899a-2f9cac25dafc"], "conference": [], "reasoning_steps": ["Find the table which compares different LLMs for NL2Code.", "Identify the three most recent decoder-only LLMs.", "Locate the respective papers.", "Find the number of programming languages in training datasets for each LLM."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"CodeGeeX": 23, "BLOOM": 13, "SantaCoder": 3}}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a4359833-c1ee-5d01-b18d-1d1a78c749f0", "question": "Is there any work that attacks language models in dialogue generation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any work that attacks language models in dialogue generation?", "reference_answer": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a49ce3ea-3977-5eb8-8598-47342bcd60a3", "question": "In figure 2, what is the next step to take after generating the supportive logical forms?Please use the name presented in this figure.And then for this step, what's the criteria held in this paper?", "answer_format": "Your answer should be a list of two strings, the first element is the name of the next step, and the second element is sevaral sentences about the criteria.", "tags": ["figure", "text", "single", "subjective"], "conference": [], "reasoning_steps": ["First, find figure 2 and identify the next step after the supportive logical forms.", "Second, locate relevant section to find the criteria."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_scoring_points_with_llm"], "eval_kwargs_list": [{"gold": ["Prompt Construction"], "lowercase": true}, {"scoring_points": ["The templates should break up the generation of a complicated question into a step by step process.", "The templates should clearly identify the subcomponent in a logical form that requires LLMs to focus on for each step."], "question": "For this step, what's the criteria held in this paper?"}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["d5242995-ff5a-54a1-a27a-a1a139974a5e"], "reference_pdf": []}
{"uuid": "a4ce44e5-a7d3-5043-981c-99695dd766e5", "question": "Which paper first proposed extracting the pair of target and stance from sentences?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposed extracting the pair of target and stance from sentences?", "reference_answer": "A New Direction in Stance Detection: Target-Stance Extraction in the Wild"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a629b08b-2d1e-5a0e-a39a-007749de7759", "question": "Is there a paper that uses the tree structure of math equations in autoregressive language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that uses the tree structure of math equations in autoregressive language models?", "reference_answer": "Tree-Based Representation and Generation of Natural and Mathematical Language"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a64654b4-b4c5-5167-b58b-529530c1be68", "question": "Is there any paper about style transfer for stories?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper about style transfer for stories?", "reference_answer": "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a69b7df9-1ecd-579e-85ae-17de9f0dfbba", "question": "Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?", "reference_answer": "Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a6e178bd-06c1-58c3-b6f1-e72b0cab6a03", "question": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?", "reference_answer": "PRIVACY-PRESERVING IN-CONTEXT LEARNING FOR LARGE LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a743e85d-4b2c-5671-a371-578b2f0af908", "question": "Is there any paper that explores and annotates the effectiveness of using testimonials or anecdotes in discussions?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that explores and annotates the effectiveness of using testimonials or anecdotes in discussions?", "reference_answer": "StoryARG: a corpus of narratives and personal experiences in argumentative texts"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a762550d-b54a-5e5f-8fcf-d3be3058cd28", "question": "Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?", "reference_answer": "Robust Multi-bit Natural Language Watermarking through Invariant Features"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "a803c5e9-ad61-5580-8819-66875022e19b", "question": "In order to improve Parrot's abilities, what is used in the anchor_pdf to train the model? What are the other two related methods in the anchor_pdf?", "answer_format": "Your answer should be a Python list of two strings, answering the two questions respectively. You should use abbreviation and use \"and\" to connect the two methods.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["a8508753-17fc-5f40-8abf-245ecbfe151e"], "reference_pdf": ["d35109d5-9f0a-5d99-ae90-dcaabf4bb74e"], "conference": [], "reasoning_steps": ["First of all, locate the paper related to Parrot.", "Secondly, read the anchor_pdf and find the methods used to train Parrot.", "Finally, find the two methods in the anchor_pdf."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["DPO", "PPO and SFT"]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a903623b-95ca-5dc2-a8a8-3c9851d02779", "question": "Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?", "reference_answer": "Self-Edit: Fault-Aware Code Editor for Code Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a911834c-2700-51fa-8a37-ab3649fdd8d7", "question": "In section 4, what research quesitions do the authors aim to answer?", "answer_format": "Your answer should be be plein text DIRECTLY FROM THE PDF.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Read section 4 to find the research questions the authors aim to answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In this section, we aim to answer three research questions: (RQ1) How do different medical generation models perform under our evaluation? (RQ2) How is the evaluation quality of DOCLENS compared to existing metrics? (RQ3) How is the evaluation quality of DOCLENS computed with open-source evaluators compared to proprietary ones?", "question": "In section 4, what research quesitions do the authors aim to answer?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["b8ba5cee-e8d9-504b-a2a7-dc210b814ece"], "reference_pdf": []}
{"uuid": "a9275d5c-ec5c-5fd2-b8de-0866aaee4fb8", "question": "Which paper combines the advantages of different frameworks for grammar error correction (GEC) and achieves good performance?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper combines the advantages of different frameworks for grammar error correction (GEC) and achieves good performance?", "reference_answer": "TemplateGEC: Improving Grammatical Error Correction with Detection Template"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "a927fcee-f0a7-50cd-b5a7-ca8ab5c8eb23", "question": "What are the institutions of the first author and corresponding author of this paper?", "answer_format": "Your answer should be a Python list of length 2 containing the institution names respectively, e.g., [\"first_author_institute\", \"corresponding_author_institute\"]. If there are multiple first authors or corresponding authors, please replace the corresponding institution name with a name list, e.g., [[\"first_author1_institute\", \"first_author2_institute\", ...], [\"corresponding_author1_institute\", \"corresponding_author2_institute\", ...]].", "tags": ["single", "metadata", "objective"], "conference": [], "reasoning_steps": ["The authors and their institutions are usually listed in the first page.", "And the co-first author or corresponding author information is usually marked with a symbol in the author region, and explained below in the footnote.", "After selecting the relevant authors, you can extract their institution information from the first page, usually nearby the author list or in the footnote."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Beihang University", ["The University of Sydney", "The Hong Kong Polytechnic University"]], "ignore_order": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3d2fcb43-2cda-5645-99aa-da78c6cfd23f"], "reference_pdf": []}
{"uuid": "a93430e0-ae3b-585d-8622-ed9b5844da8c", "question": "In Experiment Section of the anchor PDF, what is the overall framework of the baseline model achieving the second best BERTScore on the dataset LOCOMO?", "answer_format": "Your answer should be a python strings about the detailed overall framework of the baseline model.", "tags": ["multiple", "subjective", "text", "table"], "anchor_pdf": ["dbad7ff2-b141-56da-869e-e2eacc675417"], "reference_pdf": ["ffa706cf-0129-55f6-b463-6c5a458933c2", "b7f2cb42-c26b-5b4d-b8dd-6365498dbd01"], "conference": [], "reasoning_steps": ["Find the baseline model achieving the second best BERTScore on the dataset LOCOMO, which is usually in the table of the experimental section.", "Locate the relative papers about the baseline model.", "Find the overall framework of the baseline model in the paper, which is usually in the methodology section."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The overall framework of MemoChat pipeline is a memorization-retrieval-response loop inner thinking. Very different from traditional methods that retrieve directly on these accumulated dialogues, the chatbot will automatically builds and updates a structured on-the-fly memo, storing past dialogues in categories. Then, the retrieval is conducted over all recordings according to their topics and summaries", "question": "In Experiment Section of the anchor PDF, what is the overall framework of the baseline model achieving the second best BERTScore on the dataset LOCOMO?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "a96944de-c900-55f0-9b3b-cccb597a8b71", "question": "Which category of website takes up the most proportion in the dataset MC2?", "answer_format": "Your answer should be a phrase indicating the category DIRECTLY FROM THE PDF WITHOUT ANY MODIFICATION OR EXPLANATION.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the figure that shows the distribution of the dataset MC2.", "Finally, answer the question."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "News"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["16142be2-ac28-58e5-9271-8af299b18d91"], "reference_pdf": []}
{"uuid": "a9aba86b-c608-5d87-9039-cc130911a03d", "question": "What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?", "reference_answer": "Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "aa4ec90c-b162-5319-9a00-ca47101c24f8", "question": "Which paper showed that social relationships were helpful for identifying inappropriate messages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper showed that social relationships were helpful for identifying inappropriate messages?", "reference_answer": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "aa5598d0-e570-5f39-afd6-159fd696bdc6", "question": "What paper mitigates language model sampling errors due to the softmax bottleneck?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper mitigates language model sampling errors due to the softmax bottleneck?", "reference_answer": "CLOSING THE CURIOUS CASE OF NEURAL TEXT DEGENERATION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "aa73c1ee-05cb-5570-bfac-bf86eb94caeb", "question": "What is the reason why Soft MoE cannot be applied to autoregressive models currently?", "answer_format": "Your answer should be a python string", "tags": ["single", "subjective", "text"], "anchor_pdf": ["36f7c548-f8c2-5fc9-ba12-a35ac045bc25"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Section 2.5 Current Limitations", "Second, get the limitation about autoregressive decoding and get the answer"], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "What is the reason why Soft MoE cannot be applied to autoregressive models currently?", "reference_answer": "Soft MoE cannot be applied to autoregressive models because it needs to merge all input tokens together, which would break the causality requirement between past and future tokens that must be maintained during training in autoregressive models."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "aadd5754-71f6-5b8d-ad9e-d7d8e24975ce", "question": "Who is the first author of WebArena? How many papers of his/hers are cited in the paper? Which are they?", "answer_format": "Your answer should be a Python list of 3 elements. The first one is a string serving as the first author name of WebArena. The second one is an interger indicating the number of self-referenced papers. The third one is a string list storing the titles of self-referenced papers.", "tags": ["single", "text", "metadata", "objective"], "anchor_pdf": ["5a2b0d5c-6b51-5bbd-a001-a15f19f65a98"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the author list of the paper and locate the first author", "Find the reference list of the paper", "Search the first author name in the reference list and enumerate the paper with the first author in the co-author list"], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_int_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "Shuyan Zhou"}, {"gold": 6}, {"gold": ["Pal: Program-aided language models", "Language models of code are few-shot commonsense learners", "Hierarchical prompting assists large language model on web navigation", "Execution-based evaluation for open-domain code generation", "Hierarchical control of situated agents through natural language", "Show me more details: Discovering hierarchies of procedures from semi-structured web data"], "ignore_order": true, "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "aaf4f321-f2c3-5cd3-9924-d22ed02ed43c", "question": "Calculate the increase in throughput when the batch size increases from 24 to 64 for H2O (20%) at a sequence length of 2048+2048 on A100 GPU.", "answer_format": "Your answer should be a Python float number rounded to 1 decimal place. e.g. 20.3", "tags": ["table", "objective", "single"], "anchor_pdf": ["5e63d90b-a1a4-5e6e-9845-5177eba99970"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the throughput results for H2O (20%) at a sequence length of 2048+2048 on the A100 GPU.", "Retrieve the throughput results for batch sizes of 24 and 64, respectively.", "Calculate the increase in throughput when the batch size is increased from 24 to 64."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 242.1, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ab369ade-a399-5f3a-82ba-13c02f4a91a7", "question": "Whether the code and data of this paper are publicly available or not?", "answer_format": "Your answer should be a simple \"yes\" or \"no\" WITHOUT PUNCTUATION OR EXPLANATION.", "tags": ["single", "metadata", "objective"], "conference": [], "reasoning_steps": ["To find the information of code and data, you need to look at the first page of the paper.", "Usually, the URL of the code and data is provided in the Abstract section."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "yes", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["ae9b4a06-0642-5512-9150-656cf166c470"], "reference_pdf": []}
{"uuid": "ab9138b7-f6f2-5fd0-9430-1d0664ceb5c3", "question": "Which paper first studied the efficiency robustness of multi-exit language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first studied the efficiency robustness of multi-exit language models?", "reference_answer": "Dynamic Transformers Provide a False Sense of Efficiency"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ab9cd1cf-213f-5551-b4fb-104a3ba51266", "question": "Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?", "reference_answer": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ac041b6a-467d-53ce-8419-f283f3e0d7aa", "question": "Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?", "reference_answer": "Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ac2f076d-19cf-5703-b728-dc3077dd410e", "question": "The experiment section of the anchor PDF introduces a new metric, S^2MATCH, what is the main difference between it and the metric used in Appendix D? Answer with one formula.", "answer_format": "You only need to provide one definition formula of S2MATCH in Python strings. You don't need to explain the formula or variables.", "tags": ["multiple", "subjective", "formula", "text"], "conference": [], "reasoning_steps": ["Find the name of the metric used in Appendix D in the anchor PDF.", "Locate the experiment section to find information about the new metric S^2MATCH.", "Locate the reference paper introducing S^2MATCH.", "Find the main different formula between S^2MATCH and the metric used in Appendix D."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": "softMATCH = 1 - d(\\mathbf{x} ,\\mathbf{y} )", "question": "The experiment section of the anchor PDF introduces a new metric, S^2MATCH, what is the main difference between it and the metric used in Appendix D? Answer with one formula."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["aad25ce3-2d26-5a48-9653-a41e0a749e55"], "reference_pdf": ["8a4a1a61-43e9-5e08-a31f-4b7c71754ae4"]}
{"uuid": "ad341e2b-cb59-5695-b41a-9912be57ea77", "question": "What is the shape of $W$ in Equation (3)? And what about $W_l$ and $W_v$ in Equation (5)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (3) and paragraph 'Encoding' of Section 3.2.", "Second, summarize the answer for question 1.", "Third, get the content of Equation (5) and paragraph 'Interaction' of Section 3.2.", "Finally, summarize the answer for question 2."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The shape of $W$ in Equation (3) is $\\mathbb{R}^{d_s \\times d_l}$, where $d_s$ is the dimension of the vision features, and $d_l$ is the dimension of the language features. The shapes of $W_l$ and $W_v$ in Equation (5) are both $\\mathbb{R}^{d \\times 1}$.", "question": "What is the shape of $W$ in Equation (3)? And what about $W_l$ and $W_v$ in Equation (5)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["648e3d50-375b-5189-b6b0-e0520626716e"], "reference_pdf": []}
{"uuid": "ad6b9fa5-cac1-531f-8b8c-c82fe6665863", "question": "For the VQA DOC task, what are the optimal values of alpha and beta?", "answer_format": "Your answer should be a python list of two numbers", "tags": ["single", "objective", "table"], "anchor_pdf": ["60f440ff-3076-5b6e-96d7-0819845b691b"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the table 4 and get the content of table 4", "Second, find the optimal values of alpha and beta in the content of table 4", "Third, get the final answer"], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [0, 5]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ae713f72-a3ae-5bdd-8704-f849359fe19b", "question": "Which model did both anchor_pdfs use for experiments?", "answer_format": "Your answer should be a python string, and it should be the model name.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["a4114462-9b4a-51c8-ae5e-a1591a301c88", "357d672a-550b-5dc9-9bc7-fb8b429b07f6"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Retrival the model used in anchor_pdfs.", "Find the model that the anchor_pdfs both used for experiments."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "BERT"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "af63f4bc-4bf0-5521-aa2d-c032a1b947c8", "question": "Is there any paper that address attacks on code models by leveraging the semantic information of the source code through attention scores, while also guaranteeing that the generated adversarial examples can always be compiled successfully?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that address attacks on code models by leveraging the semantic information of the source code through attention scores, while also guaranteeing that the generated adversarial examples can always be compiled successfully?", "reference_answer": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b0b2b9a1-fa76-5027-9ba7-84a9876c07ac", "question": "Is there any paper that uses token-level loss to enhance sentence-level embedding learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that uses token-level loss to enhance sentence-level embedding learning?", "reference_answer": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b10c0e3a-48e4-5878-bbff-1611969ca685", "question": "What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?", "reference_answer": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b11ab881-9ac0-5b77-9b27-394744cf06e1", "question": "What are the most important optimizations of transformer network in this paper?", "answer_format": "Your answer should be a Python list of text strings, with each element being one important optimization that this paper proposes, e.g., [\"optimization 1\", \"optimization 2\", ...].", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Usually, the most important optimization of network are proposed in the abstract, introduction or methodology. Search the correpsonding paragraphs of these parts.", "If the optimiazations are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core optimizations from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Removal of the one-to-one mapping constraint among queries and keys in multiple subspaces", "Allowing each query to attend to multiple keys", "Introduction of inner-subspace interaction and cross-subspace interaction to encourage consensus among heads"], "question": "What are the most important optimizations of transformer network in this paper?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["e281fc3b-cdaa-5565-8997-6a6c8f198000"], "reference_pdf": []}
{"uuid": "b15e2f1e-a31c-58b9-8d53-1910e0d28391", "question": "On what devices is StreamVoice trained?", "answer_format": "Your answer should be a plein text directly from the PDF without explanation.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Locate the section where training details are provided.", "Read the section to find the devices used for training."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "StreamVoice is trained using 8 V100 GPUs with a batch size of 7 utterances per GPU for 700k steps.", "question": "On what devices is StreamVoice trained?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["039b3a9f-1e97-5579-bc23-fcd0d2f01c19"], "reference_pdf": []}
{"uuid": "b1724696-f143-5f5a-a58d-2f4086212016", "question": "Is there any paper that utilizes graph structure to model conversation history?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that utilizes graph structure to model conversation history?", "reference_answer": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b1ee7930-cebf-5b6d-8ebc-bbc0a6246aca", "question": "Which language model comparisons in paper \"On Robustness of Finetuned Transformer-based NLP Models\" are reflected in anchor_pdf?", "answer_format": "Your answer should be a python list of string. The string should be language model name.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["fc6daddf-131f-59b0-adc2-85b97b4ecd82"], "reference_pdf": ["2773b9a9-f232-5acb-a1dd-a0168e52cf0c"], "conference": [], "reasoning_steps": ["Find the language Models that the paper \"On Robustness of Finetuned Transformer-based NLP Models\" compares.", "Retreive the language Models in the anchor_pdf and get the answer."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["GPT-2", "BERT"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b2711e57-f28a-5955-9413-35717769b3c1", "question": "For retrieval evaluation, what metrics applied by MTEB are not used by LocalRQA?", "answer_format": "Your answer should be a python list of strings, each string is the name of a metric, as given in the MTEB paper.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["3e62472f-aacc-591c-bd3a-9d3e71b79363", "be946a16-54d6-5d5c-82ac-8aba4b2952cc"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read the section that introduces the retrieval evaluation to identify the metrics used by MTEB.", "Read the section that introduces the retrieval evaluation to identify the metrics used by LocalRQA.", "Identify the metrics used by MTEB that are not used by LocalRQA."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["MRR@k", "MAP@k", "precision@k"], "lowercase": true, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b300fae4-e575-5062-9f11-2c8f320463cb", "question": "How was the data for the latest text classification tasks used in the anchor pdf collected?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["1a023c1a-97ca-5ba9-aea9-781f1cfbb346"], "reference_pdf": ["c8fca681-edde-571e-885c-e186e7b4ae80", "64c155da-e4cc-5de8-95e9-4715738d5b1d", "9ada7bff-c684-55ab-ae9b-04f836247ddc", "ab8d017f-8645-5337-aa84-f52783391b99"], "conference": [], "reasoning_steps": ["Find the text classification tasks in the experiment section of Anchor PDF.", "Locate the relevant papers in the reference PDF.", "Identify the latest paper about the mentioned text classification tasks.", "Find the data collection methodology, which is in the corpus section."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The data collection methodology is to create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis.", "question": "How was the data for the latest text classification tasks used in the anchor pdf collected?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b33b2cf3-a27a-5b2a-a1ca-5f08d8b1e75e", "question": "Which paper makes sure that the questions used in the paper are all from real users that are genuinely curious about a specific topic or concept?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper makes sure that the questions used in the paper are all from real users that are genuinely curious about a specific topic or concept?", "reference_answer": "CREPE: Open-Domain Question Answering with False Presuppositions"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b39cbbdd-8489-53f0-a9ca-d0dbc46c8ead", "question": "What limitations do large language models have in evaluating information-seeking question answering?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What limitations do large language models have in evaluating information-seeking question answering?", "reference_answer": "Evaluating Open-Domain Question Answering in the Era of Large Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b3a5fb63-2a87-5e0c-bd8d-29f25772319c", "question": "What paper first associate the modeling frequency with input human skeletons under the NeRF framework?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first associate the modeling frequency with input human skeletons under the NeRF framework?", "reference_answer": "POSE MODULATED AVATARS FROM VIDEO"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b4dcc93d-635a-54c4-be8f-c5ec443d08db", "question": "The training dataset used in the paper \"Semiparametric Token-Sequence Co-Supervision\" is filtered to 42932 instances, then what's the original size of this dataset?", "answer_format": "Your answer should be a single integer.", "tags": ["objective", "multiple", "text"], "anchor_pdf": ["e13b0b17-08cb-50fa-b144-a14b676118bf"], "reference_pdf": ["23e4f6c4-0d28-52be-8ab4-7aef1c19b5ce"], "conference": [], "reasoning_steps": ["Firstly, locate and identify the training dataset used in the anchor paper.", "Then turn to the original paper about the dataset.", "Finally, locate the section about the description of the training data to extract its size."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 150000}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b509eb3e-12e2-51cc-a02d-ed22d0c8a8b3", "question": "How does Multi-DYLE combine the three different losses as the objective of training?", "answer_format": "Your answer should be single formula in latex format, extracted from the specified pdf.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["Firstly, find the pages or sections mentioning the combined losses or training Objective of Multi-DYLE to understand the three different losses.", "Then retrieve the right formula about the combined losses."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": "$$\\mathcal{L}=\\lambda_{g}\\mathcal{L}_{g e n}+\\lambda_{o}\\sum_{j=1}^{M}\\mathcal{L}_{o r a c l e}^{(j)}+\\lambda_{c}\\mathcal{L}_{c o n s i s t}$$", "question": "How does Multi-DYLE combine the three different losses as the objective of training?"}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["8535fa9f-0253-5e2b-b3a4-5167aeeae4c6"], "reference_pdf": []}
{"uuid": "b50d066a-9ed9-5aac-b79c-a32e3bef9734", "question": "Which dataset performs better on the LLaMA model, PRM800K or Math-Shepherd? In the source paper of PRM800K, which methods are compared with PRM?", "answer_format": "Your answer should be a python list of two items. The first item is a python string. The second item is a python list of strings.", "tags": ["multiple", "objective", "text", "image"], "anchor_pdf": ["85b588f5-13e2-5aaa-9ce8-76c52426b40e", "80b0a0f4-7247-5b9e-8782-0a4dd4a2ae4b"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Section 5.1 in the original paper of Math-Shepherd to find the comparison between PRM800K and Math-Shepherd.", "Next, turn to the paper of PRM800K", "Locate Figure 3 and identify the various methods compared in the figure."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_element_list_included"], "eval_kwargs_list": [{"gold": "Math-Sheperd"}, {"gold": ["ORM", "Majority Voting"], "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b5307d05-348e-50df-8932-95ccf83020f0", "question": "Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?", "reference_answer": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b551c2aa-7d01-5fbf-af59-ae4645fcba85", "question": "Which paper first proposed a cross-domain language model to automatically generate much labeled data for a unlabeled target domain?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first proposed a cross-domain language model to automatically generate much labeled data for a unlabeled target domain?", "reference_answer": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b7327d6a-9ab2-5fd7-966d-4250ce72ae00", "question": "Which family of model generally perform the best for the event conceptualization task", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which family of model generally perform the best for the event conceptualization task", "reference_answer": "CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "b8034b03-c46a-5b8c-8bdd-09e67ad45f9f", "question": "What is the composition of the training dataset in anchor PDF?", "answer_format": "Your answer should be a python strings about the dataset.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["9c7c7762-0132-583c-987a-0fbc89847c55"], "reference_pdf": ["8c267034-d2a4-53d9-a4e0-0fdc761cde75"], "conference": [], "reasoning_steps": ["Retrieve the name of the training dataset, which is mentioned in the experiment section of the anchor PDF.", "Locate the related pdf in the reference PDFs.", "Find the description of the training dataset in the reference PDF."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "WebVid-2M consists of 2.5M video-text pairs. The data was scraped from the web following a similar procedure to Google Conceptual Caption. The dataset consists of manually generated captions, that are for the most part well formed sentences. And the captions are aligned with the video and describe visual content.", "question": "What is the composition of the training dataset in anchor PDF?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "b943f9ec-685a-5bbf-b82e-65bd00415e6d", "question": "In \"MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\", what is the main advantage of \"MetaGPT\" that the authors claim to have over \"AdaMerging\"? Also, what is the most significant difference between the experiment settings of the papers which proposed these two methods?", "answer_format": "Your answer should be brief text answering the 2 questions with separate sentences.", "tags": ["multiple", "subjective", "text", "image"], "anchor_pdf": ["aeb01ff1-2543-50db-89d5-f33c70f77e96", "68bb62d4-2e15-5a27-a5c0-0938e5e9488a"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the section discussing the main advantage of \"MetaGPT\" over \"AdaMerging\".", "Locate the paper which proposed \"AdaMerging\".", "Find the section discussing experiments in these two papers.", "Identify the most significant difference between the experiment settings of these two papers."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The authors claim that MetaGPT is more computational efficient than AdaMerging. MetaGPT only experiments on NLP tasks, while AdaMerging only experiments on vision tasks.", "question": "In \"MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\", what is the main advantage of \"MetaGPT\" that the authors claim to have over \"AdaMerging\"? Also, what is the most significant difference between the experiment settings of the papers which proposed these two methods?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "bb5aedbf-7683-56e7-a348-0ee986fe0fd2", "question": "Is there any works that explores how to achieve balance between representativeness and diversity in chosen samples for few-shot data selection?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any works that explores how to achieve balance between representativeness and diversity in chosen samples for few-shot data selection?", "reference_answer": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "bb6a0f0e-0c0c-5038-b340-3044e9ffefd6", "question": "What paper evaluated the ability of visual few-shot learning models to do in-context learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper evaluated the ability of visual few-shot learning models to do in-context learning?", "reference_answer": "CONTEXT-AWARE META-LEARNING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "bb6ffb6d-2235-58cc-b04d-291818f74b05", "question": "In the dataset proposed by the authors, how many states are there per game?", "answer_format": "Your answer should be a floating-point number with one decimal place.", "tags": ["single", "table", "objective"], "anchor_pdf": ["26a6b2dc-7406-59e3-989d-cf45f151343d"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate where the additional corpus statistics of the dataset are.", "Finally, fetch the number of the states per game."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": "2463.5", "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "bb7c8889-1582-5409-95bc-74cb179506a1", "question": "What's the original annotation process drawn by the paper named \"Where Do People Tell Stories Online?Story Detection Across Online Communities?\"?", "answer_format": "Your answer should be a single string indicating the original annotation process.", "tags": ["subjective", "multiple", "text"], "anchor_pdf": ["ddf6d3e0-7c36-5ce6-ac60-67c687303e0f"], "reference_pdf": ["c7563d97-695f-5c77-8021-334bf2ff9ddb"], "conference": [], "reasoning_steps": ["Firstly, find the section about event annotation in the paper named \"Where Do People Tell Stories Online?Story Detection Across Online Communities\".", "Then turn to the original paper to find the section about the annotation process."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "All annotations were carried out by a single co-author after multiple rounds of discussions and the creation of a set of annotation guidelines. To calculate the expected inter-annotator agreement rate, a second co-author independently annotated a random sample of five texts at the end of the annotation process, using only the annotation guidelines for reference.", "question": "What's the original annotation process drawn by the paper named \"Where Do People Tell Stories Online?Story Detection Across Online Communities?\"?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "bbc522d2-649a-5660-8180-7f67728376bf", "question": "which paper first focuses on addressing the over-smoothing issue for sentence embedding?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "which paper first focuses on addressing the over-smoothing issue for sentence embedding?", "reference_answer": "Alleviating Over-smoothing for Unsupervised Sentence Representation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "bc5c4cf7-21ed-5298-9c2c-81386204608e", "question": "In the paper \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\", according to Figure 1, which two dimensions are mixed using Monarch matrices? In the source paper of Monarch matrices, what training settings can Monarch matrices be used for?", "answer_format": "Your answer should be a python list containing two items. The first item is a python list with two strings. The second item is a python list with an indefinite number of strings.", "tags": ["multiple", "image", "text", "objective"], "anchor_pdf": ["f3c0827e-c512-50bc-91f0-6d5a9e1177b6", "0b29dca5-cb4a-5cdc-a8a6-eb852b9d0bb2"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate Figure 1 in the paper \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\".", "Extract information from Figure 1 to answer which two dimensions are mixed using Monarch matrices.", "Jump to the original paper of Monarch matrices.", "Locate the Introduction section, and based on the Introduction, identify the different training settings."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": ["sequence", "channels"], "ignore_order": true, "lowercase": true, "threshold": 0.9}, {"gold": ["E2E training", "S2D training", "D2S fine-tuning"], "ignore_order": true, "lowercase": true, "threshold": 0.9}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "bd3d1dd5-7f10-5e09-aa76-486685c77180", "question": "What do formula (2) to formula (4) mean?", "answer_format": "Your answer should be a brief summarization of the meaning of these formulas, and you do not need to introduce these formulas one-by-one.", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, locate where formula (2) to formula (4) are", "Second, get the content of the corresponding section", "Third, get the content of the previous section to know the meaning of different symbols.", "Finally, analyze the meaning of these formulas."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Formula (2) to (4) describe the self-contrastive training (SELFCONT) algorithm, which is designed to mitigate the repetition problem in language models. SELFCONT modifies the training process by adding a penalty to the output of the current model \\(f_{\theta_1}\\) when it predicts a repetitive token that the premature checkpoint \\(f_{\theta_0}\\) also predicts. The penalty is controlled by the weight \\(w\\), which is only active when the true next token is non-repetitive but the premature model predicts it as repetitive. This encourages the model to learn more complex patterns and reduce its tendency to generate repetitive text.", "question": "What do formula (2) to formula (4) mean?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["aec41fde-98a1-58c4-86f3-100e408171cd"], "reference_pdf": []}
{"uuid": "be08635a-0dbc-5dab-85d0-40f45c6edfc2", "question": "Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?", "reference_answer": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "be178eef-403f-5633-8cbd-5b876059fce4", "question": "For each test split in Figure 5, provide the name and type of the website with the highest step success rate.", "answer_format": "Your answer should be a Python dictionary. e.g. {\"split1\": [\"web1\", \"type1\"], \"split2\": [\"web2\", \"type2\"], ...}. YOU MUST USE THE EXACT WORDS FROM PDF WITHOUT CHANGING CAPITALIZATION.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the figure.", "Search in the caption to find test split and corresponding websites.", "Finally, read the figure to get the websites with the highest step success rate."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Cross-Task": ["travelzoo", "General"], "Cross-Website": ["exploretock", "Restaurant"], "Cross-Subdomain": ["koa", "Hotel"]}}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["32a52b98-370b-5bc4-87ab-5193405b723b"], "reference_pdf": []}
{"uuid": "bec9b106-831a-5e17-97b6-8af2636194d3", "question": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?", "reference_answer": "Learning to Substitute Spans towards Improving Compositional Generalization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "befbacf1-d163-5021-bb6c-2ba79257c81c", "question": "Which was the first paper to explore the online adaptation of neural MT metrics for use during the inference stage?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which was the first paper to explore the online adaptation of neural MT metrics for use during the inference stage?", "reference_answer": "Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "bf391f7a-d33b-5b00-9001-ee92284a15ec", "question": "According to Figure 1, with the increasing of the number of few-shot training samples, which setting keeps getting a better score?", "answer_format": "Your answer should be the name of the setting appearing in the legend of the image.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the content of Figure 1.", "Second, analyze the trend of scores as the number of samples increases and get the answer."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "KB score, curie"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["ebf4682e-2653-5589-9458-da26d00d9f5b"], "reference_pdf": []}
{"uuid": "bf7fe85f-b409-5a58-ac9e-ba738e5390c7", "question": "In FewRel's 10-way 5-shot setting, what is the maximum decrease of AOD+ROD between ConPL and AGCKD across all task indexes?", "answer_format": "Your answer should be a positive floating-point number with two decimal place.", "tags": ["single", "image", "objective"], "anchor_pdf": ["3e92cc84-5991-5ac2-aa13-f92ccbfcb03b"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, get numbers from the tabular", "Second, calculate decrease of AOD+ROD for each task index", "Last, return the maximum decrease"], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 1.88, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "bfa70a42-daa5-52db-aa3f-8ceb0960739a", "question": "Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?", "reference_answer": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "bfb209f1-da03-5d97-a7e8-aa3bd63e257d", "question": "How to better attract readers to news articles by generating personalized headlines?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "How to better attract readers to news articles by generating personalized headlines?", "reference_answer": "Generating User-Engaging News Headlines"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c0e96750-91fe-5f24-aee3-74ea8706654a", "question": "For each category of PQA in terms of the form of provided answers, from what aspects does the author analyze it?", "answer_format": "Your answer should be a Python list, where each element is a string representing an aspect DIRECTLY FROM THE PDF. Note that the aspects are the same for each category. e.g. [\"aspect1\", \"aspect2\", ...]", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Locate the section where each category of PQA is discussed.", "Identify the aspects from the titles of subsections."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Problem Definition", "Datasets & Evaluation Protocols", "Methods", "Pros and Cons"], "ignore_order": true, "lowercase": true, "ignore_blank": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["2cc7f650-699e-580d-aad4-04fc17b5868f"], "reference_pdf": []}
{"uuid": "c100db0f-bb91-514e-af99-6c6efcf22cd3", "question": "How much data do the author use in total in million for the main experiment conducted on WMT17 ZhEn?", "answer_format": "Your answer should be a Python float, rounding to 1 decimal places.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Locate the section where experimental data is mentioned.", "Identify the data used for the specific experiment.", "Calculate the total data used."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 39.5, "ndigits": 1, "tolerance": 0.01}}, "state": {"gui-gpt-4o-2024-11-20": true}, "annotator": "human", "anchor_pdf": ["03166771-5ae8-57b9-9c10-3120423adc5c"], "reference_pdf": []}
{"uuid": "c17c03e2-ea11-5472-be57-c7ead3b8605f", "question": "Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?", "reference_answer": "Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c181315e-0268-53f9-a982-60eb5747f0e5", "question": "Which paper first attempts to take potential dependencies among same-level labels into account in Hierarchical Text Classification?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first attempts to take potential dependencies among same-level labels into account in Hierarchical Text Classification?", "reference_answer": "Peer-Label Assisted Hierarchical Text Classification"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c1acd5a0-7a76-5605-996d-0191bda04f6c", "question": "Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?", "reference_answer": "Introducing Semantics into Speech Encoders"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c1cbcf5c-632c-5424-a1ef-d9add6094746", "question": "What are the Low resource languages in INDICGENBENCH?", "answer_format": "Your answer should be a Python list, where each element is a string representing a language. e.g. [\"language1\", \"language2\", ...]", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Locate the section which introduces INDICGENBENCH.", "Find the paragraph that talks about the divide of languages.", "List the low resource languages."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Awadhi", "Haryanvi", "Tibetan", "Garhwali", "Konkani", "Chhattisgarhi", "Rajasthani", "Maithili", "Manipuri", "Malvi", "Marwari", "Santali", "Bodo"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["1e6eeeab-ba5c-508e-a693-62a9b39f2d92"], "reference_pdf": []}
{"uuid": "c1f769f6-eaff-5441-9f1c-d62445efe58d", "question": "What's the total number of augmented training samples across all datasets used in the MINPROMPT work?", "answer_format": "Your answer should be a single integer number.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, locate the table that describes the data augmentation statistics in the paper.", "Then, find the number of augmented data samples for each dataset. Special attention should be paid to some aggregation columns or rows, e.g., \"average\" or \"total\". They should be excluded from the calculation.", "Finally, sum up the numbers of augmented data samples across all datasets."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 251387}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0b77c64c-6b36-5501-848f-79a062be2a45"], "reference_pdf": []}
{"uuid": "c2089236-8909-5a22-9eaa-35644720a87b", "question": "According to the author, how does Cross Entropy contribute to miscalibration?", "answer_format": "Your answer should be a string.", "tags": ["single", "text", "subjective"], "anchor_pdf": ["abc391d1-1098-5b04-b9aa-5f0de5a2dd41"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that discusses the contribution of Cross Entropy to miscalibration", "Second, find the contribution of Cross Entropy to miscalibration mentioned in this part of text."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"question": "According to the author, how does Cross Entropy contribute to miscalibration?", "reference_answer": "According to the author, Cross Entropy contributes to miscalibration by encouraging the model to assign a probability of 1 to the ground-truth token and 0 to all other tokens. This leads to overestimating the probability of the correct token (over-confidence) and underestimating the probabilities of the other tokens (under-confidence), causing miscalibration during the fine-tuning process."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c21e6d8e-865c-5544-8177-49b48d723934", "question": "Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?", "reference_answer": "DISCO: Distilling Counterfactuals with Large Language Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c2412c63-0fda-5e8c-95c0-615c415d5ff9", "question": "What is the accuracy of the base model used in the experiment on the RACE test set?", "answer_format": "Your answer should be a python float with one decimal places.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["ff302319-a8f9-58f6-a16a-44971ae06a5a"], "reference_pdf": ["40076536-9fb5-50c7-acb2-93db2c59e1d7"], "conference": [], "reasoning_steps": ["Find the basic model used in the experiment", "Find the paper concerning the basic model.", "Read the paper to find the accurarcy."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 83.2, "ndigits": 1}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c4048cbf-71e6-55ec-a0e9-ba082c5a2954", "question": "In the PPTC benchmark paper, among the works that focus on LLMs' tool-use ability to generate APIs for solving user instructions, which one doesn't apply AST accuracy?", "answer_format": "Your answer should be a string, the name of the method or model.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["3f195c86-04e5-5c9d-826b-63672b5ff9a3"], "reference_pdf": ["4261dbce-3665-5261-9125-09c96905ca64", "1dffea3e-12d5-5a96-82db-480f1579040e", "8967d40f-af4b-5754-848a-0d84d923e39d"], "conference": [], "reasoning_steps": ["Locate the three works.", "Read the sections that mention evaluation.", "Identify the work that doesn't apply AST accuracy."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Toolformer", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c40f9463-e7de-5f3e-b3ec-8f64b3289541", "question": "Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?", "reference_answer": "HOW TO CATCH AN AI LIAR: LIE DETECTION IN BLACK-BOX LLMS BY ASKING UNRELATED QUESTIONS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c4461086-1920-5037-8eb9-f7d8e00aa31b", "question": "In the paper \"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering\", dataset SPIDER was used in the training process. What were the inputs and outputs in the original design of SPIDER, and how did the authors of this work adapt the dataset for their task?", "answer_format": "Your answer should be brief text regarding the inputs and outputs of SPIDER in the two works.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["2c803a9d-d383-58d9-b87c-4d27c53eafc6", "46a88ba5-c16e-5efd-913c-3de6e749f2a9"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the paper containing the original design of SPIDER.", "Find out the inputs and outputs of SPIDER in the original design.", "Find out the inputs and outputs of SPIDER in the anchor paper."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The original design of SPIDER is a text-to-SQL dataset. The inputs are natural language queries, and the model should output corresponding SQL queries. In the anchor paper, the authors use the SQL queries as inputs, and train models to output SQL execution result tables.", "question": "In the paper \"MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering\", dataset SPIDER was used in the training process. What were the inputs and outputs in the original design of SPIDER, and how did the authors of this work adapt the dataset for their task?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c4db8fe1-fe59-5b60-af98-b3e8edd5ef16", "question": "Which language performs better on old sense IDs compared to new sense IDs during experiments?", "answer_format": "Your answer should be a string of a language name.", "tags": ["single", "table", "objective"], "anchor_pdf": ["55bc6198-c2b1-518f-9612-8d58ec050f2f"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the content that compares the performance of languages on old sense IDs and new sense IDs.", "Second, identify the language that performs better on old sense IDs in the related tabular."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Russian", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c50576c6-341e-5003-ab1a-f7ba4e61992a", "question": "Which paper is the following snippet from? snippet:{ Our basic system consists of two types of prompts, namely task description and fundamental inputs. We introduce a map-guided prompting method that builds an online-constructed topological map into prompts, activating the agent's global exploration.}", "answer_format": "Your answer should be a single string of a paper_uuid.", "tags": ["metadata", "retrieval", "objective"], "conference": ["acl2023", "acl2024"], "reasoning_steps": ["Search for the similar chunk to identify the paper the snippet belongs to.", "If several papers are found, refer to the content of papers to determine the source paper."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": ["0a412245-e6a6-5d0d-a25d-5b79b8c4faaf"], "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": [], "reference_pdf": []}
{"uuid": "c53cba22-704b-51db-ae71-0166a727b747", "question": "How to calculate the Word-pair Representation matrix in Figure 4?", "answer_format": "Your answer should be a list of formulas, representing the calculation of Word-pair Representation matrix.", "tags": ["image", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Figure 4.", "Second, get the content of Section 4.2, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The Word-pair Representation matrix in the described model is calculated through Conditional Layer Normalization (CLN). Here are the formulas representing the calculation:\n\n1. **Conditional Layer Normalization (CLN) for word-pair representation:**\n   $r_{i,j} = \\text{CLN}(h_i, h_j) = \\gamma_i \\odot \\left( \\frac{h_j - \\mu}{\\sigma} \\right) + \\lambda_i$\n\n2. **Scale factor $\\gamma_i$ and shift factor $\\lambda_i$ with additional contextual information:**\n   $\\gamma_i = W_\\gamma h_i + b_\\gamma$\n   $\\lambda_i = W_\\lambda h_i + b_\\lambda$\n\n3. **Mean $\\mu$ and standard deviation $\\sigma$ of $h_j$:**\n   $\\mu = \\frac{1}{d} \\sum_{k=1}^{d} h_{jk}$\n   $\\sigma = \\sqrt{\\frac{1}{d} \\sum_{k=1}^{d} (h_{jk} - \\mu)^2}$", "question": "How to calculate the Word-pair Representation matrix in Figure 4?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["1191a3cb-63fe-560c-bbef-c7eee0dd61d6"], "reference_pdf": []}
{"uuid": "c63f7ad7-00de-56f0-8b74-2fdf420ceaa2", "question": "How many models are evaluated in WebArena? What's the success rate of the most powerful model?", "answer_format": "Your answer should be a Python list of 2 elements. The first one is an integer. The second one is a float.", "tags": ["single", "text", "table", "objective"], "anchor_pdf": ["5a2b0d5c-6b51-5bbd-a001-a15f19f65a98"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find the section of experiment setting or baseline introduction or the experiment table", "Count the numbers of evaluated models", "Determine which model is the most powerful one", "Extract the performance of the mose powerful model"], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_int_exact_match", "eval_float_exact_match"], "eval_kwargs_list": [{"gold": 3}, {"gold": 14.41}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c67e3e4c-245b-509d-92b0-3ff41e82f9d4", "question": "What research advances are incorporated into the generative language model that used to generate associations in different languages in the anchor_pdf?", "answer_format": "Your answer should be a python list of several strings.", "tags": ["objective", "multiple", "text"], "anchor_pdf": ["fd81f90f-555d-5e99-835b-153c2cdb7303"], "reference_pdf": ["eb787b77-5188-5411-b0f8-406356623bac"], "conference": [], "reasoning_steps": ["Find the generative language model that used to generate associations in different languages in the anchor_pdf.", "Locate the paper about the model.", "Read the paper to find the research advances."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Compute-optimal scaling", "Improved dataset mixtures", "Architectural and objective improvements"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "c69c648f-bde4-5a8b-a82e-67fe2cdefe9f", "question": "What is the limitation of this work proposed by the authors themselves?", "answer_format": "Your answer should be plain text.", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the limitation section in the paper to locate the limitations of the work.", "Summarize or paraphrase the limitations mentioned by the authors."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The authors have only explored applying the approach to encoder models, leaving room for applications on decoder models.", "Despite the variety of existing contrastive learning methodologies, this work still adheres to utilizing the contrastive learning objectives provided by the tasks."], "question": "What is the limitation of this work proposed by the authors themselves?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["1a472825-70b5-5b91-a9a1-f60fcb8d89f5"], "reference_pdf": []}
{"uuid": "c6d527e4-0a3f-5c85-86a7-3b9bf155fa0a", "question": "Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?", "reference_answer": "Multi-target Backdoor Attacks for Code Pre-trained Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c7fd4be1-7261-5b8c-bdb8-0da621536182", "question": "Can we learn to represent an image with arbitary numbers of tokens?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Can we learn to represent an image with arbitary numbers of tokens?", "reference_answer": "SparseFormer: Sparse Visual Recognition via Limited Latent Tokens"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c8457d49-b7c2-5ed3-880e-be91d064d1d8", "question": "What baselines are used in this paper? Note that a baseline is counted only once, even if different variants are provided based on it.", "answer_format": "Your answer should be a python list of strings, every element of the list is the name of a baseline directly mentionned in this paper.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Firstly, locate the section(s) or page(s) about the baselines.", "Then identify the baselines used in this paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["NP-SpanBERT", "QA-SRL Parser", "TNE-Parser", "Mistral"], "lowercase": true, "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["7eb41d67-59e3-542a-8f03-93ab8f53216a"], "reference_pdf": []}
{"uuid": "c912d1d0-dced-53f0-9a89-5c982701fbb5", "question": "Is there any paper exploring real speakers and thus performing multimodal emotion recognition task?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper exploring real speakers and thus performing multimodal emotion recognition task?", "reference_answer": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "c95b6bb1-3445-5378-b465-2b4d4da30a17", "question": "What are the core parameters L, H, A, and the total number of parameters(params) of the base model of the classifier in this anchor paper?", "answer_format": "Your answer should be a python dictionary with keys 'L', 'H', 'A', and 'params', and the corresponding value should be a number, e.g., {'L': 1, 'H': 1, 'A': 1, 'params': 1}.", "tags": ["multiple", "objective", "text"], "anchor_pdf": ["2a4dd0fe-b10b-5155-82ea-3a28ba29a4fa"], "reference_pdf": ["ccf560db-a30b-552f-ab16-80026764a35e"], "conference": [], "reasoning_steps": ["Find the section mentioned the base model of the classifier, which is usually in method section.", "Locate the respective papers about this model.", "Find the parameters L, H, A and params of this model, which are usually in the model section.", "Translate the values of these parameters into numbers."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"L": 24, "H": 1024, "A": 16, "params": 550000000}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ca95aa28-b131-5cea-880a-63b9357ba912", "question": "Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?", "reference_answer": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "cb721bd4-b219-50b7-99f9-1f0a5f5da438", "question": "Which paper found that mutual learning benefits multlingual models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper found that mutual learning benefits multlingual models?", "reference_answer": "Towards Higher Pareto Frontier in Multilingual Machine Translation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "cb9cb4ee-c76a-5b00-bb19-9f238ac88b5f", "question": "When discussing \"Engagingness\" in the anchor PDF, what is the definition of engagingness with interestingness from prior research?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["1f63dd31-2d16-5885-84e4-a55d7c03ea8c"], "reference_pdf": ["8bd7983c-5a5b-50cb-99ab-62297274885c", "61add12c-1a79-5ef2-a38e-00e843271ad0", "450c1e1c-8f69-5d85-9a26-df3a876f65e1"], "conference": [], "reasoning_steps": ["Find the section discussing engagingness in the anchor PDF, especially engagingness with interestingness from prior research.", "Locate the related pdf in the reference PDFs.", "Find the detailed definition of engagingness with interestingness in the reference PDF."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The definition of engagingness should engage the partner in conversation, such as presenting an interesting factTherefore, an engaging response y should provide high volume of information that acknowledges both the history x to engage the partner and the context c which we assume contains relevant facts. This naturally leads to the following metric definition: ENGAGINGNESS (\\mathbf{y} , \\mathbf{x} , \\mathbf{c} ) = sum (align(\\mathbf{y} \\to [\\mathbf{x} , \\mathbf{c} ])), where we concatenate the history x and knowledge context c, and measure the extent of response y's acknowledgement of the information.", "question": "When discussing \"Engagingness\" in the anchor PDF, what is the definition of engagingness with interestingness from prior research?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "cc8b6743-e4fd-5365-b027-f6a70a30187e", "question": "Name a paper which proposes a probabilsitic formulation of retrosynthesis.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Name a paper which proposes a probabilsitic formulation of retrosynthesis.", "reference_answer": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "cd235027-4032-5403-964a-b2c7e7550966", "question": "How much percent does VerifiNER improve the F1 score of the three baseline models on average on GENIA?", "answer_format": "Your answer should be a Python float rounded to two decimal places WITHOUT ANY PUNCTUATION OR EXPLANATION. e.g. 21.30", "tags": ["objective", "single", "table"], "anchor_pdf": ["220f0021-1bf8-599f-ab3d-5b46d56cb03e"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Firstly, locate the result of VerifiNER on GENIA.", "Fetch the improvement on three baseline models respectively.", "Finally, calculate the average improvement and round it to two decimal places."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 7.05, "ndigits": 2, "tolerance": 1e-06}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "cd63b251-d7ef-58a6-83be-75d95099d550", "question": "Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?", "reference_answer": "Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "cd837558-b900-5448-9c36-9a0c0f29924d", "question": "Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?", "reference_answer": "CAME: Confidence-guided Adaptive Memory Efficient Optimization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "cd837c4f-07d1-5db8-84c1-f258aa7985ea", "question": "Considering both benefits and costs, what is the best size of generation pool for the proposed method?", "answer_format": "Your answer should be a single integer number.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Locate the experiment table or figure that shows the method performances with different sizes of generation pool.", "Think about what 'best' means, considering both benefits and costs comprehensively.", "Find out the best size of generation pool for the proposed method based on the experiment results."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["ca116924-bf11-5529-a43f-bf68e9745c5c"], "reference_pdf": []}
{"uuid": "cdfcefb3-e2be-515b-aa68-6baf717b17a2", "question": "What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?", "reference_answer": "From Sparse to Soft Mixtures of Experts"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ce769caf-b9cd-58c7-9b38-ee23d0d17f9b", "question": "How many more turns per dialogue are there in MMDU Benchmark than in MMDialog?", "answer_format": "Your answer should be an integer, the difference of average turns rounded to integer.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["1d672477-cc66-5013-8bd5-8180c44a884f", "954a1c0e-b7f6-5871-b284-492da7703fc2"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Read both papers to find the average turns per dialogue."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 10.5, "ndigits": 1, "tolerance": 0.6}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ce95db65-95c3-55d5-8eda-3e80ef6d0775", "question": "Using only task-level prompts or using only example-specific prompts, which is better on the Multi-Domain test set?", "answer_format": "Your answer should be a single string, either \"task-level\" or \"example-specific\".", "tags": ["objective", "single", "table", "text"], "conference": [], "reasoning_steps": ["Locate the experiment table that shows the model performances on the Multi-Domain test set with different distributions of prompts.", "Find out the mathematical symbols indicating the numbers of two types of prompts.", "Locate all the rows in the table related to the question.", "Compare the result data and determine the conclusion."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "example-specific", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": true}, "annotator": "human", "anchor_pdf": ["5a2b95c1-12d6-5b77-82a1-ee24180d27ae"], "reference_pdf": []}
{"uuid": "cff35edd-a526-59ea-a003-787ebabcd2d7", "question": "Which paper first applied the chain-of-thought technique in the text summarization field?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first applied the chain-of-thought technique in the text summarization field?", "reference_answer": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d011e781-2e29-52ea-8281-e7bc25c68622", "question": "What paper first proposed a robust perceptual similarity metric with certificates?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first proposed a robust perceptual similarity metric with certificates?", "reference_answer": "LIPSIM: A PROVABLY ROBUST PERCEPTUAL SIMILARITY METRIC"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d0157667-a921-5e91-8948-4e0f31b3010c", "question": "Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?", "reference_answer": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d170af87-1580-52a1-b6d1-814f2ddbfac4", "question": "What's the evaluation baseline used in the anchor paper? What's the contributions that makes this baseline different from existing adversarial datasets?", "answer_format": "Your answer should be a python list of two strings, the first element is the baseline name(the abbrievation format is enough), and the second element is the contributions.", "tags": ["text", "multiple", "subjective"], "conference": [], "reasoning_steps": ["First, locate the section about evaluation in the anchor paper.", "Second, find the evaluation baseline used in the anchor paper and its source paper.", "Finally, turn to the source paper to find the contributions that makes this baseline different from existing adversarial datasets."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_partial_scoring_points_with_llm"], "eval_kwargs_list": [{"gold": ["AdvGLUE"], "lowercase": true}, {"scoring_points": ["Comprehensive Coverage:AdvGLUE is able to cover as many adversarial linguistic phenomena as possible.", "Systematic Annotations: this is the first work that performs systematic and comprehensive evaluation and annotation over 14 different textual adversarial examples. ", "General Compatibility:  AdvGLUE covers the widely-used GLUE tasks and creates an adversarial version of the GLUE benchmark to evaluate the robustness of language models", "High Transferability and Effectiveness: AdvGLUE has high adversarial transferability and can effectively attack a wide range of state-of-the-art models."], "question": "What's the contributions that makes this baseline different from existing adversarial datasets?", "count": 3}]}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["8c5db97f-f499-5641-8d74-d0d64d980f53"], "reference_pdf": ["32a1dee2-310a-5ead-8d2f-c957cc59e3dc"]}
{"uuid": "d1df78e0-b32e-5878-b302-ae1d5408e8a7", "question": "What is a paper studying data being collected in bundles in reinforcement learning ?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What is a paper studying data being collected in bundles in reinforcement learning ?", "reference_answer": "Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d2369c7a-ea10-5299-818e-78c80de60a82", "question": "Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?", "reference_answer": "TOWARDS FOUNDATION MODELS FOR KNOWLEDGE GRAPH REASONING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d2ce712c-a887-538c-a0fd-4cf01de110d4", "question": "Is there any paper that leverages syntactic rules to explicitly guide text generation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that leverages syntactic rules to explicitly guide text generation?", "reference_answer": "Explicit Syntactic Guidance for Neural Text Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d2f3c57b-d05d-522b-b8bf-21651c72b837", "question": "What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?", "reference_answer": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d35568c3-eed9-5383-a49a-c363470c175d", "question": "In the main evaluation results of the different baselines in the anchor PDF, both of which use CodeLLaMA as the base model, which one performs better? In the paper introducing this model, aside from the datasets mentioned in the anchor PDF, what other in-domain datasets are used?", "answer_format": "Your answer should be a python list, the first element is the name of the baseline model, and the following elements are the in-domain datasets used in the paper, e.g.,[\"baseline_model_name\", \"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "objective", "text", "table"], "conference": [], "reasoning_steps": ["Find the section mentioned the main evaluation results of the different baselines in the anchor PDF, which is usually in the table of the experiments section.", "Compare the performance of the two baselines that use CodeLLaMA as the base model to find the one that performs better.", "Locate the respective paper introducing this model.", "Find the section mentioned all the in-domain datasets used in the paper introducing this model, which is usually in the table of the experiments section.", "Compare the datasets mentioned in the anchor PDF with the datasets mentioned in the paper introducing this model to identify the other in-domain datasets used in the paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["MAmmoTH-Coder", "AQuA-RAT", "NumGLUE"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["7d6f212e-3d4c-5cb2-877c-5d233ae46f3b"], "reference_pdf": ["ecba768d-4b87-58ca-968b-2a375793a798", "b846c66a-a177-5119-af8d-ec4757d6a06c"]}
{"uuid": "d4046885-386a-5ea9-a53e-44a4d33ab4b4", "question": "Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?", "reference_answer": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d49c4e91-ace9-5ba1-a728-6083ffc72194", "question": "According to Table 3, on which single-task and on which metric, no multi-task model can outperform the corresponding single-task model?", "answer_format": "Your answer should be a Python list of two strings. The first string is the name of the task, and the second string is the name of the metric.", "tags": ["single", "text", "table", "objective"], "anchor_pdf": ["c3936fc4-4cf3-5550-b694-4cdc10986752"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate Table 3 in the paper and read the text in its caption.", "Determine the row corresponding to the single-task models.", "Find the column where a single-task model performs best."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["TextLM", "sBLIMP"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "d4cca186-6fd8-5b84-a38a-6145ceaec283", "question": "What's the second method discussed in the \"Related Work\" of the anchor PDF? In the work that proposed that method, how many fewer sabotages per game in average did the method propsed by the authors make than their baseline, under \"Non-repulser\" condition?", "answer_format": "Your answer should be a Python list of 2 elements, the first is a string, the name of the method, and the second is a float rounding to 2 decimal places, the difference of sabotages per game in average.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["00c08aac-4af9-5950-a3c2-9a0837e1cc1b"], "reference_pdf": ["d374f4a0-de68-572c-ba4f-166ff5ee6f28"], "conference": [], "reasoning_steps": ["Read the anchor_pdf, locate the \"Related Work\" section, and find the second method discussed.", "Locate the correct reference_pdf", "Find the table that shows the number of sabotages per game for different methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ADVERSITY", 1.52], "ndigits": 2, "lowercase": true, "ignore_order": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "d5e3a89b-4ef9-5ce5-b80b-76cba7c02e76", "question": "What are the detailed hyperparameters of the sentence-level scorer in Fine-grained Evaluation System of the anchor PDF?", "answer_format": "Your answer should be a python dictionary about the hyperparameters, e.g. {\"hyperparameter1\": \"value1\", \"hyperparameter2\": \"value2\", ...}. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["597bfc37-ca03-5675-905f-52e2f69e1a8c"], "reference_pdf": ["ded3a96f-b919-5ad4-88d2-50547ed66c96"], "conference": [], "reasoning_steps": ["Find the name of the sentence-level scorer in the Fine-grained Evaluation System section of the anchor PDF.", "Locate the relevant paper about the model.", "Identify the detailed hyperparameters of the sentence-level scorer, which are usually presented as a table in the architecture section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Hidden size": "2048", "Immediate Hidden Size": "5632", "Context Len": "2048", "Heads": "32", "Layers": "32", "Vocab Size": "32000"}, "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "d5ea5e23-0a82-5621-9932-ff0f19a68885", "question": "The anchor paper use two pre-trained models for experiment. For the newer one, what is its name and based on what task is it pre-trained?", "answer_format": "Your answer should be a single python list, the first element is the name of the model, the second element is the task name it is pre-trained on.", "tags": ["text", "multiple", "objective"], "conference": [], "reasoning_steps": ["First, locate the section about experiment in the anchor paper.", "Second, find the two pre-trained models used and the source paper.", "Finally, find the pre-training task of the newer model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ELECTRA", "replaced token detection"], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["3aaa5bca-d686-5f64-b1ef-92d8b28fb733"], "reference_pdf": ["c4d02102-b1c7-5b72-a414-9c175a49be48"]}
{"uuid": "d60762bb-75c6-57d0-918b-9df1525d7269", "question": "In the introduction of the anchor paper, five tracks are mentioned. What are the detailed definitions of Track 2 and Track 3?", "answer_format": "Your answer should be a python strings about the detailed definition of Track2 and Track3.", "tags": ["multiple", "subjective", "text"], "conference": [], "reasoning_steps": ["Find five tracks mentioned in the introduction of the anchor paper.", "Locate the reference paper which introduces the five tracks.", "Retrieve the detailed definitions of Track 2 and Track 3."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The formulation of this track2 is to predict, for each essay, Batson's empathic concern (\"feeling for someone\") and personal distress (\"suffering with someone\") scores. Teams are expected to develop models that predict the empathy score for each essay (self-report data from the essay writer). Both empathy and distress scores are real values between 1 and 7. Empathy score is an average of 7-point scale ratings, representing each of the following states (warm, tender, sympathetic, softhearted, moved, compassionate); distress score is an average of 7-point scale ratings, representing each of the following states (worried, upset, troubled, perturbed, grieved, disturbed, alarmed, distressed). These are state measures: measures that vary within people across time. The formulation of track3 is to predict, for each essay, one or more emotion labels from the following Ekman's six basic emotions (sadness, joy, disgust, surprise, anger, or fear) as well as neutral, and we also added hope.", "question": "In the introduction of the anchor paper, five tracks are mentioned. What are the detailed definitions of Track 2 and Track 3, and how do they differ from each other?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["b498ac0f-d8db-56e7-8809-1ef9c7e25e02"], "reference_pdf": ["699b8024-5de8-5f10-9b89-94bdc13e3e68"]}
{"uuid": "d68e9387-5bbd-5a7d-9091-bc67f849d296", "question": "Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?", "reference_answer": "PEACOK: Persona Commonsense Knowledge for Consistent and Engaging Narratives"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d74c128e-d9ec-545c-b10b-d3d3116d9ec9", "question": "How many samples are there in SeeClick's general vision-language instruction-following data?", "answer_format": "Your answer should be a single number rounding to thousands, e.g. 15000.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["6e9001d2-7637-5049-ad35-2adfdfc9c8d1"], "reference_pdf": ["86922a0e-7874-5f9a-926b-0f886076d6e8"], "conference": [], "reasoning_steps": ["Read the section that discusses data construction to find where general data comes from.", "Read the corresponding paper to identify the number of samples."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 158000}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "d7aa4317-7e09-53d4-9b5d-61b51995b83f", "question": "Which paper first applied the chain of thought concepts in 3D localization problem?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first applied the chain of thought concepts in 3D localization problem?", "reference_answer": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "d82a4438-587e-5405-9351-319110cd89de", "question": "What is the average proportion of papers in the ACL anthology in recent ten years which mention the words speech, spoken or audio in the title?", "answer_format": "Your answer should be a Python float number roudning to 3 decimal places, e.g., 0.001.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the section that discusses the proportion of papers in the ACL anthology which mention the words speech, spoken or audio in the title.", "Find the exact value of annual propotion in recent ten years.", "Finally, calculate or estimate the average propotion."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.035, "nidigits": 3, "tolerance": 0.0001}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["a88d4d17-b2f9-520e-a611-c2c4ba178be5"], "reference_pdf": []}
{"uuid": "da0eec1f-57e5-5fd8-aff2-cd21493eb60c", "question": "Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues?", "reference_answer": "PAED: Zero-Shot Persona Attribute Extraction in Dialogues"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "da8f996b-f289-5718-ac05-36ba34285a28", "question": "Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?", "reference_answer": "MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "da9752f5-e86d-577d-a99b-88a399197e6a", "question": "How many multi-modal baselines excluding the method this paper proposed do authors use? Among this baselines, who reaches the highest F1 score on Twitter2015 dataset? And what about Twitter2017 dataset?", "answer_format": "Your answer should be a python list, the first item is the number of multi-modal baselines the paper used, the second item and the third item are the name of methods reaching the highest F1 score on Twitter2015 and Twitter2017 dataset respectively.", "tags": ["metadata", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the content of Table 2.", "Second, get the content of row 'Multimodal' and count the number of multi-modal baselines.", "Third, check the 'F1' column under 'Twitter2015' and 'Twitter2017' multi-column and find the one reaching highest F1 score."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_int_exact_match", "eval_string_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": 8}, {"gold": "VLP-MABSA"}, {"gold": "CMMT"}]}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["6a24d7f4-430d-5c92-b259-f62f76490147"], "reference_pdf": []}
{"uuid": "db1901ae-ae9a-5f74-9479-c1846458d265", "question": "Is there a paper which applies Bayesian optimization to modular continual learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper which applies Bayesian optimization to modular continual learning?", "reference_answer": "A Probabilistic Framework for Modular Continual Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "db9b0fe4-a8e1-5344-8fab-77bbea36c1f1", "question": "Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?", "reference_answer": "Pre-Training to Learn in Context"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "dc151869-421a-5d8e-b56b-af7266c08585", "question": "What training acceleration methods are compared in the original paper that describes the methods used in training the FLM-101B model?", "answer_format": "Your answer should be a python list of strings.", "tags": ["multiple", "objective", "text", "table"], "anchor_pdf": ["0207c0f7-4f0a-5aca-a744-749680da8934", "58fb66aa-41bf-59a9-8e27-d9effa1f81aa"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the section on training methods in the FM-101B paper to learn about the training acceleration algorithms used.", "Turn to the original paper describing the training acceleration algorithms.", "Look for the related work section to find several training acceleration algorithms that are compared."], "evaluator": {"eval_func": "eval_element_list_included", "eval_kwargs": {"gold": ["Stacking", "CompoundGrow", "Staged", "Bert2BERT", "LiGO"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "dc634e00-e936-527b-b3e1-93565fa0178b", "question": "What are some data-efficient ways to learn text embeddings thru contrastive learning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What are some data-efficient ways to learn text embeddings thru contrastive learning?", "reference_answer": "Composition-contrastive Learning for Sentence Embeddings"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "dcd9e737-6a5f-519f-8357-0a0e6d002c1e", "question": "Which two subsets does the benchmark used as evaluation set for BLOOM and BLOOMZ models in the anchor paper merged from?(The anchor paper is named \"An Empirical Study of In-context Learning in LLMs for Machine Translation\")", "answer_format": "Your answer should be a single python list, every element of the list is a string of the abbrievation name of the subset, e.g.[\"TAT-Conv\",\"TAT-Web\"].", "tags": ["objective", "multiple", "text"], "anchor_pdf": ["bffc816d-612c-5fea-83bb-1ac6b290480b"], "reference_pdf": ["b40e8ca1-d3e8-5553-ac3c-d6ca0b21c628"], "conference": [], "reasoning_steps": ["Firstly, locate and identify the benchmark used as evaluation set for BLOOM and BLOOMZ models in the anchor paper.", "Then find the most relevant reference paper about this benchmark.", "Finally, locate the section in the reference paper about the description of the benchmark to extract the information about the merged subsets."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["IN22-Wiki", "IN22-Web"], "lowercase": true, "ignore_blank": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ddd6cf56-5026-5482-b637-f8dd9a20acf6", "question": "Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?", "reference_answer": "The Marginal Value of Momentum for Small Learning Rate SGD"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "de8d9b7f-4117-53d1-988d-77036e001339", "question": "Which approach was first proposed to solve the text-conditioned image retrieval task according to the anchor_pdf? In the approach, how to compute gating connection?", "answer_format": "Your answer should be a python string and a python strings about a fomula in latex format.", "tags": ["multiple", "text", "formula", "subjective"], "anchor_pdf": ["246cdbd7-d6bf-5e4f-8836-5b975f544162"], "reference_pdf": ["c7a72a50-58c0-5468-a9b1-e606af443337"], "conference": [], "reasoning_steps": ["First, find the approach in the anchor_pdf.", "Then, locate the relevant paper about the approach.", "Finally, find the fomula calculating the gating connection according to the approach."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_complex_math_formula_with_llm"], "eval_kwargs_list": [{"gold": "TIRG"}, {"fomulas": "f_{\\text{gate}}(\\phi_x, \\phi_t) = \\sigma \\left( W_{g2} * \\text{RELU} \\left( W_{g1} * [\\phi_x, \\phi_t] \\right) \\right) \\circ \\phi_x", "question": "Which approach was first proposed to solve the text-conditioned image retrieval task according to the anchor_pdf? In the approach, how to compute gating connection?"}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "df2d7dce-b86a-5805-b524-3f453268240f", "question": "According to Figure 1, in which year has the highest proportion of NLP papers that explicitly mention speech-related terms in their title?", "answer_format": "Your answer should be the year number with the highest proportion, e.g. 2000.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["First, get the content of Figure 1.", "Second, identify the answer and check it with the content of section 2.2."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 1989}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["a88d4d17-b2f9-520e-a611-c2c4ba178be5"], "reference_pdf": []}
{"uuid": "df46a4db-9a21-55a7-b84a-7764604b47c5", "question": "Is there any paper that uses Lipschitz continuity in learning a dynamics model?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that uses Lipschitz continuity in learning a dynamics model?", "reference_answer": "CCIL: CONTINUITY-BASED DATA AUGMENTATION FOR CORRECTIVE IMITATION LEARNING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "df8afde5-4e93-5e03-86a6-a98bcccdc1e7", "question": "What paper provides generalization bounds for self supervised learning models eg. CLIP", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper provides generalization bounds for self supervised learning models eg. CLIP", "reference_answer": "Understanding prompt engineering may not require rethinking generalization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e010a084-060b-5edb-8ff5-9be8bc82f010", "question": "Which baselines are chosen to study for Seperate Training based methods in the anchor paper named \"Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation\"? In the source paper of the baseline 'Prior', what control framework is proposed?", "answer_format": "Your answer should be a single python list,the first element is a list of strings of the baselines, the second element is the string about the control framework.e.g.[[\"Prior\",\"Baseline2\"],\"The paper proposes a novel control framework that introduces...\"].", "tags": ["subjective", "multiple", "text"], "anchor_pdf": ["e01387cf-ffe2-5a64-8f38-f9f07b77a4fa"], "reference_pdf": ["58a12963-0058-5883-93d9-a0abc4b9cc63"], "conference": [], "reasoning_steps": ["Firstly, locate the relevant section in the anchor paper to identify the baselines selected.", "Then turn to the source paper about the baseline 'Prior'.", "Finally, locate the section in the paper about the control framework it proposed(usually in abstract, introduction or conclusion)."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The paper proposes a novel framework that introduces a well-formed prior space for effective and flexible control via invertible transformation.", "question": "For the source paper of the baseline 'Prior', what control framework is proposed?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "e03ad6fe-d951-5f49-b4b8-6415e0c8203e", "question": "Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?", "reference_answer": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e0411ac6-86d2-52ff-bcc1-4e9dba8177c5", "question": "Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?", "reference_answer": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e1180112-dc52-5a5c-9907-6d007f17b729", "question": "I am a beginnner to the field of NLP, and I wonder roughly how many papers on average should one paper cite, given the provided paper list.", "answer_format": "Your answer should be a single integer number, rounded from the average reference number.", "tags": ["multiple", "metadata", "objective"], "conference": [], "reasoning_steps": ["Firstly, find the reference section in the paper to locate the number of citations.", "If the number of citations is not explicitly mentioned, count the number of references in the reference section.", "Finally, calculate the average number of citations per paper, and round it to an integer."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 43}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0aec0c5f-463b-5907-9e16-e637504b3c3d", "0b1a831b-f792-58bc-a07f-4e73a8fd0e60", "0b77c64c-6b36-5501-848f-79a062be2a45"], "reference_pdf": []}
{"uuid": "e1184bfe-06e5-5294-9f96-fa353008ba83", "question": "Who are the authors of this paper? What're their institutions?", "answer_format": "Your answer should be a Python dictionary, e.g. {\"Amy\": [\"Massachusetts Institute of Technology\", \"Carnegie Mellon University\"], \"Bob\": [\"Shanghai Jiaotong University\"]}. YOU MUST USE THE FULL AND EXACT WORDS FROM PDF.", "tags": ["single", "metadata", "objective"], "conference": [], "reasoning_steps": ["Firstly, locate the authors and institutions.", "Finally, "], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Gili Lior": ["Allen Institute for AI", "The Hebrew University of Jerusalem"], "Yoav Goldberg": ["Allen Institute for AI", "Bar-Ilan University"], "Gabriel Stanovsky": ["Allen Institute for AI", "The Hebrew University of Jerusalem"]}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["28f25ac1-c82a-5208-84b4-8ac1a33ea481"], "reference_pdf": []}
{"uuid": "e24d9741-a47e-5f69-8bde-ead5219761be", "question": "In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?", "reference_answer": "Seer: Language Instructed Video Prediction with Latent Diffusion Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e26e0a8e-7e6b-55b0-b658-af94309cd496", "question": "According to the experimental results, if we remove the document fact attention module and use mean pooling to fuse all document semantic representation vectors, by how much does the F1 score of FINEGRAINFACT decline in summaries generated by pre-trained language models published in or after 2020?", "answer_format": "Your answer should be a single python float", "tags": ["objective", "single", "table", "text"], "conference": [], "reasoning_steps": ["Locate the table about the performance results in summaries generated by different systems.", "If the column names of the table don't match the question, refer to the section about the experiment design for further clarification or naming of different modules and models.", "Compare and calculate the F1 score drop when switching from the attention module to mean pooling."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.33}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0e6978a1-3a5d-5fdd-808a-033cc79fb049"], "reference_pdf": []}
{"uuid": "e26fafda-6b5f-5a6c-8fe6-57647a29c7e7", "question": "Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?", "reference_answer": "EMO: EARTH MOVER DISTANCE OPTIMIZATION FOR AUTO-REGRESSIVE LANGUAGE MODELING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e2e3bd05-d47f-5602-83a5-06b21a463035", "question": "Which machine learning paper proposed certified robustness in the malware detection domain?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which machine learning paper proposed certified robustness in the malware detection domain?", "reference_answer": "DRSM: DE-RANDOMIZED SMOOTHING ON MALWARE CLASSIFIER PROVIDING CERTIFIED ROBUSTNESS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e30796f1-1fa4-516f-8295-ba45725d32de", "question": "Is there a dataset that allows to perform aspect-based sentiment classification on French news?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a dataset that allows to perform aspect-based sentiment classification on French news?", "reference_answer": "MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e36edbbf-6630-5a7c-9706-9c4932d865cf", "question": "How many GPUs would be required to train a 70M model used in Anchor PDF if we had used the batch sizes from the GPT-3 suite?", "answer_format": "Your answer should be a single integer.", "tags": ["multiple", "objective", "table"], "anchor_pdf": ["1a823707-4dc8-5954-8fe1-de9ba161f77a"], "reference_pdf": ["84eb4718-0ace-52b2-a378-eb5245708462"], "conference": [], "reasoning_steps": ["Find the model used in the Anchor PDF, which is usually in the Model section.", "Locate the relevant paper in the Reference PDF.", "Find the exact number of GPUs required to train the model, which is usually in the form of a table."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "e3a6b6b4-9899-53e5-b338-c77a80ee71a5", "question": "How to achieve zero-shot lip reading?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "How to achieve zero-shot lip reading?", "reference_answer": "OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e4945000-28d9-5f63-b821-f3def54eb88c", "question": "Has there been any recent work or competitions focused on the development of methods to counteract clickbait through spoiling, such as revealing key information upfront?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Has there been any recent work or competitions focused on the development of methods to counteract clickbait through spoiling, such as revealing key information upfront?", "reference_answer": "SemEval-2023 Task 5: Clickbait Spoiling"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "e4b24c60-eafc-51ff-96ba-7fabb64fc15d", "question": "In the training of LangBrige, when adapting finetuned LMs, is multilingual encoder trainable?", "answer_format": "Your answer should be a Python bool of true or false.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Usually, the training details of model are explained in the method or experiment section of the paper. Retrieve these sections from the paper.", "Based on more specific requirements, find the context mentioned  multilingual encoder and its training.", "Finally, based on the context, determine if the multilingual encoder is trainable or not."], "evaluator": {"eval_func": "eval_bool_exact_match", "eval_kwargs": {"gold": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["11f71fa7-fd1b-5223-8ee1-53ecd8519ed7"], "reference_pdf": []}
{"uuid": "e5b21555-1c9a-5275-be50-1a418f9a59d6", "question": "What are the meanings of function $s$ and function $F$ in Equation (3)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (3).", "Second, get the content of Section 4.1 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$s(\\cdot)$ is the cosine similarity and $F(\\cdot)$ is a non-linear mapping on the output representations of PLM.", "question": "What are the meanings of function $s$ and function $F$ in Equation (3)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["875e65e0-8e9f-52e3-9d3e-65f15fa1ea82"], "reference_pdf": []}
{"uuid": "e6a69aa0-9915-5c51-a2e8-ba90140fe58e", "question": "Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?", "reference_answer": "ConvGQR: Generative Query Reformulation for Conversational Search"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e6a78d3c-1bfe-55ea-b070-712554f7cae9", "question": "In the domain of prediciton questions, what's the largest issue for LLAMA 2?", "answer_format": "Your answer should be a word or phrase, indicating the largest issue. YOU MUST USE THE EXACT WORDS FROM PDF WITHOUT EXPLANATIONS.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the figure that lists the distribution of different issues.", "Finally, find the largest issue for given model."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Multi-Answers"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["10e37be8-8c9f-590c-b94f-e6bea03794f2"], "reference_pdf": []}
{"uuid": "e7356e42-a08e-5c65-abb0-6e00ea2a400a", "question": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?", "reference_answer": "Teaching Arithmetic to Small Transformers"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e744adb1-ce7c-5d3e-9b2f-a8790dfb6cb7", "question": "According to the anchor PDF, what are the baseline models used in the experiments of both the two most recent model papers and this paper?", "answer_format": "Your answer should be a python list of elements, each element is the baseline model name string, e.g., [\"model1\", \"model2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "table", "objective"], "conference": [], "reasoning_steps": ["Find the section mentioned the two most recent baseline models used in the experiments, which is usually in the table of the experiments section.", "Locate the respective papers.", "Check these papers to find all the baseline models used in the experiments.", "Identify the common baseline models used in the experiments of both the two most recent model papers and this paper."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["R-BERT", "ZS-BERT"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["97121a3c-fe43-541b-8a54-ae6f7eb19b58"], "reference_pdf": ["92a36b43-491e-5143-8319-e630273ccb0a", "b1038525-40b7-5470-a47c-13470cd55625"]}
{"uuid": "e8654b21-dff6-5447-90f4-afb0974ce94d", "question": "Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?", "reference_answer": "Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e87fa3e0-7d2f-5909-8e01-5c2d8de2e64c", "question": "Which dataset did not get improved performance after applying the proposed RECOST method to Alpaca-gpt4, compared to the Random baseline? Tell me this worst-performing dataset. And what's the remaining performance gap for our best-performing RECOST method compared to the reported human upper bound on the testset for that dataset?", "answer_format": "Your answer should be a Python list of two elements, where the first element is the name of the dataset, and the second element is a float number, calculated by subtracting the performance of the best-performing RECOST method from the reported human upper bound performance for that dataset.", "tags": ["metadata", "multiple", "objective", "table", "text"], "conference": [], "reasoning_steps": ["Firstly, find the main experiment section in the paper RECOST.", "According to the table content, find the worst-performing dataset name after applying the proposed RECOST method to Alpaca-gpt4.", "Search for the relevant paper for the dataset in the reference list.", "In the linked paper, find the experiment section, and locate the row or column which reports the human upper bound performance on the testset for that dataset.", "Finally, calculate the remaining performance gap by subtracting the performance of the best-performing RECOST method from the reported human upper bound performance for that dataset.", "Report the combination of the dataset name and the performance gap as a Python list."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_float_exact_match"], "eval_kwargs_list": [{"gold": "Hellaswag", "lowercase": true}, {"gold": 14.97, "ndigits": 2, "tolerance": 0.0001}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0e1f319d-ad46-5528-9559-9208708536e9"], "reference_pdf": ["7d4754c9-e8ac-51de-aa10-0bb4df7c4ff0"]}
{"uuid": "e89d9ee6-ed85-55bc-98fc-687823d1695f", "question": "What data augmentation strategies are used in the recently proposed dataset used in the anchor PDF?", "answer_format": "Your answer should be a python strings about the detailed data augmentation strategies.", "tags": ["multiple", "subjective", "text"], "conference": [], "reasoning_steps": ["Find the recently proposed dataset used in the anchor PDF, which is usually in the experimental section.", "Locate the relative papers about the dataset.", "Find the data augumentation strategies in the paper."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "To achieve the balance between the two-way translation in language pairs, two data augmentation strategies were utilized to enrich the corpus if necessary: In cases where the number of parallel corpus falls below 1 million, we flip the entire corpus to create the corpus for the opposite translation direction. In contrast, for corpora with more than 1 million instances, we randomly flip half the amount of corpus to generate the corresponding corpus. After data augmenting, the initial corpus of 142 translation directions is substantially enriched, expanding to a significantly larger corpus of 242 translation directions.", "question": "What data augmentation strategies are used in the recently proposed dataset used in the anchor PDF?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["2c601b1c-36cd-5106-9b44-889c55a377c8"], "reference_pdf": ["063cfa76-8115-5e5e-a5c3-c794ee055b2c"]}
{"uuid": "e8c52858-a386-5e87-a9ba-3a7ec32ae1e2", "question": "What are the catogories of label biases in in-context learning for text classification and what are the definitions of these categories?", "answer_format": "Your answer should be a Python list of text strings, with each element being one category that this paper defines, e.g., [\"category 1: define 1\", \"category 2: define 2\", ...].", "tags": ["single", "subjective", "text"], "conference": [], "reasoning_steps": ["Usually, the definitions of main concepts are mentioned in the introduction section. Search the correpsonding paragraphs.", "If the categories are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the definition of each category from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["vanilla label bias: The model's non-contextualized preference for the label names (e.g., the common token bias caused by different frequencies of label names in the pretraining corpus).", "context-label bias: The effects of the context prompt (e.g., LLMs tend to prefer the majority and last label of the in-context examples).", "domain-label bias: The effects of the task corpus on the model's predictions."], "question": "What are the catogories of label biases in in-context learning for text classification and what are the definitions of these categories?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["15f6962b-d927-5d71-b01e-f0664e09eeb5"], "reference_pdf": []}
{"uuid": "e910df81-f6bc-5c88-8df2-b99ce1990a47", "question": "Which work discusses an analysis of source and target contributions to output generation based on local interpretation when machine translation models experience hallucinations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which work discusses an analysis of source and target contributions to output generation based on local interpretation when machine translation models experience hallucinations?", "reference_answer": "Local Interpretation of Transformer Based on Linear Decomposition"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "e91cd875-a6a7-540d-80be-279f30dd2e4a", "question": "Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?", "reference_answer": "In-Context Learning through the Bayesian Prism"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ea3a6252-6542-58e2-85e5-0c5274fac510", "question": "Out of the four baselines used by the anchor PDF, which one is also utilized as a baseline by the other three papers? Additionally, what are the two other baselines that the three papers have in common?", "answer_format": "Your answer should be a Python list with two elements. The first element should be the full name of the baseline shared by all four papers (the anchor PDF method and the three other baselines). The second element should be a Python list containing the full names of the two additional baselines shared by the three papers. Ensure the baseline names are the full names.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["0a7bb69e-e83a-56b7-80ca-cbad1621dde4"], "reference_pdf": ["a3c5d97a-3112-50fa-b800-2dfc7c3e5fb4", "71357d95-65ac-5dea-8ff4-1f7ddd27af1b", "3cb3a9ff-b420-56a6-9948-7518501efc0a", "dec93516-8248-5c04-861d-9a380d21c9cb"], "conference": [], "reasoning_steps": ["Read the anchor_pdf and locate the section that discusses the baselines used in the paper.", "Read the reference_pdfs, locate the sections that discuss the baselines used in the papers, and identify the baselines shared by the papers."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_structured_object_exact_match"], "eval_kwargs_list": [{"gold": "Graph Query Embedding", "lowercase": true}, {"gold": ["Query2Box", "Beta Embedding"], "lowercase": true, "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ea497ecc-8bd7-5954-a3ac-d212432c7feb", "question": "Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??", "reference_answer": "A Survey on Asking Clarification Questions Datasets in Conversational Systems"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ea965a94-3dc2-58e6-93e6-6da8d839e7e8", "question": "What is a large event-coverage general-domain event argument extraction dataset?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What is a large event-coverage general-domain event argument extraction dataset?", "reference_answer": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "eb327f3c-93ea-5851-a544-9c05b109ac16", "question": "In the experiments, which datasets did the authors use, and how many samples are there in the training set of each dataset?", "answer_format": "Your answer should be a Python dictionary, where the keys are the names of datasets and the values are the number of samples in the respective training set. e.g. {\\\"dataset1\\\": 10, \\\"dataset2\\\": 20, ...} .", "tags": ["single", "table", "objective"], "anchor_pdf": ["4f6a36fe-eac7-58cf-81e9-584f786b2f38"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the datasets used in the paper, which are typically found in the experimental section.", "Then, enumerate the datasets mentioned in the paper and record the number of samples in the training set for each dataset."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"BIOSSES": 60, "CASUAL JUDGEMENT": 90, "EPISTEMIC REASONING": 500, "TEMPORAL SEQUENCE": 300, "IAC Vulnerability Detection": 166, "HOTPOTQA": 50}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "eb3a5dd5-0008-5edf-b8e7-8cebd614f282", "question": "In the survey of Large Language Models for NL2Code, what are the multi-lingual benchmarks to evaluate the NL2Code task, and how many instances do they contain per promgramming language?", "answer_format": "Your answer should be a Python dictionary of entries, each dictionary key is a string, the benchmark name DIRECTLY FROM THE PDF WITHOUT CHANGING CAPITALIZATION, and each value is an integer of the corresponding instance number, e.g., {\"benchmark1\": 10, \"benchmark2\": 100}, ....", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Usually, the details about benchmarks are mentioned in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Finally, answer the question with the benchmark names and their instances numbers."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"MBXP": 974, "MBXP-HumanEval": 164, "HumanEval-X": 164, "MultiPL-HumanEval": 164, "MultiPL-MBPP": 974}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["37758401-6101-554f-8f1e-4e2995443314"], "reference_pdf": []}
{"uuid": "ebd5482c-b856-5427-876b-fcd24759d8d4", "question": "MMD and xVal, a baseline in the anchor paper, both aim to solve the problem of embedding numbers in language models. Did the tasks focused on by the two papers belong to the same domain? If not, what types of tasks does xVal focus on?", "answer_format": "Your answer should be brief text answering whether the tasks focused on by the two papers belong to the same domain, and if not, the domain of the task focused on by xVal.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["58a12475-a7ce-5d3a-b75e-04814b025231", "98849314-13f0-558a-adf7-f2764d2bf67b"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Identify the domain of the task focused on by MMD.", "Locate the paper which proposed xVal.", "Identify the domain of the task focused on by xVal.", "Compare the domains of the tasks focused on by the two papers."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "No, they don't. xVal mainly focuses on prediction tasks involving numbers in the scientific domain.", "question": "MMD and xVal, a baseline in the anchor paper, both aim to solve the problem of embedding numbers in language models. Did the tasks focused on by the two papers belong to the same domain? If not, what types of tasks does xVal focus on?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ec05c8e8-b789-514f-802e-7c710b0bec67", "question": "In the main results of ShortGPT's source paper, which paper does the experimental results of several comparison methods come from? Does the method proposed in this paper require post-training?", "answer_format": "Your answer should be a python list of two elements. The first element is a python string, the paper's full name. The second element is a python bool.", "tags": ["multiple", "table", "text", "objective"], "anchor_pdf": ["48d9d307-c254-597f-9444-5c420a973c0d", "017b741f-d588-5124-9971-af37b2f806ae"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the main results table in the source paper of ShortGPT", "Second, find the source paper that provides the experimental results of comparison methods", "Third, turn to the paper and locate the methodology section to check if post-training is needed"], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_string_exact_match", "eval_bool_exact_match"], "eval_kwargs_list": [{"gold": "LaCo: Large Language Model Pruning via Layer Collapse", "lowercase": false}, {"gold": false}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ecef28ab-8648-51af-b77f-91d2ed598e89", "question": "Which one of the prior works on state space models by the same team that published the Mamba paper proposes FlashConv for accelerating state space model training?", "answer_format": "Your answer should be a python string, the full paper name of the prior work.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["618b736e-5c9f-5c00-8889-9589bdad0620", "f9291deb-da46-5c68-8636-0d39ead63ea5", "a2e013ab-e738-5c74-af0b-d1b313c31909", "b3b6d154-6610-59f3-989a-06c84e5e22b3"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, identify the team that published the Mamba paper", "Second, find the prior works on state space models by the same team in the related work section", "Third, turn to the source paper of each prior work and check if the prior work proposes FlashConv for accelerating state space model training", "Finally, return the full paper name of the prior work that proposes FlashConv for accelerating state space model training"], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models", "lowercase": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ed62604f-0aad-569a-9105-8381212aeb43", "question": "What is the optimal number of layers to skip for LLaMA2-13B?", "answer_format": "Your answer should be a Python integer number. e.g. 3", "tags": ["single", "image", "text", "objective"], "anchor_pdf": ["7cffa095-8d51-5490-b0f0-7a6845e37f67"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the result for number of layers skipped for LLaMA2-13B.", "Retrieve the optimal number of layers to skip for LLaMA2-13B."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 40}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "edf8b7f4-c386-5053-ac89-00bf27fc0d54", "question": "What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?", "reference_answer": "Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations"}}, "state": {}, "annotator": "litsearch_automatic"}
{"uuid": "ee44be40-1780-5f28-9fb4-7c2e626bc4a0", "question": "In the experiment section of Anchor PDF, what is the detailed procedure of back-translation for sentence reconstruction?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "subjective", "text"], "anchor_pdf": ["0c64c0b5-4466-51d6-9e84-59fa73d8a450"], "reference_pdf": ["f43e23ff-de7a-5bd7-9c1d-6361ba9b5734"], "conference": [], "reasoning_steps": ["Find the method of back-translation for sentence reconstruction in the experiment section of Anchor PDF.", "Locate the relevant paper in the reference PDF.", "Find the detailed procedure of back-translation, which is usually in the dataset section."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The procedure of back-translation is using a Czech-English NMT system to translate Czech sentences from the training data into English. Then pair the translations with the English references to form English-English paraphrase pairs. We used the pretrained Czech-English model from the NMT system.", "question": "In the experiment section of Anchor PDF, what is the detailed procedure of back-translation for sentence reconstruction?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "eed1fb76-7c69-540b-9b6f-ad67c3ce4153", "question": "According to Figure 3, what are the layers proposed by the paper(compared to existing methods) in the overall framework of AR quality predictions?", "answer_format": "Your answer should be a python list, every element of the list is a string presented in the original figure of the paper. If there are multiple layers with the same name, they only need to be mentioned once.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Locate Figure 3 in the specified PDF.", "Check the accompanying text for further clarification or naming of the layers.", "Analyze the figure to identify layers unique to the proposed framework compared to existing methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Argumentative Context (AC) Generation", "DistilRoBERTa Encoder", "ChatGPT", "2nd-pass Zero-Shot-CoT Prompt", "1st-pass Zero-Shot-CoT Prompt"], "threshold": 95, "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["0ef0a82a-0e86-5c1b-87b1-36e05e15bd76"], "reference_pdf": []}
{"uuid": "ef0d91b0-8648-519f-9181-ca56496723b6", "question": "What is the maximum gain obtained by adding more outputs in all the datasets tested?", "answer_format": "Your answer should be a floating point numbers with three decimal places.", "tags": ["single", "text", "table", "objective"], "anchor_pdf": ["7e26b1a7-5536-5b8d-b5cd-068508c15c2e"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the part of text that discusses the effect of different output numbers", "Second, locate the table that shows the performance of different output numbers", "Third, calculate the gain of adding more outputs in all the datasets tested", "Finally, find the maximum gain obtained by adding more outputs in all the datasets tested"], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.033, "ndigits": 3}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "ef85ae29-2dcf-5ccc-a0c6-a90689ba11b5", "question": "What are the three types of instruction-following data, and which one has the largest number of samples?", "answer_format": "Your answer should be a Python list of 2 elements. The first element is a Python list of 3 elements, containing the names of the three types of instruction-following data. The second element is a string, indicating the name of the type of instruction-following data that has the largest number of samples. e.g. [[\"type1\", \"type2\", \"type3\"], \"type\"].", "tags": ["single", "text", "objective"], "anchor_pdf": ["86922a0e-7874-5f9a-926b-0f886076d6e8"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the section that introduces the three types of instruction-following data.", "Identify the names of the three types of instruction-following data.", "Determine the number of samples for each type of instruction-following data.", "Identify the type of instruction-following data with the largest number of samples."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": ["conversation", "detailed description", "complex reasoning"], "ignore_order": true, "lowercase": true}, {"gold": "complex reasoning", "lowercase": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "efb108ff-2ddd-5cb2-9408-afba55df144b", "question": "How many tasks are in WebArena? Can they be categorized into classes? How many classes can they be categorized into? What are the classes? How many tasks are in theses classes, respectively?", "answer_format": "Your answer should be a Python list. The first element is an integer indicating the total task number. The second one is a boolean indicating if the tasks can be categorized. If the second one is true, there should be more elements. The third element should be an integer indicating the class number. The fourth one should be a string list storing the class names. The fifth one should be an integer list storing the task numbers in each class. If any needed information cannot be specified through the paper, give an empty string as the answer for that item.", "tags": ["single", "text", "table", "objective"], "anchor_pdf": ["5a2b0d5c-6b51-5bbd-a001-a15f19f65a98"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Find a table or the section containing the dataset statistics", "Extract the total task number", "Check if the tasks are categorized", "If categorized, find out the class names"], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_int_exact_match", "eval_bool_exact_match", "eval_int_exact_match", "eval_structured_object_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": 812}, {"gold": true}, {"gold": 3}, {"gold": ["Information-seeking", "Site navigation", "Content and configuration operation"], "lowercase": true}, {"gold": ""}]}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "efd9be34-b6b2-5abc-b686-0962c27c350c", "question": "Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.", "reference_answer": "Efficient Transformers with Dynamic Token Pooling"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f0291d22-9853-5727-b582-349739d89cbe", "question": "What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?", "reference_answer": "BAYESIAN NEURAL CONTROLLED DIFFERENTIAL EQUATIONS FOR TREATMENT EFFECT ESTIMATION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f06b7b4b-58fd-5450-9c97-00542144b8b2", "question": "Is there a paper exploring the curse of multilinguality for similar languages?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper exploring the curse of multilinguality for similar languages?", "reference_answer": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f0e4639b-09da-5581-87d4-2eb470c2dc0d", "question": "On which datasets were the best-performing Medical MLLMs (excluding the method proposed in this paper) trained and evaluated in the Medical VQA benchmark of the anchor PDF?", "answer_format": "Your answer should be a python list of the dataset names, e.g. [\"dataset1\", \"dataset2\", ...]. YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["58bd1994-d7e2-55c9-a194-9daf63eb3e6c"], "reference_pdf": ["4debbc0c-24ce-581c-9dda-6bc36877f0d8"], "conference": [], "reasoning_steps": ["Retrieve all the Medical MLLMs, which are usually in the experiment section of the anchor PDF.", "Find the best-performing Medical MLLM on the Medical VQA benchmark, which is usually presented as a table in the experiments section.", "Locate the relevant paper about the model.", "Identify the datasets on which the model were trained and evaluated, which are usually mentioned in the dataset section."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VQA-RAD", "SLAKE", "PathVQA"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "f1429616-9c0c-5f32-b39f-46a63a5f7d03", "question": "What open-source dataset combined knowledge retrieval with constraint satisfaction queries?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What open-source dataset combined knowledge retrieval with constraint satisfaction queries?", "reference_answer": "KITAB: EVALUATING LLMS ON CONSTRAINT SATISFACTION FOR INFORMATION RETRIEVAL"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f1adf502-f275-5ab0-ae01-868ef073b0bc", "question": "Which acl paper released the dataset SQuARe?", "answer_format": "Your answer should be a single string of a paper_uuid.", "tags": ["metadata", "retrieval", "objective"], "conference": ["acl2023", "acl2024"], "reasoning_steps": ["Find papers with the specified dataset name in title or abstract.", "If several papers are found, compare and determine the final paper according to the content of the papers."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": ["ca116924-bf11-5529-a43f-bf68e9745c5c"], "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": [], "reference_pdf": []}
{"uuid": "f1d19f7e-17b7-582f-b9dd-465860422e9e", "question": "Is there a paper which proposes a general data selection method based on information theory?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper which proposes a general data selection method based on information theory?", "reference_answer": "GIO: GRADIENT INFORMATION OPTIMIZATION FOR TRAINING DATASET SELECTION"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f1f24bb5-7f16-5048-86c0-3723a919a07e", "question": "Which foundation model paper first proposed a time series model with proposed financial time series and text data?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which foundation model paper first proposed a time series model with proposed financial time series and text data?", "reference_answer": "TEMPO: PROMPT-BASED GENERATIVE PRE-TRAINED TRANSFORMER FOR TIME SERIES FORECASTING"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f21f555a-1254-59ba-8cbc-11791cdab6b0", "question": "Are there any papers that use a world model for planning to ensure that decisions meet constraints?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Are there any papers that use a world model for planning to ensure that decisions meet constraints?", "reference_answer": "SAFEDREAMER: SAFE REINFORCEMENT LEARNING WITH WORLD MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f22e1e2f-bf4a-579e-a11f-f28e9226693a", "question": "In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?", "reference_answer": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f36225a8-3139-58df-843a-e89b838e1f37", "question": "Which base model does the anchor paper train as the retrieval model for the SA task? In its source paper, on how many STS tasks is it evaluated?", "answer_format": "Your answer should be a python list of two strings, the first element is the model name(one word), and the second element is an integer number.", "tags": ["text", "multiple", "objective"], "conference": [], "reasoning_steps": ["First, locate the section about the experiment in the anchor paper.", "Second, find the base model trained as the retrieval model for the SA task in the anchor paper and find its source paper.", "Finally, turn to the source paper to find the number of STS tasks on which the model is evaluated."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SimCSE", 7], "ignore_order": false, "lowercase": true}}, "state": {"gpt-4o-2024-05-13": false}, "annotator": "human", "anchor_pdf": ["0ad1dc99-4c37-5e62-8054-5a080158541e"], "reference_pdf": ["ff0d0226-2dc4-5a18-9cc9-ec5826c16eb7"]}
{"uuid": "f4154375-e94a-5623-a51d-0ae5cf5c4039", "question": "Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?", "reference_answer": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f586cf96-1650-57f8-b7c9-2436c89216f8", "question": "When we utilize decoder-only language models in understanding word meaning, does prompting styles affect performance? If so, which technique outperforms the others? If not, what is the worst one?", "answer_format": "Your answer should be a Python list of two elements, the first element is \"yes\" or \"no\", and the second element is the prompting style name string, don\"t reply abbreviations, e.g., [\"yes\", \"prompting_style_name\"].", "tags": ["image", "objective", "single", "text"], "conference": [], "reasoning_steps": ["Usually, the performance results are mentioned in the experiment or alation section, especially in the form of tables or figures. Search the correpsonding parts.", "Find the figures or tables describing the performances of different prompting styles in understanding word meaning.", "Finally, get the name of prompting styles based on comparison results."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["no", "Sentence completion"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["ef699d3b-ffef-5b18-8527-826110f880fd"], "reference_pdf": []}
{"uuid": "f61c9dbc-5058-5621-8aeb-bd83c90b296e", "question": "How to find those question samples that the model considers to be ambiguous.", "answer_format": "Your answer should be a single string", "tags": ["single", "text", "subjective"], "anchor_pdf": ["e6778360-e589-5ca8-84d1-2e19dd5d4172"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the section in the paper that describes the model’s perception of ambiguity.", "Second, read it, and summarize the formulas and ideas presented by the author."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "First, let the model generate an initial answer based on the question, then prompt the model to eliminate ambiguity and generate a second answer. If the model's average entropy of the two answers exceeds a threshold, it indicates that this question is ambiguous.", "question": "How to find those samples that the model considers to be ambiguous."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "f640029c-539b-58b1-a742-05b8bb0edacb", "question": "What's the biggest reason of incorrect action for each model?", "answer_format": "Your answer should be a Python dictionary. e.g. {\"model1\": \"answer1\", \"model2\": \"answer2\", ...}. YOU MUST USE THE EXACT AND FULL TEXT FROM PDF WITHOUT CHANGING CAPITALIZATION.", "tags": ["image", "objective", "single"], "conference": [], "reasoning_steps": ["Firstly, locate the section that analyses the incorrect actions of the models.", "Find the figure that shows the proportions of correct and incorrect actions for each model.", "Finally, identify the biggest reason of incorrect action for each model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Vicuna-13B": "Invalid Action/Object", "OpenChat-3.5": "Object-Mismatched Action", "Mixtral-7Bx8": "Invalid Action/Object", "Gemini Pro": "Object-Mismatched Action", "GPT-3.5-turbo": "Object-Mismatched Action", "GPT-4": "Dependency Violation"}}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["4ee26cdd-4e52-5090-b1c8-46f5dcdba09c"], "reference_pdf": []}
{"uuid": "f641587e-0065-54e9-92c4-d2b194535f80", "question": "Which paper first study POMDP with enhanced feedback on observations?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper first study POMDP with enhanced feedback on observations?", "reference_answer": "Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f7b228d5-cd68-555b-afee-f05a51a12165", "question": "How will the composition of the Primary System differ between the 2024 QUESPA Submission and the 2023 QUESPA Submission?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["fe24a64b-db14-558f-9685-ba7e6d3f00e9", "824b2039-b575-505c-92fe-9bf063f30a8d"], "reference_pdf": [], "conference": [], "reasoning_steps": ["Locate the related paper and locate the section of the Primary System.", "Analyze the differences between the two in terms of composition."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The Primary System in 2024 for the unconstrained setting consists of a pre-trained model called SpeechT5, and The Primary System in 2023 for the unconstrained setting consists of two systems, the ASR and the MT system.", "question": "How will the composition of the Primary System differ between the 2024 QUESPA Submission and the 2023 QUESPA Submission?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "f7b532c1-3fd7-5a2b-87b4-522592ff6dbe", "question": "When training with non-English image-text pairs, what is the loss function of the TriKD?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of section 3.2 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "When training with non-English image-text pairs, only ITC loss is applied, as the CLIP text encoder does not support non-English languages. Therefore, the loss function for the TriKD is $\\mathcal{L}_{\\text{TriKD}} = \\mathcal{L}_{\\text{ITC}}$. And the Image-Text Contrastive(ITC) loss is formulated as the average of image-to-text($\\mathcal{L}_{\\text{i2x}}$) loss and text-to-image($\\mathcal{L}_{\\text{x2i}})$) loss: $\\mathcal{L}_{\\text{ITC}} = 1/2(\\mathcal{L}_{\\text{i2x}} + \\mathcal{L}_{\\text{x2i}}) = 1/2[\\ell(h^I, h^X) + \\ell(h^X, h^I)]$. Here, $h^I$ represents the $\\ell_2$-normalized output from the CLIP image encoder and CLIP-projector, and $h^X$ represents the $\\ell_2$-normalized output from the Multilingual Text Encoder (MTE) and X-projector for a given image-text pair.", "question": "When training with non-English image-text pairs, what is the loss function of the TriKD?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["ff651d37-e725-5752-9c38-3361bc54723d"], "reference_pdf": []}
{"uuid": "f7c8f3fc-801a-5e50-9722-af38407a0b9d", "question": "What are the seven categories of tasks, which form the dataset used to conduct SFT on a Llama-2-7B model in section 2.1?", "answer_format": "Your answer should be a Python list of seven elements, containing the names of the seven categories of tasks. e.g. [\"task1\", \"task2\", ... \"task7\"]. YOU MUST USE THE EXACT AND FULL NAMES OF THE TASKS AS MENTIONED IN THE PAPER.", "tags": ["objective", "single", "text"], "conference": [], "reasoning_steps": ["Read section 2.1 and locate the implementation details.", "Find the details concerning dataset.", "List the seven categories of tasks"], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["closed-book question answering", "coreference resolution", "natural language inference", "abstract summarization", "multi-lingual translation", "reading comprehension", "text classification"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["eed48331-03ed-52de-8f87-c71da234697c"], "reference_pdf": []}
{"uuid": "f9276cd0-6c6a-5da7-a169-385a7f04ebb0", "question": "What are the main models mentioned in the anchor_pdf and what is the relationship between them?", "answer_format": "Your answer should be a python strings.", "tags": ["multiple", "text", "subjective"], "anchor_pdf": ["e45897f5-4429-5750-a8fb-dcfa9a904b5f", "f42949a1-aae5-5c65-9791-fff76a3dabd4", "eb4c8aef-aded-5cee-9cf3-805b485d85fd"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, find the main models mentioned in the anchor_pdf.", "Then, find their relationships in the details."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The main models mentioned in the anchor_pdf are T0, T5 and Tk-INSTRUCT. T0 and Tk-INSTRUCT are based on T5 and have been improved.", "question": "What are the main models mentioned in the anchor_pdf and what is the relationship between them?"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "f9866921-b6c0-55f2-874f-8bcb5d1e733b", "question": "Is there a paper that links exposure bias to distillation?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper that links exposure bias to distillation?", "reference_answer": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "f987547b-e418-5424-8f8b-f8855bdf63cc", "question": "Which two datasets it combines, the dataset that Alchemist used to evaluate image modality?", "answer_format": "Your answer should be a Python list of two strings, the abbreviations of the datasets as given in the paper.", "tags": ["multiple", "text", "objective"], "anchor_pdf": ["0e3f6c92-099b-5343-a589-7095452ddf16"], "reference_pdf": ["b085c7de-cb5b-5a8d-a0a2-6e617182ff63"], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["CUB", "Places"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "fa7f68f5-fd2e-5b0b-b099-2c09331b7c25", "question": "Which papers develop methods to make in-context learning more computationally efficient?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which papers develop methods to make in-context learning more computationally efficient?", "reference_answer": "FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "fac6cccc-3d4a-5211-9180-1a825de52b16", "question": "In the largest dataset concerning procedural graph extraction before PAGED, how many labeled sentences are there in total?", "answer_format": "Your answer should be a single integer.", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["6b82aecf-fda7-531e-8094-fc5ab6f4810d"], "reference_pdf": ["def780ec-7d73-56b2-b9b2-94253574fd00", "1dbf41f6-3c97-5468-b8cc-59eeb975b718", "04c19523-5522-52c0-ab13-e8f1ae6eb957"], "conference": [], "reasoning_steps": ["Find the table that compares different datasets.", "Identify the previous largest dataset.", "Read the corresponding paper to find the number of labeled sentences."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 4808}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "fbca5330-8955-5359-94a5-d91961e2a6d9", "question": "Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?", "reference_answer": "KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "fc882b50-9385-5452-a96a-e0e93a9cbd2f", "question": "What is the first paper to address the problem of predicting knowledge graphs whose nodes, links and attributes change with time?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "What is the first paper to address the problem of predicting knowledge graphs whose nodes, links and attributes change with time?", "reference_answer": "Holistic Prediction on a Time-Evolving Attributed Graph"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "fd391ae5-5893-5d2a-b630-55b7f0cc1fb3", "question": "According to Table 2 in the paper \"ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought\", how many times does the LLM's API need to be called to generate a SQL query in the DIN-SQL approach? What modules in the DIN-SQL approach lead to those API calls?", "answer_format": "Your answer should be a Python list like [integer, string1, string2, ...]. The first element should be an integer, representing the number of times the LLM's API needs to be called. Each subsequent element should be a string, representing a module name in the DIN-SQL approach. Note that the module names do not need to include the word \"module\".", "tags": ["multiple", "text", "table", "objective"], "anchor_pdf": ["0d110629-3064-59d4-8638-7edb53c01b9e"], "reference_pdf": ["bff546ba-646a-5bdf-b8e2-1f19e59d5162"], "conference": [], "reasoning_steps": ["Locate Table 2 in the paper \"ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought\".", "Find the number of times the LLM's API needs to be called to generate a SQL query in the DIN-SQL approach.", "Check what modules in the DIN-SQL approach lead to those API calls."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [4, "schema linking", "classification & decomposition", "sql generation", "self-correction"], "ignore_order": true, "lowercase": true, "threshold": 95}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "fe64bb38-1b46-53c8-b82b-dfc2acf75c2e", "question": "I want to contact the first author of this paper. What's the email address?", "answer_format": "Your answer should be a verbose text string representing the email address if there is only one first author. Otherwise, return a Python list of e-mail strings for each first and co-first author, e.g., [\"xxx@xxx.com\", \"yyy@yyy.com\", ...]. DO NOT INCLUDE ANY OTHER CONTEXT IN YOUR ANSWER.", "tags": ["single", "metadata", "objective"], "conference": [], "reasoning_steps": ["To find the first and possibly co-first authors, we need to look at the first page of the paper.", "Pay attention to the superscript symbols next to the author names, which usually indicate the author's affiliation and contribution (e.g., co-first or corresponding authors).", "Find the footnote of the first page to see if there is any explanation of the superscript symbols, such that we can determine all first authors.", "Combine with all authors' affiliations to find the email address(es) of the first author(s), which are usually mentioned nearby the authors or in the footnote."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["hhu4zu@virginia.edu", "qiao.jin@nih.gov"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["3ea7de2a-3312-589f-a765-01e4b9e1dcb7"], "reference_pdf": []}
{"uuid": "fe85c6f9-9ec2-5535-8ae4-d9be3e92d66c", "question": "Why can we omit $p(\\{y_l, l \\in L\\})$ in Equation (1)?", "answer_format": "", "tags": ["formula", "single", "subjective"], "conference": [], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 6.1, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In Equation (1), $p(\\{y_l, l \\in L\\})$ can be omitted because it acts as a normalization factor that is constant with respect to the ancestral form $x$. The goal of the equation is to compute a value proportional to the posterior probability $p(x | \\{y_l, l \\in L\\})$. Since $p(\\{y_l, l \\in L\\})$ does not depend on $x$, it remains the same for all possible ancestral forms and thus does not affect the relative probabilities of different $x$ values.", "question": "Why can we omit $p(\\{y_l, l \\in L\\})$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}, "annotator": "human", "anchor_pdf": ["a819666a-9e5b-5213-9efd-4f1e12225426"], "reference_pdf": []}
{"uuid": "fec0d844-3c0c-5c05-827f-cdbbf762d406", "question": "In the entity detection experiments, what is the text type of the dataset used in the training stage with highest F-Score_test?", "answer_format": "Your answer should be a short word or phrase.", "tags": ["single", "table", "objective"], "anchor_pdf": ["30aea0d7-d2e5-58e4-ada8-4f4bf479edd9"], "reference_pdf": [], "conference": [], "reasoning_steps": ["First, locate the highest score and get its setting", "Second, find the table that list all dataset information", "Third, retrieve the target information with correct key"], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "frame-theory"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
{"uuid": "fed63d9e-52d3-5a7c-89f4-ac6f37f7e02b", "question": "Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?", "reference_answer": "CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "fee3ed60-b2ce-55ce-b06d-0f4e9fe1639f", "question": "Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?", "reference_answer": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ff1576ee-d2c5-505a-964a-c2fcc94c75ff", "question": "Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["iclr2024"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?", "reference_answer": "MgNO: Efficient Parameterization of Linear Operators via Multigrid"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ff31ef9b-f07d-59a4-ac9a-4d694ff7bb13", "question": "Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?", "answer_format": "Your answer should be the title of the paper WITHOUT ANY EXPLANATION.", "tags": ["retrieval", "text", "subjective"], "anchor_pdf": [], "reference_pdf": [], "conference": ["acl2023"], "reasoning_steps": [], "evaluator": {"eval_func": "eval_paper_relevance_with_reference_answer", "eval_kwargs": {"question": "Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?", "reference_answer": "A Survey of Deep Learning for Mathematical Reasoning"}}, "state": {}, "annotator": "litsearch_manual"}
{"uuid": "ff40fb8f-a1d9-5598-91b2-2af15bbad92e", "question": "Among the specific models tested, whose performance is closest to RePe on MixATIS++?", "answer_format": "Your answer should be the name of model DIRECTLY FROM THE PDF WITHOUT ANY EXPLANATION.", "tags": ["objective", "single", "table"], "conference": [], "reasoning_steps": ["Firstly, locate the table that compares the performance of models.", "Finally, identify the model whose performance is closest to RePe on MixATIS++."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "FC-MTLF"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human", "anchor_pdf": ["95e1644a-230b-5b28-a025-31992e32e3f0"], "reference_pdf": []}
{"uuid": "ff50fbdd-645a-5d67-ae87-b02133b59ed6", "question": "In the paper that proposes NoMAD-Attention, what do the authors choose as vector database? Additionally, in the paper that proposes that vector database, how to compute the number of distance computations and when it reaches a minimum?", "answer_format": "Your answer should be a Python list of 2 strings, each string is a formula in LaTeX format, representing the equation of the number of distance computations and the condition when it reaches a minimum.", "tags": ["multiple", "text", "formula", "subjective"], "anchor_pdf": ["1ee0a3c2-c155-541c-aa3a-dd86cb24a4d8"], "reference_pdf": ["55f434b3-b41d-5f88-bf94-a6bff9a2f7f9"], "conference": [], "reasoning_steps": ["Locate the paragraph that discusses the experimental setup.", "Identify the vector database applied.", "Read the corresponding paper.", "Locate the section discussing \"Non-exhaustive search\".", "Identify the formulas."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["N_{\\text{distances}} = K_\\text{IVF} + P_\\text{IVF} \\times N/K_\\text{IVF}", "K_\\text{IVF} = \\sqrt{P_\\text{IVF}N}"], "question": "How to compute the number of distance computations and when it reaches a minimum?", "ignore_order": false}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "human"}
