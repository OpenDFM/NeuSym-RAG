{
    "uuid": "xxxx-xxxx-xxxx", // unique identifier for this data sample
    "question": "user question about ai research papers", // user question
    "answer_format": "text description on answer format, e.g., a single float number, a list of strings", // can be inserted into prompt
    "tags": [
        "tag1",
        "tag2"
    ], // different tags for the data or task sample, see below for definition
    "anchor_pdf": [
    ], // UUIDs of the papers that are explicitly mentioned in the question
    "reference_pdf": [
    ], // in "multiple" category, UUIDs of papers that may be used but not provided in the question
    "conference": [
        "acl2023"
    ], // in "retrieval" or "comprehensive" category, define the search space of papers, usually conference+year
    "evaluator": {
        "eval_func": "function_name_to_call", // all eval functions are defined under `evaluation/` folder
        "eval_kwargs": {
            "scoring_points": [
                "1.score point1",
                "2.score point1"
            ]
        } // the gold answer or how to get the gold answer should be included in `eval_kwargs` dict. Other optional keyword arguments can be used for customization and function re-use, e.g., `lowercase=True` and `threshold=0.95`.
    }, // A complex dict specifying the evaluation function and its parameters. The first parameter of the `eval_func` must be LLM predicted string.
    "annotator": "human" // whether annotated by human, machine, or adapted from other datasets
}