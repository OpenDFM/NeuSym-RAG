{"uuid": "541435a6-878e-540f-8b6a-86bf7920dc82", "question": "What is the main design of Auto-GUI framework from the aspects of the encoder, interaction, and decoder?", "answer_format": "Your answer should be a Python list of text strings, with each element being one core stage of this framework, you'd better use the origin text, e.g., ['stage 1', 'stage 2', ...].", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["648e3d50-375b-5189-b6b0-e0520626716e"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the framework of a model is mentioned in the methodology or related work section. Search the corresponding paragraphs of these two parts.", "If the core stages are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core stages from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Encoding: Acquire encoded features from both vision and language inputs. The vision input is encoded by a frozen vision encoder, the language input is encoded by a language encoder.", "Interaction: The encoded vision and language representations are integrated by a single-head self-attention network and a gated fusion", "Decoding: The fused representation is fed to the decoder to generate a chain of future action plans. The target predictions consist of a chain of future action plans and the current action prediction separated by specific prompts"], "question": "What is the main design of Auto-GUI framework?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "c1cbcf5c-632c-5424-a1ef-d9add6094746", "question": "What are the Low resource languages in INDICGENBENCH?", "answer_format": "Your answer should be a Python list, where each element is a string representing a language. e.g. ['language1', 'language2', ...]", "tags": ["single-doc details", "text understanding"], "pdf_id": ["1e6eeeab-ba5c-508e-a693-62a9b39f2d92"], "conference": ["acl2024"], "reasoning_steps": ["Locate the section which introduces INDICGENBENCH.", "Find the paragraph that talks about the divide of languages.", "List the low resource languages."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Awadhi", "Haryanvi", "Tibetan", "Garhwali", "Konkani", "Chhattisgarhi", "Rajasthani", "Maithili", "Manipuri", "Malvi", "Marwari", "Santali", "Bodo"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "891093a5-a10b-5299-a421-a77713a8886e", "question": "According to Table 1, which baselines this paper used has the highest average score? In which paper is this method proposed? And in which conference was this paper published?", "answer_format": "Your answer should be a python list with three items, the first item is the name of baseline reaching the highest average score according to Table 1, the second item is the name of paper where this method proposed, and the third item is the abbreviation of the conference name where this paper was published.", "tags": ["single-doc details", "metadata extraction", "objective"], "pdf_id": ["9c5c3a63-3042-582a-9358-d0c61de3330d"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Table 1.", "Second, find what baseline reaching the highest average score.", "Third, according to reference, get the name of paper where this method was proposed.", "Finally, find the name of conference where this paper was published."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SimCTG", "A contrastive framework for neural text generation", "NeurIPS"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}}
{"uuid": "68baa0b9-8e5e-5436-94d5-6dd0b3bbfff0", "question": "On which datasets this study surpassed the SOTA?", "answer_format": "Your answer should be a Python list of dataset, e.g., ['dataset1', 'dataset2', ...]. YOU MUST USE THE EXACT TEXT FROM THE PAPER.", "tags": ["single-doc details", "table", "objective"], "pdf_id": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find all sections in the paper that mentions SOTA.", "Identify the part that compare the performance with SOTA.", "Finally, list all the datasets that surpasse SOTA."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["DailyDialog", "CMU_DoG", "LIGHT", "EmpathicDialogue", "Wizard of Wikipedia", "CommonsenseDialog"]}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "8cc38e05-20e5-5a69-8b82-ecc09c03450a", "question": "According to the experiment result, How much better does the GPT-2 model perform on Task A compared to the CNN-BiLSTM model in terms of F1-score?", "answer_format": "Your answer should be a single python float.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["1aa5e165-0b78-582a-8f67-e459452348df"], "conference": ["acl2023"], "reasoning_steps": ["Locate the table(s) that presents experimental results for Task A.", "Compare and calculate the F1 score difference between GPT-2 and CNN-BiLSTM."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.02}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "d82a4438-587e-5405-9351-319110cd89de", "question": "What is the average proportion of papers in the ACL anthology in recent ten years which mention the words speech, spoken or audio in the title?", "answer_format": "Your answer should be a Python float number, e.g., 0.001.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["a88d4d17-b2f9-520e-a611-c2c4ba178be5"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the section that discusses the proportion of papers in the ACL anthology which mention the words speech, spoken or audio in the title.", "Find the exact value of annual propotion in recent ten years.", "Finally, calculate or estimate the average propotion."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.035, "nidigits": 3, "tolerance": 0.0001}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "ab369ade-a399-5f3a-82ba-13c02f4a91a7", "question": "Whether the code and data of this paper are publicly available or not?", "answer_format": "Your answer should be a simple 'yes' or 'no' WITHOUT PUNCTUATION OR EXPLANATION.", "tags": ["metadata query", "metadata extraction", "objective"], "pdf_id": ["ae9b4a06-0642-5512-9150-656cf166c470"], "conference": ["acl2024"], "reasoning_steps": ["To find the information of code and data, you need to look at the first page of the paper.", "Usually, the URL of the code and data is provided in the Abstract section."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "yes", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "ad341e2b-cb59-5695-b41a-9912be57ea77", "question": "What is the shape of $W$ in Equation (3)? And what about $W_l$ and $W_v$ in Equation (5)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["648e3d50-375b-5189-b6b0-e0520626716e"], "conference": ["acl2024"], "reasoning_steps": ["First, get the content of Equation (3) and paragraph 'Encoding' of Section 3.2.", "Second, summarize the answer for question 1.", "Third, get the content of Equation (5) and paragraph 'Interaction' of Section 3.2.", "Finally, summarize the answer for question 2."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The shape of $W$ in Equation (3) is $\\mathbb{R}^{d_s \\times d_l}$, where $d_s$ is the dimension of the vision features, and $d_l$ is the dimension of the language features. The shapes of $W_l$ and $W_v$ in Equation (5) are both $\\mathbb{R}^{d \\times 1}$.", "question": "What is the shape of $W$ in Equation (3)? And what about $W_l$ and $W_v$ in Equation (5)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "e8c52858-a386-5e87-a9ba-3a7ec32ae1e2", "question": "What are the catogories of label biases in in-context learning for text classification and what are the definitions of these categories?", "answer_format": "Your answer should be a Python list of text strings, with each element being one category that this paper defines, e.g., ['category 1: define 1', 'category 2: define 2', ...].", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["15f6962b-d927-5d71-b01e-f0664e09eeb5"], "conference": ["acl2023"], "reasoning_steps": ["Usually, the definitions of main concepts are mentioned in the introduction section. Search the correpsonding paragraphs.", "If the categories are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the definition of each category from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["vanilla label bias: The model's non-contextualized preference for the label names (e.g., the common token bias caused by different frequencies of label names in the pretraining corpus).", "context-label bias: The effects of the context prompt (e.g., LLMs tend to prefer the majority and last label of the in-context examples).", "domain-label bias: The effects of the task corpus on the model's predictions."], "question": "What are the catogories of label biases in in-context learning for text classification and what are the definitions of these categories?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "1bc803b2-807d-5218-891c-da60a470cd93", "question": "Which model achieves the highest accuracy of the classification when the training data consists of 512 pairs of FPQ and TPQs in this paper?", "answer_format": "Your answer should be a python string about the name of the best model. You'd better use the names as they are referred to in the paper.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["529875d6-5189-5c4d-9076-1635a01a862d"], "conference": ["acl2023"], "reasoning_steps": ["Usually, the experiment results between models are mentioned in the experiment or result section, especially in the form of tables or figures. Search the correpsonding parts.", "Find all the experiment results of models when the training data consists of 512 pairs of FPQ and TPQs.", "Finally, compare and rank to get the name of the model demonstrating the highest accuracy."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "MACAW-11B", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "cd837c4f-07d1-5db8-84c1-f258aa7985ea", "question": "Considering both benefits and costs, what is the best size of generation pool for the proposed method?", "answer_format": "Your answer should be a single integer number.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["ca116924-bf11-5529-a43f-bf68e9745c5c"], "conference": ["acl2023"], "reasoning_steps": ["Locate the experiment table or figure that shows the method performances with different sizes of generation pool.", "Think about what 'best' means, considering both benefits and costs comprehensively.", "Find out the best size of generation pool for the proposed method based on the experiment results."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "897a955d-9e8a-5836-a3b6-0ed48575f2b9", "question": "In terms of evaluation results on SuperGLUE using RoBERTaBASE, on subtask ReCoRD, which model(s) achieve the overall best results? Additionally, which model(s) perform the best among the two-stage MTL models?", "answer_format": "Your answer should be a python dict(without \\n) containing two keys 'overall best' and 'two-stage MTL best', each value of the two keys is a python string list. e.g.{'overall best':['modelname1'],'two-stage MTL best':['modelname2','modelname3']} ", "tags": ["single-doc details", "text understanding", "table calculation", "objective"], "pdf_id": ["9b06b24b-0afc-5ccb-95fc-c662395d291d"], "conference": ["acl2024"], "reasoning_steps": ["Find information about two-stage MTL models(maybe a section or table mentioning which models or baselines are two-stage MTL).", "Locate the table about the performance results on SuperGLUE using RoBERTaBASE(usually a main experiment table containing different kinds of models and subtasks).", "Compare the performance of models on the subtask ReCoRD to determine which model(s) achieves the overall best results and which model(s) perform the best among the two-stage MTL models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"overall best": ["PROPETL"], "two-stage MTL best": ["SCALEARNUNIFORM", "SCALEARN++"]}, "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "39fb54be-7c67-59c2-9179-8cd66ce19bc2", "question": "Considering the performance of ChatDev agent on DSEval-LeetCode benchmark, what is the most common cause of the errors?", "answer_format": "Your answer should be a python list of elements, the first element is the string of the main verdict, the second element is the string of the sub-verdict, e.g., ['verdict_name', 'sub-verdict_name'].", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["0fe6d2d4-00e7-596b-a80c-ffe5a6d88b97"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the performance results of agents on benchmarks are mentioned in the experiment or result section, especially in the form of tables or images. Search the correpsonding parts.", "Find the error analysis about the performance of ChatDev agent on DSEval-LeetCode benchmark.", "You can also retrieve the reference section for any additional information if you can't find in the main text.", "Finally, find the most common cause of the errors based on the information from the corresponding charts and legends."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Presentation Error", "Index Mismatch"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "eb3a5dd5-0008-5edf-b8e7-8cebd614f282", "question": "In the survey of Large Language Models for NL2Code, what are the multi-lingual benchmarks to evaluate the NL2Code task, and how many instances do they contain per promgramming language?", "answer_format": "Your answer should be a Python dictionary of entries, each dictionary key is a string, the benchmark name DIRECTLY FROM THE PDF WITHOUT CHANGING CAPITALIZATION, and each value is an integer of the corresponding instance number, e.g., {'benchmark1': 10, 'benchmark2': 100}, ....", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["37758401-6101-554f-8f1e-4e2995443314"], "conference": ["acl2023"], "reasoning_steps": ["Usually, the details about benchmarks are mentioned in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Finally, answer the question with the benchmark names and their instances numbers."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"MBXP": 974, "MBXP-HumanEval": 164, "HumanEval-X": 164, "MultiPL-HumanEval": 164, "MultiPL-MBPP": 974}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "75c1fd66-8271-5ae8-b45f-c188ae9ccf84", "question": "Which evaluation metric demonstrates the greatest improvement in the finetuned model proposed in this paper compared to GPT baseline?", "answer_format": "Your answer should be a Python string, which is the name of the evaluation metric DIRECTLY FROM THE PDF.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["481d851e-214b-5d6b-af6c-880a1be8f3b9"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the comparison results between new models and baseline are mentioned in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Find the exact values of the performance of the new model and the baseline on all evaluation metrics.", "Finally, calculate or compare to get the evaluation metric demonstrates the greatest improvement in the finetuned model compared to GPT baseline."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "sBLEU", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "6098fb2b-f951-52c7-8cf9-e17aa7124833", "question": "What is the difference between Equation (1) and Equation (2)?", "answer_format": "Your answer should be text describing the difference.", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["bbe1cd56-d6c0-5ab7-8f3e-a54ff7489d0b"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Equation 1 and its corresponding paragraphs.", "Second, get the content of Equation 2 and its corresponding paragraphs.", "Finally, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Equation (1) is used in single-vector retrieval models. It calculates the similarity score $s(q, d)$ as the dot product between the encoded vector of the query $v_q = \\eta_Q(q)$ and the encoded vector of the document $v_d = \\eta_D(d)$. In this method, the entire query and the entire document are each represented by a single vector, and their similarity is determined by the cosine of the angle between these two vectors. This approach does not consider token-level interactions, as all token embeddings are pooled into a single vector before the similarity score is computed.", "Equation (2) is used in multi-vector retrieval models, specifically in ColBERT. It calculates the similarity score by considering the interaction between each token in the query and each token in the document. The similarity score $s(q, d)$ is defined as $s(q, d) = \\sum_{i=1}^{N} \\max_{j} v_{qi}^T v_{dj}$, where $v_{qi}$ and $v_{dj}$ are the last-layer contextualized token embeddings of BERT for the i-th token in the query and the j-th token in the document, respectively. This operation, known as MaxSim, exhaustively compares each query token to all document tokens, effectively capturing the most relevant token-level interactions."], "question": "What is the difference between Equation (1) and Equation (2)?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "eed1fb76-7c69-540b-9b6f-ad67c3ce4153", "question": "According to Figure 3, what are the layers proposed by the paper(compared to existing methods) in the overall framework of AR quality predictions?", "answer_format": "Your answer should be a python list, every element of the list is a string presented in the original figure of the paper. If there are multiple layers with the same name, they only need to be mentioned once.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["0ef0a82a-0e86-5c1b-87b1-36e05e15bd76"], "conference": ["acl2023"], "reasoning_steps": ["Locate Figure 3 in the specified PDF.", "Check the accompanying text for further clarification or naming of the layers.", "Analyze the figure to identify layers unique to the proposed framework compared to existing methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Argumentative Context (AC) Generation", "DistilRoBERTa Encoder", "ChatGPT", "2nd-pass Zero-Shot-CoT Prompt", "1st-pass Zero-Shot-CoT Prompt"], "ignore_order": true, "threshold": 95, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "b11ab881-9ac0-5b77-9b27-394744cf06e1", "question": "What are the most important optimizations of transformer network in this paper?", "answer_format": "Your answer should be a Python list of text strings, with each element being one important optimization that this paper proposes, e.g., ['optimization 1', 'optimization 2', ...].", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["e281fc3b-cdaa-5565-8997-6a6c8f198000"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the most important optimization of network are proposed in the abstract, introduction or methodology. Search the correpsonding paragraphs of these parts.", "If the optimiazations are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core optimizations from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Removal of the one-to-one mapping constraint among queries and keys in multiple subspaces", "Allowing each query to attend to multiple keys", "Introduction of inner-subspace interaction and cross-subspace interaction to encourage consensus among heads"], "question": "What are the most important optimizations of transformer network in this paper?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "44db1f84-1791-509e-91ae-79b2856153ee", "question": "What are the datasets and their metrics used in this paper according to the tables?", "answer_format": "Your answer should be a Python dictionary, e.g., {'dataset1': 'metric1', 'dataset2': 'metric2', ...}. YOU MUST USE THE EXACT TEXT AND FULL DATASET NAME FROM THE PAPER WIHOUT CHANGING CAPITALIZATION.", "tags": ["single-doc details", "table", "objective"], "pdf_id": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find all tables in the paper that include dataset and metric.", "Identify the full name of the datasets and metrics from the tables.", "Finally, list the dataset-metric pairs."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Cornell Movie": "Rouge-1", "DailyDialog": "BLEU-1", "CMU_DoG": "Rouge-1", "LIGHT": "unigram-F1", "EmpathicDialogue": "Rouge-1", "ConvAI2": "unigram-F1", "Wizard of Wikipedia": "unigram-F1", "Mutual": "Rouge-L", "CommonsenseDialog": "Rouge-1"}}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "412f7530-b194-5aca-8508-22318575e1b2", "question": "According to the expression and physical meaning of formula (2), if I want the weight to be 0.5 right at the middle of the training process, what is the value of parameter s?", "answer_format": "Your answer should be a python float of the exact value of parameter s.", "tags": ["single-doc details", "formula calculation", "text understanding", "objective"], "pdf_id": ["996afc36-70e9-5f75-8e9f-6f0e5587c451"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find the formula (2) in the paper and retrieve the context to understand the physical meaning.", "Then, find the parameters and retrieve the context to understand the effect of parameters on the weight, e.g., the relationship between training process and the parameter t implies t = 0.5.", "Finally, calculate the value of s when the weight is 0.5 at the middle of the training process."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.5, "ndigits": 4}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "7696934c-fc83-504d-83d9-3716e13dfd89", "question": "How much does the average performance of the model improve on WMT'19 test sets by replacing one of example-specific prompts with a task-level prompt?", "answer_format": "Your answer should be a single float number ranging from 0 to 100, representing the subtraction result.", "tags": ["single-doc details", "table calculation", "text understanding", "objective"], "pdf_id": ["5a2b95c1-12d6-5b77-82a1-ee24180d27ae"], "conference": ["acl2023"], "reasoning_steps": ["Locate the experiment table that shows the model performances on WMT'19 test sets with different distributions of prompts.", "Find out the mathematical symbols indicating the numbers of two types of prompts.", "Locate the two rows in the table related to the question.", "Record the two average performance scores.", "Calculate the subtraction result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.34, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "4a2b4ab6-a332-5d58-b58b-b2e8405edf77", "question": "What does formula (3) in this paper mean?", "answer_format": "Your answer should be a Python strings of the detailed explanation of the formula.", "tags": ["single-doc details", "text understanding", "formula inference", "subjective"], "pdf_id": ["f0aab1fb-be5b-5b84-aa0e-a13aa814c7b0"], "conference": ["acl2023"], "reasoning_steps": ["Find the formula (3) in the paper and the surrounding text to understand the context of the formula.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "This formula is to compute dynamic parameters and transform the less important dynamic parameters into input-agnostic static parameters. Specifically, a mask M_i is utilized to indicate whether the i-th element of ˆΘ is dynamic or static. Mi = 1 means the i-th element of ˆΘ is dynamic so we update it through the dynamic function W with input x and dynamic factors Θ. Otherwise, ˆΘ remains the same.", "question": "What does formula (3) in this paper mean?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "c0e96750-91fe-5f24-aee3-74ea8706654a", "question": "For each category of PQA in terms of the form of provided answers, from what aspects does the author analyze it?", "answer_format": "Your answer should be a Python list, where each element is a string representing an aspect DIRECTLY FROM THE PDF. Note that the aspects are the same for each category. e.g. ['aspect1', 'aspect2', ...]", "tags": ["single-doc details", "text understanding"], "pdf_id": ["2cc7f650-699e-580d-aad4-04fc17b5868f"], "conference": ["acl2023"], "reasoning_steps": ["Locate the section where each category of PQA is discussed.", "Identify the aspects from the titles of subsections."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Problem Definition", "Datasets & Evaluation Protocols", "Methods", "Pros and Cons"], "ignore_order": true, "lowercase": true, "ignore_blank": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "04f6fcad-edd9-577c-b089-ae167567ef47", "question": "What is the most appropriate evaluation metric for this paper?", "answer_format": "Your answer should be a python strings of the exact name of the evaluation metric.", "tags": ["single-doc details", "text understanding", "objective"], "pdf_id": ["cc28a219-3ac4-5614-ba45-59d6aabf1af4"], "conference": ["acl2023"], "reasoning_steps": ["Find the corresponding sections about evaluation metric.", "Compare different evaluation metrics to find the most appropriate one based on the context."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "refinement", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "79b25301-76c6-5594-9f59-76e6ea48246c", "question": "In section 3, the author provides an exemplary event description. List the features in the example that correspond to the semantic roles discussed in the following paragraph.", "answer_format": "Your answer should be a Python dictionary where the keys are the semantic roles and the values are the features that correspond to the roles. e.g. {'semantic_role1': 'feature1', 'semantic_role2': 'feature2', ...}", "tags": ["single-doc details", "text understanding"], "pdf_id": ["5b81a1a3-fbe6-534d-b0c0-801e8fb2bdd6"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, read section 3 and find the exemplary event description.", "Read the following paragraphs to identify the semantic roles discussed.", "List the features in the example that correspond to the semantic roles.", "Finally, formulate the answer according to answer format."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"subject": "soldiers", "predicate": "injured", "quantifier": "two", "object": "civilians"}}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "1faadd0a-1ee9-5541-b4ca-7a0bd3cacc0e", "question": "RRCP is a new pipeline proposed to recognize retrieval complexity, on which complex QA datasets does it significantly outperform the LLM baseline, with a clear improvement in Accuracy or F1 Score of at least 0.1?", "answer_format": "Your answer should be a list of elements, each element is the QA dataset name string, e.g., ['QA dataset1', 'QA dataset2', ...].", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["7640a304-546b-5aa9-8111-8a56b9b06861"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the comparison between new design and baseline are proposed in the experiment or result section, especially in the form of tables. Search the correpsonding parts.", "Find the exact values of Accuracy and F1 Score on RRCP and baseline for differnt QA dataset.", "Finally, compare and calculate the values to find the datasets satisfying the requirements of outperforming."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["CWQ", "StrategyQA", "MuSiQue"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "bd3d1dd5-7f10-5e09-aa76-486685c77180", "question": "What do formula (2) to formula (4) mean?", "answer_format": "Your answer should be a brief summarization of the meaning of these formulas, and you do not need to introduce these formulas one-by-one.", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["aec41fde-98a1-58c4-86f3-100e408171cd"], "conference": ["acl2023"], "reasoning_steps": ["First, locate where formula (2) to formula (4) are", "Second, get the content of the corresponding section", "Third, get the content of the previous section to know the meaning of different symbols.", "Finally, analyze the meaning of these formulas."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "Formula (2) to (4) describe the self-contrastive training (SELFCONT) algorithm, which is designed to mitigate the repetition problem in language models. SELFCONT modifies the training process by adding a penalty to the output of the current model \\(f_{\theta_1}\\) when it predicts a repetitive token that the premature checkpoint \\(f_{\theta_0}\\) also predicts. The penalty is controlled by the weight \\(w\\), which is only active when the true next token is non-repetitive but the premature model predicts it as repetitive. This encourages the model to learn more complex patterns and reduce its tendency to generate repetitive text.", "question": "What do formula (2) to formula (4) mean?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "f586cf96-1650-57f8-b7c9-2436c89216f8", "question": "When we utilize decoder-only language models in understanding word meaning, does prompting styles affect performance? If so, which technique outperforms the others? If not, what is the worst one?", "answer_format": "Your answer should be a Python list of two elements, the first element is 'yes' or 'no', and the second element is the prompting style name string, don't reply abbreviations, e.g., ['yes', 'prompting_style_name'].", "tags": ["single-doc details", "text understanding", "image analysis", "objective"], "pdf_id": ["ef699d3b-ffef-5b18-8527-826110f880fd"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the performance results are mentioned in the experiment or alation section, especially in the form of tables or figures. Search the correpsonding parts.", "Find the figures or tables describing the performances of different prompting styles in understanding word meaning.", "Finally, get the name of prompting styles based on comparison results."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["no", "Sentence completion"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "8fb6f8ec-fae3-5823-ba3f-21cdca6952a9", "question": "How do the authors split the dataset for the experiments?", "answer_format": "Your answer should be a plein text.", "tags": ["single-doc details", "text understanding"], "pdf_id": ["67c78c79-7878-5ff5-b5f6-45cec4ad9bf9"], "conference": ["acl2023"], "reasoning_steps": ["Locate the section where experiments are mentioned.", "Find the subsection which discusses the dataset split."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "For the empirical modeling analysis and performance benchmarking, we randomly split the dataset into 3 sets: train (70%), dev (5%), and test (25%) sets, while ensuring both domains (fashion and furniture) have the same split distributions.", "question": "How do the authors split the dataset for the experiments?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "68855b4d-dd5d-5b33-8ddd-61b13b1b6c51", "question": "On cmudog, which one among the linguistics operators appears the most frequently? What's its distribution?", "answer_format": "Your answer should be a Python list of 2 elements. The first element is the linguistic operator's name, and the second element is its disrtibution in percent in string format. e.g. ['answer', '5%'].", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["3107f6a8-1939-5af0-b3d8-06d7aa66158d"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, locate the image that contains the distribution of linguistic operators on cmudog.", "Finally, output the most frequent linguistic operator and its distribution."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VERB_MODIFY", "36%"]}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "00608f20-e3f5-5fdc-8979-4efeb0756d8e", "question": "What distance function and transfer function do the author use for their method?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["16f7e6e6-6bfa-5c74-9c8b-1adc5bf7e3b9"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Table 2 and get the final answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The author use distance function $d(u,v) = -u^Tv/\\tau$ and transfer function $f(h) = \\frac{h}{||h||}$ for their method.", "question": "What distance function and transfer function do the author use for their method?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "8531101d-a0b8-50fa-9f0e-5a3c71417b4e", "question": "How to calculate three important parameters that appear in the second part of Figure 2?", "answer_format": "Your answer should be a Python list of three string elements, every element is a formula in latex format to calculate a parameter", "tags": ["single-doc details", "formula inference", "image analysis", "subjective"], "pdf_id": ["8712603a-e96c-5537-be18-651b29dedfb8"], "conference": ["acl2023"], "reasoning_steps": ["Find Figure 2 in the specified PDF and understand the background information.", "Identify its second part and three important parameters.", "Extract formulas related to the three parameters from the figure."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["\\alpha_{k}=\\displaystyle\\frac{\\cos\\left(\\mathbf{h}_{t},\\mathbf{h}_{o}[k]\\right)}{\\sum_{j=1}^{K}\\cos\\left(\\mathbf{h}_{t},\\mathbf{h}_{o}[j]\\right)}", "\\beta=\\mathrm{FC}_{\\beta}(\\overline{{\\mathbf{h}}}_{o})=\\mathrm{FC}_{\\beta}\\left(\\displaystyle\\sum_{k=1}^{K}\\alpha_{k}\\mathbf{h}_{o}[k]\\right)", "\\gamma=\\mathrm{tanh}\\left(\\mathrm{FC}_{\\gamma}\\left(\\mathbf{h}_{i}\\right)\\right)"], "question": "How to calculate three important parameters that appear in the second part of Figure 2?", "ignore_order": true}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "18b13577-3570-5e5f-be1c-77606cce3cf4", "question": "When the authors use ChatGPT in generating data points, what is the templated prompt?", "answer_format": "Your answer should be raw text directly demonstrating the used prompt template without any other context.", "tags": ["single-doc details", "text understanding", "table calculation", "subjective"], "pdf_id": ["09abed2f-a7af-56e5-8a34-3fc5e7130c6a"], "conference": ["acl2024"], "reasoning_steps": ["Locate the position about \"using ChatGPT in generating data points\".", "Find any text or table content that contains the templated prompt, and directly return the raw templated prompt."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "I am creating a dataset and need to generate data that is similar but not identical to the following examples. Here are 5 examples from my dataset:\n1. [Example 1]\n2. [Example 2]\n3. [Example 3]\n4. [Example 4]\n5. [Example 5]\nPlease generate [Specified Number] new data points that are similar in style and structure to these examples but are unique in content. Format the responses as a numbered list, starting from 6 onwards. Each data point should start on a new line and be prefixed with its corresponding number followed by a period and a space.\nFor example:\n6. [New Data Point 1]\n7. [New Data Point 2]\n...", "question": "When the authors use ChatGPT in generating data points, what is the templated prompt?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "8beb15f1-2c64-5aa7-a1a3-1579452b2ecc", "question": "For the specific dataset where CLiCoTEA does not outperform all models, in which languages does this occur?", "answer_format": "Your answer should be a Python list of string elements, every element is the abbreviation of a langugage mentioned in the paper, e.g. ['AR', 'ES', 'FR', ...].", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["0d6ea045-b831-520d-9b99-ba22a081a403"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, identify relevant performances (usually in the main experiment table) from the specified PDF for CLiCoTEA and other models.", "Analyse and recognize the specific dataset in which CLiCoTEA does not outperform all models.", "Extract relevant performances (possibly in the ablation table or appendices) for that dataset.", "Finally, analyze the table content to identify the languages where CLiCoTEA underperforms previous methods."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["DE", "ES", "ID", "RU", "TR"], "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "1cc5d12c-31dd-5e52-92d5-9227e8cfbfcd", "question": "What are the main questions that this paper tries to resolve or answer?", "answer_format": "Your answer should be a Python list of text strings, with each element being one critical problem that this paper analyzes, e.g., ['question 1', 'question 2', ...].", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["effede2d-ed50-597e-b1fb-d5fc1b6dc554"], "conference": ["acl2024"], "reasoning_steps": ["Usually, the main questions that one work tries to resolve are mentioned in the end of the introduction section or at the beiginning of the experiment section. Search the correpsonding paragraphs of these two parts.", "If the questions are organized as evident bullet points, directly use them as the answer.", "Otherwise, try to summarize the core questions from the main text."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Is it possible to determine data contamination by solely analyzing the inputs and outputs of existing LLMs?", "Do recent GPTs excel in Text-to-SQL tasks in a zero-shot setting both on potentially leaked data and totally unseen one?", "Is data contamination affecting the accuracy and reliability of an existing GPT in Text-to-SQL tasks?"], "question": "What are the main questions that this paper tries to resolve or answer?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "5f2de2c6-fbcd-561a-b7a4-be129671f5db", "question": "On which labeled dataset did the metric AMR not reduce to Acc? On that dataset, which model performs best on the metric AMR?", "answer_format": "Your answer should be a Python list of three elements, the first element is the name of the labeled dataset, the second and third element is the model family and the variant of the model. e.g. ['answer1', 'answer2', 'answer3'].", "tags": ["single-doc details", "table calculation", "text understanding"], "pdf_id": ["0da230cb-d487-56fa-9a85-4648f3f1e6c5"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the section that includes the relevant metrics.", "Find the labled dataset's name in the section text.", "Locate the table that compares different models.", "Finally, analyse the columns and fetch the best model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VITC-L", "GPT-3.5", "0301"], "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "2cd0cc5e-defb-51aa-b04d-1cfead682bda", "question": "For handling hallucinations with auxiliary models, what is the model they use, and what are the metrics or measures to evaluate semantic similarity of two sentences?", "answer_format": "Your answer should be a Python list of two elements, the first element is the model name string, and the second element is a list of metric names, e.g., ['model_name', ['metric1', 'metric2', ...]].", "tags": ["single-doc details", "text understanding", "objective"], "pdf_id": ["b5062515-e162-5a98-a421-ab84dfe1d930"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the section that discusses hallucinations.", "Identify the finer requirements on external or auxiliary models for handling hallucinations.", "Finally, retrieve the context for this subsection to find the model name and the concrete metrics."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["COMET-QE", ["LASER", "LaBSE", "XNLI"]]}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "f7c8f3fc-801a-5e50-9722-af38407a0b9d", "question": "What are the seven categories of tasks, which form the dataset used to conduct SFT on a Llama-2-7B model in section 2.1?", "answer_format": "Your answer should be a Python list of seven elements, containing the names of the seven categories of tasks. e.g. ['task1', 'task2', ... 'task7']. YOU MUST USE THE EXACT AND FULL NAMES OF THE TASKS AS MENTIONED IN THE PAPER.", "tags": ["single-doc details", "text understanding"], "pdf_id": ["eed48331-03ed-52de-8f87-c71da234697c"], "conference": ["acl2024"], "reasoning_steps": ["Read section 2.1 and locate the implementation details.", "Find the details concerning dataset.", "List the seven categories of tasks"], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["closed-book question answering", "coreference resolution", "natural language inference", "abstract summarization", "multi-lingual translation", "reading comprehension", "text classification"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "3cc9e70a-bd6b-525c-af61-4b66f9ef8a77", "question": "Who is the corresponding author of this paper?", "answer_format": "Your answer should be a python string about the name of the corresponding author.", "tags": ["metadata query", "metadata extraction", "objective"], "pdf_id": ["122bad91-1e5a-554e-bf1b-7f1e375aaf71"], "conference": ["acl2023"], "reasoning_steps": ["To find the corresponding authors, we need to look at the first page of the paper.", "Pay attention to the superscript symbols next to the author names, which usually indicate the author's affiliation and contribution (e.g., co-first or corresponding authors).", "Find the footnote of the first page to see if there is any explanation of the superscript symbols, such that we can determine the corresponding author."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Dongyan Zhao", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "9eb8dc14-f435-5774-ac7d-1bc63d5b72b4", "question": "Which website can I find the conversation video between Sheldon and Leonard?", "answer_format": "Your answer should be a pure text string starting with 'https'. DO NOT INCLUDE ANY OTHER INFORMATION OR CONTEXT IN THE ANSWER.", "tags": ["metadata query", "text understanding", "metadata extraction", "objective"], "pdf_id": ["1a8e9cd5-8ae1-52b9-84d5-b67bd9c07a21"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, we need to search through the paper with the query \"conversation video between Sheldon and Leonard\".", "Then, we need to find the website link surrounding the text.", "If not found nearby, we can also search for the footnote in exactly the same page where the query relevant text is found. It possibly contains the website link."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "https://giaabaoo.github.io/TPD_website/", "lowercase": false}}, "state": {"gui-gpt-4o-2024-11-20": true, "gpt-4o-2024-11-20": false}}
{"uuid": "512fd6fd-1c6a-54a9-addf-51e622e99dfe", "question": "In terms of WER values with ASR across the six different methods tested in the paper, how much higher is DD2 compared to NV1?", "answer_format": "Your answer should be a single float number.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["81b6a6b0-a195-5cae-9e30-137150b64352"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, locate the section or figure relevant to WER values with ASR across the six different methods.", "Find WER values for DD2 and NV1.If necessary, view and analyze the figure.", "Finally, calculate how much higher DD2 is than NV1."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.598}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "66e99a9b-0660-5574-b8b6-1a05b76c7396", "question": "What are the two loss functions in Equation (8) means?", "answer_format": "Your answer should a list with two items, representing the meaning of the first and the second loss function respectively.", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["85e8f01d-9273-5e20-a37a-fb9e82cf2984"], "conference": ["acl2024"], "reasoning_steps": ["First, get the content of Equation (8).", "Second, get the content of section 3.3.1.", "Third, get the content of section 2, and find what S2T and S2U mean.", "Finally, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["$\\mathcal{L}_{y}(\\theta)$: This term refers to the loss associated with the text prediction task. It is part of the multi-task learning strategy, where the model is trained to predict the textual output, such as the transcription or translation of the input speech.", "$\\mathcal{L}_{z}(\\theta)$: This term corresponds to the loss for the acoustic unit prediction task. Acoustic units are discrete representations of the speech signal, and this loss helps the model learn to generate these units, which are then used to synthesize the translated speech."], "question": "What are the two loss functions in Equation (8) means?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "cd235027-4032-5403-964a-b2c7e7550966", "question": "How much percent does VerifiNER improve the F1 score of the three baseline models on average on GENIA?", "answer_format": "Your answer should be a Python float rounded to two decimal places WITHOUT ANT PUNCTUATION OR EXPLANATION. e.g. 21.30", "tags": ["single-doc details", "table calculation"], "pdf_id": ["220f0021-1bf8-599f-ab3d-5b46d56cb03e"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the result of VerifiNER on GENIA.", "Fetch the improvement on three baseline models respectively.", "Finally, calculate the average improvement and round it to two decimal places."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 7.05, "ndigits": 2, "tolerance": 1e-06}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "775ac142-b55e-5cbb-9dc2-ebfb7aa64260", "question": "What shortcoming does REV overcome? and how?", "answer_format": "", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["d834bb23-9c22-5e94-9421-0be576081dae"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Introduction section.", "Second, summarize the answer."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The REV metric overcomes the shortcoming of existing free-text rationale evaluation methods that focus primarily on how well a rationale helps predict a label. Traditional metrics often fail to assess the new information a rationale provides beyond what is already present in the input or label.", "REV addresses this shortcoming by quantifying the additional, label-relevant information in a rationale, using an information-theoretic approach. It evaluates rationales along two dimensions: (1) Support for the Label: Whether the rationale helps predict the intended label; (2) New Information Contribution: How much unique information the rationale adds, beyond the input and label, to justify the prediction"], "question": "What shortcoming does REV overcome? and how?", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "e26e0a8e-7e6b-55b0-b658-af94309cd496", "question": "According to the experimental results, if we remove the document fact attention module and use mean pooling to fuse all document semantic representation vectors, by how much does the F1 score of FINEGRAINFACT decline in summaries generated by pre-trained language models published in or after 2020?", "answer_format": "Your answer should be a single python float", "tags": ["single-doc details", "text understanding", "table calculation", "objective"], "pdf_id": ["0e6978a1-3a5d-5fdd-808a-033cc79fb049"], "conference": ["acl2023"], "reasoning_steps": ["Locate the table about the performance results in summaries generated by different systems.", "If the column names of the table don't match the question, refer to the section about the experiment design for further clarification or naming of different modules and models.", "Compare and calculate the F1 score drop when switching from the attention module to mean pooling."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 0.33}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "e1184bfe-06e5-5294-9f96-fa353008ba83", "question": "Who are the authors of this paper? What're their institutions?", "answer_format": "Your answer should be a Python dictionary, e.g. {'Amy': ['Massachusetts Institute of Technology', 'Carnegie Mellon University'], 'Bob': ['Shanghai Jiaotong University']}. YOU MUST USE THE FULL AND EXACT WORDS FROM PDF.", "tags": ["metadata query", "metadata extraction"], "pdf_id": ["28f25ac1-c82a-5208-84b4-8ac1a33ea481"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the authors and institutions.", "Finally, "], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Gili Lior": ["Allen Institute for AI", "The Hebrew University of Jerusalem"], "Yoav Goldberg": ["Allen Institute for AI", "Bar-Ilan University"], "Gabriel Stanovsky": ["Allen Institute for AI", "The Hebrew University of Jerusalem"]}, "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "78feef9e-1c36-5824-9c47-544c65f73c86", "question": "Which domain in the GRBench dataset does not have any hard questions?", "answer_format": "Your answer should be a single string representing the domain name.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "conference": ["acl2024"], "reasoning_steps": ["Locate the statistics table or the experimental figure related to GRBench samples of different difficulties.", "Check the statistics table if it reveals the domain with no hard questions.", "Check the experimental figure if it reveals the domain with no result data on hard questions."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "healthcare", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "23cb6726-c7b6-56f0-86bc-4939eac49e1d", "question": "What is the innovation of the formula (6) in this paper?", "answer_format": "Your answer should be a Python strings of innovation of the formula.", "tags": ["single-doc details", "formula inference", "text understanding", "subjective"], "pdf_id": ["7e6c6c6a-f0a6-59ee-8734-af8a912dcf09"], "conference": ["acl2023"], "reasoning_steps": ["Find the formula (6) in the Methodology section and the surrounding text to understand the meaning of the formula.", "Identify the innovation or just the difference of the formula based on the context and the formula itself."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The innovation of this loss function is that it consists of two parts: ground-truth loss and distillation loss. The ground-truth loss is to use one-hot labels to predict connectives, and LKD is the knowledge distillation loss utilizing the Kullback-Leibler divergence to quantify the difference of output distribution from student's soft predictions to teacher's soft labels, which means the student model S is required to match not only the groundtruth one-hot labels but also the probability outputs of the teacher model T.", "question": "What is the innovation of the formula (6) in this paper?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "139b4a99-bd26-5162-a087-d19ee079ebd2", "question": "For the four types of evaluation mentioned in the paper, provide their names and corresponding overall score range. Besides, explain how to obtain overall score for JWP-SAQs.", "answer_format": "Your answer should be a Python list of 2 elements, the first element is a Pyhon dictionary, where the keys are the names of the four types of evaluation and the values are the corresponding overall score range. The second element is a string that explains how to obtain the overall score for JWP-SAQs. e.g. [{'evaluation1': [-1, 1], 'evaluation2': [0, 0.5], ...}, 'reason'] . For the first element, YOU MUST USE THE EXACT NAMES FROM THE PDF WITHOUT CHANGING THE CAPITALIZATION. For the second element, YOU MUST USE DATA FROM THE PDF TO SUPPORT YOUR REASON.", "tags": ["single-doc details", "image analysis", "text understanding"], "pdf_id": ["a3a0b636-d0ab-5dab-a2c3-49f666755896"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the section and figure that introduces the four types of evaluation.", "Identify the names of the four types of evaluation and their corresponding overall score range.", "Read the image to get the criteria for JWP-SAQs.", "Finally, explain how to obtain overall score for JWP-SAQs."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_scoring_points_with_llm"], "eval_kwargs_list": [{"gold": {"SA-SAQs": [0, 1], "JWP-SAQs": [0.75, 2.5], "SA-MCQs": [0, 1], "JWP-MCQs": [0, 1]}, "ndigits": 2, "tolerance": 0.001}, {"scoring_points": ["JWP Short Answers are evaluated by LLM according to the 4 JWP Criteria Facets", "The 4 JWP Criteria Facets include Language Fluency, Style Alignment, Logical Coherence and Instruction Fulfillment.", "The 4 criteria's score ranges are [1, 3], [1, 3], [1, 3] and [0, 1] respectively, leading to an average overall score range of [0.75, 2.5]."], "question": "How to obtain overall score for JWP-SAQs?", "ignore_order": true}]}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "3d3d6314-7069-5382-b942-830f22b0b94c", "question": "Which conference was the paper 'Fact-Checking Complex Claims with Program-Guided Reasoning' published in? Is it a long paper, a short paper or findings?", "answer_format": "Your answer should be a Python list of two elements, the first element is the abbreviation of the conference name (including the year), e.g. EMNLP 2022, and the second element is the type of this paper, i.e. long paper, short paper or findings.", "tags": ["single-doc details", "metadata extraction", "objective"], "pdf_id": ["ed5d7873-1891-50ad-a358-d976054e12f7"], "conference": ["acl2023"], "reasoning_steps": ["First, get the metadata of this paper.", "Second, according to key 'conference', 'year' and 'volume', get the answer"], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["ACL 2023", "long paper"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "38a692e8-8566-539a-aecd-b3e7df04dbcf", "question": "According to the methods proposed by this paper,how to calculate the bias Scores when aggregating attributions for tokens, instances and instructions respectively?", "answer_format": "Your answer should be a python list of three elements, every element is a formula string in latex format.", "tags": ["single-doc details", "formula inference"], "pdf_id": ["2b003f7e-a995-57d1-af78-78432ed96561"], "conference": ["acl2024"], "reasoning_steps": ["Locate the sections about the aggregation of bias scores.", "Extract formulas that correspond to aggregating tokens, instances and instructions respectively."], "evaluator": {"eval_func": "eval_complex_math_formula_with_llm", "eval_kwargs": {"formulas": ["$$\\nB_{i}^{(\\iota,x_{j})}(h)=\\operatorname*{max}_{k}B_{i}^{(\\iota,x_{j},t_{k})}(h)\n$$", "$$\\n\\begin{array}{c}{{B_{i}^{(\\iota,\\mathcal{D})}(h)=\\displaystyle\\sum_{j}^{N}\\alpha^{(\\iota,x_{j})}B_{i}^{(\\iota,x_{j})}(h)}}\\\\ {{\\alpha^{(\\iota,x_{j})}=\\mathcal{P}(\\hat{y_{j}}|\\iota,x_{j})}}\\end{array}\\n$$", "$$\\nB_{i}^{(\\mathbb{Z},\\mathcal{D})}(h)=\\frac{1}{M}\\sum_{\\iota}^{\\mathcal{Z}}B_{i}^{(\\iota,\\mathcal{D})}(h)\\n$$"], "question": "According to the methods proposed by this paper,how to calculate the bias Scores when aggregating attributions for tokens, instances and instructions respectively?", "ignore_order": false}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "0212a0ea-5029-52d9-bd26-cdcf61a1ff42", "question": "How can we get the bias attribution without skill knowledge according to formula (2)?", "answer_format": "Your answer should be a Python strings of the calculation method of bias attribution without skill knowledge.", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["2b003f7e-a995-57d1-af78-78432ed96561"], "conference": ["acl2024"], "reasoning_steps": ["Find the formula (2) in the Methods section and the surrounding text to understand the calculation method of the bias attribution.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The bias attribution without skill knowledge can be calculated by the difference of between the the attribution score between the output text ^y and the golden label text y. Because A(i,x,^y)(h) includes skill knowledge in addition to biased knowledge since estimating the biased text also contains the knowledge of language modeling, such as understanding instruction knowledge. Therefore, we should disentangle the skill knowledge to compute the clean bias attribution.", "question": "How can we get the bias attribution without skill knowledge according to formula (2)?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "2166da5e-be09-5f2b-a8e9-7fed58ede51d", "question": "According to Table 2, which models perform the highest on each of the 8 tasks of GLUE?", "answer_format": "Your answer should a python list of the name of models reaching highest performance on MNLI, QQP, QNLI, SST-2, STS-B, MRPC, RTE, and CoLA respectively. If two models get the same score, you can use 'and' to connect their names, e.g. A and B.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["9b06b24b-0afc-5ccb-95fc-c662395d291d"], "conference": ["acl2024"], "reasoning_steps": ["First, get the content of Table.", "Second, identify the models with the highest performance on eight tasks."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["SCALEARN++", "SCALEARNUNIFORM and SCALEARNUNIFORM++", "SCALEARNUNIFORM and SCALEARNUNIFORM++", "SCALEARN++", "SCALEARN", "ADAPTERFUSION", "SCALEARN", "SCALEARN++"], "ignore_order": false}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}}
{"uuid": "b15e2f1e-a31c-58b9-8d53-1910e0d28391", "question": "On what devices is StreamVoice trained?", "answer_format": "Your answer should be a plein text directly from the PDF without explanation.", "tags": ["single-doc details", "text understanding"], "pdf_id": ["039b3a9f-1e97-5579-bc23-fcd0d2f01c19"], "conference": ["acl2024"], "reasoning_steps": ["Locate the section where training details are provided.", "Read the section to find the devices used for training."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "StreamVoice is trained using 8 V100 GPUs with a batch size of 7 utterances per GPU for 700k steps.", "question": "On what devices is StreamVoice trained?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "4c9a32c4-52df-56cf-bbcf-0a10a18d594f", "question": "According to Table 1, how many times is the average number of tokens for the dataset with the highest average number of tokens versus the one with the least average number of tokens?", "answer_format": "Your answer should be a floating-point number with two decimal places.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["9c96433d-817e-5dad-a394-d7d80a428ed0"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Table 1.", "Second, identify the dataset with the highest average number of tokens and the dataset with the least average number of tokens.", "Finally, calculate the result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 1210.75, "ndigits": 2, "tolerance": 1e-06}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "a927fcee-f0a7-50cd-b5a7-ca8ab5c8eb23", "question": "What are the institutions of the first author and corresponding author of this paper?", "answer_format": "Your answer should be a Python list of length 2 containing the institution names respectively, e.g., [\"first_author_institute\", \"corresponding_author_institute\"]. If there are multiple first authors or corresponding authors, please replace the corresponding institution name with a name list, e.g., [[\"first_author1_institute\", \"first_author2_institute\", ...], [\"corresponding_author1_institute\", \"corresponding_author2_institute\", ...]].", "tags": ["metadata query", "metadata extraction", "objective"], "pdf_id": ["3d2fcb43-2cda-5645-99aa-da78c6cfd23f"], "conference": ["acl2024"], "reasoning_steps": ["The authors and their institutions are usually listed in the first page.", "And the co-first author or corresponding author information is usually marked with a symbol in the author region, and explained below in the footnote.", "After selecting the relevant authors, you can extract their institution information from the first page, usually nearby the author list or in the footnote."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Beihang University", ["The University of Sydney", "The Hong Kong Polytechnic University"]], "ignore_order": false}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "198666fc-a067-52c2-b80f-fb804bc80034", "question": "What is the ranking of the average performance of the models compared in the experiment across all languages where each model has a value in the all-language finetuning, from highest to lowest?", "answer_format": "Your answer should be a Python list of elements, each element is a model name string, e.g., ['model_name 1', 'model_name 2', ...].", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["ff651d37-e725-5752-9c38-3361bc54723d"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, find all section titles in the paper to locate the experiment section.", "Identify the finer requirements on all-language finetuning.", "Next, find the relevant table, figure or text that contains the average performance of the models across all languages.", "Finally, calculate or retrieve the exact value to figure out the ranking of the models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["mCLIP", "mCLIP+", "UC2", "M3P"]}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "27873950-73ca-554c-be4b-88fc723841e7", "question": "How to calculate $l^{\\prime}(c,c^{\\prime})$ in the equation under Section 3.2?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["d23ce539-adb7-5709-b341-169c0dcd5871"], "conference": ["acl2024"], "reasoning_steps": ["First, get the content of the equation under 3.2.", "Second, get the content of Section 3.1 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$l^{\\prime}(c,c^{\\prime}) = \\mathbbm{1}(l(c) \\geq l(c^{\\prime}))$ is a pairwise comparator, which takes a pair of comparisons $(c,c^{\\prime})$ and determines the more consistent one. $l(c)$ evaluates the consistency of $c$, and a higher value of $l(c)$ indicates a greater degree of consistency.", "question": "How to calculate $l^{\\prime}(c,c^{\\prime})$ in the equation under Section 3.2?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "da9752f5-e86d-577d-a99b-88a399197e6a", "question": "How many multi-modal baselines excluding the method this paper proposed do authors use? Among this baselines, who reaches the highest F1 score on Twitter2015 dataset? And what about Twitter2017 dataset?", "answer_format": "Your answer should be a python list, the first item is the number of multi-modal baselines the paper used, the second item and the third item are the name of methods reaching the highest F1 score on Twitter2015 and Twitter2017 dataset respectively.", "tags": ["single-doc details", "metadata extraction", "objective"], "pdf_id": ["6a24d7f4-430d-5c92-b259-f62f76490147"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Table 2.", "Second, get the content of row 'Multimodal' and count the number of multi-modal baselines.", "Third, check the 'F1' column under 'Twitter2015' and 'Twitter2017' multi-column and find the one reaching highest F1 score."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_int_exact_match", "eval_string_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": 8}, {"gold": "VLP-MABSA"}, {"gold": "CMMT"}]}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}}
{"uuid": "17fe77cb-688f-52d4-be70-a66eaadc17ff", "question": "What are the main works at the table-level when performing data augmentation on the MMTab dataset in this paper?", "answer_format": "Your answer should be a Python strings about the main works at the table-level when performing data augmentation on the MMTab dataset.", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["a7f69ed5-2c8a-5892-bf66-c2599380f805"], "conference": ["acl2024"], "reasoning_steps": ["Find the corresponding sections about the data augmentation on the MMTab dataset.", "Retrieve for the texts discussing the main works of data augmentation at the table-level.", "If there is no clear expression, try to summarize it from the main text."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The main work is to separately design scripts to render table images with three different styles: Web-page (70.8%), Excel (19.4%) and Markdown (9.8%). Fine-grained adjustments such as font type and cell colors are also considered", "question": "What are the main works at the table-level when performing data augmentation on the MMTab dataset in this paper?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "43ab49eb-020d-5c64-a210-7f0931d39224", "question": "How can I get $h_i$ or $h_j$ in Equation (1)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["32d1e04a-e87c-5179-8b13-4ad86585c55f"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 3.1, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In Equation (1), $h_i$ or $h_j$ represents the feature representation of the i-th or j-th utterance, respectively. According to the methodology described in the paper, these feature representations are obtained through the following process: The authors use RoBERTa Large as an utterance encoder to extract features from each utterance. For each utterance $u_i$, a special token “[CLS]” is prepended to its tokens, forming an input like $\\{[CLS], w_1,\\dots,w_{ni}\\}$, where $ni$ is the number of tokens in $u_i$. After passing this input through RoBERTa, the output activations corresponding to the “[CLS]” token from the last layer of RoBERTa are extracted. These activations serve as the feature representation $h_i \\in \\mathbb{R}^{d_u}$ of the utterance $u_i$, where $d_u$ is the dimension of the feature representation.", "question": "How can I get $h_i$ or $h_j$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "f640029c-539b-58b1-a742-05b8bb0edacb", "question": "What's the biggest reason of incorrect action for each model?", "answer_format": "Your answer should be a Python dictionary. e.g. {'model1': 'answer1', 'model2': 'answer2', ...}. YOU MUST USE THE EXACT AND FULL TEXT FROM PDF WITHOUT CHANGING CAPITALIZATION.", "tags": ["single-doc details", "image analysis"], "pdf_id": ["4ee26cdd-4e52-5090-b1c8-46f5dcdba09c"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the section that analyses the incorrect actions of the models.", "Find the figure that shows the proportions of correct and incorrect actions for each model.", "Finally, identify the biggest reason of incorrect action for each model."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Vicuna-13B": "Invalid Action/Object", "OpenChat-3.5": "Object-Mismatched Action", "Mixtral-7Bx8": "Invalid Action/Object", "Gemini Pro": "Object-Mismatched Action", "GPT-3.5-turbo": "Object-Mismatched Action", "GPT-4": "Dependency Violation"}}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "55534d55-ed7c-5240-96a5-cde7fd739de8", "question": "What're the related domains of this paper according to related works?", "answer_format": "Your answer should be a Python list of strings where each string is a related domain. e.g. ['domain1', 'domain2', ...]", "tags": ["single-doc details", "text understanding"], "pdf_id": ["9cf8bd4d-0120-5a3b-b926-1a2d7d7b4f0a"], "conference": ["acl2023"], "reasoning_steps": ["Locate the related works section of the paper.", "Identify the subsections in the related works section.", "Use the subsections titles as the related domains."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["Prompt-based Learning", "Lexical Relation Classification"], "ignore_order": true, "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "ce95db65-95c3-55d5-8eda-3e80ef6d0775", "question": "Using only task-level prompts or using only example-specific prompts, which is better on the Multi-Domain test set?", "answer_format": "Your answer should be a single string, either \"task-level\" or \"example-specific\".", "tags": ["single-doc details", "table calculation", "text understanding", "objective"], "pdf_id": ["5a2b95c1-12d6-5b77-82a1-ee24180d27ae"], "conference": ["acl2023"], "reasoning_steps": ["Locate the experiment table that shows the model performances on the Multi-Domain test set with different distributions of prompts.", "Find out the mathematical symbols indicating the numbers of two types of prompts.", "Locate all the rows in the table related to the question.", "Compare the result data and determine the conclusion."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "example-specific", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": true}}
{"uuid": "fe64bb38-1b46-53c8-b82b-dfc2acf75c2e", "question": "I want to contact the first author of this paper. What's the email address?", "answer_format": "Your answer should be a verbose text string representing the email address if there is only one first author. Otherwise, return a Python list of e-mail strings for each first and co-first author, e.g., [\"xxx@xxx.com\", \"yyy@yyy.com\", ...]. DO NOT INCLUDE ANY OTHER CONTEXT IN YOUR ANSWER.", "tags": ["metadata query", "metadata extraction", "objective"], "pdf_id": ["3ea7de2a-3312-589f-a765-01e4b9e1dcb7"], "conference": ["acl2024"], "reasoning_steps": ["To find the first and possibly co-first authors, we need to look at the first page of the paper.", "Pay attention to the superscript symbols next to the author names, which usually indicate the author's affiliation and contribution (e.g., co-first or corresponding authors).", "Find the footnote of the first page to see if there is any explanation of the superscript symbols, such that we can determine all first authors.", "Combine with all authors' affiliations to find the email address(es) of the first author(s), which are usually mentioned nearby the authors or in the footnote."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["hhu4zu@virginia.edu", "qiao.jin@nih.gov"], "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "12a70e18-aa46-5779-bd69-2f3620d7f484", "question": "Which downstream tasks does CLiCoTEA outperform other models in terms of zero-shot performance on the IGLUE benchmark?", "answer_format": "Your answer should be a Python list of string elements, every element is the abbreviation of a downstream task type mentioned in the paper.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["0d6ea045-b831-520d-9b99-ba22a081a403"], "conference": ["acl2023"], "reasoning_steps": ["Firstly, identify downstream tasks relevant to the question.", "Find tables zero-shot performance on the IGLUE benchmark.", "Finally, compare the performances on different tasks to identify where CLiCoTEA outperforms other models."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": ["VE", "VR"], "ignore_order": true, "ignore_blank": true}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "f7b532c1-3fd7-5a2b-87b4-522592ff6dbe", "question": "When training with non-English image-text pairs, what is the loss function os the TriKD?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["ff651d37-e725-5752-9c38-3361bc54723d"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of section 3.2 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "When training with non-English image-text pairs, only ITC loss is applied, as the CLIP text encoder does not support non-English languages. Therefore, the loss function for the TriKD is $\\mathcal{L}_{\\text{TriKD}} = \\mathcal{L}_{\\text{ITC}}$. And the Image-Text Contrastive(ITC) loss is formulated as the average of image-to-text($\\mathcal{L}_{\\text{i2x}}$) loss and text-to-image($\\mathcal{L}_{\\text{x2i}})$) loss: $\\mathcal{L}_{\\text{ITC}} = 1/2(\\mathcal{L}_{\\text{i2x}} + \\mathcal{L}_{\\text{x2i}}) = 1/2[\\ell(h^I, h^X) + \\ell(h^X, h^I)]$. Here, $h^I$ represents the $\\ell_2$-normalized output from the CLIP image encoder and CLIP-projector, and $h^X$ represents the $\\ell_2$-normalized output from the Multilingual Text Encoder (MTE) and X-projector for a given image-text pair.", "question": "When training with non-English image-text pairs, what is the loss function os the TriKD?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "df2d7dce-b86a-5805-b524-3f453268240f", "question": "According to Figure 1, in which year has the highest proportion of NLP papers that explicitly mention speech-related terms in their title?", "answer_format": "Your answer should be the year number with the highest proportion, e.g. 2000.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["a88d4d17-b2f9-520e-a611-c2c4ba178be5"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Figure 1.", "Second, identify the answer and check it with the content of section 2.2."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 1989}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "c1f769f6-eaff-5441-9f1c-d62445efe58d", "question": "What's the total number of augmented training samples across all datasets used in the MINPROMPT work?", "answer_format": "Your answer should be a single integer number.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["0b77c64c-6b36-5501-848f-79a062be2a45"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the table that describes the data augmentation statistics in the paper.", "Then, find the number of augmented data samples for each dataset. Special attention should be paid to some aggregation columns or rows, e.g., \"average\" or \"total\". They should be excluded from the calculation.", "Finally, sum up the numbers of augmented data samples across all datasets."], "evaluator": {"eval_func": "eval_int_exact_match", "eval_kwargs": {"gold": 251387}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "bf391f7a-d33b-5b00-9001-ee92284a15ec", "question": "According to Figure 1, with the increasing of the number of few-shot training samples, which setting keeps getting a better score?", "answer_format": "Your answer should be the name of the setting appearing in the legend of the image.", "tags": ["single-doc details", "image analysis", "objective"], "pdf_id": ["ebf4682e-2653-5589-9458-da26d00d9f5b"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Figure 1.", "Second, analyze the trend of scores as the number of samples increases and get the answer."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "KB score, curie"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": false}}
{"uuid": "0b1cad92-b6c6-51a8-b7f2-5c844e572024", "question": "On which language does LLaMA-2 13B with no removal reaches its second highest perplexity?", "answer_format": "Your answer should be a word DIRECTLY FROM THE PDF WITHOUT ANY EXPLANATION.", "tags": ["single-doc details", "table calculation"], "pdf_id": ["a20a1a7a-b335-54ef-82a5-b97be4405604"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the table that compares LLaMA-2 perplexity.", "Find the correct column and rows that correspond to the model LLaMA-2 13B with no removal.", "Finally, identify the language that has the second highest perplexity."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Italian", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "3fd4d805-ae6c-527d-b6d2-30a18fb0ab12", "question": "On the dataset proposed by this work, how much does the GPT-3.5-turbo model improve its GPT4score after using Graph-CoT?", "answer_format": "Your answer should be a single float number ranging from 0 to 100, representing the subtraction result.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "conference": ["acl2024"], "reasoning_steps": ["Find the name of the dataset proposed by this work.", "Locate the experiment table related to the proposed dataset that demonstrates the GPT4score of the GPT-3.5-turbo model before and after using Graph-CoT.", "Record the two GPT4scores respectively.", "Calculate the subtraction result."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 16.81, "ndigits": 2}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "a96944de-c900-55f0-9b3b-cccb597a8b71", "question": "Which category of website takes up the most proportion in the dataset MC2?", "answer_format": "Your answer should be a phrase indicating the category DIRECTLY FROM THE PDF WITHOUT ANY MODIFICATION OR EXPLANATION.", "tags": ["single-doc details", "image analysis"], "pdf_id": ["16142be2-ac28-58e5-9271-8af299b18d91"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the figure that shows the distribution of the dataset MC2.", "Finally, answer the question."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "News"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "c53cba22-704b-51db-ae71-0166a727b747", "question": "How to calculate the Word-pair Representation matrix in Figure 4?", "answer_format": "Your answer should be a list of formulas, representing the calculation of Word-pair Representation matrix.", "tags": ["single-doc details", "image analysis", "subjective"], "pdf_id": ["1191a3cb-63fe-560c-bbef-c7eee0dd61d6"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Figure 4.", "Second, get the content of Section 4.2, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The Word-pair Representation matrix in the described model is calculated through Conditional Layer Normalization (CLN). Here are the formulas representing the calculation:\n\n1. **Conditional Layer Normalization (CLN) for word-pair representation:**\n   $r_{i,j} = \\text{CLN}(h_i, h_j) = \\gamma_i \\odot \\left( \\frac{h_j - \\mu}{\\sigma} \\right) + \\lambda_i$\n\n2. **Scale factor $\\gamma_i$ and shift factor $\\lambda_i$ with additional contextual information:**\n   $\\gamma_i = W_\\gamma h_i + b_\\gamma$\n   $\\lambda_i = W_\\lambda h_i + b_\\lambda$\n\n3. **Mean $\\mu$ and standard deviation $\\sigma$ of $h_j$:**\n   $\\mu = \\frac{1}{d} \\sum_{k=1}^{d} h_{jk}$\n   $\\sigma = \\sqrt{\\frac{1}{d} \\sum_{k=1}^{d} (h_{jk} - \\mu)^2}$", "question": "How to calculate the Word-pair Representation matrix in Figure 4?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "82bdaa47-a2cb-5fbd-a827-83d981f4bb52", "question": "According to Table 2, what is the difference of the percentage of over 80% agreement threshold for both concessive and causal relations between the first and second iteration on the English dataset?", "answer_format": "Your answer should be a percentage with two decimal places, indicating the difference in proportion.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["757dc667-413b-5207-8463-5deb4fedf073"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Table 2.", "Second, find the '>80% both' column and calculate the difference of first and second iteration.", "Third, calculate the total number of samples.", "Finally, calculate the percentage."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "58.06%"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "08bd3ae0-1594-510d-b67b-bfbd9fb73b56", "question": "What baselines do authors use? And according to Table 2, which baseline can reach the best performance?", "answer_format": "Your answer should a list with two items, the first item is a python list of the name of baselines, and the second item is the name of baseline reaching the best performance, e.g. [[baseline 1, baseline 2, ...], baseline 1].", "tags": ["single-doc details", "text understanding", "objective"], "pdf_id": ["3d2fcb43-2cda-5645-99aa-da78c6cfd23f"], "conference": ["acl2024"], "reasoning_steps": ["First, get the content of Section 4.", "Second, get the content of Table 2, and get the final answer."], "evaluator": {"eval_func": "eval_conjunction", "eval_kwargs": {"eval_func_list": ["eval_structured_object_exact_match", "eval_string_exact_match"], "eval_kwargs_list": [{"gold": ["Prompting", "Random", "BM25", "TopK", "TopK + MDL"], "ignore_order": true, "ignore_blank": true}, {"gold": "TopK + MDL", "ignore_blank": true}]}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "07f7da2d-c8d9-581d-bf9e-ba8f9a562968", "question": "What are the meanings of different loss items in equation (9)? i.e. $\\mathcal{L}_{c}^{A}$, $\\mathcal{L}_{s}^{A}$, $\\mathcal{L}_{a}^{A}$, and $\\mathcal{L}_{c}^{B}$", "answer_format": "Your answer should be a Python list of four elements, where each of the item represent the meaning of the corresponding loss item.", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["c51a33ed-cd35-5426-b37e-6864a1b66a35"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of equation (9).", "Second, get the related content of each loss item in the paper.", "Third, analyze the meaning of each loss item."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["$\\mathcal{L}_{c}^{A}$: The content reconstruction loss, which ensures that the model can accurately reconstruct the input text from the latent representations.", "$\\mathcal{L}_{s}^{A}$: The stance classification loss, which guides the model to predict the correct stance of the text. It typically uses cross-entropy loss to align predictions with true stance labels.", "$\\mathcal{L}_{a}^{A}$: The aspect span reconstruction loss, which helps the model generate a specific aspect-related text span using a Gaussian negative log-likelihood (NLL) loss.", "$\\mathcal{L}_{c}^{B}$: The swapped reconstruction loss, derived from a swapping autoencoder mechanism. It ensures that the disentanglement is maintained even after swapping aspect embeddings across samples with the same aspect but different stances."], "question": "What are the meanings of different loss items in equation (9)? i.e. $\\mathcal{L}_{c}^{A}$, $\\mathcal{L}_{s}^{A}$, $\\mathcal{L}_{a}^{A}$, and $\\mathcal{L}_{c}^{B}$", "ignore_order": true}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "e5b21555-1c9a-5275-be50-1a418f9a59d6", "question": "What are the meanings of function $s$ and function $F$ in Equation (3)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["875e65e0-8e9f-52e3-9d3e-65f15fa1ea82"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Equation (3).", "Second, get the content of Section 4.1 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$s(\\cdot)$ is the cosine similarity and $F(\\cdot)$ is a non-linear mapping on the output representations of PLM.", "question": "What are the meanings of function $s$ and function $F$ in Equation (3)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "3cdb56d6-cdfc-57d0-8b3d-736aee6fa4c7", "question": "How to initialize $h_{i,l-1}^S$ in Equation (11)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["6a24d7f4-430d-5c92-b259-f62f76490147"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of the Equation (11).", "Second, get the content of section 3.3, and find the definition of $h_i^S$.", "Finally, get the content of Section 3.2, and find the definition of $\\hat{h_i}$."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "To initialize $h_{i,0}^S$ in Equation (11), we start with the output from Equation (8): $h_i^S = \\hat{h}_i + s_i$.\n\nHere, $\\hat{h}_i$ is the output from the Aspect-Aware Attention Module (A3M), and $s_i$ is the sentiment feature obtained by projecting the affective score of word $w_i$ from SenticNet into the same dimensional space as $\\hat{h}_i$. Specifically:\n\n1. For each word $w_i$ in the sentence, obtain its affective score $w_i^S$ from SenticNet.\n2. Project this affective score into the same dimensional space as $\\hat{h}_i$: $s_i = W_S w_i^S + b_S$, where $W_S$ and $b_S$ are learned parameters.\n3. Add the sentiment feature $s_i$ to $\\hat{h}_i$: $h_i^S = \\hat{h}_i + s_i$\n\nThis $h_i^S$ serves as the initial node representation $h_{i,0}^S$ for the Aspect-Guided Graph Convolutional Network (AG-GCN). Therefore, for the first layer $l=0$: $h_{i,0}^S = h_i^S$.", "question": "How to initialize $h_{i,l-1}^S$ in Equation (11)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "e6a78d3c-1bfe-55ea-b070-712554f7cae9", "question": "In the domain of prediciton questions, what's the largest issue for LLAMA 2?", "answer_format": "Your answer should be a word or phrase, indicating the largest issue. YOU MUST USE THE EXACT WORDS FROM PDF WITHOUT EXPLANATIONS.", "tags": ["single-doc details", "image analysis"], "pdf_id": ["10e37be8-8c9f-590c-b94f-e6bea03794f2"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the figure that lists the distribution of different issues.", "Finally, find the largest issue for given model."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "Multi-Answers"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "7870d38f-1d3b-57d0-b0a0-bdcf9c1cd381", "question": "What are the advantages and disadvantages of MUX-PLMs mentioned in the paper?", "answer_format": "Your answer should be a string list of advantages and disadvantages. ", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["457688f9-deb0-54fb-8531-6ed175a556d0"], "conference": ["acl2023"], "reasoning_steps": ["Locate the section in the specified PDF that discusses MUX-PLMs, possibly in the analysis section or conclusion section.", "Extract content from the relevant sections that highlight the advantages of MUX-PLMs and discusses the disadvantages of MUX-PLMs..", "Summarize the advantages and disadvantages."], "evaluator": {"eval_func": "eval_partial_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["Significant Throughput Improvement: MUX-PLMs leverage data multiplexing to process multiple inputs in a single forward pass, resulting in a dramatic increase in throughput.", "Comparable Performance to PLMs: Despite the increased throughput, MUX-PLMs maintain performance close to traditional PLMs on downstream tasks.", "Model Generalizability: MUX-PLMs can be fine-tuned like traditional PLMs for various downstream tasks, such as text classification and named entity recognition.", "Performance-Throughput Trade-off: As the number of multiplexed inputs (N) increases, the throughput improves further, but the performance may slightly degrade.", "Model Size Limitations: While the paper demonstrates the effectiveness of MUX-PLMs across different model sizes, the model size still impacts performance and throughput. Larger models may offer better performance but with a potentially smaller throughput improvement compared to smaller models.", "Data Sampling Strategy: The paper employs a random data sampling strategy, but a more sophisticated approach could potentially enhance performance, for example, clustering similar instances based on similarity metrics and multiplexing them."], "question": "What are the advantages and disadvantages of MUX-PLMs mentioned in the paper?", "count": 4}}, "state": {"gpt-4o-2024-11-20": false}}
{"uuid": "c69c648f-bde4-5a8b-a82e-67fe2cdefe9f", "question": "What is the limitation of this work proposed by the authors themselves?", "answer_format": "Your answer should be plain text.", "tags": ["single-doc details", "text understanding", "subjective"], "pdf_id": ["1a472825-70b5-5b91-a9a1-f60fcb8d89f5"], "conference": ["acl2024"], "reasoning_steps": ["Find the limitation section in the paper to locate the limitations of the work.", "Summarize or paraphrase the limitations mentioned by the authors."], "evaluator": {"eval_func": "eval_scoring_points_with_llm", "eval_kwargs": {"scoring_points": ["The authors have only explored applying the approach to encoder models, leaving room for applications on decoder models.", "Despite the variety of existing contrastive learning methodologies, this work still adheres to utilizing the contrastive learning objectives provided by the tasks."], "question": "What is the limitation of this work proposed by the authors themselves?", "ignore_order": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "85f81535-fc10-575c-995f-4739a4470d4b", "question": "Where can I get the code of GreenKGC method?", "answer_format": "Your answer should be the url of the code of GreenKGC method.", "tags": ["single-doc details", "metadata extraction", "objective"], "pdf_id": ["a3e3cee1-d140-5dc1-9608-2f1a1d924229"], "conference": ["acl2023"], "reasoning_steps": ["First, get the footer of page 1 and answer the question."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "https://github.com/yunchengwang/GreenKGC"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "c100db0f-bb91-514e-af99-6c6efcf22cd3", "question": "How much data do the author use in total in million for the main experiment conducted on WMT17 ZhEn?", "answer_format": "Your answer should be a Python float, rounding to 1 decimal places.", "tags": ["single-doc details", "text understanding"], "pdf_id": ["03166771-5ae8-57b9-9c10-3120423adc5c"], "conference": ["acl2023"], "reasoning_steps": ["Locate the section where experimental data is mentioned.", "Identify the data used for the specific experiment.", "Calculate the total data used."], "evaluator": {"eval_func": "eval_float_exact_match", "eval_kwargs": {"gold": 39.5, "ndigits": 1, "tolerance": 0.01}}, "state": {"gui-gpt-4o-2024-11-20": true}}
{"uuid": "fe85c6f9-9ec2-5535-8ae4-d9be3e92d66c", "question": "Why can we omit $p(\\{y_l, l \\in L\\})$ in Equation (1)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["a819666a-9e5b-5213-9efd-4f1e12225426"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 6.1, and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "In Equation (1), $p(\\{y_l, l \\in L\\})$ can be omitted because it acts as a normalization factor that is constant with respect to the ancestral form $x$. The goal of the equation is to compute a value proportional to the posterior probability $p(x | \\{y_l, l \\in L\\})$. Since $p(\\{y_l, l \\in L\\})$ does not depend on $x$, it remains the same for all possible ancestral forms and thus does not affect the relative probabilities of different $x$ values.", "question": "Why can we omit $p(\\{y_l, l \\in L\\})$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "4ab4e4dc-fc8a-5749-a2cf-171f0a0bc4e3", "question": "What are the meanings of $h_i, r_i, t_i$ in Equation (1)?", "answer_format": "", "tags": ["single-doc details", "formula inference", "subjective"], "pdf_id": ["a3e3cee1-d140-5dc1-9608-2f1a1d924229"], "conference": ["acl2023"], "reasoning_steps": ["First, get the content of Equation (1).", "Second, get the content of Section 3.2 and summarize the answer."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "$h_i, r_i, t_i$ represent the $i$-th dimension in the head, relation, and tail representations respectively.", "question": "What are the meanings of $h_i, r_i, t_i$ in Equation (1)?"}}, "state": {"gpt-4o-2024-05-13": false, "gui-gpt-4o-2024-05-13": true}}
{"uuid": "ff40fb8f-a1d9-5598-91b2-2af15bbad92e", "question": "Among the specific models tested, whose performance is closest to RePe on MixATIS++?", "answer_format": "Your answer should be the name of model DIRECTLY FROM THE PDF WITHOUT ANY EXPLANATION.", "tags": ["single-doc details", "table calculation"], "pdf_id": ["95e1644a-230b-5b28-a025-31992e32e3f0"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the table that compares the performance of models.", "Finally, identify the model whose performance is closest to RePe on MixATIS++."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "FC-MTLF"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "65d3fbf5-5319-5490-9686-537924c3c4ee", "question": "I want to replicate the experiment in this paper. Please list all the datasets and baselines that I should prepare.", "answer_format": "Your answer should be plain text", "tags": ["single-doc details", "text understanding"], "pdf_id": ["4f88e7de-b217-5d6b-a315-a872e927bdfe"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the experiment section.", "Identify the datasets and baselines used in the experiment.", "Finally, answer with the list of datasets and baselines."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "1. For datasets, this paper select five challenging logical reasoning benchmarks: (1)LogiQA (2)ProofWriter (3)FOLIO (4)PrOntoQA (5)LogicalDeduction(LD). 2. For baselines, (1)Standard prompting (2)Chain-of-Thought(CoT) (3)Chain-of-Thought with Self-Consistency(CoT-SC) (4)Selection-Inference(SI) (5)LAMBADA (6)Tree-of-Thought(ToT) (7)Cumulative Reasoning(CR).", "question": "I want to replicate the experiment in this paper. Please list all the datasets and baselines that I should prepare."}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "7369f690-c9b9-52d7-8698-3b38d8c2baf1", "question": "For dataset MultiDialog, what's the number of dialogues,the number of turns, total length in hours, number of speakers, and the name of source dataset of dialogue scripts?Please adopt the statistics the most accurate you can find in the paper.", "answer_format": "Your answer should be a python list,of which the elements are in the following order: number of dialogues(int), number of turns(int), total length in hours(float), number of speakers(int), and the name of source dataset of dialogue scripts(str), every element of the list is an int or float or string representing the relevant dataset information.", "tags": ["single-doc details", "text understanding", "objective"], "pdf_id": ["4beb073a-74b4-563a-82a7-da7513acaff0"], "conference": ["acl2024"], "reasoning_steps": ["Locate the sections or tables in the specified PDF relevant to the basic information of MultiDialog.", "Extract the exact number of dialogues, turns, total length in hours, number of speakers, and source dataset name.", "Summarize the extracted information in the specified order."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": [8733, 187859, 339.71, 12, "TopicalChat"], "ignore_order": false, "threshold": 95}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "1797020c-d6b0-5909-a17b-25688d7bc433", "question": "According to this paper, what type will the question \"How to keep strawberries fresh\" be classified into?", "answer_format": "Your answer should be a single string representing the question type.", "tags": ["single-doc details", "text understanding", "objective"], "pdf_id": ["010e09cf-8f86-5759-8446-0f7e6558997c"], "conference": ["acl2023"], "reasoning_steps": ["Read section titles and locate the section that discusses the question type classification.", "Understand the definition of each question type.", "Identify the question type that the question falls into based on the definition."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "open", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "57082e3d-1fcd-54f5-8985-370723fcc4c2", "question": "Which domain in the GRBench dataset has the most number of questions?", "answer_format": "Your answer should be a single string representing the domain name.", "tags": ["single-doc details", "table calculation", "objective"], "pdf_id": ["fb154467-1ce1-5c1f-9d4f-b4f5c76312ee"], "conference": ["acl2024"], "reasoning_steps": ["Locate the statistics table for the GRBench dataset.", "Record or calculate the total number of questions in each domain.", "Identify the domain with the highest number of questions."], "evaluator": {"eval_func": "eval_string_exact_match", "eval_kwargs": {"gold": "academic", "lowercase": true}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "6bdd99b0-3976-5029-9b64-d82b7bfb4276", "question": "What does formula (1) mean in the Methodology section?", "answer_format": "Your answer should be a python strings of the detailed explanation of the formula.", "tags": ["single-doc details", "text understanding", "formula inference", "subjective"], "pdf_id": ["cd91a901-384e-597b-bb20-2d4f9fa0c4f9"], "conference": ["acl2023"], "reasoning_steps": ["Find the formula (1) in the Methodology section and the surrounding text to understand the meaning of the formula.", "If the formula is fully explained in the text, directly use the explanation as the answer.", "Otherwise, try to explain it based on the formula itself and the context."], "evaluator": {"eval_func": "eval_reference_answer_with_llm", "eval_kwargs": {"reference_answer": "The idea of this formula is decomposing token representations into their constituent vectors based on vector-based approaches. Decompose the i-th token representation in layer l into elemental vectors attributable to each of the N input tokens. So we can compute the norm of the attribution vector of the k-th input to quantify its total attribution to xi.", "question": "What does formula (1) mean in the Methodology section?"}}, "state": {"gui-gpt-4o-2024-11-20": false}}
{"uuid": "be178eef-403f-5633-8cbd-5b876059fce4", "question": "For each test split in Figure 5, provide the name and type of the website with the highest step success rate.", "answer_format": "Your answer should be a Python dictionary. e.g. {'split1': ['web1', 'type1'], 'split2': ['web2', 'type2'], ...}. YOU MUST USE THE EXACT WORDS FROM PDF WITHOUT CHANGING CAPITALIZATION.", "tags": ["single-doc details", "image analysis"], "pdf_id": ["32a52b98-370b-5bc4-87ab-5193405b723b"], "conference": ["acl2024"], "reasoning_steps": ["Firstly, locate the figure.", "Search in the caption to find test split and corresponding websites.", "Finally, read the figure to get the websites with the highest step success rate."], "evaluator": {"eval_func": "eval_structured_object_exact_match", "eval_kwargs": {"gold": {"Cross-Task": ["travelzoo", "General"], "Cross-Website": ["exploretock", "Restaurant"], "Cross-Subdomain": ["koa", "Hotel"]}}}, "state": {"gui-gpt-4o-2024-11-20": false}}
