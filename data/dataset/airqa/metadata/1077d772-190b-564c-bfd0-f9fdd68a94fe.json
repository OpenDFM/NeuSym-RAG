{
    "uuid": "1077d772-190b-564c-bfd0-f9fdd68a94fe",
    "title": "Analysis of LLM’s “Spurious” Correct Answers Using Evidence Information of Multi-hop QA Datasets",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
    "bibtex": "@inproceedings{ishii-etal-2024-analysis,\n    title = \"Analysis of {LLM}{'}s {``}Spurious{''} Correct Answers Using Evidence Information of Multi-hop {QA} Datasets\",\n    author = \"Ishii, Ai  and\n      Inoue, Naoya  and\n      Suzuki, Hisami  and\n      Sekine, Satoshi\",\n    editor = \"Biswas, Russa  and\n      Kaffee, Lucie-Aim{\\'e}e  and\n      Agarwal, Oshin  and\n      Minervini, Pasquale  and\n      Singh, Sameer  and\n      de Melo, Gerard\",\n    booktitle = \"Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.kallm-1.3\",\n    doi = \"10.18653/v1/2024.kallm-1.3\",\n    pages = \"24--34\",\n    abstract = \"Recent LLMs show an impressive accuracy on one of the hallmark tasks of language understanding, namely Question Answering (QA). However, it is not clear if the correct answers provided by LLMs are actually grounded on the correct knowledge related to the question. In this paper, we use multi-hop QA datasets to evaluate the accuracy of the knowledge LLMs use to answer questions, and show that as much as 31{\\%} of the correct answers by the LLMs are in fact spurious, i.e., the knowledge LLMs used to ground the answer is wrong while the answer is correct. We present an analysis of these spurious correct answers by GPT-4 using three datasets in two languages, while suggesting future pathways to correct the grounding information using existing external knowledge bases.\",\n}\n",
    "authors": [
        "Ai Ishii",
        "Naoya Inoue",
        "Hisami Suzuki",
        "Satoshi Sekine"
    ],
    "pdf_url": "https://aclanthology.org/2024.kallm-1.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1077d772-190b-564c-bfd0-f9fdd68a94fe.pdf",
    "abstract": "Recent LLMs show an impressive accuracy on one of the hallmark tasks of language understanding, namely Question Answering (QA). However, it is not clear if the correct answers provided by LLMs are actually grounded on the correct knowledge related to the question. In this paper, we use multi-hop QA datasets to evaluate the accuracy of the knowledge LLMs use to answer questions, and show that as much as 31% of the correct answers by the LLMs are in fact spurious, i.e., the knowledge LLMs used to ground the answer is wrong while the answer is correct. We present an analysis of these spurious correct answers by GPT-4 using three datasets in two languages, while suggesting future pathways to correct the grounding information using existing external knowledge bases.",
    "num_pages": 11
}