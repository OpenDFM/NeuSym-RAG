{
    "uuid": "31880db4-2032-566f-90ee-222ebab5094e",
    "title": "Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-serial,\n    title = \"Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction\",\n    author = \"Wang, Xinyi  and\n      Wang, Zitao  and\n      Hu, Wei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.804\",\n    doi = \"10.18653/v1/2023.findings-acl.804\",\n    pages = \"12693--12706\",\n    abstract = \"Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models.\",\n}\n",
    "authors": [
        "Xinyi Wang",
        "Zitao Wang",
        "Wei Hu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.804.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/31880db4-2032-566f-90ee-222ebab5094e.pdf",
    "abstract": "Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models.",
    "num_pages": 14
}