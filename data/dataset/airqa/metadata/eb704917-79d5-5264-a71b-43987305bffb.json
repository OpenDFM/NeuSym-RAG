{
    "uuid": "eb704917-79d5-5264-a71b-43987305bffb",
    "title": "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{huang-etal-2024-chat,\n    title = \"Chat Vector: A Simple Approach to Equip {LLM}s with Instruction Following and Model Alignment in New Languages\",\n    author = \"Huang, Shih-Cheng  and\n      Li, Pin-Zu  and\n      Hsu, Yu-chi  and\n      Chen, Kuang-Ming  and\n      Lin, Yu Tung  and\n      Hsiao, Shih-Kai  and\n      Tsai, Richard  and\n      Lee, Hung-yi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.590\",\n    doi = \"10.18653/v1/2024.acl-long.590\",\n    pages = \"10943--10959\",\n    abstract = \"Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of $\\textit{chat vector}$ to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model{'}s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector{'}s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.\",\n}\n",
    "authors": [
        "Shih-Cheng Huang",
        "Pin-Zu Li",
        "Yu-chi Hsu",
        "Kuang-Ming Chen",
        "Yu Tung Lin",
        "Shih-Kai Hsiao",
        "Richard Tsai",
        "Hung-yi Lee"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.590.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/eb704917-79d5-5264-a71b-43987305bffb.pdf",
    "abstract": "Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of chat vector to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model’s weights, we can endow the model with chat capabilities in new languages without the need for further training.Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector’s simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models. Our code is available at https://github.com/aqweteddy/ChatVector.",
    "num_pages": 17
}