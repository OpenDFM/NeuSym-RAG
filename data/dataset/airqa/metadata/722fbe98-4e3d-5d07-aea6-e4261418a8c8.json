{
    "uuid": "722fbe98-4e3d-5d07-aea6-e4261418a8c8",
    "title": "Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{song-etal-2024-trial,\n    title = \"Trial and Error: Exploration-Based Trajectory Optimization of {LLM} Agents\",\n    author = \"Song, Yifan  and\n      Yin, Da  and\n      Yue, Xiang  and\n      Huang, Jie  and\n      Li, Sujian  and\n      Lin, Bill Yuchen\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.409\",\n    doi = \"10.18653/v1/2024.acl-long.409\",\n    pages = \"7584--7600\",\n    abstract = \"Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.\",\n}\n",
    "authors": [
        "Yifan Song",
        "Da Yin",
        "Xiang Yue",
        "Jie Huang",
        "Sujian Li",
        "Bill Yuchen Lin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.409.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/722fbe98-4e3d-5d07-aea6-e4261418a8c8.pdf",
    "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.",
    "num_pages": 17
}