{
    "uuid": "d3e45f18-8921-5ba3-92de-2ae9ff2062a9",
    "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yong-etal-2023-bloom,\n    title = \"{BLOOM}+1: Adding Language Support to {BLOOM} for Zero-Shot Prompting\",\n    author = \"Yong, Zheng Xin  and\n      Schoelkopf, Hailey  and\n      Muennighoff, Niklas  and\n      Aji, Alham Fikri  and\n      Adelani, David Ifeoluwa  and\n      Almubarak, Khalid  and\n      Bari, M Saiful  and\n      Sutawika, Lintang  and\n      Kasai, Jungo  and\n      Baruwa, Ahmed  and\n      Winata, Genta  and\n      Biderman, Stella  and\n      Raff, Edward  and\n      Radev, Dragomir  and\n      Nikoulina, Vassilina\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.653\",\n    doi = \"10.18653/v1/2023.acl-long.653\",\n    pages = \"11682--11703\",\n    abstract = \"The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at \\url{https://github.com/bigscience-workshop/multilingual-modeling}.\",\n}\n",
    "authors": [
        "Zheng Xin Yong",
        "Hailey Schoelkopf",
        "Niklas Muennighoff",
        "Alham Fikri Aji",
        "David Ifeoluwa Adelani",
        "Khalid Almubarak",
        "M Saiful Bari",
        "Lintang Sutawika",
        "Jungo Kasai",
        "Ahmed Baruwa",
        "Genta Winata",
        "Stella Biderman",
        "Edward Raff",
        "Dragomir Radev",
        "Vassilina Nikoulina"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.653.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d3e45f18-8921-5ba3-92de-2ae9ff2062a9.pdf",
    "abstract": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling.",
    "num_pages": 22
}