{
    "uuid": "b9e11e38-fff0-5cab-963c-2b45de74092d",
    "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{soldaini-etal-2024-dolma,\n    title = \"Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research\",\n    author = \"Soldaini, Luca  and\n      Kinney, Rodney  and\n      Bhagia, Akshita  and\n      Schwenk, Dustin  and\n      Atkinson, David  and\n      Authur, Russell  and\n      Bogin, Ben  and\n      Chandu, Khyathi  and\n      Dumas, Jennifer  and\n      Elazar, Yanai  and\n      Hofmann, Valentin  and\n      Jha, Ananya  and\n      Kumar, Sachin  and\n      Lucy, Li  and\n      Lyu, Xinxi  and\n      Lambert, Nathan  and\n      Magnusson, Ian  and\n      Morrison, Jacob  and\n      Muennighoff, Niklas  and\n      Naik, Aakanksha  and\n      Nam, Crystal  and\n      Peters, Matthew  and\n      Ravichander, Abhilasha  and\n      Richardson, Kyle  and\n      Shen, Zejiang  and\n      Strubell, Emma  and\n      Subramani, Nishant  and\n      Tafjord, Oyvind  and\n      Walsh, Evan  and\n      Zettlemoyer, Luke  and\n      Smith, Noah  and\n      Hajishirzi, Hannaneh  and\n      Beltagy, Iz  and\n      Groeneveld, Dirk  and\n      Dodge, Jesse  and\n      Lo, Kyle\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.840\",\n    doi = \"10.18653/v1/2024.acl-long.840\",\n    pages = \"15725--15788\",\n    abstract = \"Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.\",\n}\n",
    "authors": [
        "Luca Soldaini",
        "Rodney Kinney",
        "Akshita Bhagia",
        "Dustin Schwenk",
        "David Atkinson",
        "Russell Authur",
        "Ben Bogin",
        "Khyathi Chandu",
        "Jennifer Dumas",
        "Yanai Elazar",
        "Valentin Hofmann",
        "Ananya Jha",
        "Sachin Kumar",
        "Li Lucy",
        "Xinxi Lyu",
        "Nathan Lambert",
        "Ian Magnusson",
        "Jacob Morrison",
        "Niklas Muennighoff",
        "Aakanksha Naik",
        "Crystal Nam",
        "Matthew Peters",
        "Abhilasha Ravichander",
        "Kyle Richardson",
        "Zejiang Shen",
        "Emma Strubell",
        "Nishant Subramani",
        "Oyvind Tafjord",
        "Evan Walsh",
        "Luke Zettlemoyer",
        "Noah Smith",
        "Hannaneh Hajishirzi",
        "Iz Beltagy",
        "Dirk Groeneveld",
        "Jesse Dodge",
        "Kyle Lo"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.840.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b9e11e38-fff0-5cab-963c-2b45de74092d.pdf",
    "abstract": "Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.",
    "num_pages": 64
}