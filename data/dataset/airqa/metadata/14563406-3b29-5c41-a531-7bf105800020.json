{
    "uuid": "14563406-3b29-5c41-a531-7bf105800020",
    "title": "A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)",
    "bibtex": "@inproceedings{lee-etal-2023-side,\n    title = \"A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification\",\n    author = \"Lee, Bruce W.  and\n      Yang, Bongseok  and\n      Lee, Jason\",\n    editor = \"Strube, Michael  and\n      Braud, Chloe  and\n      Hardmeier, Christian  and\n      Li, Junyi Jessy  and\n      Loaiciga, Sharid  and\n      Zeldes, Amir\",\n    booktitle = \"Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.codi-1.2\",\n    doi = \"10.18653/v1/2023.codi-1.2\",\n    pages = \"16--23\",\n    abstract = \"Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of 7 pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best-performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance. Our code is publicly released.\",\n}\n",
    "authors": [
        "Bruce W. Lee",
        "Bongseok Yang",
        "Jason Lee"
    ],
    "pdf_url": "https://aclanthology.org/2023.codi-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/14563406-3b29-5c41-a531-7bf105800020.pdf",
    "abstract": "Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of 7 pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best-performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance. Our code is publicly released.",
    "num_pages": 8
}