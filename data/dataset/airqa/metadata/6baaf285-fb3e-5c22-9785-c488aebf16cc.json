{
    "uuid": "6baaf285-fb3e-5c22-9785-c488aebf16cc",
    "title": "Proving membership in LLM pretraining data via data watermarks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wei-etal-2024-proving,\n    title = \"Proving membership in {LLM} pretraining data via data watermarks\",\n    author = \"Wei, Johnny  and\n      Wang, Ryan  and\n      Jia, Robin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.788\",\n    doi = \"10.18653/v1/2024.findings-acl.788\",\n    pages = \"13306--13320\",\n    abstract = \"Detecting whether copyright holders{'} works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design - watermark length, number of duplications, and interference - affect the power of the hypothesis test. Next, we study how a watermark{'}s detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B{'}s training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.\",\n}\n",
    "authors": [
        "Johnny Wei",
        "Ryan Wang",
        "Robin Jia"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.788.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6baaf285-fb3e-5c22-9785-c488aebf16cc.pdf",
    "abstract": "Detecting whether copyright holders’ works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design - watermark length, number of duplications, and interference - affect the power of the hypothesis test. Next, we study how a watermark’s detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B’s training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.",
    "num_pages": 15
}