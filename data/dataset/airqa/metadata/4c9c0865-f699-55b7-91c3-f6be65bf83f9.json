{
    "uuid": "4c9c0865-f699-55b7-91c3-f6be65bf83f9",
    "title": "NLP_DI at NADI 2024 shared task: Multi-label Arabic Dialect Classifications with an Unsupervised Cross-Encoder",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{kanjirangat-etal-2024-nlp,\n    title = \"{NLP}{\\_}{DI} at {NADI} 2024 shared task: Multi-label {A}rabic Dialect Classifications with an Unsupervised Cross-Encoder\",\n    author = \"Kanjirangat, Vani  and\n      Samardzic, Tanja  and\n      Dolamic, Ljiljana  and\n      Rinaldi, Fabio\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.82\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.82\",\n    pages = \"742--747\",\n    abstract = \"We report the approaches submitted to the NADI 2024 Subtask 1: Multi-label country-level Dialect Identification (MLDID). The core part was to adapt the information from multi-class data for a multi-label dialect classification task. We experimented with supervised and unsupervised strategies to tackle the task in this challenging setting. Under the supervised setup, we used the model trained using NADI 2023 data and devised approaches to convert the multi-class predictions to multi-label by using information from the confusion matrix or using calibrated probabilities. Under unsupervised settings, we used the Arabic-based sentence encoders and multilingual cross-encoders to retrieve similar samples from the training set, considering each test input as a query. The associated labels are then assigned to the input query. We also tried different variations, such as co-occurring dialects derived from the provided development set. We obtained the best validation performance of 48.5{\\%} F-score using one of the variations with an unsupervised approach and the same approach yielded the best test result of 43.27{\\%} (Ranked 2).\",\n}\n",
    "authors": [
        "Vani Kanjirangat",
        "Tanja Samardzic",
        "Ljiljana Dolamic",
        "Fabio Rinaldi"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.82.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4c9c0865-f699-55b7-91c3-f6be65bf83f9.pdf",
    "abstract": "We report the approaches submitted to the NADI 2024 Subtask 1: Multi-label country-level Dialect Identification (MLDID). The core part was to adapt the information from multi-class data for a multi-label dialect classification task. We experimented with supervised and unsupervised strategies to tackle the task in this challenging setting. Under the supervised setup, we used the model trained using NADI 2023 data and devised approaches to convert the multi-class predictions to multi-label by using information from the confusion matrix or using calibrated probabilities. Under unsupervised settings, we used the Arabic-based sentence encoders and multilingual cross-encoders to retrieve similar samples from the training set, considering each test input as a query. The associated labels are then assigned to the input query. We also tried different variations, such as co-occurring dialects derived from the provided development set. We obtained the best validation performance of 48.5% F-score using one of the variations with an unsupervised approach and the same approach yielded the best test result of 43.27% (Ranked 2).",
    "num_pages": 6
}