{
    "uuid": "4c9649d2-ada5-5824-9c7a-e4f112e63ebe",
    "title": "SIMSUM: Document-level Text Simplification via Simultaneous Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{blinova-etal-2023-simsum,\n    title = \"{SIMSUM}: Document-level Text Simplification via Simultaneous Summarization\",\n    author = \"Blinova, Sofia  and\n      Zhou, Xinyu  and\n      Jaggi, Martin  and\n      Eickhoff, Carsten  and\n      Bahrainian, Seyed Ali\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.552\",\n    doi = \"10.18653/v1/2023.acl-long.552\",\n    pages = \"9927--9944\",\n    abstract = \"Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Our model is designed with explicit summarization and simplification models and guides the generation using the main keywords of a source text. In order to evaluate our new model, we use two existing benchmark datasets for simplification, namely D-Wikipedia and Wiki-Doc. We compare our model{'}s performance with state of the art and show that SIMSUM achieves top results on the D-Wikipedia dataset SARI (+1.20), D-SARI (+1.64), and FKGL (-0.35) scores, improving over the best baseline models. In order to evaluate the quality of the generated text, we analyze the outputs from different models qualitatively and demonstrate the merit of our new model. Our code and datasets are available.\",\n}\n",
    "authors": [
        "Sofia Blinova",
        "Xinyu Zhou",
        "Martin Jaggi",
        "Carsten Eickhoff",
        "Seyed Ali Bahrainian"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.552.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4c9649d2-ada5-5824-9c7a-e4f112e63ebe.pdf",
    "abstract": "Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Our model is designed with explicit summarization and simplification models and guides the generation using the main keywords of a source text. In order to evaluate our new model, we use two existing benchmark datasets for simplification, namely D-Wikipedia and Wiki-Doc. We compare our modelâ€™s performance with state of the art and show that SIMSUM achieves top results on the D-Wikipedia dataset SARI (+1.20), D-SARI (+1.64), and FKGL (-0.35) scores, improving over the best baseline models. In order to evaluate the quality of the generated text, we analyze the outputs from different models qualitatively and demonstrate the merit of our new model. Our code and datasets are available.",
    "num_pages": 18
}