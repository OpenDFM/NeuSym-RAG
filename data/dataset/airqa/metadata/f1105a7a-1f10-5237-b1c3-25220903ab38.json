{
    "uuid": "f1105a7a-1f10-5237-b1c3-25220903ab38",
    "title": "Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{liang-etal-2024-debatrix,\n    title = \"Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on {LLM}\",\n    author = \"Liang, Jingcong  and\n      Ye, Rong  and\n      Han, Meng  and\n      Lai, Ruofei  and\n      Zhang, Xinyu  and\n      Huang, Xuanjing  and\n      Wei, Zhongyu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.868\",\n    doi = \"10.18653/v1/2024.findings-acl.868\",\n    pages = \"14575--14595\",\n    abstract = \"How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments.At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate.In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration.To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system{'}s performance to actual debate outcomes.The findings indicate a notable enhancement over directly using LLMs for debate evaluation.Source code and benchmark data are available at https://github.com/ljcleo/debatrix.\",\n}\n",
    "authors": [
        "Jingcong Liang",
        "Rong Ye",
        "Meng Han",
        "Ruofei Lai",
        "Xinyu Zhang",
        "Xuanjing Huang",
        "Zhongyu Wei"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.868.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f1105a7a-1f10-5237-b1c3-25220903ab38.pdf",
    "abstract": "How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments.At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate.In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration.To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our systemâ€™s performance to actual debate outcomes.The findings indicate a notable enhancement over directly using LLMs for debate evaluation.Source code and benchmark data are available at https://github.com/ljcleo/debatrix.",
    "num_pages": 21
}