{
    "uuid": "daa82de6-fc77-54cf-b1b3-5774fb169916",
    "title": "Dodo: Dynamic Contextual Compression for Decoder-only LMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{qin-etal-2024-dodo,\n    title = \"Dodo: Dynamic Contextual Compression for Decoder-only {LM}s\",\n    author = \"Qin, Guanghui  and\n      Rosset, Corby  and\n      Chau, Ethan  and\n      Rao, Nikhil  and\n      Van Durme, Benjamin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.536\",\n    doi = \"10.18653/v1/2024.acl-long.536\",\n    pages = \"9961--9975\",\n    abstract = \"Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98{\\%} for reconstruction, achieving nearly lossless encoding.\",\n}\n",
    "authors": [
        "Guanghui Qin",
        "Corby Rosset",
        "Ethan Chau",
        "Nikhil Rao",
        "Benjamin Van Durme"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.536.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/daa82de6-fc77-54cf-b1b3-5774fb169916.pdf",
    "abstract": "Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.",
    "num_pages": 15
}