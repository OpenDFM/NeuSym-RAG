{
    "uuid": "f945a29d-d226-554d-b81c-902277b07e97",
    "title": "Feature-Adaptive and Data-Scalable In-Context Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-feature-adaptive,\n    title = \"Feature-Adaptive and Data-Scalable In-Context Learning\",\n    author = \"Li, Jiahao  and\n      Wang, Quan  and\n      Zhang, Licheng  and\n      Jin, Guoqing  and\n      Mao, Zhendong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.81\",\n    doi = \"10.18653/v1/2024.acl-long.81\",\n    pages = \"1481--1494\",\n    abstract = \"In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4{\\textasciitilde}128 shots) and LLM scale (0.8{\\textasciitilde}70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve \\textbf{+14.3} average accuracy from feature adaptation over vanilla ICL on 10 datasets, with \\textbf{+6.2} average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data.\",\n}\n",
    "authors": [
        "Jiahao Li",
        "Quan Wang",
        "Licheng Zhang",
        "Guoqing Jin",
        "Zhendong Mao"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.81.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f945a29d-d226-554d-b81c-902277b07e97.pdf",
    "abstract": "In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4~128 shots) and LLM scale (0.8~70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve +14.3 average accuracy from feature adaptation over vanilla ICL on 10 datasets, with +6.2 average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data.",
    "num_pages": 14
}