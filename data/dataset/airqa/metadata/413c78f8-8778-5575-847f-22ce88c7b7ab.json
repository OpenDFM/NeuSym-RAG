{
    "uuid": "413c78f8-8778-5575-847f-22ce88c7b7ab",
    "title": "Joint Generator-Ranker Learning for Natural Language Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{shen-etal-2023-joint,\n    title = \"Joint Generator-Ranker Learning for Natural Language Generation\",\n    author = \"Shen, Weizhou  and\n      Gong, Yeyun  and\n      Shen, Yelong  and\n      Wang, Song  and\n      Quan, Xiaojun  and\n      Duan, Nan  and\n      Chen, Weizhu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.486\",\n    doi = \"10.18653/v1/2023.findings-acl.486\",\n    pages = \"7681--7699\",\n    abstract = \"Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are publicly available at \\url{https://github.com/microsoft/ProphetNet/tree/master/JGR}.\",\n}\n",
    "authors": [
        "Weizhou Shen",
        "Yeyun Gong",
        "Yelong Shen",
        "Song Wang",
        "Xiaojun Quan",
        "Nan Duan",
        "Weizhu Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.486.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/413c78f8-8778-5575-847f-22ce88c7b7ab.pdf",
    "abstract": "Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are publicly available at https://github.com/microsoft/ProphetNet/tree/master/JGR.",
    "num_pages": 19
}