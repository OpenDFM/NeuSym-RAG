{
    "uuid": "095bc186-5661-5083-a735-d1fe3f2650cc",
    "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{michael-etal-2023-nlp,\n    title = \"What Do {NLP} Researchers Believe? Results of the {NLP} Community Metasurvey\",\n    author = \"Michael, Julian  and\n      Holtzman, Ari  and\n      Parrish, Alicia  and\n      Mueller, Aaron  and\n      Wang, Alex  and\n      Chen, Angelica  and\n      Madaan, Divyam  and\n      Nangia, Nikita  and\n      Pang, Richard Yuanzhe  and\n      Phang, Jason  and\n      Bowman, Samuel R.\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.903\",\n    doi = \"10.18653/v1/2023.acl-long.903\",\n    pages = \"16334--16368\",\n    abstract = \"We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community{'}s predictions don{'}t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.\",\n}\n",
    "authors": [
        "Julian Michael",
        "Ari Holtzman",
        "Alicia Parrish",
        "Aaron Mueller",
        "Alex Wang",
        "Angelica Chen",
        "Divyam Madaan",
        "Nikita Nangia",
        "Richard Yuanzhe Pang",
        "Jason Phang",
        "Samuel R. Bowman"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.903.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/095bc186-5661-5083-a735-d1fe3f2650cc.pdf",
    "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",
    "num_pages": 35
}