{
    "uuid": "6967cbd6-e465-5a7a-9d48-e49754694163",
    "title": "KaPQA: Knowledge-Augmented Product Question-Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP",
    "bibtex": "@inproceedings{eppalapally-etal-2024-kapqa,\n    title = \"{K}a{PQA}: Knowledge-Augmented Product Question-Answering\",\n    author = \"Eppalapally, Swetha  and\n      Dangi, Daksh  and\n      Bhat, Chaithra  and\n      Gupta, Ankita  and\n      Zhang, Ruiyi  and\n      Agarwal, Shubham  and\n      Bagga, Karishma  and\n      Yoon, Seunghyun  and\n      Lipka, Nedim  and\n      Rossi, Ryan  and\n      Dernoncourt, Franck\",\n    editor = \"Yu, Wenhao  and\n      Shi, Weijia  and\n      Yasunaga, Michihiro  and\n      Jiang, Meng  and\n      Zhu, Chenguang  and\n      Hajishirzi, Hannaneh  and\n      Zettlemoyer, Luke  and\n      Zhang, Zhihan\",\n    booktitle = \"Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.knowledgenlp-1.2\",\n    doi = \"10.18653/v1/2024.knowledgenlp-1.2\",\n    pages = \"15--29\",\n    abstract = \"Question-answering for domain-specific applications has recently attracted much interest due to the latest advancements in large language models (LLMs). However, accurately assessing the performance of these applications remains a challenge, mainly due to the lack of suitable benchmarks that effectively simulate real-world scenarios. To address this challenge, we introduce two product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop products to help evaluate the performance of existing models on domain-specific product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA framework to enhance the performance of the models in the product QA task. Our experiments demonstrated that inducing domain knowledge through query reformulation allowed for increased retrieval and generative performance when compared to standard RAG-QA methods. This improvement, however, is slight, and thus illustrates the challenge posed by the datasets introduced.\",\n}\n",
    "authors": [
        "Swetha Eppalapally",
        "Daksh Dangi",
        "Chaithra Bhat",
        "Ankita Gupta",
        "Ruiyi Zhang",
        "Shubham Agarwal",
        "Karishma Bagga",
        "Seunghyun Yoon",
        "Nedim Lipka",
        "Ryan Rossi",
        "Franck Dernoncourt"
    ],
    "pdf_url": "https://aclanthology.org/2024.knowledgenlp-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6967cbd6-e465-5a7a-9d48-e49754694163.pdf",
    "abstract": "Question-answering for domain-specific applications has recently attracted much interest due to the latest advancements in large language models (LLMs). However, accurately assessing the performance of these applications remains a challenge, mainly due to the lack of suitable benchmarks that effectively simulate real-world scenarios. To address this challenge, we introduce two product question-answering (QA) datasets focused on Adobe Acrobat and Photoshop products to help evaluate the performance of existing models on domain-specific product QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA framework to enhance the performance of the models in the product QA task. Our experiments demonstrated that inducing domain knowledge through query reformulation allowed for increased retrieval and generative performance when compared to standard RAG-QA methods. This improvement, however, is slight, and thus illustrates the challenge posed by the datasets introduced.",
    "num_pages": 15
}