{
    "uuid": "3e2d4845-0dd6-50f0-9f7e-7bc120140c3f",
    "title": "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{gaido-etal-2024-speech,\n    title = \"Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?\",\n    author = \"Gaido, Marco  and\n      Papi, Sara  and\n      Negri, Matteo  and\n      Bentivogli, Luisa\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.789\",\n    doi = \"10.18653/v1/2024.acl-long.789\",\n    pages = \"14760--14778\",\n    abstract = \"The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.\",\n}\n",
    "authors": [
        "Marco Gaido",
        "Sara Papi",
        "Matteo Negri",
        "Luisa Bentivogli"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.789.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3e2d4845-0dd6-50f0-9f7e-7bc120140c3f.pdf",
    "abstract": "The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.",
    "num_pages": 19
}