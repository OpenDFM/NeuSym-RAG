{
    "uuid": "eed48331-03ed-52de-8f87-c71da234697c",
    "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{dou-etal-2024-loramoe,\n    title = \"{L}o{RAM}o{E}: Alleviating World Knowledge Forgetting in Large Language Models via {M}o{E}-Style Plugin\",\n    author = \"Dou, Shihan  and\n      Zhou, Enyu  and\n      Liu, Yan  and\n      Gao, Songyang  and\n      Shen, Wei  and\n      Xiong, Limao  and\n      Zhou, Yuhao  and\n      Wang, Xiao  and\n      Xi, Zhiheng  and\n      Fan, Xiaoran  and\n      Pu, Shiliang  and\n      Zhu, Jiang  and\n      Zheng, Rui  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.106\",\n    doi = \"10.18653/v1/2024.acl-long.106\",\n    pages = \"1932--1945\",\n    abstract = \"Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/Ablustrund/LoRAMoE.\",\n}\n",
    "authors": [
        "Shihan Dou",
        "Enyu Zhou",
        "Yan Liu",
        "Songyang Gao",
        "Wei Shen",
        "Limao Xiong",
        "Yuhao Zhou",
        "Xiao Wang",
        "Zhiheng Xi",
        "Xiaoran Fan",
        "Shiliang Pu",
        "Jiang Zhu",
        "Rui Zheng",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.106.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/eed48331-03ed-52de-8f87-c71da234697c.pdf",
    "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/Ablustrund/LoRAMoE.",
    "num_pages": 14
}