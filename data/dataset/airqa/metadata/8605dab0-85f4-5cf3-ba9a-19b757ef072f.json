{
    "uuid": "8605dab0-85f4-5cf3-ba9a-19b757ef072f",
    "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{uppaal-etal-2023-fine,\n    title = \"Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection\",\n    author = \"Uppaal, Rheeya  and\n      Hu, Junjie  and\n      Li, Yixuan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.717\",\n    doi = \"10.18653/v1/2023.acl-long.717\",\n    pages = \"12813--12832\",\n    abstract = \"Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0{\\%} FPR95 in many cases), strongly outperforming the fine-tuned counterpart.\",\n}\n",
    "authors": [
        "Rheeya Uppaal",
        "Junjie Hu",
        "Yixuan Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.717.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8605dab0-85f4-5cf3-ba9a-19b757ef072f.pdf",
    "abstract": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart.",
    "num_pages": 20
}