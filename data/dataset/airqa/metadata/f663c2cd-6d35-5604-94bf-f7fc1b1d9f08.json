{
    "uuid": "f663c2cd-6d35-5604-94bf-f7fc1b1d9f08",
    "title": "Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hono-etal-2024-integrating,\n    title = \"Integrating Pre-Trained Speech and Language Models for End-to-End Speech Recognition\",\n    author = \"Hono, Yukiya  and\n      Mitsuda, Koh  and\n      Zhao, Tianyu  and\n      Mitsui, Kentaro  and\n      Wakatsuki, Toshiaki  and\n      Sawada, Kei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.787\",\n    doi = \"10.18653/v1/2024.findings-acl.787\",\n    pages = \"13289--13305\",\n    abstract = \"Advances in machine learning have made it possible to perform various text and speech processing tasks, such as automatic speech recognition (ASR), in an end-to-end (E2E) manner. E2E approaches utilizing pre-trained models are gaining attention for conserving training data and resources. However, most of their applications in ASR involve only one of either a pre-trained speech or a language model. This paper proposes integrating a pre-trained speech representation model and a large language model (LLM) for E2E ASR. The proposed model enables the optimization of the entire ASR process, including acoustic feature extraction and acoustic and language modeling, by combining pre-trained models with a bridge network and also enables the application of remarkable developments in LLM utilization, such as parameter-efficient domain adaptation and inference optimization. Experimental results demonstrate that the proposed model achieves a performance comparable to that of modern E2E ASR models by utilizing powerful pre-training models with the proposed integrated approach.\",\n}\n",
    "authors": [
        "Yukiya Hono",
        "Koh Mitsuda",
        "Tianyu Zhao",
        "Kentaro Mitsui",
        "Toshiaki Wakatsuki",
        "Kei Sawada"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.787.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f663c2cd-6d35-5604-94bf-f7fc1b1d9f08.pdf",
    "abstract": "Advances in machine learning have made it possible to perform various text and speech processing tasks, such as automatic speech recognition (ASR), in an end-to-end (E2E) manner. E2E approaches utilizing pre-trained models are gaining attention for conserving training data and resources. However, most of their applications in ASR involve only one of either a pre-trained speech or a language model. This paper proposes integrating a pre-trained speech representation model and a large language model (LLM) for E2E ASR. The proposed model enables the optimization of the entire ASR process, including acoustic feature extraction and acoustic and language modeling, by combining pre-trained models with a bridge network and also enables the application of remarkable developments in LLM utilization, such as parameter-efficient domain adaptation and inference optimization. Experimental results demonstrate that the proposed model achieves a performance comparable to that of modern E2E ASR models by utilizing powerful pre-training models with the proposed integrated approach.",
    "num_pages": 17
}