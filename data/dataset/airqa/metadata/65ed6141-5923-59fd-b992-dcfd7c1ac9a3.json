{
    "uuid": "65ed6141-5923-59fd-b992-dcfd7c1ac9a3",
    "title": "Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{akyurek-etal-2024-deductive,\n    title = \"Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability\",\n    author = {Aky{\\\"u}rek, Afra Feyza  and\n      Aky{\\\"u}rek, Ekin  and\n      Choshen, Leshem  and\n      Wijaya, Derry  and\n      Andreas, Jacob},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.584\",\n    doi = \"10.18653/v1/2024.findings-acl.584\",\n    pages = \"9802--9818\",\n    abstract = \"While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQuAKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26{\\%}; on CREAK, fully unsupervised DCT improves verification accuracy by 12{\\%}. These results show that LMs{'} reasoning capabilities during inference can be leveraged during training to improve their reliability.\",\n}\n",
    "authors": [
        "Afra Feyza Akyürek",
        "Ekin Akyürek",
        "Leshem Choshen",
        "Derry Wijaya",
        "Jacob Andreas"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.584.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/65ed6141-5923-59fd-b992-dcfd7c1ac9a3.pdf",
    "abstract": "While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQuAKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK, fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs’ reasoning capabilities during inference can be leveraged during training to improve their reliability.",
    "num_pages": 17
}