{
    "uuid": "f5b72d33-3210-5234-b4cb-bd6ac92cbb7b",
    "title": "Comparing Neural Question Generation Architectures for Reading Comprehension",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{perkoff-etal-2023-comparing,\n    title = \"Comparing Neural Question Generation Architectures for Reading Comprehension\",\n    author = \"Perkoff, E. Margaret  and\n      Bhattacharyya, Abhidip  and\n      Cai, Jon  and\n      Cao, Jie\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.47\",\n    doi = \"10.18653/v1/2023.bea-1.47\",\n    pages = \"556--566\",\n    abstract = \"In recent decades, there has been a significant push to leverage technology to aid both teachers and students in the classroom. Language processing advancements have been harnessed to provide better tutoring services, automated feedback to teachers, improved peer-to-peer feedback mechanisms, and measures of student comprehension for reading. Automated question generation systems have the potential to significantly reduce teachers{'} workload in the latter. In this paper, we compare three differ- ent neural architectures for question generation across two types of reading material: narratives and textbooks. For each architecture, we explore the benefits of including question attributes in the input representation. Our models show that a T5 architecture has the best overall performance, with a RougeL score of 0.536 on a narrative corpus and 0.316 on a textbook corpus. We break down the results by attribute and discover that the attribute can improve the quality of some types of generated questions, including Action and Character, but this is not true for all models.\",\n}\n",
    "authors": [
        "E. Margaret Perkoff",
        "Abhidip Bhattacharyya",
        "Jon Cai",
        "Jie Cao"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.47.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f5b72d33-3210-5234-b4cb-bd6ac92cbb7b.pdf",
    "abstract": "In recent decades, there has been a significant push to leverage technology to aid both teachers and students in the classroom. Language processing advancements have been harnessed to provide better tutoring services, automated feedback to teachers, improved peer-to-peer feedback mechanisms, and measures of student comprehension for reading. Automated question generation systems have the potential to significantly reduce teachersâ€™ workload in the latter. In this paper, we compare three differ- ent neural architectures for question generation across two types of reading material: narratives and textbooks. For each architecture, we explore the benefits of including question attributes in the input representation. Our models show that a T5 architecture has the best overall performance, with a RougeL score of 0.536 on a narrative corpus and 0.316 on a textbook corpus. We break down the results by attribute and discover that the attribute can improve the quality of some types of generated questions, including Action and Character, but this is not true for all models.",
    "num_pages": 11
}