{
    "uuid": "549c5f7c-b494-538e-a9ae-4a2c00dd9d2d",
    "title": "Reconstruction Probing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{kim-etal-2023-reconstruction,\n    title = \"Reconstruction Probing\",\n    author = \"Kim, Najoung  and\n      Khilnani, Jatin  and\n      Warstadt, Alex  and\n      Qaddoumi, Abdelrahim\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.523\",\n    doi = \"10.18653/v1/2023.findings-acl.523\",\n    pages = \"8240--8255\",\n    abstract = \"We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decontextualized lexical prior of the model. This comparison can be understood as quantifying the contribution of contextualization towards reconstruction{---}the difference in the reconstruction probabilities can only be attributed to the representational change of the single token induced by contextualization. We apply this analysis to three MLMs and find that contextualization boosts reconstructability of tokens that are close to the token being reconstructed in terms of linear and syntactic distance. Furthermore, we extend our analysis to finer-grained decomposition of contextualized representations, and we find that these boosts are largely attributable to static and positional embeddings at the input layer.\",\n}\n",
    "authors": [
        "Najoung Kim",
        "Jatin Khilnani",
        "Alex Warstadt",
        "Abdelrahim Qaddoumi"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.523.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/549c5f7c-b494-538e-a9ae-4a2c00dd9d2d.pdf",
    "abstract": "We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decontextualized lexical prior of the model. This comparison can be understood as quantifying the contribution of contextualization towards reconstructionâ€”the difference in the reconstruction probabilities can only be attributed to the representational change of the single token induced by contextualization. We apply this analysis to three MLMs and find that contextualization boosts reconstructability of tokens that are close to the token being reconstructed in terms of linear and syntactic distance. Furthermore, we extend our analysis to finer-grained decomposition of contextualized representations, and we find that these boosts are largely attributable to static and positional embeddings at the input layer.",
    "num_pages": 16
}