{
    "uuid": "1baaf02e-dff6-56a7-9c2f-b30921777b9d",
    "title": "Promoting Fairness in Classification of Quality of Medical Evidence",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    "bibtex": "@inproceedings{suster-etal-2023-promoting,\n    title = \"Promoting Fairness in Classification of Quality of Medical Evidence\",\n    author = \"Suster, Simon  and\n      Baldwin, Timothy  and\n      Verspoor, Karin\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.39\",\n    doi = \"10.18653/v1/2023.bionlp-1.39\",\n    pages = \"413--426\",\n    abstract = \"Automatically rating the quality of published research is a critical step in medical evidence synthesis. While several methods have been proposed, their algorithmic fairness has been overlooked even though significant risks may follow when such systems are deployed in biomedical contexts. In this work, we study fairness on two systems along two sensitive attributes, participant sex and medical area. In some cases, we find important inequalities, leading us to apply various debiasing methods. Upon examining an interplay of systems{'} predictive performance, fairness, as well as medically critical selective classification capabilities and calibration performance, we find that fairness can sometimes improve through debiasing, but at a cost in other performance measures.\",\n}\n",
    "authors": [
        "Simon Suster",
        "Timothy Baldwin",
        "Karin Verspoor"
    ],
    "pdf_url": "https://aclanthology.org/2023.bionlp-1.39.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1baaf02e-dff6-56a7-9c2f-b30921777b9d.pdf",
    "abstract": "Automatically rating the quality of published research is a critical step in medical evidence synthesis. While several methods have been proposed, their algorithmic fairness has been overlooked even though significant risks may follow when such systems are deployed in biomedical contexts. In this work, we study fairness on two systems along two sensitive attributes, participant sex and medical area. In some cases, we find important inequalities, leading us to apply various debiasing methods. Upon examining an interplay of systemsâ€™ predictive performance, fairness, as well as medically critical selective classification capabilities and calibration performance, we find that fairness can sometimes improve through debiasing, but at a cost in other performance measures.",
    "num_pages": 14
}