{
    "uuid": "7ebb9d11-544c-5dbe-bbe9-3ec264857524",
    "title": "Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{patidar-etal-2024-shot,\n    title = \"Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning\",\n    author = \"Patidar, Mayur  and\n      Sawhney, Riya  and\n      Singh, Avinash  and\n      Chatterjee, Biswajit  and\n      ., Mausam  and\n      Bhattacharya, Indrajit\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.495\",\n    doi = \"10.18653/v1/2024.acl-long.495\",\n    pages = \"9147--9165\",\n    abstract = \"Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.\",\n}\n",
    "authors": [
        "Mayur Patidar",
        "Riya Sawhney",
        "Avinash Singh",
        "Biswajit Chatterjee",
        "Mausam .",
        "Indrajit Bhattacharya"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.495.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7ebb9d11-544c-5dbe-bbe9-3ec264857524.pdf",
    "abstract": "Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.",
    "num_pages": 19
}