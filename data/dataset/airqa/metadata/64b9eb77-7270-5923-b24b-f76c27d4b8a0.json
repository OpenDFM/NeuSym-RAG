{
    "uuid": "64b9eb77-7270-5923-b24b-f76c27d4b8a0",
    "title": "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{chen-etal-2023-improving-cross,\n    title = \"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations\",\n    author = \"Chen, Jifan  and\n      Zhang, Yuhao  and\n      Liu, Lan  and\n      Dong, Rui  and\n      Chen, Xinchi  and\n      Ng, Patrick  and\n      Wang, William Yang  and\n      Huang, Zhiheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.341\",\n    doi = \"10.18653/v1/2023.findings-acl.341\",\n    pages = \"5523--5539\",\n    abstract = \"There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model{'}s ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models. We design the task configurations to explicitly specify the task type, as well as its input and output types. We show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner. We demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively.\",\n}\n",
    "authors": [
        "Jifan Chen",
        "Yuhao Zhang",
        "Lan Liu",
        "Rui Dong",
        "Xinchi Chen",
        "Patrick Ng",
        "William Yang Wang",
        "Zhiheng Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.341.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/64b9eb77-7270-5923-b24b-f76c27d4b8a0.pdf",
    "abstract": "There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the modelâ€™s ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models. We design the task configurations to explicitly specify the task type, as well as its input and output types. We show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner. We demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively.",
    "num_pages": 17
}