{
    "uuid": "5854c5c5-dfb3-5966-a3a8-77eca9328b92",
    "title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{siledar-etal-2024-one,\n    title = \"One Prompt To Rule Them All: {LLM}s for Opinion Summary Evaluation\",\n    author = \"Siledar, Tejpalsingh  and\n      Nath, Swaroop  and\n      Muddu, Sankara  and\n      Rangaraju, Rupasai  and\n      Nath, Swaprava  and\n      Bhattacharyya, Pushpak  and\n      Banerjee, Suman  and\n      Patil, Amey  and\n      Singh, Sudhanshu  and\n      Chelliah, Muthusamy  and\n      Garera, Nikesh\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.655\",\n    doi = \"10.18653/v1/2024.acl-long.655\",\n    pages = \"12119--12134\",\n    abstract = \"Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the SUMMEVAL-OP dataset, encompassing 7 dimensions crucial to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We propose OP-I-PROMPT, a dimension-independent prompt, along with OP-PROMPTS, a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that OP-I-PROMPT emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70 with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.\",\n}\n",
    "authors": [
        "Tejpalsingh Siledar",
        "Swaroop Nath",
        "Sankara Muddu",
        "Rupasai Rangaraju",
        "Swaprava Nath",
        "Pushpak Bhattacharyya",
        "Suman Banerjee",
        "Amey Patil",
        "Sudhanshu Singh",
        "Muthusamy Chelliah",
        "Nikesh Garera"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.655.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5854c5c5-dfb3-5966-a3a8-77eca9328b92.pdf",
    "abstract": "Evaluation of opinion summaries using conventional reference-based metrics often fails to provide a comprehensive assessment and exhibits limited correlation with human judgments. While Large Language Models (LLMs) have shown promise as reference-free metrics for NLG evaluation, their potential remains unexplored for opinion summary evaluation. Furthermore, the absence of sufficient opinion summary evaluation datasets hinders progress in this area. In response, we introduce the SUMMEVAL-OP dataset, encompassing 7 dimensions crucial to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We propose OP-I-PROMPT, a dimension-independent prompt, along with OP-PROMPTS, a dimension-dependent set of prompts for opinion summary evaluation. Our experiments demonstrate that OP-I-PROMPT emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation of 0.70 with human judgments, surpassing prior methodologies. Remarkably, we are the first to explore the efficacy of LLMs as evaluators, both on closed-source and open-source models, in the opinion summary evaluation domain.",
    "num_pages": 16
}