{
    "uuid": "83c2d7b3-cac1-5a33-82a5-ec1cf6accf32",
    "title": "The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-music,\n    title = \"The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models\",\n    author = \"Li, Jiajia  and\n      Yang, Lu  and\n      Tang, Mingni  and\n      Chenchong, Chenchong  and\n      Li, Zuchao  and\n      Wang, Ping  and\n      Zhao, Hai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.194\",\n    doi = \"10.18653/v1/2024.findings-acl.194\",\n    pages = \"3246--3257\",\n    abstract = \"Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs{'} capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs.ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to evaluate and analyze LLMs{'} performance in the domain of music.Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities.With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs{'} music-related abilities. The dataset is available at GitHub and HuggingFace.\",\n}\n",
    "authors": [
        "Jiajia Li",
        "Lu Yang",
        "Mingni Tang",
        "Chenchong Chenchong",
        "Zuchao Li",
        "Ping Wang",
        "Hai Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.194.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/83c2d7b3-cac1-5a33-82a5-ec1cf6accf32.pdf",
    "abstract": "Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs’ capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs.ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to evaluate and analyze LLMs’ performance in the domain of music.Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities.With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs’ music-related abilities. The dataset is available at GitHub and HuggingFace.",
    "num_pages": 12
}