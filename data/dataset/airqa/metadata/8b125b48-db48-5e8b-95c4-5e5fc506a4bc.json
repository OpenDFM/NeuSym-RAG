{
    "uuid": "8b125b48-db48-5e8b-95c4-5e5fc506a4bc",
    "title": "GroundingGPT: Language Enhanced Multi-modal Grounding Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-groundinggpt,\n    title = \"{G}rounding{GPT}: Language Enhanced Multi-modal Grounding Model\",\n    author = \"Li, Zhaowei  and\n      Xu, Qi  and\n      Zhang, Dong  and\n      Song, Hang  and\n      Cai, YiQing  and\n      Qi, Qi  and\n      Zhou, Ran  and\n      Pan, Junting  and\n      Li, Zefeng  and\n      Tu, Vu  and\n      Huang, Zhida  and\n      Wang, Tao\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.360\",\n    doi = \"10.18653/v1/2024.acl-long.360\",\n    pages = \"6657--6678\",\n    abstract = \"Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose \\textbf{GroundingGPT}, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model{'}s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model{'}s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT.\",\n}\n",
    "authors": [
        "Zhaowei Li",
        "Qi Xu",
        "Dong Zhang",
        "Hang Song",
        "YiQing Cai",
        "Qi Qi",
        "Ran Zhou",
        "Junting Pan",
        "Zefeng Li",
        "Vu Tu",
        "Zhida Huang",
        "Tao Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.360.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8b125b48-db48-5e8b-95c4-5e5fc506a4bc.pdf",
    "abstract": "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose GroundingGPT, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model’s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model’s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT.",
    "num_pages": 22
}