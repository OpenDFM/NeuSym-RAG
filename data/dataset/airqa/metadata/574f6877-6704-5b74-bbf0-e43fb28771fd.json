{
    "uuid": "574f6877-6704-5b74-bbf0-e43fb28771fd",
    "title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{pryor-etal-2023-using,\n    title = \"Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic\",\n    author = \"Pryor, Connor  and\n      Yuan, Quan  and\n      Liu, Jeremiah  and\n      Kazemi, Mehran  and\n      Ramachandran, Deepak  and\n      Bedrax-Weiss, Tania  and\n      Getoor, Lise\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.422\",\n    doi = \"10.18653/v1/2023.acl-long.422\",\n    pages = \"7631--7652\",\n    abstract = \"Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.\",\n}\n",
    "authors": [
        "Connor Pryor",
        "Quan Yuan",
        "Jeremiah Liu",
        "Mehran Kazemi",
        "Deepak Ramachandran",
        "Tania Bedrax-Weiss",
        "Lise Getoor"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.422.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/574f6877-6704-5b74-bbf0-e43fb28771fd.pdf",
    "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.",
    "num_pages": 22
}