{
    "uuid": "15baba11-9239-54a7-a2fc-accae9d907df",
    "title": "Uncovering and Categorizing Social Biases in Text-to-SQL",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-uncovering,\n    title = \"Uncovering and Categorizing Social Biases in Text-to-{SQL}\",\n    author = \"Liu, Yan  and\n      Gao, Yan  and\n      Su, Zhe  and\n      Chen, Xiaokang  and\n      Ash, Elliott  and\n      Lou, Jian-Guang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.759\",\n    doi = \"10.18653/v1/2023.acl-long.759\",\n    pages = \"13573--13584\",\n    abstract = \"Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task.\",\n}\n",
    "authors": [
        "Yan Liu",
        "Yan Gao",
        "Zhe Su",
        "Xiaokang Chen",
        "Elliott Ash",
        "Jian-Guang Lou"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.759.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/15baba11-9239-54a7-a2fc-accae9d907df.pdf",
    "abstract": "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task.",
    "num_pages": 12
}