{
    "uuid": "79daca49-1cbd-508f-8363-1cd94b9f3ef6",
    "title": "Robustness-Aware Word Embedding Improves Certified Robustness to Adversarial Word Substitutions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-robustness,\n    title = \"Robustness-Aware Word Embedding Improves Certified Robustness to Adversarial Word Substitutions\",\n    author = \"Wang, Yibin  and\n      Yang, Yichen  and\n      He, Di  and\n      He, Kun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.42\",\n    doi = \"10.18653/v1/2023.findings-acl.42\",\n    pages = \"673--687\",\n    abstract = \"Natural Language Processing (NLP) models have gained great success on clean texts, but they are known to be vulnerable to adversarial examples typically crafted by synonym substitutions. In this paper, we target to solve this problem and find that word embedding is important to the certified robustness of NLP models. Given the findings, we propose the Embedding Interval Bound Constraint (EIBC) triplet loss to train robustness-aware word embeddings for better certified robustness. We optimize the EIBC triplet loss to reduce distances between synonyms in the embedding space, which is theoretically proven to make the verification boundary tighter. Meanwhile, we enlarge distances among non-synonyms, maintaining the semantic representation of word embeddings. Our method is conceptually simple and componentized. It can be easily combined with IBP training and improves the certified robust accuracy from 76.73{\\%} to 84.78{\\%} on the IMDB dataset. Experiments demonstrate that our method outperforms various state-of-the-art certified defense baselines and generalizes well to unseen substitutions. The code is available at \\url{https://github.com/JHL-HUST/EIBC-IBP/}.\",\n}\n",
    "authors": [
        "Yibin Wang",
        "Yichen Yang",
        "Di He",
        "Kun He"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.42.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/79daca49-1cbd-508f-8363-1cd94b9f3ef6.pdf",
    "abstract": "Natural Language Processing (NLP) models have gained great success on clean texts, but they are known to be vulnerable to adversarial examples typically crafted by synonym substitutions. In this paper, we target to solve this problem and find that word embedding is important to the certified robustness of NLP models. Given the findings, we propose the Embedding Interval Bound Constraint (EIBC) triplet loss to train robustness-aware word embeddings for better certified robustness. We optimize the EIBC triplet loss to reduce distances between synonyms in the embedding space, which is theoretically proven to make the verification boundary tighter. Meanwhile, we enlarge distances among non-synonyms, maintaining the semantic representation of word embeddings. Our method is conceptually simple and componentized. It can be easily combined with IBP training and improves the certified robust accuracy from 76.73% to 84.78% on the IMDB dataset. Experiments demonstrate that our method outperforms various state-of-the-art certified defense baselines and generalizes well to unseen substitutions. The code is available at https://github.com/JHL-HUST/EIBC-IBP/.",
    "num_pages": 15
}