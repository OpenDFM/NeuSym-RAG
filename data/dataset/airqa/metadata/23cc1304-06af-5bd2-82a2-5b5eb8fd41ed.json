{
    "uuid": "23cc1304-06af-5bd2-82a2-5b5eb8fd41ed",
    "title": "Bridging the Preference Gap between Retrievers and LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ke-etal-2024-bridging,\n    title = \"Bridging the Preference Gap between Retrievers and {LLM}s\",\n    author = \"Ke, Zixuan  and\n      Kong, Weize  and\n      Li, Cheng  and\n      Zhang, Mingyang  and\n      Mei, Qiaozhu  and\n      Bendersky, Michael\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.562\",\n    doi = \"10.18653/v1/2024.acl-long.562\",\n    pages = \"10438--10451\",\n    abstract = \"Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-{''}friendly{''} information and assembling a LLM-{''}friendly{''} context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.\",\n}\n",
    "authors": [
        "Zixuan Ke",
        "Weize Kong",
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Michael Bendersky"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.562.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/23cc1304-06af-5bd2-82a2-5b5eb8fd41ed.pdf",
    "abstract": "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-”friendly” information and assembling a LLM-”friendly” context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.",
    "num_pages": 14
}