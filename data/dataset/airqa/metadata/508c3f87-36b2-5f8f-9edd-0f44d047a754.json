{
    "uuid": "508c3f87-36b2-5f8f-9edd-0f44d047a754",
    "title": "Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chen-etal-2024-improving-retrieval,\n    title = \"Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts\",\n    author = \"Chen, Zhuo  and\n      Wang, Xinyu  and\n      Jiang, Yong  and\n      Xie, Pengjun  and\n      Huang, Fei  and\n      Tu, Kewei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.458\",\n    doi = \"10.18653/v1/2024.findings-acl.458\",\n    pages = \"7683--7694\",\n    abstract = \"In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to cover longer contexts in Open-Domain Question-Answering tasks. {\\%}It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs.It leverages a small encoder and cross-attention mechanism and effectively encodes contexts. With our method, the original language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings. Our code will be released at https://github.com/Alibaba-NLP/Vec-RA-ODQA.\",\n}\n",
    "authors": [
        "Zhuo Chen",
        "Xinyu Wang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Kewei Tu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.458.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/508c3f87-36b2-5f8f-9edd-0f44d047a754.pdf",
    "abstract": "In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to cover longer contexts in Open-Domain Question-Answering tasks. %It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs.It leverages a small encoder and cross-attention mechanism and effectively encodes contexts. With our method, the original language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings. Our code will be released at https://github.com/Alibaba-NLP/Vec-RA-ODQA.",
    "num_pages": 12
}