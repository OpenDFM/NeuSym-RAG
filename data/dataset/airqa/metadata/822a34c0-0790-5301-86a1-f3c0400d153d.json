{
    "uuid": "822a34c0-0790-5301-86a1-f3c0400d153d",
    "title": "Unlocking the Power of Large Language Models for Entity Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jiang-etal-2024-unlocking,\n    title = \"Unlocking the Power of Large Language Models for Entity Alignment\",\n    author = \"Jiang, Xuhui  and\n      Shen, Yinghan  and\n      Shi, Zhichao  and\n      Xu, Chengjin  and\n      Li, Wei  and\n      Li, Zixuan  and\n      Guo, Jian  and\n      Shen, Huawei  and\n      Wang, Yuanzhuo\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.408\",\n    doi = \"10.18653/v1/2024.acl-long.408\",\n    pages = \"7566--7583\",\n    abstract = \"Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs{'} capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA{'}s superior performance, highlighting LLMs{'} potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/.\",\n}\n",
    "authors": [
        "Xuhui Jiang",
        "Yinghan Shen",
        "Zhichao Shi",
        "Chengjin Xu",
        "Wei Li",
        "Zixuan Li",
        "Jian Guo",
        "Huawei Shen",
        "Yuanzhuo Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.408.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/822a34c0-0790-5301-86a1-f3c0400d153d.pdf",
    "abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs’ capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA’s superior performance, highlighting LLMs’ potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/.",
    "num_pages": 18
}