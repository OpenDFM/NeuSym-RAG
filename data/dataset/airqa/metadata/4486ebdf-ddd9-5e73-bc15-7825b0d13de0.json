{
    "uuid": "4486ebdf-ddd9-5e73-bc15-7825b0d13de0",
    "title": "Towards Zero-Shot Multilingual Transfer for Code-Switched Responses",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2023-towards,\n    title = \"Towards Zero-Shot Multilingual Transfer for Code-Switched Responses\",\n    author = \"Wu, Ting-Wei  and\n      Zhao, Changsheng  and\n      Chang, Ernie  and\n      Shi, Yangyang  and\n      Chuang, Pierce  and\n      Chandra, Vikas  and\n      Juang, Biing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.417\",\n    doi = \"10.18653/v1/2023.acl-long.417\",\n    pages = \"7551--7563\",\n    abstract = \"Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of code-switched responses. We propose a new adapter-based framework that allows for efficient transfer by learning task-specific representations and encapsulating source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness of our approach.\",\n}\n",
    "authors": [
        "Ting-Wei Wu",
        "Changsheng Zhao",
        "Ernie Chang",
        "Yangyang Shi",
        "Pierce Chuang",
        "Vikas Chandra",
        "Biing Juang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.417.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4486ebdf-ddd9-5e73-bc15-7825b0d13de0.pdf",
    "abstract": "Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of code-switched responses. We propose a new adapter-based framework that allows for efficient transfer by learning task-specific representations and encapsulating source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness of our approach.",
    "num_pages": 13
}