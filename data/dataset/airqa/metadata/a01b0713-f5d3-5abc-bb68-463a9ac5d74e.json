{
    "uuid": "a01b0713-f5d3-5abc-bb68-463a9ac5d74e",
    "title": "Exploring Defeasibility in Causal Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cui-etal-2024-exploring,\n    title = \"Exploring Defeasibility in Causal Reasoning\",\n    author = \"Cui, Shaobo  and\n      Milikic, Lazar  and\n      Feng, Yiyang  and\n      Ismayilzada, Mete  and\n      Paul, Debjit  and\n      Bosselut, Antoine  and\n      Faltings, Boi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.384\",\n    doi = \"10.18653/v1/2024.findings-acl.384\",\n    pages = \"6433--6452\",\n    abstract = \"Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present $\\delta$-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. $\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, namely, cause-effect pairs accompanied by supporters and defeaters. We further show that current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that measures causal strength based on token-level causal relationships. CESAR achieves a significant 69.7{\\%} relative improvement over existing metrics, increasing from 47.2{\\%} to 80.1{\\%} in capturing the causal strength change brought by supporters and defeaters. We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by $\\delta$-CAUSAL.\",\n}\n",
    "authors": [
        "Shaobo Cui",
        "Lazar Milikic",
        "Yiyang Feng",
        "Mete Ismayilzada",
        "Debjit Paul",
        "Antoine Bosselut",
        "Boi Faltings"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.384.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a01b0713-f5d3-5abc-bb68-463a9ac5d74e.pdf",
    "abstract": "Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present ùõø-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. ùõø-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, namely, cause-effect pairs accompanied by supporters and defeaters. We further show that current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in ùõø-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating), a metric that measures causal strength based on token-level causal relationships. CESAR achieves a significant 69.7% relative improvement over existing metrics, increasing from 47.2% to 80.1% in capturing the causal strength change brought by supporters and defeaters. We further demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and 10.7 points behind humans in generating supporters and defeaters, emphasizing the challenge posed by ùõø-CAUSAL.",
    "num_pages": 20
}