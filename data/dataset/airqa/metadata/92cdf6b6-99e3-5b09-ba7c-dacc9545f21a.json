{
    "uuid": "92cdf6b6-99e3-5b09-ba7c-dacc9545f21a",
    "title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{selvam-etal-2023-tail,\n    title = \"The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks\",\n    author = \"Selvam, Nikil  and\n      Dev, Sunipa  and\n      Khashabi, Daniel  and\n      Khot, Tushar  and\n      Chang, Kai-Wei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.118\",\n    doi = \"10.18653/v1/2023.acl-short.118\",\n    pages = \"1373--1386\",\n    abstract = \"How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias. We hope these troubling observations motivate more robust measures of social biases.\",\n}\n",
    "authors": [
        "Nikil Selvam",
        "Sunipa Dev",
        "Daniel Khashabi",
        "Tushar Khot",
        "Kai-Wei Chang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.118.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/92cdf6b6-99e3-5b09-ba7c-dacc9545f21a.pdf",
    "abstract": "How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias. We hope these troubling observations motivate more robust measures of social biases.",
    "num_pages": 14
}