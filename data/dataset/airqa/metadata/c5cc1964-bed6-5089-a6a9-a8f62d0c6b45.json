{
    "uuid": "c5cc1964-bed6-5089-a6a9-a8f62d0c6b45",
    "title": "Pragmatic Inference with a CLIP Listener for Contrastive Captioning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ou-etal-2023-pragmatic,\n    title = \"Pragmatic Inference with a {CLIP} Listener for Contrastive Captioning\",\n    author = \"Ou, Jiefu  and\n      Krojer, Benno  and\n      Fried, Daniel\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.120\",\n    doi = \"10.18653/v1/2023.findings-acl.120\",\n    pages = \"1904--1917\",\n    abstract = \"We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision-language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity {---} outperforming past methods for discriminative captioning by 11{\\%} to 15{\\%} accuracy in human evaluations.\",\n}\n",
    "authors": [
        "Jiefu Ou",
        "Benno Krojer",
        "Daniel Fried"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.120.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c5cc1964-bed6-5089-a6a9-a8f62d0c6b45.pdf",
    "abstract": "We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision-language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity â€” outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations.",
    "num_pages": 14
}