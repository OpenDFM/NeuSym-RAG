{
    "uuid": "1a81bab8-25ed-5da4-bb27-58ae70a8a8f8",
    "title": "A Study on the Efficiency and Generalization of Light Hybrid Retrievers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{luo-etal-2023-study,\n    title = \"A Study on the Efficiency and Generalization of Light Hybrid Retrievers\",\n    author = \"Luo, Man  and\n      Jain, Shashank  and\n      Gupta, Anchit  and\n      Einolghozati, Arash  and\n      Oguz, Barlas  and\n      Chatterjee, Debojeet  and\n      Chen, Xilun  and\n      Baral, Chitta  and\n      Heidari, Peyman\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.139\",\n    doi = \"10.18653/v1/2023.acl-short.139\",\n    pages = \"1617--1626\",\n    abstract = \"Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study {``}Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance{''}? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves $13\\times$ memory while maintaining 98.0{\\%} performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individual sparse and dense retrievers. Nevertheless, our analysis shows that there is a large room to improve the robustness of retrievers, suggesting a new research direction.\",\n}\n",
    "authors": [
        "Man Luo",
        "Shashank Jain",
        "Anchit Gupta",
        "Arash Einolghozati",
        "Barlas Oguz",
        "Debojeet Chatterjee",
        "Xilun Chen",
        "Chitta Baral",
        "Peyman Heidari"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.139.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1a81bab8-25ed-5da4-bb27-58ae70a8a8f8.pdf",
    "abstract": "Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study “Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance”? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves 13× memory while maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individual sparse and dense retrievers. Nevertheless, our analysis shows that there is a large room to improve the robustness of retrievers, suggesting a new research direction.",
    "num_pages": 10
}