{
    "uuid": "54ed03ed-dbcc-5cf9-97fc-a2e061345f35",
    "title": "Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    "bibtex": "@inproceedings{stewart-mihalcea-2024-whose,\n    title = \"Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation\",\n    author = \"Stewart, Ian  and\n      Mihalcea, Rada\",\n    editor = \"Fale{\\'n}ska, Agnieszka  and\n      Basta, Christine  and\n      Costa-juss{\\`a}, Marta  and\n      Goldfarb-Tarrant, Seraphina  and\n      Nozza, Debora\",\n    booktitle = \"Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.gebnlp-1.23\",\n    doi = \"10.18653/v1/2024.gebnlp-1.23\",\n    pages = \"365--375\",\n    abstract = \"Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output. While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g., {``}the lawyer kissed her wife.{''} We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g., Spanish) and comprised of popular occupation nouns. We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between entities of the same gender. The error rate varies considerably based on the context, and same-gender sentences referencing high female-representation occupations are translated with lower accuracy. We provide this work as a case study in the evaluation of intrinsic bias in NLP systems with respect to social relationships.\",\n}\n",
    "authors": [
        "Ian Stewart",
        "Rada Mihalcea"
    ],
    "pdf_url": "https://aclanthology.org/2024.gebnlp-1.23.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/54ed03ed-dbcc-5cf9-97fc-a2e061345f35.pdf",
    "abstract": "Machine translation often suffers from biased data and algorithms that can lead to unacceptable errors in system output. While bias in gender norms has been investigated, less is known about whether MT systems encode bias about social relationships, e.g., “the lawyer kissed her wife.” We investigate the degree of bias against same-gender relationships in MT systems, using generated template sentences drawn from several noun-gender languages (e.g., Spanish) and comprised of popular occupation nouns. We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between entities of the same gender. The error rate varies considerably based on the context, and same-gender sentences referencing high female-representation occupations are translated with lower accuracy. We provide this work as a case study in the evaluation of intrinsic bias in NLP systems with respect to social relationships.",
    "num_pages": 11
}