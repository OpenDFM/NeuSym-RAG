{
    "uuid": "ba92d5a9-d6a1-5d5b-99b9-13b05d30a0ee",
    "title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-plug,\n    title = \"{PLUG}: Leveraging Pivot Language in Cross-Lingual Instruction Tuning\",\n    author = \"Zhang, Zhihan  and\n      Lee, Dong-Ho  and\n      Fang, Yuwei  and\n      Yu, Wenhao  and\n      Jia, Mengzhao  and\n      Jiang, Meng  and\n      Barbieri, Francesco\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.379\",\n    doi = \"10.18653/v1/2024.acl-long.379\",\n    pages = \"7025--7046\",\n    abstract = \"Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29{\\%} on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Code and data are available at https://github.com/ytyz1307zzh/PLUG.\",\n}\n",
    "authors": [
        "Zhihan Zhang",
        "Dong-Ho Lee",
        "Yuwei Fang",
        "Wenhao Yu",
        "Mengzhao Jia",
        "Meng Jiang",
        "Francesco Barbieri"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.379.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ba92d5a9-d6a1-5d5b-99b9-13b05d30a0ee.pdf",
    "abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Code and data are available at https://github.com/ytyz1307zzh/PLUG.",
    "num_pages": 22
}