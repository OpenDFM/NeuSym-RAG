{
    "uuid": "6bff3144-3a71-5438-8ddd-bf16347a303d",
    "title": "LAVIS: A One-stop Library for Language-Vision Intelligence",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{li-etal-2023-lavis,\n    title = \"{LAVIS}: A One-stop Library for Language-Vision Intelligence\",\n    author = \"Li, Dongxu  and\n      Li, Junnan  and\n      Le, Hung  and\n      Wang, Guangsen  and\n      Savarese, Silvio  and\n      Hoi, Steven C.H.\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.3\",\n    doi = \"10.18653/v1/2023.acl-demo.3\",\n    pages = \"31--41\",\n    abstract = \"We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.\",\n}\n",
    "authors": [
        "Dongxu Li",
        "Junnan Li",
        "Hung Le",
        "Guangsen Wang",
        "Silvio Savarese",
        "Steven C.H. Hoi"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6bff3144-3a71-5438-8ddd-bf16347a303d.pdf",
    "abstract": "We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.",
    "num_pages": 11
}