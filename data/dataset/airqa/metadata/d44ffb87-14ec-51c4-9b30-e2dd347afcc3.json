{
    "uuid": "d44ffb87-14ec-51c4-9b30-e2dd347afcc3",
    "title": "Dual Prompt Tuning based Contrastive Learning for Hierarchical Text Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{xiong-etal-2024-dual,\n    title = \"Dual Prompt Tuning based Contrastive Learning for Hierarchical Text Classification\",\n    author = \"Xiong, Sishi  and\n      Zhao, Yu  and\n      Zhang, Jie  and\n      Mengxiang, Li  and\n      He, Zhongjiang  and\n      Li, Xuelong  and\n      Song, Shuangyong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.723\",\n    doi = \"10.18653/v1/2024.findings-acl.723\",\n    pages = \"12146--12158\",\n    abstract = \"Hierarchical text classification aims at categorizing texts into a multi-tiered tree-structured hierarchy of labels. Existing methods pay more attention to capture hierarchy-aware text feature by exploiting explicit parent-child relationships, while interactions between peer labels are rarely taken into account, resulting in severe label confusion within each layer. In this work, we propose a novel Dual Prompt Tuning (DPT) method, which emphasizes identifying discrimination among peer labels by performing contrastive learning on each hierarchical layer. We design an innovative hand-crafted prompt containing slots for both positive and negative label predictions to cooperate with contrastive learning. In addition, we introduce a label hierarchy self-sensing auxiliary task to ensure cross-layer label consistency. Extensive experiments demonstrate that DPT achieves significant improvements and outperforms the current state-of-the-art methods on BGC and RCV1-V2 benchmark datasets.\",\n}\n",
    "authors": [
        "Sishi Xiong",
        "Yu Zhao",
        "Jie Zhang",
        "Li Mengxiang",
        "Zhongjiang He",
        "Xuelong Li",
        "Shuangyong Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.723.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d44ffb87-14ec-51c4-9b30-e2dd347afcc3.pdf",
    "abstract": "Hierarchical text classification aims at categorizing texts into a multi-tiered tree-structured hierarchy of labels. Existing methods pay more attention to capture hierarchy-aware text feature by exploiting explicit parent-child relationships, while interactions between peer labels are rarely taken into account, resulting in severe label confusion within each layer. In this work, we propose a novel Dual Prompt Tuning (DPT) method, which emphasizes identifying discrimination among peer labels by performing contrastive learning on each hierarchical layer. We design an innovative hand-crafted prompt containing slots for both positive and negative label predictions to cooperate with contrastive learning. In addition, we introduce a label hierarchy self-sensing auxiliary task to ensure cross-layer label consistency. Extensive experiments demonstrate that DPT achieves significant improvements and outperforms the current state-of-the-art methods on BGC and RCV1-V2 benchmark datasets.",
    "num_pages": 13
}