{
    "uuid": "05e69f51-7311-57c4-b177-8e0b80b3bb29",
    "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cheng-etal-2023-improving,\n    title = \"Improving Contrastive Learning of Sentence Embeddings from {AI} Feedback\",\n    author = \"Cheng, Qinyuan  and\n      Yang, Xiaogui  and\n      Sun, Tianxiang  and\n      Li, Linyang  and\n      Qiu, Xipeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.707\",\n    doi = \"10.18653/v1/2023.findings-acl.707\",\n    pages = \"11122--11138\",\n    abstract = \"Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods.\",\n}\n",
    "authors": [
        "Qinyuan Cheng",
        "Xiaogui Yang",
        "Tianxiang Sun",
        "Linyang Li",
        "Xipeng Qiu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.707.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/05e69f51-7311-57c4-b177-8e0b80b3bb29.pdf",
    "abstract": "Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods.",
    "num_pages": 17
}