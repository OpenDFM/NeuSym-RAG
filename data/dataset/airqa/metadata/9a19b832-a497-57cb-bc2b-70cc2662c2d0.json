{
    "uuid": "9a19b832-a497-57cb-bc2b-70cc2662c2d0",
    "title": "Improving Named Entity Recognition via Bridge-based Domain Adaptation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{xu-etal-2023-improving,\n    title = \"Improving Named Entity Recognition via Bridge-based Domain Adaptation\",\n    author = \"Xu, Jingyun  and\n      Zheng, Changmeng  and\n      Cai, Yi  and\n      Chua, Tat-Seng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.238\",\n    doi = \"10.18653/v1/2023.findings-acl.238\",\n    pages = \"3869--3882\",\n    abstract = \"Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification. To this end, we attempt to utilize contrastive learning to refine the original representations and propose a model-agnostic framework named MoCL for cross-domain NER. Additionally, we respectively combine MoCL with two distinctive cross-domain NER methods and two pre-training language models to explore its generalization ability. Empirical results on seven domains show the effectiveness and good generalization ability of MoCL.\",\n}\n",
    "authors": [
        "Jingyun Xu",
        "Changmeng Zheng",
        "Yi Cai",
        "Tat-Seng Chua"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.238.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9a19b832-a497-57cb-bc2b-70cc2662c2d0.pdf",
    "abstract": "Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification. To this end, we attempt to utilize contrastive learning to refine the original representations and propose a model-agnostic framework named MoCL for cross-domain NER. Additionally, we respectively combine MoCL with two distinctive cross-domain NER methods and two pre-training language models to explore its generalization ability. Empirical results on seven domains show the effectiveness and good generalization ability of MoCL.",
    "num_pages": 14
}