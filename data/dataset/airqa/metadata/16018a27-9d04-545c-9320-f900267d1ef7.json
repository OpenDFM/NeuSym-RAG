{
    "uuid": "16018a27-9d04-545c-9320-f900267d1ef7",
    "title": "Self-Evolution Learning for Discriminative Language Model Pretraining",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhong-etal-2023-self,\n    title = \"Self-Evolution Learning for Discriminative Language Model Pretraining\",\n    author = \"Zhong, Qihuang  and\n      Ding, Liang  and\n      Liu, Juhua  and\n      Du, Bo  and\n      Tao, Dacheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.254\",\n    doi = \"10.18653/v1/2023.findings-acl.254\",\n    pages = \"4130--4145\",\n    abstract = \"Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43 2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization.\",\n}\n",
    "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Bo Du",
        "Dacheng Tao"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.254.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/16018a27-9d04-545c-9320-f900267d1ef7.pdf",
    "abstract": "Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43 2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization.",
    "num_pages": 16
}