{
    "uuid": "57f45420-1725-50b0-b7cf-a231586518cb",
    "title": "RePALM: Popular Quote Tweet Generation via Auto-Response Augmentation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{yu-etal-2024-repalm,\n    title = \"{R}e{PALM}: Popular Quote Tweet Generation via Auto-Response Augmentation\",\n    author = \"Yu, Erxin  and\n      Li, Jing  and\n      Xu, Chunpu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.570\",\n    doi = \"10.18653/v1/2024.findings-acl.570\",\n    pages = \"9566--9579\",\n    abstract = \"A quote tweet enables users to share others{'} content while adding their own commentary. In order to enhance public engagement through quote tweets, we investigate the task of generating popular quote tweets. This task aims to produce quote tweets that garner higher popularity, as indicated by increased likes, replies, and retweets. Despite the impressive language generation capabilities of large language models (LLMs), there has been limited research on how LLMs can effectively learn the popularity of text to better engage the public. Therefore, we introduce a novel approach called Response-augmented Popularity-Aligned Language Model (RePALM), which aligns language generation with popularity by leveraging insights from augmented auto-responses provided by readers. We utilize the Proximal Policy Optimization framework with a dual-reward mechanism to jointly optimize for the popularity of the quote tweet and its consistency with the auto-responses. In our experiments, we collected two datasets consisting of quote tweets containing external links and those referencing others{'} tweets. Extensive results demonstrate the superiority of RePALM over advanced language models that do not incorporate response augmentation.\",\n}\n",
    "authors": [
        "Erxin Yu",
        "Jing Li",
        "Chunpu Xu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.570.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/57f45420-1725-50b0-b7cf-a231586518cb.pdf",
    "abstract": "A quote tweet enables users to share others’ content while adding their own commentary. In order to enhance public engagement through quote tweets, we investigate the task of generating popular quote tweets. This task aims to produce quote tweets that garner higher popularity, as indicated by increased likes, replies, and retweets. Despite the impressive language generation capabilities of large language models (LLMs), there has been limited research on how LLMs can effectively learn the popularity of text to better engage the public. Therefore, we introduce a novel approach called Response-augmented Popularity-Aligned Language Model (RePALM), which aligns language generation with popularity by leveraging insights from augmented auto-responses provided by readers. We utilize the Proximal Policy Optimization framework with a dual-reward mechanism to jointly optimize for the popularity of the quote tweet and its consistency with the auto-responses. In our experiments, we collected two datasets consisting of quote tweets containing external links and those referencing others’ tweets. Extensive results demonstrate the superiority of RePALM over advanced language models that do not incorporate response augmentation.",
    "num_pages": 14
}