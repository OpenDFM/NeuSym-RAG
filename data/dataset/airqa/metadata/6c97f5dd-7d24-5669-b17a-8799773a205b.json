{
    "uuid": "6c97f5dd-7d24-5669-b17a-8799773a205b",
    "title": "InfiMM: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{liu-etal-2024-infimm,\n    title = \"{I}nfi{MM}: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model\",\n    author = \"Liu, Haogeng  and\n      You, Quanzeng  and\n      Wang, Yiqi  and\n      Han, Xiaotian  and\n      Zhai, Bohan  and\n      Liu, Yongfei  and\n      Chen, Wentao  and\n      Jian, Yiren  and\n      Tao, Yunzhe  and\n      Yuan, Jianbo  and\n      He, Ran  and\n      Yang, Hongxia\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.27\",\n    doi = \"10.18653/v1/2024.findings-acl.27\",\n    pages = \"485--492\",\n    abstract = \"In this work, we present InfiMM, an advanced Multimodal Large Language Model that adapts to intricate vision-language tasks. InfiMM, inspired by the Flamingo architecture, distinguishes itself through the utilization of large-scale training data, comprehensive training strategies, and diverse large language models. This approach ensures the preservation of Flamingo{'}s foundational strengths while simultaneously introducing augmented capabilities. Empirical evaluations across a variety of benchmarks underscore InfiMM{'}s remarkable capability in multimodal understanding. The code can be found at: https://anonymous.4open.science/r/infimm-zephyr-F60C/.\",\n}\n",
    "authors": [
        "Haogeng Liu",
        "Quanzeng You",
        "Yiqi Wang",
        "Xiaotian Han",
        "Bohan Zhai",
        "Yongfei Liu",
        "Wentao Chen",
        "Yiren Jian",
        "Yunzhe Tao",
        "Jianbo Yuan",
        "Ran He",
        "Hongxia Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.27.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6c97f5dd-7d24-5669-b17a-8799773a205b.pdf",
    "abstract": "In this work, we present InfiMM, an advanced Multimodal Large Language Model that adapts to intricate vision-language tasks. InfiMM, inspired by the Flamingo architecture, distinguishes itself through the utilization of large-scale training data, comprehensive training strategies, and diverse large language models. This approach ensures the preservation of Flamingo’s foundational strengths while simultaneously introducing augmented capabilities. Empirical evaluations across a variety of benchmarks underscore InfiMM’s remarkable capability in multimodal understanding. The code can be found at: https://anonymous.4open.science/r/infimm-zephyr-F60C/.",
    "num_pages": 8
}