{
    "uuid": "53958ef3-ee2f-5624-9d91-b4413da6dc1f",
    "title": "EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{ou-etal-2024-easyinstruct,\n    title = \"{E}asy{I}nstruct: An Easy-to-use Instruction Processing Framework for Large Language Models\",\n    author = \"Ou, Yixin  and\n      Zhang, Ningyu  and\n      Gui, Honghao  and\n      Xu, Ziwen  and\n      Qiao, Shuofei  and\n      Fang, Runnan  and\n      Li, Lei  and\n      Bi, Zhen  and\n      Zheng, Guozhou  and\n      Chen, Huajun\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.10\",\n    doi = \"10.18653/v1/2024.acl-demos.10\",\n    pages = \"94--106\",\n    abstract = \"In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at Github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.\",\n}\n",
    "authors": [
        "Yixin Ou",
        "Ningyu Zhang",
        "Honghao Gui",
        "Ziwen Xu",
        "Shuofei Qiao",
        "Runnan Fang",
        "Lei Li",
        "Zhen Bi",
        "Guozhou Zheng",
        "Huajun Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/53958ef3-ee2f-5624-9d91-b4413da6dc1f.pdf",
    "abstract": "In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at Github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.",
    "num_pages": 13
}