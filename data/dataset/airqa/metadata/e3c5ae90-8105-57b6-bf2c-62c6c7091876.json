{
    "uuid": "e3c5ae90-8105-57b6-bf2c-62c6c7091876",
    "title": "A Simple Concatenation can Effectively Improve Speech Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{zhang-etal-2023-simple,\n    title = \"A Simple Concatenation can Effectively Improve Speech Translation\",\n    author = \"Zhang, Linlin  and\n      Fan, Kai  and\n      Chen, Boxing  and\n      Si, Luo\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.153\",\n    doi = \"10.18653/v1/2023.acl-short.153\",\n    pages = \"1793--1802\",\n    abstract = \"A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to construct the ST model architecture, however, the two modalities are independently fed and trained via different losses. This situation does not match ST{'}s properties across two modalities and also limits the upper bound of the performance. Inspired by the works of video Transformer, we propose a simple unified cross-modal ST method, which concatenates speech and text as the input, and builds a teacher that can utilize both cross-modal information simultaneously. Experimental results show that in our unified ST framework, models can effectively utilize the auxiliary information from speech and text, and achieve compelling results on MuST-C datasets.\",\n}\n",
    "authors": [
        "Linlin Zhang",
        "Kai Fan",
        "Boxing Chen",
        "Luo Si"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.153.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e3c5ae90-8105-57b6-bf2c-62c6c7091876.pdf",
    "abstract": "A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to construct the ST model architecture, however, the two modalities are independently fed and trained via different losses. This situation does not match STâ€™s properties across two modalities and also limits the upper bound of the performance. Inspired by the works of video Transformer, we propose a simple unified cross-modal ST method, which concatenates speech and text as the input, and builds a teacher that can utilize both cross-modal information simultaneously. Experimental results show that in our unified ST framework, models can effectively utilize the auxiliary information from speech and text, and achieve compelling results on MuST-C datasets.",
    "num_pages": 10
}