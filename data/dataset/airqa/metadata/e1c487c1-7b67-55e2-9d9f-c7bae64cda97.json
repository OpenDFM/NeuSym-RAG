{
    "uuid": "e1c487c1-7b67-55e2-9d9f-c7bae64cda97",
    "title": "Unsupervised Summarization Re-ranking",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ravaut-etal-2023-unsupervised,\n    title = \"Unsupervised Summarization Re-ranking\",\n    author = \"Ravaut, Mathieu  and\n      Joty, Shafiq  and\n      Chen, Nancy\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.529\",\n    doi = \"10.18653/v1/2023.findings-acl.529\",\n    pages = \"8341--8376\",\n    abstract = \"With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27{\\%} and ChatGPT by up to 6.86{\\%} relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51{\\%} (up to 23.73{\\%} from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).\",\n}\n",
    "authors": [
        "Mathieu Ravaut",
        "Shafiq Joty",
        "Nancy Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.529.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e1c487c1-7b67-55e2-9d9f-c7bae64cda97.pdf",
    "abstract": "With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).",
    "num_pages": 36
}