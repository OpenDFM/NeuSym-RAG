{
    "uuid": "bdf0761b-8607-55f0-b79c-bc4821b12bfb",
    "title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs’ Pragmatics Capabilities",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{sravanthi-etal-2024-pub,\n    title = \"{PUB}: A Pragmatics Understanding Benchmark for Assessing {LLM}s{'} Pragmatics Capabilities\",\n    author = \"Sravanthi, Settaluri  and\n      Doshi, Meet  and\n      Tankala, Pavan  and\n      Murthy, Rudra  and\n      Dabre, Raj  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.719\",\n    doi = \"10.18653/v1/2024.findings-acl.719\",\n    pages = \"12075--12097\",\n    abstract = \"LLMs have demonstrated remarkable capability for understanding semantics, but their understanding of pragmatics is not well studied. To this end, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely; Implicature, Presupposition, Reference, and Deixis. We curate high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k are newly annotated. We evaluate nine models varying in the number of parameters and type of training. Our study reveals several key observations about the pragmatic capabilities of LLMs: 1. chat-fine-tuning strongly benefits smaller models, 2. large base models are competitive with their chat-fine-tuned counterparts, 3. there is a huge variance in performance across different pragmatics phenomena, and 4. a noticeable performance gap between human capabilities and model capabilities. We hope that PUB will enable comprehensive evaluation of LLM{'}s pragmatic reasoning capabilities.\",\n}\n",
    "authors": [
        "Settaluri Sravanthi",
        "Meet Doshi",
        "Pavan Tankala",
        "Rudra Murthy",
        "Raj Dabre",
        "Pushpak Bhattacharyya"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.719.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bdf0761b-8607-55f0-b79c-bc4821b12bfb.pdf",
    "abstract": "LLMs have demonstrated remarkable capability for understanding semantics, but their understanding of pragmatics is not well studied. To this end, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely; Implicature, Presupposition, Reference, and Deixis. We curate high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k are newly annotated. We evaluate nine models varying in the number of parameters and type of training. Our study reveals several key observations about the pragmatic capabilities of LLMs: 1. chat-fine-tuning strongly benefits smaller models, 2. large base models are competitive with their chat-fine-tuned counterparts, 3. there is a huge variance in performance across different pragmatics phenomena, and 4. a noticeable performance gap between human capabilities and model capabilities. We hope that PUB will enable comprehensive evaluation of LLM’s pragmatic reasoning capabilities.",
    "num_pages": 23
}