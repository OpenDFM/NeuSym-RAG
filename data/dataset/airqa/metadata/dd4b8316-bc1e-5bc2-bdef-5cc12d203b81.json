{
    "uuid": "dd4b8316-bc1e-5bc2-bdef-5cc12d203b81",
    "title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{tang-etal-2023-less,\n    title = \"Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses\",\n    author = \"Tang, Liyan  and\n      Peng, Yifan  and\n      Wang, Yanshan  and\n      Ding, Ying  and\n      Durrett, Greg  and\n      Rousseau, Justin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.794\",\n    doi = \"10.18653/v1/2023.findings-acl.794\",\n    pages = \"12532--12555\",\n    abstract = \"A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, {``}less likely brainstorming,{''} that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models{'} capability of generating less likely outputs is improved.\",\n}\n",
    "authors": [
        "Liyan Tang",
        "Yifan Peng",
        "Yanshan Wang",
        "Ying Ding",
        "Greg Durrett",
        "Justin Rousseau"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.794.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dd4b8316-bc1e-5bc2-bdef-5cc12d203b81.pdf",
    "abstract": "A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models’ capability of generating less likely outputs is improved.",
    "num_pages": 24
}