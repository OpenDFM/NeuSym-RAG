{
    "uuid": "180d2943-812c-537d-ab3d-ea0e40caddef",
    "title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{wang-etal-2024-rescue,\n    title = \"Rescue: Ranking {LLM} Responses with Partial Ordering to Improve Response Generation\",\n    author = \"Wang, Yikun  and\n      Zheng, Rui  and\n      Li, Haoming  and\n      Zhang, Qi  and\n      Gui, Tao  and\n      Liu, Fei\",\n    editor = \"Fu, Xiyan  and\n      Fleisig, Eve\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-srw.32\",\n    doi = \"10.18653/v1/2024.acl-srw.32\",\n    pages = \"261--272\",\n    abstract = \"Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert-annotated data is costly for most tasks. In this paper, we explore a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system{'}s improved response generation ability using benchmark datasets, including textual entailment and multi-document question answering. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses for a specific task, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RESCUE, offers a promising avenue for enhancing the response generation and task accuracy of LLMs.\",\n}\n",
    "authors": [
        "Yikun Wang",
        "Rui Zheng",
        "Haoming Li",
        "Qi Zhang",
        "Tao Gui",
        "Fei Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-srw.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/180d2943-812c-537d-ab3d-ea0e40caddef.pdf",
    "abstract": "Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining a large volume of expert-annotated data is costly for most tasks. In this paper, we explore a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our systemâ€™s improved response generation ability using benchmark datasets, including textual entailment and multi-document question answering. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses for a specific task, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RESCUE, offers a promising avenue for enhancing the response generation and task accuracy of LLMs.",
    "num_pages": 12
}