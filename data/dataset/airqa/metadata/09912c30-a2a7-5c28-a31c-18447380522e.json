{
    "uuid": "09912c30-a2a7-5c28-a31c-18447380522e",
    "title": "Learning to Plan and Generate Text with Citations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fierro-etal-2024-learning,\n    title = \"Learning to Plan and Generate Text with Citations\",\n    author = \"Fierro, Constanza  and\n      Amplayo, Reinald Kim  and\n      Huot, Fantine  and\n      De Cao, Nicola  and\n      Maynez, Joshua  and\n      Narayan, Shashi  and\n      Lapata, Mirella\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.615\",\n    doi = \"10.18653/v1/2024.acl-long.615\",\n    pages = \"11397--11417\",\n    abstract = \"The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.\",\n}\n",
    "authors": [
        "Constanza Fierro",
        "Reinald Kim Amplayo",
        "Fantine Huot",
        "Nicola De Cao",
        "Joshua Maynez",
        "Shashi Narayan",
        "Mirella Lapata"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.615.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/09912c30-a2a7-5c28-a31c-18447380522e.pdf",
    "abstract": "The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the attribution capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as blueprints of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an abstractive model where questions are generated from scratch, and an extractive model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component.",
    "num_pages": 21
}