{
    "uuid": "f09c41a2-285b-5e95-ba4a-f98f37901b58",
    "title": "AntContentTech at SemEval-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task Learning for Understanding Indian Legal Texts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{huo-etal-2023-antcontenttech,\n    title = \"{A}nt{C}ontent{T}ech at {S}em{E}val-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task Learning for Understanding {I}ndian Legal Texts\",\n    author = \"Huo, Jingjing  and\n      Zhang, Kezun  and\n      Liu, Zhengyong  and\n      Lin, Xuan  and\n      Xu, Wenqiang  and\n      Zheng, Maozong  and\n      Wang, Zhaoguo  and\n      Li, Song\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.54\",\n    doi = \"10.18653/v1/2023.semeval-1.54\",\n    pages = \"402--408\",\n    abstract = \"The objective of this shared task is to gain an understanding of legal texts, and it is beset with difficulties such as the comprehension of lengthy noisy legal documents, domain specificity as well as the scarcity of annotated data. To address these challenges, we propose a system that employs a hierarchical model and integrates domain-adaptive pretraining, data augmentation, and auxiliary-task learning techniques. Moreover, to enhance generalization and robustness, we ensemble the models that utilize these diverse techniques. Our system ranked first on the RR sub-task and in the middle for the other two sub-tasks.\",\n}\n",
    "authors": [
        "Jingjing Huo",
        "Kezun Zhang",
        "Zhengyong Liu",
        "Xuan Lin",
        "Wenqiang Xu",
        "Maozong Zheng",
        "Zhaoguo Wang",
        "Song Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.54.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f09c41a2-285b-5e95-ba4a-f98f37901b58.pdf",
    "abstract": "The objective of this shared task is to gain an understanding of legal texts, and it is beset with difficulties such as the comprehension of lengthy noisy legal documents, domain specificity as well as the scarcity of annotated data. To address these challenges, we propose a system that employs a hierarchical model and integrates domain-adaptive pretraining, data augmentation, and auxiliary-task learning techniques. Moreover, to enhance generalization and robustness, we ensemble the models that utilize these diverse techniques. Our system ranked first on the RR sub-task and in the middle for the other two sub-tasks.",
    "num_pages": 7
}