{
    "uuid": "89e103d2-5167-5aba-8f87-22251712fd65",
    "title": "Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{polak-etal-2023-towards,\n    title = \"Towards Efficient Simultaneous Speech Translation: {CUNI}-{KIT} System for Simultaneous Track at {IWSLT} 2023\",\n    author = \"Pol{\\'a}k, Peter  and\n      Liu, Danni  and\n      Pham, Ngoc-Quan  and\n      Niehues, Jan  and\n      Waibel, Alexander  and\n      Bojar, Ond{\\v{r}}ej\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.37\",\n    doi = \"10.18653/v1/2023.iwslt-1.37\",\n    pages = \"389--396\",\n    abstract = \"In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45{\\%} relative RTF).\",\n}\n",
    "authors": [
        "Peter Polák",
        "Danni Liu",
        "Ngoc-Quan Pham",
        "Jan Niehues",
        "Alexander Waibel",
        "Ondřej Bojar"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.37.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/89e103d2-5167-5aba-8f87-22251712fd65.pdf",
    "abstract": "In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).",
    "num_pages": 8
}