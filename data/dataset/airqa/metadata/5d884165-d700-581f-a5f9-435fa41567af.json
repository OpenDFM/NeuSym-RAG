{
    "uuid": "5d884165-d700-581f-a5f9-435fa41567af",
    "title": "DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{tan-etal-2023-damo,\n    title = \"{DAMO}-{NLP} at {S}em{E}val-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition\",\n    author = \"Tan, Zeqi  and\n      Huang, Shen  and\n      Jia, Zixia  and\n      Cai, Jiong  and\n      Li, Yinghui  and\n      Lu, Weiming  and\n      Zhuang, Yueting  and\n      Tu, Kewei  and\n      Xie, Pengjun  and\n      Huang, Fei  and\n      Jiang, Yong\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.277\",\n    doi = \"10.18653/v1/2023.semeval-1.277\",\n    pages = \"2014--2028\",\n    abstract = \"The MultiCoNER II shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER I task. To cope with these problems, the previous top systems in the MultiCoNER I either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team DAMO-NLP proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model. Also, we explore various search strategies and refine the quality of retrieval knowledge. Our system wins 9 out of 13 tracks in the MultiCoNER II shared task. Additionally, we compared our system with ChatGPT, one of the large language models which have unlocked strong capabilities on many tasks. The results show that there is still much room for improvement for ChatGPT on the extraction task.\",\n}\n",
    "authors": [
        "Zeqi Tan",
        "Shen Huang",
        "Zixia Jia",
        "Jiong Cai",
        "Yinghui Li",
        "Weiming Lu",
        "Yueting Zhuang",
        "Kewei Tu",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.277.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5d884165-d700-581f-a5f9-435fa41567af.pdf",
    "abstract": "The MultiCoNER II shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER I task. To cope with these problems, the previous top systems in the MultiCoNER I either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team DAMO-NLP proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model. Also, we explore various search strategies and refine the quality of retrieval knowledge. Our system wins 9 out of 13 tracks in the MultiCoNER II shared task. Additionally, we compared our system with ChatGPT, one of the large language models which have unlocked strong capabilities on many tasks. The results show that there is still much room for improvement for ChatGPT on the extraction task.",
    "num_pages": 15
}