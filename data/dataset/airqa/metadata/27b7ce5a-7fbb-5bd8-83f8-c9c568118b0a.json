{
    "uuid": "27b7ce5a-7fbb-5bd8-83f8-c9c568118b0a",
    "title": "Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{nguyen-etal-2024-video,\n    title = \"Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives\",\n    author = \"Nguyen, Thong  and\n      Bin, Yi  and\n      Xiao, Junbin  and\n      Qu, Leigang  and\n      Li, Yicong  and\n      Wu, Jay Zhangjie  and\n      Nguyen, Cong-Duy  and\n      Ng, See-Kiong  and\n      Luu, Anh Tuan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.217\",\n    doi = \"10.18653/v1/2024.findings-acl.217\",\n    pages = \"3636--3657\",\n    abstract = \"Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research.\",\n}\n",
    "authors": [
        "Thong Nguyen",
        "Yi Bin",
        "Junbin Xiao",
        "Leigang Qu",
        "Yicong Li",
        "Jay Zhangjie Wu",
        "Cong-Duy Nguyen",
        "See-Kiong Ng",
        "Anh Tuan Luu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.217.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/27b7ce5a-7fbb-5bd8-83f8-c9c568118b0a.pdf",
    "abstract": "Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research.",
    "num_pages": 22
}