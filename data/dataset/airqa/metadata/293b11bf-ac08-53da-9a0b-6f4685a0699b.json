{
    "uuid": "293b11bf-ac08-53da-9a0b-6f4685a0699b",
    "title": "Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hu-etal-2024-listen,\n    title = \"Listen Again and Choose the Right Answer: A New Paradigm for Automatic Speech Recognition with Large Language Models\",\n    author = \"Hu, Yuchen  and\n      Chen, Chen  and\n      Qin, Chengwei  and\n      Zhu, Qiushi  and\n      Chng, EngSiong  and\n      Li, Ruizhe\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.37\",\n    doi = \"10.18653/v1/2024.findings-acl.37\",\n    pages = \"666--679\",\n    abstract = \"Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses. Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results. However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection. In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction. First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source speech as extra input to improve the fidelity of correction output. Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions. Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets.\",\n}\n",
    "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Chengwei Qin",
        "Qiushi Zhu",
        "EngSiong Chng",
        "Ruizhe Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.37.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/293b11bf-ac08-53da-9a0b-6f4685a0699b.pdf",
    "abstract": "Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which aims to predict the ground-truth transcription from the decoded N-best hypotheses. Thanks to the strong language generation ability of LLMs and rich information in the N-best list, GER shows great effectiveness in enhancing ASR results. However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection. In this paper, we propose ClozeGER, a new paradigm for ASR generative error correction. First, we introduce a multimodal LLM (i.e., SpeechGPT) to receive source speech as extra input to improve the fidelity of correction output. Then, we reformat GER as a cloze test with logits calibration to remove the input information redundancy and simplify GER with clear instructions. Experiments show that ClozeGER achieves a new breakthrough over vanilla GER on 9 popular ASR datasets.",
    "num_pages": 14
}