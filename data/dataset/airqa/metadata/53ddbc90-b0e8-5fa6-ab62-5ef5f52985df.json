{
    "uuid": "53ddbc90-b0e8-5fa6-ab62-5ef5f52985df",
    "title": "Iterative Forward Tuning Boosts In-Context Learning in Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yang-etal-2024-iterative-forward,\n    title = \"Iterative Forward Tuning Boosts In-Context Learning in Language Models\",\n    author = \"Yang, Jiaxi  and\n      Hui, Binyuan  and\n      Yang, Min  and\n      Wang, Bailin  and\n      Li, Bowen  and\n      Li, Binhua  and\n      Huang, Fei  and\n      Li, Yongbin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.825\",\n    doi = \"10.18653/v1/2024.acl-long.825\",\n    pages = \"15460--15473\",\n    abstract = \"Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.\",\n}\n",
    "authors": [
        "Jiaxi Yang",
        "Binyuan Hui",
        "Min Yang",
        "Bailin Wang",
        "Bowen Li",
        "Binhua Li",
        "Fei Huang",
        "Yongbin Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.825.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/53ddbc90-b0e8-5fa6-ab62-5ef5f52985df.pdf",
    "abstract": "Despite the advancements in in-context learning (ICL) for large language models (LLMs), current research centers on specific prompt engineering, such as demonstration selection, with the expectation that a single iteration of demonstrations processing can generalize effectively to a given test sample. However, this perspective overlooks the potential benefits derived from multiple iterations involving demonstrations, a practice aligning more closely with the iterative decision-making process exhibited by humans, who often learn through analogy. In this study, we introduce a novel two-stage framework to boost ICL in LLMs. Specifically, our framework delineates the ICL process into two distinct stages: Deep-Thinking and test stages. The Deep-Thinking stage incorporates a unique attention mechanism, i.e., iterative enhanced attention, which enables multiple rounds of information accumulation. This mechanism operates by manipulating the Key-Value matrices without training, fostering enhanced understanding capabilities in LLMs by thinking demonstrations multiple times. We evaluated Deep-Thinking across a range of benchmarks and LLMs, showing its superior performance over vanilla ICL methods and its effectiveness in challenging tasks where demonstration selection is infeasible.",
    "num_pages": 14
}