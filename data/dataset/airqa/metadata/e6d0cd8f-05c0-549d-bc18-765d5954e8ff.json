{
    "uuid": "e6d0cd8f-05c0-549d-bc18-765d5954e8ff",
    "title": "Few-shot Classification with Hypersphere Modeling of Prototypes",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ding-etal-2023-shot,\n    title = \"Few-shot Classification with Hypersphere Modeling of Prototypes\",\n    author = \"Ding, Ning  and\n      Chen, Yulin  and\n      Cui, Ganqu  and\n      Wang, Xiaobin  and\n      Zheng, Haitao  and\n      Liu, Zhiyuan  and\n      Xie, Pengjun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.57\",\n    doi = \"10.18653/v1/2023.findings-acl.57\",\n    pages = \"895--917\",\n    abstract = \"Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However, using embeddings in space lacks expressivity and cannot capture class information robustly, while statistical complex modeling poses difficulty to metric designs. In this work, we use tensor fields ({``}areas{''}) to model classes from the geometrical perspective for few-shot learning. We present a simple and effective method, dubbed as hypersphere prototypes (HyperProto), where class information is represented by hyperspheres with dynamic sizes with two sets of learnable parameters: the hypersphere{'}s center and the radius. Extending from points to areas, hyperspheres are much more expressive than embeddings. Moreover, it is more convenient to perform metric-based classification with hypersphere prototypes than statistical modeling, as we only need to calculate the distance from a data point to the surface of the hypersphere. Following this idea, we also develop two variants of prototypes under other measurements. Extensive experiments and analysis on few-shot NLP tasks and comparison with 20+ competitive baselines demonstrate the effectiveness of our approach.\",\n}\n",
    "authors": [
        "Ning Ding",
        "Yulin Chen",
        "Ganqu Cui",
        "Xiaobin Wang",
        "Haitao Zheng",
        "Zhiyuan Liu",
        "Pengjun Xie"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.57.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e6d0cd8f-05c0-549d-bc18-765d5954e8ff.pdf",
    "abstract": "Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However, using embeddings in space lacks expressivity and cannot capture class information robustly, while statistical complex modeling poses difficulty to metric designs. In this work, we use tensor fields (“areas”) to model classes from the geometrical perspective for few-shot learning. We present a simple and effective method, dubbed as hypersphere prototypes (HyperProto), where class information is represented by hyperspheres with dynamic sizes with two sets of learnable parameters: the hypersphere’s center and the radius. Extending from points to areas, hyperspheres are much more expressive than embeddings. Moreover, it is more convenient to perform metric-based classification with hypersphere prototypes than statistical modeling, as we only need to calculate the distance from a data point to the surface of the hypersphere. Following this idea, we also develop two variants of prototypes under other measurements. Extensive experiments and analysis on few-shot NLP tasks and comparison with 20+ competitive baselines demonstrate the effectiveness of our approach.",
    "num_pages": 23
}