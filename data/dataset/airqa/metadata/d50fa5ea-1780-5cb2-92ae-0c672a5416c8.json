{
    "uuid": "d50fa5ea-1780-5cb2-92ae-0c672a5416c8",
    "title": "Stealthy Attack on Large Language Model based Recommendation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-stealthy,\n    title = \"Stealthy Attack on Large Language Model based Recommendation\",\n    author = \"Zhang, Jinghao  and\n      Liu, Yuting  and\n      Liu, Qiang  and\n      Wu, Shu  and\n      Guo, Guibing  and\n      Wang, Liang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.318\",\n    doi = \"10.18653/v1/2024.acl-long.318\",\n    pages = \"5839--5857\",\n    abstract = \"Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item{'}s exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model{'}s training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.\",\n}\n",
    "authors": [
        "Jinghao Zhang",
        "Yuting Liu",
        "Qiang Liu",
        "Shu Wu",
        "Guibing Guo",
        "Liang Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.318.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d50fa5ea-1780-5cb2-92ae-0c672a5416c8.pdf",
    "abstract": "Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item’s exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model’s training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.",
    "num_pages": 19
}