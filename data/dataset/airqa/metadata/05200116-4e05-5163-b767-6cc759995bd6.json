{
    "uuid": "05200116-4e05-5163-b767-6cc759995bd6",
    "title": "Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-side,\n    title = \"Which Side Are You On? A Multi-task Dataset for End-to-End Argument Summarisation and Evaluation\",\n    author = \"Li, Hao  and\n      Wu, Yuping  and\n      Schlegel, Viktor  and\n      Batista-Navarro, Riza  and\n      Madusanka, Tharindu  and\n      Zahid, Iqra  and\n      Zeng, Jiayan  and\n      Wang, Xiaochi  and\n      He, Xinran  and\n      Li, Yizhi  and\n      Nenadic, Goran\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.9\",\n    doi = \"10.18653/v1/2024.findings-acl.9\",\n    pages = \"133--150\",\n    abstract = \"With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset.\",\n}\n",
    "authors": [
        "Hao Li",
        "Yuping Wu",
        "Viktor Schlegel",
        "Riza Batista-Navarro",
        "Tharindu Madusanka",
        "Iqra Zahid",
        "Jiayan Zeng",
        "Xiaochi Wang",
        "Xinran He",
        "Yizhi Li",
        "Goran Nenadic"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/05200116-4e05-5163-b767-6cc759995bd6.pdf",
    "abstract": "With the recent advances of large language models (LLMs), it is no longer infeasible to build an automated debate system that helps people to synthesise persuasive arguments. Previous work attempted this task by integrating multiple components. In our work, we introduce an argument mining dataset that captures the end-to-end process of preparing an argumentative essay for a debate, which covers the tasks of claim and evidence identification (Task 1 ED), evidence convincingness ranking (Task 2 ECR), argumentative essay summarisation and human preference ranking (Task 3 ASR) and metric learning for automated evaluation of resulting essays, based on human feedback along argument quality dimensions (Task 4 SQE). Our dataset contains 14k examples of claims that are fully annotated with various properties supporting the aforementioned tasks. We evaluate multiple generative baselines for each of these tasks, including representative LLMs. We find, that while they show promising results on individual tasks in our benchmark, their end-to-end performance on all four tasks in succession deteriorates significantly, both in automated measures as well as in human-centred evaluation. This challenge presented by our proposed dataset motivates future research on end-to-end argument mining and summarisation. The repository of this project is available at https://github.com/HarrywillDr/ArgSum-Datatset.",
    "num_pages": 18
}