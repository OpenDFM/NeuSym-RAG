{
    "uuid": "214bf236-7ec8-5632-a5f2-1a52e94b91dc",
    "title": "NLPeople at TextGraphs-17 Shared Task: Chain of Thought Questioning to Elicit Decompositional Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
    "bibtex": "@inproceedings{moses-etal-2024-nlpeople,\n    title = \"{NLP}eople at {T}ext{G}raphs-17 Shared Task: Chain of Thought Questioning to Elicit Decompositional Reasoning\",\n    author = \"Moses, Movina  and\n      Kuruvanthodi, Vishnudev  and\n      Elkaref, Mohab  and\n      Tanaka, Shinnosuke  and\n      Barry, James  and\n      Mel, Geeth  and\n      Watson, Campbell\",\n    editor = \"Ustalov, Dmitry  and\n      Gao, Yanjun  and\n      Panchenko, Alexander  and\n      Tutubalina, Elena  and\n      Nikishina, Irina  and\n      Ramesh, Arti  and\n      Sakhovskiy, Andrey  and\n      Usbeck, Ricardo  and\n      Penn, Gerald  and\n      Valentino, Marco\",\n    booktitle = \"Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.textgraphs-1.13\",\n    pages = \"142--148\",\n    abstract = \"This paper presents the approach of the NLPeople team for the Text-Graph Representations for KGQA Shared Task at TextGraphs-17. The task involved selecting an answer for a given question from a list of candidate entities. We show that prompting Large Language models (LLMs) to break down a natural language question into a series of sub-questions, allows models to understand complex questions. The LLMs arrive at the final answer by answering the intermediate questions using their internal knowledge and without needing additional context. Our approach to the task uses an ensemble of prompting strategies to guide how LLMs interpret various types of questions. Our submission achieves an F1 score of 85.90, ranking 1st among the other participants in the task.\",\n}\n",
    "authors": [
        "Movina Moses",
        "Vishnudev Kuruvanthodi",
        "Mohab Elkaref",
        "Shinnosuke Tanaka",
        "James Barry",
        "Geeth Mel",
        "Campbell Watson"
    ],
    "pdf_url": "https://aclanthology.org/2024.textgraphs-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/214bf236-7ec8-5632-a5f2-1a52e94b91dc.pdf",
    "abstract": "This paper presents the approach of the NLPeople team for the Text-Graph Representations for KGQA Shared Task at TextGraphs-17. The task involved selecting an answer for a given question from a list of candidate entities. We show that prompting Large Language models (LLMs) to break down a natural language question into a series of sub-questions, allows models to understand complex questions. The LLMs arrive at the final answer by answering the intermediate questions using their internal knowledge and without needing additional context. Our approach to the task uses an ensemble of prompting strategies to guide how LLMs interpret various types of questions. Our submission achieves an F1 score of 85.90, ranking 1st among the other participants in the task.",
    "num_pages": 7
}