{
    "uuid": "f2bc241b-389f-5b16-a178-565f2c32bbd5",
    "title": "Multi-modal Stance Detection: New Datasets and Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{liang-etal-2024-multi,\n    title = \"Multi-modal Stance Detection: New Datasets and Model\",\n    author = \"Liang, Bin  and\n      Li, Ang  and\n      Zhao, Jingqian  and\n      Gui, Lin  and\n      Yang, Min  and\n      Yu, Yue  and\n      Wong, Kam-Fai  and\n      Xu, Ruifeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.736\",\n    doi = \"10.18653/v1/2024.findings-acl.736\",\n    pages = \"12373--12387\",\n    abstract = \"Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in today{'}s fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our five benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.\",\n}\n",
    "authors": [
        "Bin Liang",
        "Ang Li",
        "Jingqian Zhao",
        "Lin Gui",
        "Min Yang",
        "Yue Yu",
        "Kam-Fai Wong",
        "Ruifeng Xu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.736.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f2bc241b-389f-5b16-a178-565f2c32bbd5.pdf",
    "abstract": "Stance detection is a challenging task that aims to identify public opinion from social media platforms with respect to specific targets. Previous work on stance detection largely focused on pure texts. In this paper, we study multi-modal stance detection for tweets consisting of texts and images, which are prevalent in todayâ€™s fast-growing social media platforms where people often post multi-modal messages. To this end, we create five new multi-modal stance detection datasets of different domains based on Twitter, in which each example consists of a text and an image. In addition, we propose a simple yet effective Targeted Multi-modal Prompt Tuning framework (TMPT), where target information is leveraged to learn multi-modal stance features from textual and visual modalities. Experimental results on our five benchmark datasets show that the proposed TMPT achieves state-of-the-art performance in multi-modal stance detection.",
    "num_pages": 15
}