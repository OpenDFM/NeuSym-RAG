{
    "uuid": "25169617-0360-5bf1-9046-d7b5a0a7251d",
    "title": "Improving Autoregressive Grammatical Error Correction with Non-autoregressive Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cao-etal-2023-improving,\n    title = \"Improving Autoregressive Grammatical Error Correction with Non-autoregressive Models\",\n    author = \"Cao, Hang  and\n      Cao, Zhiquan  and\n      Hu, Chi  and\n      Hou, Baoyu  and\n      Xiao, Tong  and\n      Zhu, Jingbo\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.760\",\n    doi = \"10.18653/v1/2023.findings-acl.760\",\n    pages = \"12014--12027\",\n    abstract = \"Grammatical Error Correction (GEC) aims to correct grammatical errors in sentences. We find that autoregressive models tend to assign low probabilities to tokens that need corrections. Here we introduce additional signals to the training of GEC models so that these systems can learn to better predict at ambiguous positions. To do this, we use a non-autoregressive model as an auxiliary model, and develop a new regularization term of training by considering the difference in predictions between the autoregressive and non-autoregressive models. We experiment with this method on both English and Chinese GEC tasks. Experimental results show that our GEC system outperforms the baselines on all the data sets significantly.\",\n}\n",
    "authors": [
        "Hang Cao",
        "Zhiquan Cao",
        "Chi Hu",
        "Baoyu Hou",
        "Tong Xiao",
        "Jingbo Zhu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.760.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/25169617-0360-5bf1-9046-d7b5a0a7251d.pdf",
    "abstract": "Grammatical Error Correction (GEC) aims to correct grammatical errors in sentences. We find that autoregressive models tend to assign low probabilities to tokens that need corrections. Here we introduce additional signals to the training of GEC models so that these systems can learn to better predict at ambiguous positions. To do this, we use a non-autoregressive model as an auxiliary model, and develop a new regularization term of training by considering the difference in predictions between the autoregressive and non-autoregressive models. We experiment with this method on both English and Chinese GEC tasks. Experimental results show that our GEC system outperforms the baselines on all the data sets significantly.",
    "num_pages": 14
}