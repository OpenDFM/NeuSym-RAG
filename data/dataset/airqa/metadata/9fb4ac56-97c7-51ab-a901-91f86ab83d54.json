{
    "uuid": "9fb4ac56-97c7-51ab-a901-91f86ab83d54",
    "title": "Continual Knowledge Distillation for Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2023-continual,\n    title = \"Continual Knowledge Distillation for Neural Machine Translation\",\n    author = \"Zhang, Yuanchi  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Liu, Yang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.443\",\n    doi = \"10.18653/v1/2023.acl-long.443\",\n    pages = \"7978--7996\",\n    abstract = \"While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on Chinese-English and German-English datasets show that our method achieves significant and consistent improvements over strong baselines under both homogeneous and heterogeneous trained model settings and is robust to malicious models.\",\n}\n",
    "authors": [
        "Yuanchi Zhang",
        "Peng Li",
        "Maosong Sun",
        "Yang Liu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.443.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9fb4ac56-97c7-51ab-a901-91f86ab83d54.pdf",
    "abstract": "While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on Chinese-English and German-English datasets show that our method achieves significant and consistent improvements over strong baselines under both homogeneous and heterogeneous trained model settings and is robust to malicious models.",
    "num_pages": 19
}