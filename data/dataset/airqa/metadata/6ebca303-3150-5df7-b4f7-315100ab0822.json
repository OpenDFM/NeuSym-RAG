{
    "uuid": "6ebca303-3150-5df7-b4f7-315100ab0822",
    "title": "Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhou-etal-2023-modular,\n    title = \"Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference\",\n    author = \"Zhou, Wangchunshu  and\n      Le Bras, Ronan  and\n      Choi, Yejin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.664\",\n    doi = \"10.18653/v1/2023.findings-acl.664\",\n    pages = \"10452--10465\",\n    abstract = \"Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible sequence-to-sequence model compression. Modular Transformers trains modularized layers that have the same function of two or more consecutive layers in the original model via module replacing and knowledge distillation. After training, the modularized layers can be flexibly assembled into sequence-to-sequence models that meet different performance-efficiency trade-offs. Experimental results show that after a single training phase, by simply varying the assemble strategy, Modular Transformers can achieve flexible compression ratios from 1.1x to 6x with little to moderate relative performance drop.\",\n}\n",
    "authors": [
        "Wangchunshu Zhou",
        "Ronan Le Bras",
        "Yejin Choi"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.664.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6ebca303-3150-5df7-b4f7-315100ab0822.pdf",
    "abstract": "Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible sequence-to-sequence model compression. Modular Transformers trains modularized layers that have the same function of two or more consecutive layers in the original model via module replacing and knowledge distillation. After training, the modularized layers can be flexibly assembled into sequence-to-sequence models that meet different performance-efficiency trade-offs. Experimental results show that after a single training phase, by simply varying the assemble strategy, Modular Transformers can achieve flexible compression ratios from 1.1x to 6x with little to moderate relative performance drop.",
    "num_pages": 14
}