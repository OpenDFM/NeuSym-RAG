{
    "uuid": "84bf5fcd-cae3-5db6-8695-a65b4e59a5fd",
    "title": "DP-MLM: Differentially Private Text Rewriting Using Masked Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{meisenbacher-etal-2024-dp,\n    title = \"{DP}-{MLM}: Differentially Private Text Rewriting Using Masked Language Models\",\n    author = \"Meisenbacher, Stephen  and\n      Chevli, Maulik  and\n      Vladika, Juraj  and\n      Matthes, Florian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.554\",\n    doi = \"10.18653/v1/2024.findings-acl.554\",\n    pages = \"9314--9328\",\n}\n",
    "authors": [
        "Stephen Meisenbacher",
        "Maulik Chevli",
        "Juraj Vladika",
        "Florian Matthes"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.554.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/84bf5fcd-cae3-5db6-8695-a65b4e59a5fd.pdf",
    "abstract": "The task of text privatization using Differential Privacy has recently taken the form of text rewriting, in which an input text is obfuscated via the use of generative (large) language models. While these methods have shown promising results in the ability to preserve privacy, these methods rely on autoregressive models which lack a mechanism to contextualize the private rewriting process. In response to this, we propose DP-MLM, a new method for differentially private text rewriting based on leveraging masked language models (MLMs) to rewrite text in a semantically similar and obfuscated manner. We accomplish this with a simple contextualization technique, whereby we rewrite a text one token at a time. We find that utilizing encoder-only MLMs provides better utility preservation at lower Îµ levels, as compared to previous methods relying on larger models with a decoder. In addition, MLMs allow for greater customization of the rewriting mechanism, as opposed to generative approaches. We make the code for DP-MLM public and reusable, found at https://github.com/sjmeis/DPMLM.",
    "num_pages": 15
}