{
    "uuid": "5efebbdf-8fb7-508d-9387-d78f30c786a9",
    "title": "Causal and Temporal Inference in Visual Question Generation by Utilizing Pre-trained Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)",
    "bibtex": "@inproceedings{hu-keller-2024-causal,\n    title = \"Causal and Temporal Inference in Visual Question Generation by Utilizing Pre-trained Models\",\n    author = \"Hu, Zhanghao  and\n      Keller, Frank\",\n    editor = \"Gu, Jing  and\n      Fu, Tsu-Jui (Ray)  and\n      Hudson, Drew  and\n      Celikyilmaz, Asli  and\n      Wang, William\",\n    booktitle = \"Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.alvr-1.12\",\n    doi = \"10.18653/v1/2024.alvr-1.12\",\n    pages = \"138--154\",\n    abstract = \"Visual Question Generation is a task at the crossroads of visual and language learning, impacting broad domains like education, medicine, and social media. While existing pre-trained models excel in fact-based queries with image pairs, they fall short of capturing human-like inference, particularly in understanding causal and temporal relationships within videos. Additionally, the computational demands of prevalent pre-training methods pose challenges. In response, our study introduces a framework that leverages vision-text matching pre-trained models to guide language models in recognizing event-entity relationships within videos and generating inferential questions. Demonstrating efficacy on the NExT-QA dataset, which is designed for causal and temporal inference in visual question answering, our method successfully guides pre-trained language models in recognizing video content. We present methodologies for abstracting causal and temporal relationships between events and entities, pointing out the importance of consistent relationships among input frames during training and inference phases and suggesting an avenue for future exploration.\",\n}\n",
    "authors": [
        "Zhanghao Hu",
        "Frank Keller"
    ],
    "pdf_url": "https://aclanthology.org/2024.alvr-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5efebbdf-8fb7-508d-9387-d78f30c786a9.pdf",
    "abstract": "Visual Question Generation is a task at the crossroads of visual and language learning, impacting broad domains like education, medicine, and social media. While existing pre-trained models excel in fact-based queries with image pairs, they fall short of capturing human-like inference, particularly in understanding causal and temporal relationships within videos. Additionally, the computational demands of prevalent pre-training methods pose challenges. In response, our study introduces a framework that leverages vision-text matching pre-trained models to guide language models in recognizing event-entity relationships within videos and generating inferential questions. Demonstrating efficacy on the NExT-QA dataset, which is designed for causal and temporal inference in visual question answering, our method successfully guides pre-trained language models in recognizing video content. We present methodologies for abstracting causal and temporal relationships between events and entities, pointing out the importance of consistent relationships among input frames during training and inference phases and suggesting an avenue for future exploration.",
    "num_pages": 17
}