{
    "uuid": "7a863bff-df10-5f7a-af2e-76a59e5e48e0",
    "title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-analyzing,\n    title = \"Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding\",\n    author = \"Zhang, Zhihan  and\n      Cao, Yixin  and\n      Ye, Chenchen  and\n      Ma, Yunshan  and\n      Liao, Lizi  and\n      Chua, Tat-Seng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.87\",\n    doi = \"10.18653/v1/2024.acl-long.87\",\n    pages = \"1588--1606\",\n    abstract = \"The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.\",\n}\n",
    "authors": [
        "Zhihan Zhang",
        "Yixin Cao",
        "Chenchen Ye",
        "Yunshan Ma",
        "Lizi Liao",
        "Tat-Seng Chua"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.87.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7a863bff-df10-5f7a-af2e-76a59e5e48e0.pdf",
    "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",
    "num_pages": 19
}