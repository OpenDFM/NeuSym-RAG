{
    "uuid": "72e26170-c5b0-5a1d-afe2-9f08d1f99a27",
    "title": "VISREAS: Complex Visual Reasoning with Unanswerable Questions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{akter-etal-2024-visreas,\n    title = \"{VISREAS}: Complex Visual Reasoning with Unanswerable Questions\",\n    author = \"Akter, Syeda Nahida  and\n      Lee, Sangwu  and\n      Chang, Yingshan  and\n      Bisk, Yonatan  and\n      Nyberg, Eric\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.402\",\n    doi = \"10.18653/v1/2024.findings-acl.402\",\n    pages = \"6735--6752\",\n    abstract = \"Verifying a question{'}s validity before answering is crucial in real-world applications, where users may provide imperfect instructions. In this scenario, an ideal model should address the discrepancies in the query and convey them to the users rather than generating the best possible answer. Addressing this requirement, we introduce a new compositional visual question-answering dataset, VisReas, that consists of answerable and unanswerable visual queries formulated by traversing and perturbing commonalities and differences among objects, attributes, and relations. VisReas contains 2.07M semantically diverse queries generated automatically using Visual Genome scene graphs. The unique feature of this task, validating question answerability with respect to an image before answering, and the poor performance of state-of-the-art models inspired the design of a new modular baseline, Logic2Vision that reasons by producing and executing pseudocode without any external modules to generate the answer. Logic2Vision outperforms generative models in VisReas (+4.82{\\%} over LLaVA-1.5; +12.23{\\%} over InstructBLIP) and achieves a significant gain in performance against the classification models.\",\n}\n",
    "authors": [
        "Syeda Nahida Akter",
        "Sangwu Lee",
        "Yingshan Chang",
        "Yonatan Bisk",
        "Eric Nyberg"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.402.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/72e26170-c5b0-5a1d-afe2-9f08d1f99a27.pdf",
    "abstract": "Verifying a questionâ€™s validity before answering is crucial in real-world applications, where users may provide imperfect instructions. In this scenario, an ideal model should address the discrepancies in the query and convey them to the users rather than generating the best possible answer. Addressing this requirement, we introduce a new compositional visual question-answering dataset, VisReas, that consists of answerable and unanswerable visual queries formulated by traversing and perturbing commonalities and differences among objects, attributes, and relations. VisReas contains 2.07M semantically diverse queries generated automatically using Visual Genome scene graphs. The unique feature of this task, validating question answerability with respect to an image before answering, and the poor performance of state-of-the-art models inspired the design of a new modular baseline, Logic2Vision that reasons by producing and executing pseudocode without any external modules to generate the answer. Logic2Vision outperforms generative models in VisReas (+4.82% over LLaVA-1.5; +12.23% over InstructBLIP) and achieves a significant gain in performance against the classification models.",
    "num_pages": 18
}