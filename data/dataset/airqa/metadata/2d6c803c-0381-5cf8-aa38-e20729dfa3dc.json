{
    "uuid": "2d6c803c-0381-5cf8-aa38-e20729dfa3dc",
    "title": "Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yang-etal-2023-shot-joint,\n    title = \"Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt\",\n    author = \"Yang, Xiaocui  and\n      Feng, Shi  and\n      Wang, Daling  and\n      Sun, Qi  and\n      Wu, Wenfang  and\n      Zhang, Yifei  and\n      Hong, Pengfei  and\n      Poria, Soujanya\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.735\",\n    doi = \"10.18653/v1/2023.findings-acl.735\",\n    pages = \"11575--11589\",\n    abstract = \"We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt. Extensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.\",\n}\n",
    "authors": [
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Qi Sun",
        "Wenfang Wu",
        "Yifei Zhang",
        "Pengfei Hong",
        "Soujanya Poria"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.735.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2d6c803c-0381-5cf8-aa38-e20729dfa3dc.pdf",
    "abstract": "We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt. Extensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.",
    "num_pages": 15
}