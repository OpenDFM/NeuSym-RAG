{
    "uuid": "396b53c3-f396-50e0-8d56-20c7922491fc",
    "title": "Improved Visual Story Generation with Adaptive Context Modeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{feng-etal-2023-improved,\n    title = \"Improved Visual Story Generation with Adaptive Context Modeling\",\n    author = \"Feng, Zhangyin  and\n      Ren, Yuchen  and\n      Yu, Xinmiao  and\n      Feng, Xiaocheng  and\n      Tang, Duyu  and\n      Shi, Shuming  and\n      Qin, Bing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.305\",\n    doi = \"10.18653/v1/2023.findings-acl.305\",\n    pages = \"4939--4955\",\n    abstract = \"Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all preceding images contribute equally to the generation of the characters and scenes at the current stage. To address this, we present a simple method that improves the leading system with adaptive context modeling, which is not only incorporated in the encoder but also adopted as additional guidance in the sampling stage to boost the global consistency of the generated story. We evaluate our model on PororoSV and FlintstonesSV datasets and show that our approach achieves state-of-the-art FID scores on both story visualization and continuation scenarios. We conduct detailed model analysis and show that our model excels at generating semantically consistent images for stories.\",\n}\n",
    "authors": [
        "Zhangyin Feng",
        "Yuchen Ren",
        "Xinmiao Yu",
        "Xiaocheng Feng",
        "Duyu Tang",
        "Shuming Shi",
        "Bing Qin"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.305.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/396b53c3-f396-50e0-8d56-20c7922491fc.pdf",
    "abstract": "Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all preceding images contribute equally to the generation of the characters and scenes at the current stage. To address this, we present a simple method that improves the leading system with adaptive context modeling, which is not only incorporated in the encoder but also adopted as additional guidance in the sampling stage to boost the global consistency of the generated story. We evaluate our model on PororoSV and FlintstonesSV datasets and show that our approach achieves state-of-the-art FID scores on both story visualization and continuation scenarios. We conduct detailed model analysis and show that our model excels at generating semantically consistent images for stories.",
    "num_pages": 17
}