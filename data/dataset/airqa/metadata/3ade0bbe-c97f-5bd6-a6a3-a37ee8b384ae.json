{
    "uuid": "3ade0bbe-c97f-5bd6-a6a3-a37ee8b384ae",
    "title": "Pause-Aware Automatic Dubbing using LLM and Voice Cloning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    "bibtex": "@inproceedings{li-etal-2024-pause,\n    title = \"Pause-Aware Automatic Dubbing using {LLM} and Voice Cloning\",\n    author = \"Li, Yuang  and\n      Guo, Jiaxin  and\n      Zhang, Min  and\n      Miaomiao, Ma  and\n      Rao, Zhiqiang  and\n      Zhang, Weidong  and\n      He, Xianghui  and\n      Wei, Daimeng  and\n      Yang, Hao\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.iwslt-1.2\",\n    doi = \"10.18653/v1/2024.iwslt-1.2\",\n    pages = \"12--16\",\n    abstract = \"Automatic dubbing aims to translate the speech of a video into another language, ensuring the new speech naturally fits the original video. This paper details Huawei Translation Services Center{'}s (HW-TSC) submission for IWSLT 2024{'}s automatic dubbing task, under an unconstrained setting. Our system{'}s machine translation (MT) component utilizes a Transformer-based MT model and an LLM-based post-editor to produce translations of varying lengths. The text-to-speech (TTS) component employs a VITS-based TTS model and a voice cloning module to emulate the original speaker{'}s vocal timbre. For enhanced dubbing synchrony, we introduce a parsing-informed pause selector. Finally, we rerank multiple results based on lip-sync error distance (LSE-D) and character error rate (CER). Our system achieves LSE-D of 10.75 and 12.19 on subset1 and subset2 of DE-EN test sets respectively, superior to last year{'}s best system.\",\n}\n",
    "authors": [
        "Yuang Li",
        "Jiaxin Guo",
        "Min Zhang",
        "Ma Miaomiao",
        "Zhiqiang Rao",
        "Weidong Zhang",
        "Xianghui He",
        "Daimeng Wei",
        "Hao Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.iwslt-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3ade0bbe-c97f-5bd6-a6a3-a37ee8b384ae.pdf",
    "abstract": "Automatic dubbing aims to translate the speech of a video into another language, ensuring the new speech naturally fits the original video. This paper details Huawei Translation Services Center’s (HW-TSC) submission for IWSLT 2024’s automatic dubbing task, under an unconstrained setting. Our system’s machine translation (MT) component utilizes a Transformer-based MT model and an LLM-based post-editor to produce translations of varying lengths. The text-to-speech (TTS) component employs a VITS-based TTS model and a voice cloning module to emulate the original speaker’s vocal timbre. For enhanced dubbing synchrony, we introduce a parsing-informed pause selector. Finally, we rerank multiple results based on lip-sync error distance (LSE-D) and character error rate (CER). Our system achieves LSE-D of 10.75 and 12.19 on subset1 and subset2 of DE-EN test sets respectively, superior to last year’s best system.",
    "num_pages": 5
}