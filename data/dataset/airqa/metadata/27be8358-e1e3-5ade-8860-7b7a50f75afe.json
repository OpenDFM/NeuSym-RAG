{
    "uuid": "27be8358-e1e3-5ade-8860-7b7a50f75afe",
    "title": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{sedova-roth-2023-actc,\n    title = \"{ACTC}: Active Threshold Calibration for Cold-Start Knowledge Graph Completion\",\n    author = \"Sedova, Anastasiia  and\n      Roth, Benjamin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.158\",\n    doi = \"10.18653/v1/2023.acl-short.158\",\n    pages = \"1853--1863\",\n    abstract = \"Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7{\\%} points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4{\\%} points over different budgets.\",\n}\n",
    "authors": [
        "Anastasiia Sedova",
        "Benjamin Roth"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.158.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/27be8358-e1e3-5ade-8860-7b7a50f75afe.pdf",
    "abstract": "Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7% points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4% points over different budgets.",
    "num_pages": 11
}