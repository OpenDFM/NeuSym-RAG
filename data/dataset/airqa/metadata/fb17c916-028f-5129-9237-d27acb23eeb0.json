{
    "uuid": "fb17c916-028f-5129-9237-d27acb23eeb0",
    "title": "Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{jang-etal-2023-figurative,\n    title = \"Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans\",\n    author = \"Jang, Hyewon  and\n      Yu, Qi  and\n      Frassinelli, Diego\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.622\",\n    doi = \"10.18653/v1/2023.findings-acl.622\",\n    pages = \"9816--9832\",\n    abstract = \"Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to provide insights into the inner workings of such models. With our first analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four figurative language types.\",\n}\n",
    "authors": [
        "Hyewon Jang",
        "Qi Yu",
        "Diego Frassinelli"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.622.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fb17c916-028f-5129-9237-d27acb23eeb0.pdf",
    "abstract": "Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to provide insights into the inner workings of such models. With our first analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four figurative language types.",
    "num_pages": 17
}