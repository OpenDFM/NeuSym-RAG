{
    "uuid": "87973b41-eba0-5d80-9a4d-8b015d05504f",
    "title": "Hire a Linguist!: Learning Endangered Languages in LLMs with In-Context Linguistic Descriptions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-hire,\n    title = \"Hire a Linguist!: Learning Endangered Languages in {LLM}s with In-Context Linguistic Descriptions\",\n    author = \"Zhang, Kexun  and\n      Choi, Yee  and\n      Song, Zhenqiao  and\n      He, Taiqi  and\n      Wang, William Yang  and\n      Li, Lei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.925\",\n    doi = \"10.18653/v1/2024.findings-acl.925\",\n    pages = \"15654--15669\",\n    abstract = \"How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LingoLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM{'}s prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LingoLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LingoLLM elevates translation capability from GPT-4{'}s 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages. Our data, code, and model generations will be released to the public. Our data, code, and model generations can be found at \\url{https://github.com/LLiLab/llm4endangeredlang}.\",\n}\n",
    "authors": [
        "Kexun Zhang",
        "Yee Choi",
        "Zhenqiao Song",
        "Taiqi He",
        "William Yang Wang",
        "Lei Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.925.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/87973b41-eba0-5d80-9a4d-8b015d05504f.pdf",
    "abstract": "How can large language models (LLMs) process and translate endangered languages? Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose LingoLLM, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM’s prompt, including a dictionary, a grammar book, and morphologically analyzed input text. We implement LingoLLM on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that LingoLLM elevates translation capability from GPT-4’s 0 to 10.5 BLEU for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages. Our data, code, and model generations will be released to the public. Our data, code, and model generations can be found at https://github.com/LLiLab/llm4endangeredlang.",
    "num_pages": 16
}