{
    "uuid": "8da9f658-7601-52d0-a283-74ef408a9dfb",
    "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2023-contrastive,\n    title = \"Contrastive Decoding: Open-ended Text Generation as Optimization\",\n    author = \"Li, Xiang Lisa  and\n      Holtzman, Ari  and\n      Fried, Daniel  and\n      Liang, Percy  and\n      Eisner, Jason  and\n      Hashimoto, Tatsunori  and\n      Zettlemoyer, Luke  and\n      Lewis, Mike\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.687\",\n    doi = \"10.18653/v1/2023.acl-long.687\",\n    pages = \"12286--12312\",\n    abstract = \"Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.\",\n}\n",
    "authors": [
        "Xiang Lisa Li",
        "Ari Holtzman",
        "Daniel Fried",
        "Percy Liang",
        "Jason Eisner",
        "Tatsunori Hashimoto",
        "Luke Zettlemoyer",
        "Mike Lewis"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.687.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8da9f658-7601-52d0-a283-74ef408a9dfb.pdf",
    "abstract": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.",
    "num_pages": 27
}