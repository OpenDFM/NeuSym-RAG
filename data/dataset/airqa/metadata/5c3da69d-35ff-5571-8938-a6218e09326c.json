{
    "uuid": "5c3da69d-35ff-5571-8938-a6218e09326c",
    "title": "Recyclable Tuning for Continual Pre-training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{qin-etal-2023-recyclable,\n    title = \"Recyclable Tuning for Continual Pre-training\",\n    author = \"Qin, Yujia  and\n      Qian, Cheng  and\n      Han, Xu  and\n      Lin, Yankai  and\n      Wang, Huadong  and\n      Xie, Ruobing  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.723\",\n    doi = \"10.18653/v1/2023.findings-acl.723\",\n    pages = \"11403--11426\",\n    abstract = \"Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance.\",\n}\n",
    "authors": [
        "Yujia Qin",
        "Cheng Qian",
        "Xu Han",
        "Yankai Lin",
        "Huadong Wang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.723.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5c3da69d-35ff-5571-8938-a6218e09326c.pdf",
    "abstract": "Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance.",
    "num_pages": 24
}