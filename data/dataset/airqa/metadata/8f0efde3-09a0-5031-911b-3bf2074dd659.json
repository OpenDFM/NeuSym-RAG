{
    "uuid": "8f0efde3-09a0-5031-911b-3bf2074dd659",
    "title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{dou-etal-2024-stepcoder,\n    title = \"{S}tep{C}oder: Improving Code Generation with Reinforcement Learning from Compiler Feedback\",\n    author = \"Dou, Shihan  and\n      Liu, Yan  and\n      Jia, Haoxiang  and\n      Zhou, Enyu  and\n      Xiong, Limao  and\n      Shan, Junjie  and\n      Huang, Caishuang  and\n      Wang, Xiao  and\n      Fan, Xiaoran  and\n      Xi, Zhiheng  and\n      Zhou, Yuhao  and\n      Ji, Tao  and\n      Zheng, Rui  and\n      Zhang, Qi  and\n      Gui, Tao  and\n      Huang, Xuanjing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.251\",\n    doi = \"10.18653/v1/2024.acl-long.251\",\n    pages = \"4571--4585\",\n    abstract = \"The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce \\textbf{StepCoder}, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.\",\n}\n",
    "authors": [
        "Shihan Dou",
        "Yan Liu",
        "Haoxiang Jia",
        "Enyu Zhou",
        "Limao Xiong",
        "Junjie Shan",
        "Caishuang Huang",
        "Xiao Wang",
        "Xiaoran Fan",
        "Zhiheng Xi",
        "Yuhao Zhou",
        "Tao Ji",
        "Rui Zheng",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.251.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8f0efde3-09a0-5031-911b-3bf2074dd659.pdf",
    "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.",
    "num_pages": 15
}