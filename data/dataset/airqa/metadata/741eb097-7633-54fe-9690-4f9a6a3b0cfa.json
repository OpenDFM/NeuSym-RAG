{
    "uuid": "741eb097-7633-54fe-9690-4f9a6a3b0cfa",
    "title": "MsBERT: A New Model for the Reconstruction of Lacunae in Hebrew Manuscripts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024)",
    "bibtex": "@inproceedings{shmidman-etal-2024-msbert,\n    title = \"{M}s{BERT}: A New Model for the Reconstruction of Lacunae in {H}ebrew Manuscripts\",\n    author = \"Shmidman, Avi  and\n      Shmidman, Ometz  and\n      Gershuni, Hillel  and\n      Koppel, Moshe\",\n    editor = \"Pavlopoulos, John  and\n      Sommerschield, Thea  and\n      Assael, Yannis  and\n      Gordin, Shai  and\n      Cho, Kyunghyun  and\n      Passarotti, Marco  and\n      Sprugnoli, Rachele  and\n      Liu, Yudong  and\n      Li, Bin  and\n      Anderson, Adam\",\n    booktitle = \"Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Hybrid in Bangkok, Thailand and online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.ml4al-1.2\",\n    doi = \"10.18653/v1/2024.ml4al-1.2\",\n    pages = \"13--18\",\n    abstract = \"Hebrew manuscripts preserve thousands of textual transmissions of post-Biblical Hebrew texts from the first millennium. In many cases, the text in the manuscripts is not fully decipherable, whether due to deterioration, perforation, burns, or otherwise. Existing BERT models for Hebrew struggle to fill these gaps, due to the many orthographical deviations found in Hebrew manuscripts. We have pretrained a new dedicated BERT model, dubbed MsBERT (short for: Manuscript BERT), designed from the ground up to handle Hebrew manuscript text. MsBERT substantially outperforms all existing Hebrew BERT models regarding the prediction of missing words in fragmentary Hebrew manuscript transcriptions in multiple genres, as well as regarding the task of differentiating between quoted passages and exegetical elaborations. We provide MsBERT for free download and unrestricted use, and we also provide an interactive and user-friendly website to allow manuscripts scholars to leverage the power of MsBERT in their scholarly work of reconstructing fragmentary Hebrew manuscripts.\",\n}\n",
    "authors": [
        "Avi Shmidman",
        "Ometz Shmidman",
        "Hillel Gershuni",
        "Moshe Koppel"
    ],
    "pdf_url": "https://aclanthology.org/2024.ml4al-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/741eb097-7633-54fe-9690-4f9a6a3b0cfa.pdf",
    "abstract": "Hebrew manuscripts preserve thousands of textual transmissions of post-Biblical Hebrew texts from the first millennium. In many cases, the text in the manuscripts is not fully decipherable, whether due to deterioration, perforation, burns, or otherwise. Existing BERT models for Hebrew struggle to fill these gaps, due to the many orthographical deviations found in Hebrew manuscripts. We have pretrained a new dedicated BERT model, dubbed MsBERT (short for: Manuscript BERT), designed from the ground up to handle Hebrew manuscript text. MsBERT substantially outperforms all existing Hebrew BERT models regarding the prediction of missing words in fragmentary Hebrew manuscript transcriptions in multiple genres, as well as regarding the task of differentiating between quoted passages and exegetical elaborations. We provide MsBERT for free download and unrestricted use, and we also provide an interactive and user-friendly website to allow manuscripts scholars to leverage the power of MsBERT in their scholarly work of reconstructing fragmentary Hebrew manuscripts.",
    "num_pages": 6
}