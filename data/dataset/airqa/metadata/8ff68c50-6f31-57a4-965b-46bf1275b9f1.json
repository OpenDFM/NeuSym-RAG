{
    "uuid": "8ff68c50-6f31-57a4-965b-46bf1275b9f1",
    "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{shi-etal-2024-reslora,\n    title = \"{R}es{L}o{RA}: Identity Residual Mapping in Low-Rank Adaption\",\n    author = \"Shi, Shuhua  and\n      Huang, Shaohan  and\n      Song, Minghui  and\n      Li, Zhoujun  and\n      Zhang, Zihan  and\n      Huang, Haizhen  and\n      Wei, Furu  and\n      Deng, Weiwei  and\n      Sun, Feng  and\n      Zhang, Qi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.525\",\n    doi = \"10.18653/v1/2024.findings-acl.525\",\n    pages = \"8870--8884\",\n    abstract = \"As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at [this url](https://github.com/microsoft/LMOps/tree/main/reslora).\",\n}\n",
    "authors": [
        "Shuhua Shi",
        "Shaohan Huang",
        "Minghui Song",
        "Zhoujun Li",
        "Zihan Zhang",
        "Haizhen Huang",
        "Furu Wei",
        "Weiwei Deng",
        "Feng Sun",
        "Qi Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.525.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8ff68c50-6f31-57a4-965b-46bf1275b9f1.pdf",
    "abstract": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at [this url](https://github.com/microsoft/LMOps/tree/main/reslora).",
    "num_pages": 15
}