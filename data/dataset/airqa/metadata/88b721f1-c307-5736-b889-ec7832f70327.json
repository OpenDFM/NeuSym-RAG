{
    "uuid": "88b721f1-c307-5736-b889-ec7832f70327",
    "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-plad,\n    title = \"{PL}a{D}: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs\",\n    author = \"Zhang, Rongzhi  and\n      Shen, Jiaming  and\n      Liu, Tianqi  and\n      Wang, Haorui  and\n      Qin, Zhen  and\n      Han, Feng  and\n      Liu, Jialu  and\n      Baumgartner, Simon  and\n      Bendersky, Michael  and\n      Zhang, Chao\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.923\",\n    doi = \"10.18653/v1/2024.findings-acl.923\",\n    pages = \"15623--15636\",\n    abstract = \"Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate the student{'}s estimation of sequence likelihood, which steers the student{'}s focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM{'}s internal states, tackles the student{'}s expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.\",\n}\n",
    "authors": [
        "Rongzhi Zhang",
        "Jiaming Shen",
        "Tianqi Liu",
        "Haorui Wang",
        "Zhen Qin",
        "Feng Han",
        "Jialu Liu",
        "Simon Baumgartner",
        "Michael Bendersky",
        "Chao Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.923.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/88b721f1-c307-5736-b889-ec7832f70327.pdf",
    "abstract": "Large Language Models (LLMs) have exhibited impressive capabilities in various tasks, yet their vast parameter sizes restrict their applicability in resource-constrained settings. Knowledge distillation (KD) offers a viable solution by transferring expertise from large teacher models to compact student models. However, traditional KD techniques face specific challenges when applied to LLMs, including restricted access to LLM outputs, significant teacher-student capacity gaps, and the inherited mis-calibration issue. In this work, we present PLaD, a novel preference-based LLM distillation framework. PLaD exploits the teacher-student capacity discrepancy to generate pseudo-preference pairs where teacher outputs are preferred over student outputs. Then, PLaD leverages a ranking loss to re-calibrate the student’s estimation of sequence likelihood, which steers the student’s focus towards understanding the relative quality of outputs instead of simply imitating the teacher. PLaD bypasses the need for access to teacher LLM’s internal states, tackles the student’s expressivity limitations, and mitigates the student mis-calibration issue. Through extensive experiments on two sequence generation tasks and with various LLMs, we demonstrate the effectiveness of our proposed PLaD framework.",
    "num_pages": 14
}