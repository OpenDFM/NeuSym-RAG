{
    "uuid": "e41133e4-5721-50b3-a15b-8ff135b0541e",
    "title": "Context-Aware Transformer Pre-Training for Answer Sentence Selection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{di-liello-etal-2023-context,\n    title = \"Context-Aware Transformer Pre-Training for Answer Sentence Selection\",\n    author = \"Di Liello, Luca  and\n      Garg, Siddhant  and\n      Moschitti, Alessandro\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.40\",\n    doi = \"10.18653/v1/2023.acl-short.40\",\n    pages = \"458--468\",\n    abstract = \"Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large annotated datasets, while using local contextual information around the candidate sentence. In this paper, we propose three pre-training objectives designed to mimic the downstream fine-tuning task of contextual AS2. This allows for specializing LMs when fine-tuning for contextual AS2. Our experiments on three public and two large-scale industrial datasets show that our pre-training approaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2 accuracy by up to 8{\\%} on some datasets.\",\n}\n",
    "authors": [
        "Luca Di Liello",
        "Siddhant Garg",
        "Alessandro Moschitti"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.40.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e41133e4-5721-50b3-a15b-8ff135b0541e.pdf",
    "abstract": "Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large annotated datasets, while using local contextual information around the candidate sentence. In this paper, we propose three pre-training objectives designed to mimic the downstream fine-tuning task of contextual AS2. This allows for specializing LMs when fine-tuning for contextual AS2. Our experiments on three public and two large-scale industrial datasets show that our pre-training approaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2 accuracy by up to 8% on some datasets.",
    "num_pages": 11
}