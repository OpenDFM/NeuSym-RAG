{
    "uuid": "596c2d97-9ac8-5478-a942-937d1c51c7a1",
    "title": "When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{papi-etal-2024-good,\n    title = \"When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in {NLP}\",\n    author = \"Papi, Sara  and\n      Gaido, Marco  and\n      Pilzer, Andrea  and\n      Negri, Matteo\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.200\",\n    doi = \"10.18653/v1/2024.acl-long.200\",\n    pages = \"3657--3672\",\n    abstract = \"Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments with a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research. As countermeasures, we release pangoliNN, a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.\",\n}\n",
    "authors": [
        "Sara Papi",
        "Marco Gaido",
        "Andrea Pilzer",
        "Matteo Negri"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.200.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/596c2d97-9ac8-5478-a942-937d1c51c7a1.pdf",
    "abstract": "Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments with a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research. As countermeasures, we release pangoliNN, a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.",
    "num_pages": 16
}