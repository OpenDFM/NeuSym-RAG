{
    "uuid": "440084b7-345c-5e32-ba63-1da68444269d",
    "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{xia-etal-2024-unlocking,\n    title = \"Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding\",\n    author = \"Xia, Heming  and\n      Yang, Zhe  and\n      Dong, Qingxiu  and\n      Wang, Peiyi  and\n      Li, Yongqi  and\n      Ge, Tao  and\n      Liu, Tianyu  and\n      Li, Wenjie  and\n      Sui, Zhifang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.456\",\n    doi = \"10.18653/v1/2024.findings-acl.456\",\n    pages = \"7655--7671\",\n    abstract = \"To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.\",\n}\n",
    "authors": [
        "Heming Xia",
        "Zhe Yang",
        "Qingxiu Dong",
        "Peiyi Wang",
        "Yongqi Li",
        "Tao Ge",
        "Tianyu Liu",
        "Wenjie Li",
        "Zhifang Sui"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.456.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/440084b7-345c-5e32-ba63-1da68444269d.pdf",
    "abstract": "To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",
    "num_pages": 17
}