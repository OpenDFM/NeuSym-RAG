{
    "uuid": "f0a611d5-a5c6-5967-91cb-338e6b74ffd3",
    "title": "Sparsity-Accelerated Training for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ma-etal-2024-sparsity,\n    title = \"Sparsity-Accelerated Training for Large Language Models\",\n    author = \"Ma, Da  and\n      Chen, Lu  and\n      Wang, Pengyu  and\n      Xu, Hongshen  and\n      Li, Hanqi  and\n      Sun, Liangtai  and\n      Zhu, Su  and\n      Fan, Shuai  and\n      Yu, Kai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.875\",\n    doi = \"10.18653/v1/2024.findings-acl.875\",\n    pages = \"14696--14707\",\n    abstract = \"Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \\textit{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a 45{\\%} throughput improvement in continual pre-training and saves 38{\\%} training time in supervised fine-tuning. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training.\",\n}\n",
    "authors": [
        "Da Ma",
        "Lu Chen",
        "Pengyu Wang",
        "Hongshen Xu",
        "Hanqi Li",
        "Liangtai Sun",
        "Su Zhu",
        "Shuai Fan",
        "Kai Yu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.875.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f0a611d5-a5c6-5967-91cb-338e6b74ffd3.pdf",
    "abstract": "Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging sparsity in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a 45% throughput improvement in continual pre-training and saves 38% training time in supervised fine-tuning. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training.",
    "num_pages": 12
}