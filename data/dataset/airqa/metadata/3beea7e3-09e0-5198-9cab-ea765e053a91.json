{
    "uuid": "3beea7e3-09e0-5198-9cab-ea765e053a91",
    "title": "Language Model Adaption for Reinforcement Learning with Natural Language Action Space",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-language-model,\n    title = \"Language Model Adaption for Reinforcement Learning with Natural Language Action Space\",\n    author = \"Wang, Jiangxing  and\n      Li, Jiachen  and\n      Han, Xiao  and\n      Ye, Deheng  and\n      Lu, Zongqing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.89\",\n    doi = \"10.18653/v1/2024.acl-long.89\",\n    pages = \"1620--1634\",\n    abstract = \"Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the combinatorial nature of the natural language. Previous research leverages pretrained language models to capture action semantics and reduce the size of the action space. However, since pretrained models are typically trained on general corpora, there can be an unpredictable mismatch between the priors encoded in pretrained models and the characteristics of the specific RL environment. To address this issue, we propose Mutual-Information Regularized Policy Optimization, MIPO. MIPO enables implicit and dynamic reduction of the action space. Starting from the prior provided by the pretrained language model, our method dynamically adjusts the prior during the learning process based on the guidance of mutual information regularization. Theoretically, we demonstrate that this policy optimization process leads to the monotonic improvement on the mutual-information regularized RL objective. Empirically, we conduct experiments in various environments and demonstrate the effectiveness of MIPO.\",\n}\n",
    "authors": [
        "Jiangxing Wang",
        "Jiachen Li",
        "Xiao Han",
        "Deheng Ye",
        "Zongqing Lu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.89.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3beea7e3-09e0-5198-9cab-ea765e053a91.pdf",
    "abstract": "Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the combinatorial nature of the natural language. Previous research leverages pretrained language models to capture action semantics and reduce the size of the action space. However, since pretrained models are typically trained on general corpora, there can be an unpredictable mismatch between the priors encoded in pretrained models and the characteristics of the specific RL environment. To address this issue, we propose Mutual-Information Regularized Policy Optimization, MIPO. MIPO enables implicit and dynamic reduction of the action space. Starting from the prior provided by the pretrained language model, our method dynamically adjusts the prior during the learning process based on the guidance of mutual information regularization. Theoretically, we demonstrate that this policy optimization process leads to the monotonic improvement on the mutual-information regularized RL objective. Empirically, we conduct experiments in various environments and demonstrate the effectiveness of MIPO.",
    "num_pages": 15
}