{
    "uuid": "954a1c0e-b7f6-5871-b284-492da7703fc2",
    "title": "MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{feng-etal-2023-mmdialog,\n    title = \"{MMD}ialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation\",\n    author = \"Feng, Jiazhan  and\n      Sun, Qingfeng  and\n      Xu, Can  and\n      Zhao, Pu  and\n      Yang, Yaming  and\n      Tao, Chongyang  and\n      Zhao, Dongyan  and\n      Lin, Qingwei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.405\",\n    doi = \"10.18653/v1/2023.acl-long.405\",\n    pages = \"7348--7363\",\n    abstract = \"Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in \\url{https://github.com/victorsungo/MMDialog}.\",\n}\n",
    "authors": [
        "Jiazhan Feng",
        "Qingfeng Sun",
        "Can Xu",
        "Pu Zhao",
        "Yaming Yang",
        "Chongyang Tao",
        "Dongyan Zhao",
        "Qingwei Lin"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.405.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/954a1c0e-b7f6-5871-b284-492da7703fc2.pdf",
    "abstract": "Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in https://github.com/victorsungo/MMDialog.",
    "num_pages": 16
}