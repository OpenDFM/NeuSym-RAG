{
    "uuid": "9852f10f-98e3-5c47-98c9-c976612ae607",
    "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{tang-etal-2024-medagents,\n    title = \"{M}ed{A}gents: Large Language Models as Collaborators for Zero-shot Medical Reasoning\",\n    author = \"Tang, Xiangru  and\n      Zou, Anni  and\n      Zhang, Zhuosheng  and\n      Li, Ziming  and\n      Zhao, Yilun  and\n      Zhang, Xingyao  and\n      Cohan, Arman  and\n      Gerstein, Mark\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.33\",\n    doi = \"10.18653/v1/2024.findings-acl.33\",\n    pages = \"599--621\",\n    abstract = \"Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.\",\n}\n",
    "authors": [
        "Xiangru Tang",
        "Anni Zou",
        "Zhuosheng Zhang",
        "Ziming Li",
        "Yilun Zhao",
        "Xingyao Zhang",
        "Arman Cohan",
        "Mark Gerstein"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.33.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9852f10f-98e3-5c47-98c9-c976612ae607.pdf",
    "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.",
    "num_pages": 23
}