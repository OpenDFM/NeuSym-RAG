{
    "uuid": "dd35ae4e-5fc3-5335-a590-72739bcf6943",
    "title": "More than Minorities and Majorities: Understanding Multilateral Bias in Language Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhao-etal-2024-minorities,\n    title = \"More than Minorities and Majorities: Understanding Multilateral Bias in Language Generation\",\n    author = \"Zhao, Jiaxu  and\n      Shi, Zijing  and\n      Li, Yitong  and\n      Pei, Yulong  and\n      Chen, Ling  and\n      Fang, Meng  and\n      Pechenizkiy, Mykola\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.594\",\n    doi = \"10.18653/v1/2024.findings-acl.594\",\n    pages = \"9987--10001\",\n    abstract = \"Pretrained models learned from real corpora can often capture undesirable features, leading to bias issues against different demographic groups. Most existing studies on bias dataset construction or bias mitigation methods only focus on one demographic group pair to study a certain bias, e.g. \\textit{black} vs. \\textit{white} for racial bias. However, in real-world applications, there are more than two demographic groups that are at risk of the same bias. In this paper, we propose to analyze and reduce biases across multiple demographic groups. We collect and build a multi-demographic bias dataset including five commonly discussed bias dimensions. To mitigate multi-demographic bias, we adopt several novel debiasing methods, including regularisation-based and augmentation-based methods, as well as appropriate evaluation metrics for multi-demographic bias measurement. Experimental results on the proposed multi-demographic dataset show that a fairer model can be achieved using a multi-demographic debiasing approach. Also, the model debiased using the proposed multi-demographic debiasing methods can better transfer to unseen demographics without sacrificing the performance of the pretrained model.\",\n}\n",
    "authors": [
        "Jiaxu Zhao",
        "Zijing Shi",
        "Yitong Li",
        "Yulong Pei",
        "Ling Chen",
        "Meng Fang",
        "Mykola Pechenizkiy"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.594.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/dd35ae4e-5fc3-5335-a590-72739bcf6943.pdf",
    "abstract": "Pretrained models learned from real corpora can often capture undesirable features, leading to bias issues against different demographic groups. Most existing studies on bias dataset construction or bias mitigation methods only focus on one demographic group pair to study a certain bias, e.g. black vs. white for racial bias. However, in real-world applications, there are more than two demographic groups that are at risk of the same bias. In this paper, we propose to analyze and reduce biases across multiple demographic groups. We collect and build a multi-demographic bias dataset including five commonly discussed bias dimensions. To mitigate multi-demographic bias, we adopt several novel debiasing methods, including regularisation-based and augmentation-based methods, as well as appropriate evaluation metrics for multi-demographic bias measurement. Experimental results on the proposed multi-demographic dataset show that a fairer model can be achieved using a multi-demographic debiasing approach. Also, the model debiased using the proposed multi-demographic debiasing methods can better transfer to unseen demographics without sacrificing the performance of the pretrained model.",
    "num_pages": 15
}