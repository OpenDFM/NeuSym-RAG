{
    "uuid": "dbadedcd-da18-597a-81f9-e157dafcb783",
    "title": "WikiHowQA: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bolotova-baranova-etal-2023-wikihowqa,\n    title = \"{W}iki{H}ow{QA}: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering\",\n    author = \"Bolotova-Baranova, Valeriia  and\n      Blinov, Vladislav  and\n      Filippova, Sofya  and\n      Scholer, Falk  and\n      Sanderson, Mark\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.290\",\n    doi = \"10.18653/v1/2023.acl-long.290\",\n    pages = \"5291--5314\",\n    abstract = \"Answering non-factoid questions (NFQA) is a challenging task, requiring passage-level answers that are difficult to construct and evaluate. Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents. Meanwhile, modern models can generate highly coherent and fluent, but often factually incorrect answers that can deceive even non-expert humans. There is a critical need for high-quality resources for multi-document NFQA (MD-NFQA) to train new models and evaluate answers{'} grounding and factual consistency in relation to supporting documents. To address this gap, we introduce WikiHowQA, a new multi-document NFQA benchmark built on WikiHow, a website dedicated to answering {``}how-to{''} questions. The benchmark includes 11,746 human-written answers along with 74,527 supporting documents. We describe the unique challenges of the resource, provide strong baselines, and propose a novel human evaluation framework that utilizes highlighted relevant supporting passages to mitigate issues such as assessor unfamiliarity with the question topic. All code and data, including the automatic code for preparing the human evaluation, are publicly available.\",\n}\n",
    "authors": [
        "Valeriia Bolotova-Baranova",
        "Vladislav Blinov",
        "Sofya Filippova",
        "Falk Scholer",
        "Mark Sanderson"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.290.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dbadedcd-da18-597a-81f9-e157dafcb783.pdf",
    "abstract": "Answering non-factoid questions (NFQA) is a challenging task, requiring passage-level answers that are difficult to construct and evaluate. Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents. Meanwhile, modern models can generate highly coherent and fluent, but often factually incorrect answers that can deceive even non-expert humans. There is a critical need for high-quality resources for multi-document NFQA (MD-NFQA) to train new models and evaluate answers’ grounding and factual consistency in relation to supporting documents. To address this gap, we introduce WikiHowQA, a new multi-document NFQA benchmark built on WikiHow, a website dedicated to answering “how-to” questions. The benchmark includes 11,746 human-written answers along with 74,527 supporting documents. We describe the unique challenges of the resource, provide strong baselines, and propose a novel human evaluation framework that utilizes highlighted relevant supporting passages to mitigate issues such as assessor unfamiliarity with the question topic. All code and data, including the automatic code for preparing the human evaluation, are publicly available.",
    "num_pages": 24
}