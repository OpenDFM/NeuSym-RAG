{
    "uuid": "c00c795c-293a-5f85-89c3-8f2dd8aa8842",
    "title": "Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lin-etal-2023-enhancing,\n    title = \"Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words\",\n    author = \"Lin, Yang  and\n      Gao, Xin  and\n      Chu, Xu  and\n      Wang, Yasha  and\n      Zhao, Junfeng  and\n      Chen, Chao\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.845\",\n    doi = \"10.18653/v1/2023.findings-acl.845\",\n    pages = \"13361--13377\",\n    abstract = \"Efforts have been made to apply topic seed words to improve the topic interpretability of topic models. However, due to the semantic diversity of natural language, supervisions from seed words could be ambiguous, making it hard to be incorporated into the current neural topic models. In this paper, we propose SeededNTM, a neural topic model enhanced with supervisions from seed words on both word and document levels. We introduce a context-dependency assumption to alleviate the ambiguities with context document information, and an auto-adaptation mechanism to automatically balance between multi-level information. Moreover, an intra-sample consistency regularizer is proposed to deal with noisy supervisions via encouraging perturbation and semantic consistency. Extensive experiments on multiple datasets show that SeededNTM can derive semantically meaningful topics and outperforms the state-of-the-art seeded topic models in terms of topic quality and classification accuracy.\",\n}\n",
    "authors": [
        "Yang Lin",
        "Xin Gao",
        "Xu Chu",
        "Yasha Wang",
        "Junfeng Zhao",
        "Chao Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.845.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c00c795c-293a-5f85-89c3-8f2dd8aa8842.pdf",
    "abstract": "Efforts have been made to apply topic seed words to improve the topic interpretability of topic models. However, due to the semantic diversity of natural language, supervisions from seed words could be ambiguous, making it hard to be incorporated into the current neural topic models. In this paper, we propose SeededNTM, a neural topic model enhanced with supervisions from seed words on both word and document levels. We introduce a context-dependency assumption to alleviate the ambiguities with context document information, and an auto-adaptation mechanism to automatically balance between multi-level information. Moreover, an intra-sample consistency regularizer is proposed to deal with noisy supervisions via encouraging perturbation and semantic consistency. Extensive experiments on multiple datasets show that SeededNTM can derive semantically meaningful topics and outperforms the state-of-the-art seeded topic models in terms of topic quality and classification accuracy.",
    "num_pages": 17
}