{
    "uuid": "c2383bd4-72e1-5f88-86f4-0bbdcccbadfe",
    "title": "KGQA Without Retraining",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{mckenna-sen-2023-kgqa,\n    title = \"{KGQA} Without Retraining\",\n    author = \"Mckenna, Nick  and\n      Sen, Priyanka\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.15\",\n    doi = \"10.18653/v1/2023.sustainlp-1.15\",\n    pages = \"212--218\",\n}\n",
    "authors": [
        "Nick Mckenna",
        "Priyanka Sen"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c2383bd4-72e1-5f88-86f4-0bbdcccbadfe.pdf",
    "abstract": "Popular models for Knowledge Graph Question Answering (KGQA), including semantic parsing and End-to-End (E2E) models, decode into a constrained space of KG relations. Although E2E models accommodate novel entities at test-time, this constraint means they cannot access novel relations, requiring expensive and time-consuming retraining whenever a new relation is added to the KG. We propose KG-Flex, a new architecture for E2E KGQA that instead decodes into a continuous embedding space of relations, which enables use of novel relations at test-time. KG-Flex is the first to support KG updates with entirely novel triples, free of retraining, while still supporting end-to-end training with simple, weak supervision of (Q, A) pairs. Our architecture saves on time, energy, and data resources for retraining, yet we retain performance on standard benchmarks. We further demonstrate zero-shot use of novel relations, achieving up to 82% of baseline hit@1 on three QA datasets. KG-Flex can also fine-tune, requiring significantly shorter time than full retraining; fine-tuning on target data for 10% of full training increases hit@1 to 89-100% of baseline.",
    "num_pages": 7
}