{
    "uuid": "74221fe0-5a9f-52a6-817a-8ef5a7867743",
    "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{anschutz-etal-2023-language,\n    title = \"Language Models for {G}erman Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training\",\n    author = {Ansch{\\\"u}tz, Miriam  and\n      Oehms, Joshua  and\n      Wimmer, Thomas  and\n      Jezierski, Bart{\\l}omiej  and\n      Groh, Georg},\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.74\",\n    doi = \"10.18653/v1/2023.findings-acl.74\",\n    pages = \"1147--1158\",\n    abstract = \"Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.\",\n}\n",
    "authors": [
        "Miriam Anschütz",
        "Joshua Oehms",
        "Thomas Wimmer",
        "Bartłomiej Jezierski",
        "Georg Groh"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.74.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/74221fe0-5a9f-52a6-817a-8ef5a7867743.pdf",
    "abstract": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks.",
    "num_pages": 12
}