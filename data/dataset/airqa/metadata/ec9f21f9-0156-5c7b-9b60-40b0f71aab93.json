{
    "uuid": "ec9f21f9-0156-5c7b-9b60-40b0f71aab93",
    "title": "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{moskvoretskii-etal-2024-taxollama,\n    title = \"{T}axo{LL}a{MA}: {W}ord{N}et-based Model for Solving Multiple Lexical Semantic Tasks\",\n    author = \"Moskvoretskii, Viktor  and\n      Neminova, Ekaterina  and\n      Lobanova, Alina  and\n      Panchenko, Alexander  and\n      Nikishina, Irina\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.127\",\n    doi = \"10.18653/v1/2024.acl-long.127\",\n    pages = \"2331--2350\",\n    abstract = \"In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the {``}all-in-one{''} model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)\",\n}\n",
    "authors": [
        "Viktor Moskvoretskii",
        "Ekaterina Neminova",
        "Alina Lobanova",
        "Alexander Panchenko",
        "Irina Nikishina"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.127.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ec9f21f9-0156-5c7b-9b60-40b0f71aab93.pdf",
    "abstract": "In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the “all-in-one” model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)",
    "num_pages": 20
}