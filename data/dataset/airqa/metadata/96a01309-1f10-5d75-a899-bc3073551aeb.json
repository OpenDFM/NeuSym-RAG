{
    "uuid": "96a01309-1f10-5d75-a899-bc3073551aeb",
    "title": "CuReD: Deep Learning Optical Character Recognition for Cuneiform Text Editions and Legacy Materials",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024)",
    "bibtex": "@inproceedings{gordin-etal-2024-cured,\n    title = \"{C}u{R}e{D}: Deep Learning Optical Character Recognition for Cuneiform Text Editions and Legacy Materials\",\n    author = \"Gordin, Shai  and\n      Alper, Morris  and\n      Romach, Avital  and\n      Saenz Santos, Luis  and\n      Yochai, Naama  and\n      Lalazar, Roey\",\n    editor = \"Pavlopoulos, John  and\n      Sommerschield, Thea  and\n      Assael, Yannis  and\n      Gordin, Shai  and\n      Cho, Kyunghyun  and\n      Passarotti, Marco  and\n      Sprugnoli, Rachele  and\n      Liu, Yudong  and\n      Li, Bin  and\n      Anderson, Adam\",\n    booktitle = \"Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Hybrid in Bangkok, Thailand and online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.ml4al-1.14\",\n    doi = \"10.18653/v1/2024.ml4al-1.14\",\n    pages = \"130--140\",\n    abstract = \"Cuneiform documents, the earliest known form of writing, are prolific textual sources of the ancient past. Experts publish editions of these texts in transliteration using specialized typesetting, but most remain inaccessible for computational analysis in traditional printed books or legacy materials. Off-the-shelf OCR systems are insufficient for digitization without adaptation. We present CuReD (Cuneiform Recognition-Documents), a deep learning-based human-in-the-loop OCR pipeline for digitizing scanned transliterations of cuneiform texts. CuReD has a character error rate of 9{\\%} on clean data and 11{\\%} on representative scans. We digitized a challenging sample of transliterated cuneiform documents, as well as lexical index cards from the University of Pennsylvania Museum, demonstrating the feasibility of our platform for enabling computational analysis and bolstering machine-readable cuneiform text datasets. Our result provide the first human-in-the-loop pipeline and interface for digitizing transliterated cuneiform sources and legacy materials, enabling the enrichment of digital sources of these low-resource languages.\",\n}\n",
    "authors": [
        "Shai Gordin",
        "Morris Alper",
        "Avital Romach",
        "Luis Saenz Santos",
        "Naama Yochai",
        "Roey Lalazar"
    ],
    "pdf_url": "https://aclanthology.org/2024.ml4al-1.14.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/96a01309-1f10-5d75-a899-bc3073551aeb.pdf",
    "abstract": "Cuneiform documents, the earliest known form of writing, are prolific textual sources of the ancient past. Experts publish editions of these texts in transliteration using specialized typesetting, but most remain inaccessible for computational analysis in traditional printed books or legacy materials. Off-the-shelf OCR systems are insufficient for digitization without adaptation. We present CuReD (Cuneiform Recognition-Documents), a deep learning-based human-in-the-loop OCR pipeline for digitizing scanned transliterations of cuneiform texts. CuReD has a character error rate of 9% on clean data and 11% on representative scans. We digitized a challenging sample of transliterated cuneiform documents, as well as lexical index cards from the University of Pennsylvania Museum, demonstrating the feasibility of our platform for enabling computational analysis and bolstering machine-readable cuneiform text datasets. Our result provide the first human-in-the-loop pipeline and interface for digitizing transliterated cuneiform sources and legacy materials, enabling the enrichment of digital sources of these low-resource languages.",
    "num_pages": 11
}