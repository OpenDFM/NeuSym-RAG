{
    "uuid": "e8d44b6e-5c2f-5677-ab19-9bebf1a73ed9",
    "title": "Harnessing Toulmin’s theory for zero-shot argument explication",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{gupta-etal-2024-harnessing,\n    title = \"Harnessing Toulmin{'}s theory for zero-shot argument explication\",\n    author = \"Gupta, Ankita  and\n      Zuckerman, Ethan  and\n      O{'}Connor, Brendan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.552\",\n    doi = \"10.18653/v1/2024.acl-long.552\",\n    pages = \"10259--10276\",\n    abstract = \"To better analyze informal arguments on public forums, we propose the task of argument explication, which makes explicit a text{'}s argumentative structure and implicit reasoning by outputting triples of propositions â¨claim, reason warrantâ©. The three slots, or argument components, are derived from the widely known Toulmin (1958) model of argumentation. While prior research applies Toulmin or related theories to annotate datasets and train supervised models, we develop an effective method to prompt generative large language models (LMs) to output explicitly named argument components proposed by Toulmin by prompting with the theory name (e.g., {`}According to Toulmin model{'}). We evaluate the outputs{'} coverage and validity through a human study and automatic evaluation based on prior argumentation datasets and perform robustness checks over alternative LMs, prompts, and argumentation theories. Finally, we conduct a proof-of-concept case study to extract an interpretable argumentation (hyper)graph from a large corpus of critical public comments on whether to allow the COVID-19 vaccine for children, suggesting future directions for corpus analysis and argument visualization.\",\n}\n",
    "authors": [
        "Ankita Gupta",
        "Ethan Zuckerman",
        "Brendan O’Connor"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.552.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e8d44b6e-5c2f-5677-ab19-9bebf1a73ed9.pdf",
    "abstract": "To better analyze informal arguments on public forums, we propose the task of argument explication, which makes explicit a text’s argumentative structure and implicit reasoning by outputting triples of propositions ⟨claim, reason warrant⟩. The three slots, or argument components, are derived from the widely known Toulmin (1958) model of argumentation. While prior research applies Toulmin or related theories to annotate datasets and train supervised models, we develop an effective method to prompt generative large language models (LMs) to output explicitly named argument components proposed by Toulmin by prompting with the theory name (e.g., ‘According to Toulmin model’). We evaluate the outputs’ coverage and validity through a human study and automatic evaluation based on prior argumentation datasets and perform robustness checks over alternative LMs, prompts, and argumentation theories. Finally, we conduct a proof-of-concept case study to extract an interpretable argumentation (hyper)graph from a large corpus of critical public comments on whether to allow the COVID-19 vaccine for children, suggesting future directions for corpus analysis and argument visualization.",
    "num_pages": 18
}