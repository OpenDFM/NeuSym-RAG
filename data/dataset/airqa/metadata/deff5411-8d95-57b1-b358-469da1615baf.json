{
    "uuid": "deff5411-8d95-57b1-b358-469da1615baf",
    "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ustun-etal-2024-aya,\n    title = \"Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model\",\n    author = {{\\\"U}st{\\\"u}n, Ahmet  and\n      Aryabumi, Viraat  and\n      Yong, Zheng  and\n      Ko, Wei-Yin  and\n      D{'}souza, Daniel  and\n      Onilude, Gbemileke  and\n      Bhandari, Neel  and\n      Singh, Shivalika  and\n      Ooi, Hui-Lee  and\n      Kayid, Amr  and\n      Vargus, Freddie  and\n      Blunsom, Phil  and\n      Longpre, Shayne  and\n      Muennighoff, Niklas  and\n      Fadaee, Marzieh  and\n      Kreutzer, Julia  and\n      Hooker, Sara},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.845\",\n    doi = \"10.18653/v1/2024.acl-long.845\",\n    pages = \"15894--15939\",\n    abstract = \"Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50{\\%} are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages {---}{---} including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.\",\n}\n",
    "authors": [
        "Ahmet Üstün",
        "Viraat Aryabumi",
        "Zheng Yong",
        "Wei-Yin Ko",
        "Daniel D’souza",
        "Gbemileke Onilude",
        "Neel Bhandari",
        "Shivalika Singh",
        "Hui-Lee Ooi",
        "Amr Kayid",
        "Freddie Vargus",
        "Phil Blunsom",
        "Shayne Longpre",
        "Niklas Muennighoff",
        "Marzieh Fadaee",
        "Julia Kreutzer",
        "Sara Hooker"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.845.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/deff5411-8d95-57b1-b358-469da1615baf.pdf",
    "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages —— including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models.",
    "num_pages": 46
}