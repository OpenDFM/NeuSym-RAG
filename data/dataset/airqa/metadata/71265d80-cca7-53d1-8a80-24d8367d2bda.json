{
    "uuid": "71265d80-cca7-53d1-8a80-24d8367d2bda",
    "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{sertkan-etal-2023-ranger,\n    title = \"Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation\",\n    author = {Sertkan, Mete  and\n      Althammer, Sophia  and\n      Hofst{\\\"a}tter, Sebastian},\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.56\",\n    doi = \"10.18653/v1/2023.acl-demo.56\",\n    pages = \"581--587\",\n    abstract = \"In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger{'}s benefits.\",\n}\n",
    "authors": [
        "Mete Sertkan",
        "Sophia Althammer",
        "Sebastian Hofstätter"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.56.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/71265d80-cca7-53d1-8a80-24d8367d2bda.pdf",
    "abstract": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger’s benefits.",
    "num_pages": 7
}