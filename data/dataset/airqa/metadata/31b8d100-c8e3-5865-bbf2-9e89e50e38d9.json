{
    "uuid": "31b8d100-c8e3-5865-bbf2-9e89e50e38d9",
    "title": "LinguaLinked: Distributed Large Language Model Inference on Mobile Devices",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{zhao-etal-2024-lingualinked,\n    title = \"{L}ingua{L}inked: Distributed Large Language Model Inference on Mobile Devices\",\n    author = \"Zhao, Junchen  and\n      Song, Yurun  and\n      Simenl3@uci.edu, Simenl3@uci.edu  and\n      Harris, Ian  and\n      Abdu Jyothi, Sangeetha\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.16\",\n    doi = \"10.18653/v1/2024.acl-demos.16\",\n    pages = \"160--171\",\n    abstract = \"Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices and ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each deviceï¿½s capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the systemï¿½s overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices.\",\n}\n",
    "authors": [
        "Junchen Zhao",
        "Yurun Song",
        "Simenl3@uci.edu Simenl3@uci.edu",
        "Ian Harris",
        "Sangeetha Abdu Jyothi"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/31b8d100-c8e3-5865-bbf2-9e89e50e38d9.pdf",
    "abstract": "Deploying Large Language Models (LLMs) locally on mobile devices presents a significant challenge due to their extensive memory requirements. In this paper, we introduce LinguaLinked, a system for decentralized, distributed LLM inference on mobile devices. LinguaLinked enables collaborative execution of the inference task across multiple trusted devices and ensures data privacy by processing information locally. LinguaLinked uses three key strategies. First, an optimized model assignment technique segments LLMs and uses linear optimization to align segments with each device�s capabilities. Second, an optimized data transmission mechanism ensures efficient and structured data flow between model segments while also maintaining the integrity of the original model structure. Finally, LinguaLinked incorporates a runtime load balancer that actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing the system�s overall efficiency and responsiveness. We demonstrate that LinguaLinked facilitates efficient LLM inference while maintaining consistent throughput and minimal latency through extensive testing across various mobile devices, from high-end to low-end Android devices.",
    "num_pages": 12
}