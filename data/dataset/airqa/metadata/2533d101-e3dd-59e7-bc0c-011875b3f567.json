{
    "uuid": "2533d101-e3dd-59e7-bc0c-011875b3f567",
    "title": "UniCOQE: Unified Comparative Opinion Quintuple Extraction As A Set",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yang-etal-2023-unicoqe,\n    title = \"{U}ni{COQE}: Unified Comparative Opinion Quintuple Extraction As A Set\",\n    author = \"Yang, Zinong  and\n      Xu, Feng  and\n      Yu, Jianfei  and\n      Xia, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.775\",\n    doi = \"10.18653/v1/2023.findings-acl.775\",\n    pages = \"12229--12240\",\n    abstract = \"Comparative Opinion Quintuple Extraction (COQE) aims to identify comparative opinion sentences in product reviews, extract comparative opinion elements in the sentences, and then incorporate them into quintuples. Existing methods decompose the COQE task into multiple primary subtasks and then solve them in a pipeline manner. However, these approaches ignore the intrinsic connection between subtasks and the error propagation among stages. This paper proposes a unified generative model, UniCOQE, to solve the COQE task in one shot. We design a generative template where all the comparative tuples are concatenated as the target output sequence. However, the multiple tuples are inherently not an ordered sequence but an unordered set. The pre-defined order will force the generative model to learn a false order bias and hinge the model{'}s training. To alleviate this bias, we introduce a new {``}predict-and-assign{''} training paradigm that models the golden tuples as a set. Specifically, we utilize a set-matching strategy to find the optimal order of tuples. The experimental results on multiple benchmarks show that our unified generative model significantly outperforms the SOTA method, and ablation experiments prove the effectiveness of the set-matching strategy.\",\n}\n",
    "authors": [
        "Zinong Yang",
        "Feng Xu",
        "Jianfei Yu",
        "Rui Xia"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.775.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2533d101-e3dd-59e7-bc0c-011875b3f567.pdf",
    "abstract": "Comparative Opinion Quintuple Extraction (COQE) aims to identify comparative opinion sentences in product reviews, extract comparative opinion elements in the sentences, and then incorporate them into quintuples. Existing methods decompose the COQE task into multiple primary subtasks and then solve them in a pipeline manner. However, these approaches ignore the intrinsic connection between subtasks and the error propagation among stages. This paper proposes a unified generative model, UniCOQE, to solve the COQE task in one shot. We design a generative template where all the comparative tuples are concatenated as the target output sequence. However, the multiple tuples are inherently not an ordered sequence but an unordered set. The pre-defined order will force the generative model to learn a false order bias and hinge the model’s training. To alleviate this bias, we introduce a new “predict-and-assign” training paradigm that models the golden tuples as a set. Specifically, we utilize a set-matching strategy to find the optimal order of tuples. The experimental results on multiple benchmarks show that our unified generative model significantly outperforms the SOTA method, and ablation experiments prove the effectiveness of the set-matching strategy.",
    "num_pages": 12
}