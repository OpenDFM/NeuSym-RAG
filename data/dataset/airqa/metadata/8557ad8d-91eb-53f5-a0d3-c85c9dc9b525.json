{
    "uuid": "8557ad8d-91eb-53f5-a0d3-c85c9dc9b525",
    "title": "StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cong-etal-2024-styledubber,\n    title = \"{S}tyle{D}ubber: Towards Multi-Scale Style Learning for Movie Dubbing\",\n    author = \"Cong, Gaoxiang  and\n      Qi, Yuankai  and\n      Li, Liang  and\n      Beheshti, Amin  and\n      Zhang, Zhedong  and\n      Hengel, Anton  and\n      Yang, Ming-Hsuan  and\n      Yan, Chenggang  and\n      Huang, Qingming\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.404\",\n    doi = \"10.18653/v1/2024.findings-acl.404\",\n    pages = \"6767--6779\",\n    abstract = \"Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync. Extensive experiments on two of the primary benchmarks, V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art. The code will be made available at https://github.com/GalaxyCong/StyleDubber.\",\n}\n",
    "authors": [
        "Gaoxiang Cong",
        "Yuankai Qi",
        "Liang Li",
        "Amin Beheshti",
        "Zhedong Zhang",
        "Anton Hengel",
        "Ming-Hsuan Yang",
        "Chenggang Yan",
        "Qingming Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.404.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8557ad8d-91eb-53f5-a0d3-c85c9dc9b525.pdf",
    "abstract": "Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync. Extensive experiments on two of the primary benchmarks, V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art. The code will be made available at https://github.com/GalaxyCong/StyleDubber.",
    "num_pages": 13
}