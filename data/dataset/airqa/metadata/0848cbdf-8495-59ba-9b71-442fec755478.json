{
    "uuid": "0848cbdf-8495-59ba-9b71-442fec755478",
    "title": "Integrating Table Representations into Large Language Models for Improved Scholarly Document Comprehension",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)",
    "bibtex": "@inproceedings{korkmaz-del-rio-chanona-2024-integrating,\n    title = \"Integrating Table Representations into Large Language Models for Improved Scholarly Document Comprehension\",\n    author = \"Korkmaz, Buse Sibel  and\n      Del Rio Chanona, Antonio\",\n    editor = \"Ghosal, Tirthankar  and\n      Singh, Amanpreet  and\n      Waard, Anita  and\n      Mayr, Philipp  and\n      Naik, Aakanksha  and\n      Weller, Orion  and\n      Lee, Yoonjoo  and\n      Shen, Shannon  and\n      Qin, Yanxia\",\n    booktitle = \"Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sdp-1.28\",\n    pages = \"293--306\",\n    abstract = \"We address the challenge of interpreting and reasoning over scientific tables with Large Language Models (LLMs), a crucial aspect of scholarly documents. Despite significant progress in natural language processing, the integration of tabular data into scientific LLMs remains limited. We propose an innovative approach leveraging intermediate task pre-training on table question-answering datasets, followed by model adaptation to comprehend tables in computer science literature. Our findings reveal that incorporating table understanding substantially improves the performance of LLMs on scientific literature understanding tasks, which we showcase in peer-review score prediction. This improvement underscores the importance of utilizing tabular data in the training of scientific language models. The code and models are publicly available at [this link](https://github.com/buseskorkmaz/Integrating-Table-Representations-into-LLMs).\",\n}\n",
    "authors": [
        "Buse Sibel Korkmaz",
        "Antonio Del Rio Chanona"
    ],
    "pdf_url": "https://aclanthology.org/2024.sdp-1.28.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0848cbdf-8495-59ba-9b71-442fec755478.pdf",
    "abstract": "We address the challenge of interpreting and reasoning over scientific tables with Large Language Models (LLMs), a crucial aspect of scholarly documents. Despite significant progress in natural language processing, the integration of tabular data into scientific LLMs remains limited. We propose an innovative approach leveraging intermediate task pre-training on table question-answering datasets, followed by model adaptation to comprehend tables in computer science literature. Our findings reveal that incorporating table understanding substantially improves the performance of LLMs on scientific literature understanding tasks, which we showcase in peer-review score prediction. This improvement underscores the importance of utilizing tabular data in the training of scientific language models. The code and models are publicly available at [this link](https://github.com/buseskorkmaz/Integrating-Table-Representations-into-LLMs).",
    "num_pages": 14
}