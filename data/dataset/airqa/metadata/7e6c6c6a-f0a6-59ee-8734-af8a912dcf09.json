{
    "uuid": "7e6c6c6a-f0a6-59ee-8734-af8a912dcf09",
    "title": "Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2023-connective,\n    title = \"Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation\",\n    author = \"Wu, Hongyi  and\n      Zhou, Hao  and\n      Lan, Man  and\n      Wu, Yuanbin  and\n      Zhang, Yadong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.325\",\n    doi = \"10.18653/v1/2023.acl-long.325\",\n    pages = \"5908--5923\",\n    abstract = \"Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance.\",\n}\n",
    "authors": [
        "Hongyi Wu",
        "Hao Zhou",
        "Man Lan",
        "Yuanbin Wu",
        "Yadong Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.325.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7e6c6c6a-f0a6-59ee-8734-af8a912dcf09.pdf",
    "abstract": "Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance.",
    "num_pages": 16
}