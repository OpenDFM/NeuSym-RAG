{
    "uuid": "15073f20-80bf-52c6-841b-93478da3cbd7",
    "title": "On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{xiao-etal-2023-isotropy,\n    title = \"On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning\",\n    author = \"Xiao, Chenghao  and\n      Long, Yang  and\n      Al Moubayed, Noura\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.778\",\n    doi = \"10.18653/v1/2023.findings-acl.778\",\n    pages = \"12266--12283\",\n    abstract = \"Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as {``}spurious contextualization{''} is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamics with different training temperatures, batch sizes and pooling methods.\",\n}\n",
    "authors": [
        "Chenghao Xiao",
        "Yang Long",
        "Noura Al Moubayed"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.778.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/15073f20-80bf-52c6-841b-93478da3cbd7.pdf",
    "abstract": "Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as “spurious contextualization” is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamics with different training temperatures, batch sizes and pooling methods.",
    "num_pages": 18
}