{
    "uuid": "6f371107-e717-5266-8be1-69ef4f6b1f83",
    "title": "Class-Incremental Learning based on Label Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{shao-etal-2023-class,\n    title = \"Class-Incremental Learning based on Label Generation\",\n    author = \"Shao, Yijia  and\n      Guo, Yiduo  and\n      Zhao, Dongyan  and\n      Liu, Bing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.109\",\n    doi = \"10.18653/v1/2023.acl-short.109\",\n    pages = \"1263--1276\",\n    abstract = \"Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.\",\n}\n",
    "authors": [
        "Yijia Shao",
        "Yiduo Guo",
        "Dongyan Zhao",
        "Bing Liu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.109.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6f371107-e717-5266-8be1-69ef4f6b1f83.pdf",
    "abstract": "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.",
    "num_pages": 14
}