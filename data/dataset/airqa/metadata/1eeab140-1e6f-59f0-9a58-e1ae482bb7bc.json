{
    "uuid": "1eeab140-1e6f-59f0-9a58-e1ae482bb7bc",
    "title": "Aligning Large Language Models via Fine-grained Supervision",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{xu-etal-2024-aligning,\n    title = \"Aligning Large Language Models via Fine-grained Supervision\",\n    author = \"Xu, Dehong  and\n      Qiu, Liang  and\n      Kim, Minseok  and\n      Ladhak, Faisal  and\n      Do, Jaeyoung\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.62\",\n    doi = \"10.18653/v1/2024.acl-short.62\",\n    pages = \"673--680\",\n    abstract = \"Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can improve LLM performance by up to 5.1{\\%} in terms of win rate against the reference model, compared with the traditional PPO model.\",\n}\n",
    "authors": [
        "Dehong Xu",
        "Liang Qiu",
        "Minseok Kim",
        "Faisal Ladhak",
        "Jaeyoung Do"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.62.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1eeab140-1e6f-59f0-9a58-e1ae482bb7bc.pdf",
    "abstract": "Pre-trained large-scale language models (LLMs) excel at producing coherent articles, yet their outputs may be untruthful, toxic, or fail to align with user expectations. Current approaches focus on using reinforcement learning with human feedback (RLHF) to improve model alignment, which works by transforming coarse human preferences of LLM outputs into a feedback signal that guides the model learning process. However, because this approach operates on sequence-level feedback, it lacks the precision to identify the exact parts of the output affecting user preferences. To address this gap, we propose a method to enhance LLM alignment through fine-grained token-level supervision. Specifically, we ask annotators to minimally edit less preferred responses within the standard reward modeling dataset to make them more favorable, ensuring changes are made only where necessary while retaining most of the original content. The refined dataset is used to train a token-level reward model, which is then used for training our fine-grained Proximal Policy Optimization (PPO) model. Our experiment results demonstrate that this approach can improve LLM performance by up to 5.1% in terms of win rate against the reference model, compared with the traditional PPO model.",
    "num_pages": 8
}