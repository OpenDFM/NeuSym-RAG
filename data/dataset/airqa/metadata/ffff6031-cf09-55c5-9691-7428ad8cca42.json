{
    "uuid": "ffff6031-cf09-55c5-9691-7428ad8cca42",
    "title": "MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{murugesan-etal-2023-mismatch,\n    title = \"{MISMATCH}: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types\",\n    author = \"Murugesan, Keerthiram  and\n      Swaminathan, Sarathkrishna  and\n      Dan, Soham  and\n      Chaudhury, Subhajit  and\n      Gunasekara, Chulaka  and\n      Crouse, Maxwell  and\n      Mahajan, Diwakar  and\n      Abdelaziz, Ibrahim  and\n      Fokoue, Achille  and\n      Kapanipathi, Pavan  and\n      Roukos, Salim  and\n      Gray, Alexander\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.274\",\n    doi = \"10.18653/v1/2023.findings-acl.274\",\n    pages = \"4485--4503\",\n    abstract = \"With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated text captured by the existing metrics. In this work, we propose a new evaluation scheme to model human judgments in 7 NLP tasks, based on the fine-grained mismatches between a pair of texts. Inspired by the recent efforts in several NLP tasks for fine-grained evaluation, we introduce a set of 13 mismatch error types such as spatial/geographic errors, entity errors, etc, to guide the model for better prediction of human judgments. We propose a neural framework for evaluating machine texts that uses these mismatch error types as auxiliary tasks and re-purposes the existing single-number evaluation metrics as additional scalar features, in addition to textual features extracted from the machine and reference texts. Our experiments reveal key insights about the existing metrics via the mismatch errors. We show that the mismatch errors between the sentence pairs on the held-out datasets from 7 NLP tasks align well with the human evaluation.\",\n}\n",
    "authors": [
        "Keerthiram Murugesan",
        "Sarathkrishna Swaminathan",
        "Soham Dan",
        "Subhajit Chaudhury",
        "Chulaka Gunasekara",
        "Maxwell Crouse",
        "Diwakar Mahajan",
        "Ibrahim Abdelaziz",
        "Achille Fokoue",
        "Pavan Kapanipathi",
        "Salim Roukos",
        "Alexander Gray"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.274.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ffff6031-cf09-55c5-9691-7428ad8cca42.pdf",
    "abstract": "With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated text captured by the existing metrics. In this work, we propose a new evaluation scheme to model human judgments in 7 NLP tasks, based on the fine-grained mismatches between a pair of texts. Inspired by the recent efforts in several NLP tasks for fine-grained evaluation, we introduce a set of 13 mismatch error types such as spatial/geographic errors, entity errors, etc, to guide the model for better prediction of human judgments. We propose a neural framework for evaluating machine texts that uses these mismatch error types as auxiliary tasks and re-purposes the existing single-number evaluation metrics as additional scalar features, in addition to textual features extracted from the machine and reference texts. Our experiments reveal key insights about the existing metrics via the mismatch errors. We show that the mismatch errors between the sentence pairs on the held-out datasets from 7 NLP tasks align well with the human evaluation.",
    "num_pages": 19
}