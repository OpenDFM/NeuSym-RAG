{
    "uuid": "feca8e3a-0787-5af1-948c-901bbf03eb51",
    "title": "The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-impact,\n    title = \"The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis\",\n    author = \"Zhang, Miaoran  and\n      Gautam, Vagrant  and\n      Wang, Mingyang  and\n      Alabi, Jesujoba  and\n      Shen, Xiaoyu  and\n      Klakow, Dietrich  and\n      Mosbach, Marius\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.438\",\n    doi = \"10.18653/v1/2024.findings-acl.438\",\n    pages = \"7342--7371\",\n    abstract = \"In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.\",\n}\n",
    "authors": [
        "Miaoran Zhang",
        "Vagrant Gautam",
        "Mingyang Wang",
        "Jesujoba Alabi",
        "Xiaoyu Shen",
        "Dietrich Klakow",
        "Marius Mosbach"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.438.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/feca8e3a-0787-5af1-948c-901bbf03eb51.pdf",
    "abstract": "In-context learning is a popular inference strategy where large language models solve a task using only a few labeled demonstrations without needing any parameter updates. Although there have been extensive studies on English in-context learning, multilingual in-context learning remains under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that strong instruction-following models including Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.",
    "num_pages": 30
}