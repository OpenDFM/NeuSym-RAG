{
    "uuid": "d6b62ed4-0989-5067-a64d-43851cc191c7",
    "title": "Generating Dialog Responses with Specified Grammatical Items for Second Language Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{okano-etal-2023-generating,\n    title = \"Generating Dialog Responses with Specified Grammatical Items for Second Language Learning\",\n    author = \"Okano, Yuki  and\n      Funakoshi, Kotaro  and\n      Nagata, Ryo  and\n      Okumura, Manabu\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.16\",\n    doi = \"10.18653/v1/2023.bea-1.16\",\n    pages = \"184--194\",\n    abstract = \"This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement learning, we examine combinations of three reward functions that consider grammatical items, diversity, and fluency. Our experiments confirm that both approaches can generate responses including the specified grammatical items and that it is crucial to consider fluency rather than diversity as the reward function.\",\n}\n",
    "authors": [
        "Yuki Okano",
        "Kotaro Funakoshi",
        "Ryo Nagata",
        "Manabu Okumura"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d6b62ed4-0989-5067-a64d-43851cc191c7.pdf",
    "abstract": "This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement learning, we examine combinations of three reward functions that consider grammatical items, diversity, and fluency. Our experiments confirm that both approaches can generate responses including the specified grammatical items and that it is crucial to consider fluency rather than diversity as the reward function.",
    "num_pages": 11
}