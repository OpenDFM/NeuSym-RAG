{
    "uuid": "d4f85224-4645-59aa-8350-4be3d859e2b6",
    "title": "Improving Neural Machine Translation Formality Control with Domain Adaptation and Reranking-based Transductive Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{wu-etal-2023-improving,\n    title = \"Improving Neural Machine Translation Formality Control with Domain Adaptation and Reranking-based Transductive Learning\",\n    author = \"Wu, Zhanglin  and\n      Li, Zongyao  and\n      Wei, Daimeng  and\n      Shang, Hengchao  and\n      Guo, Jiaxin  and\n      Chen, Xiaoyu  and\n      Rao, Zhiqiang  and\n      Yu, Zhengzhe  and\n      Yang, Jinlong  and\n      Li, Shaojun  and\n      Xie, Yuhao  and\n      Wei, Bin  and\n      Zheng, Jiawei  and\n      Zhu, Ming  and\n      Lei, Lizhi  and\n      Yang, Hao  and\n      Jiang, Yanfei\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.13\",\n    doi = \"10.18653/v1/2023.iwslt-1.13\",\n    pages = \"180--186\",\n    abstract = \"This paper presents Huawei Translation Service Center (HW-TSC){'}s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets constrained and unconstrained conditions. We train the formality control models for these four language pairs under these two conditions respectively, and submit the corresponding translation results. Our efforts are divided into two fronts: enhancing general translation quality and improving formality control capability. According to the different requirements of the formality control task, we use a multi-stage pre-training method to train a bilingual or multilingual neural machine translation (NMT) model as the basic model, which can improve the general translation quality of the base model to a relatively high level. Then, under the premise of affecting the general translation quality of the basic model as little as possible, we adopt domain adaptation and reranking-based transductive learning methods to improve the formality control capability of the model.\",\n}\n",
    "authors": [
        "Zhanglin Wu",
        "Zongyao Li",
        "Daimeng Wei",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Xiaoyu Chen",
        "Zhiqiang Rao",
        "Zhengzhe Yu",
        "Jinlong Yang",
        "Shaojun Li",
        "Yuhao Xie",
        "Bin Wei",
        "Jiawei Zheng",
        "Ming Zhu",
        "Lizhi Lei",
        "Hao Yang",
        "Yanfei Jiang"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d4f85224-4645-59aa-8350-4be3d859e2b6.pdf",
    "abstract": "This paper presents Huawei Translation Service Center (HW-TSC)â€™s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets constrained and unconstrained conditions. We train the formality control models for these four language pairs under these two conditions respectively, and submit the corresponding translation results. Our efforts are divided into two fronts: enhancing general translation quality and improving formality control capability. According to the different requirements of the formality control task, we use a multi-stage pre-training method to train a bilingual or multilingual neural machine translation (NMT) model as the basic model, which can improve the general translation quality of the base model to a relatively high level. Then, under the premise of affecting the general translation quality of the basic model as little as possible, we adopt domain adaptation and reranking-based transductive learning methods to improve the formality control capability of the model.",
    "num_pages": 7
}