{
    "uuid": "b81f177f-efc1-5aad-b02f-85b9eb83d0f5",
    "title": "I already said that! Degenerating redundant questions in open-domain dialogue systems.",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{mai-carson-berndsen-2023-already,\n    title = \"{I} already said that! Degenerating redundant questions in open-domain dialogue systems.\",\n    author = \"Mai, Long  and\n      Carson-berndsen, Julie\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.33\",\n    doi = \"10.18653/v1/2023.acl-srw.33\",\n    pages = \"226--236\",\n    abstract = \"Neural text generation models have achieved remarkable success in carrying on short open-domain conversations. However, their performance degrades significantly in the long term, especially in their ability to ask coherent questions. A significant issue is the generation of redundant questions where the answer has already been provided by the user. We adapt and evaluate different methods, including negative training, decoding, and classification, to mitigate the redundancy problem. We also propose a simple yet effective method for generating training data without the need for crowdsourcing human-human or human-bot conversations. Experiments with the BlenderBot model show that our combined method significantly reduces the rate of redundant questions from 27.2{\\%} to 8.7{\\%}, while improving the quality of the original model. The code, dataset, and trained models can be found at our repository.\",\n}\n",
    "authors": [
        "Long Mai",
        "Julie Carson-berndsen"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.33.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b81f177f-efc1-5aad-b02f-85b9eb83d0f5.pdf",
    "abstract": "Neural text generation models have achieved remarkable success in carrying on short open-domain conversations. However, their performance degrades significantly in the long term, especially in their ability to ask coherent questions. A significant issue is the generation of redundant questions where the answer has already been provided by the user. We adapt and evaluate different methods, including negative training, decoding, and classification, to mitigate the redundancy problem. We also propose a simple yet effective method for generating training data without the need for crowdsourcing human-human or human-bot conversations. Experiments with the BlenderBot model show that our combined method significantly reduces the rate of redundant questions from 27.2% to 8.7%, while improving the quality of the original model. The code, dataset, and trained models can be found at our repository.",
    "num_pages": 11
}