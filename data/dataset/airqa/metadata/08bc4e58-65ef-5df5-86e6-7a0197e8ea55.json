{
    "uuid": "08bc4e58-65ef-5df5-86e6-7a0197e8ea55",
    "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{dai-etal-2024-deepseekmoe,\n    title = \"{D}eep{S}eek{M}o{E}: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\",\n    author = \"Dai, Damai  and\n      Deng, Chengqi  and\n      Zhao, Chenggang  and\n      Xu, R.x.  and\n      Gao, Huazuo  and\n      Chen, Deli  and\n      Li, Jiashi  and\n      Zeng, Wangding  and\n      Yu, Xingkai  and\n      Wu, Y.  and\n      Xie, Zhenda  and\n      Li, Y.k.  and\n      Huang, Panpan  and\n      Luo, Fuli  and\n      Ruan, Chong  and\n      Sui, Zhifang  and\n      Liang, Wenfeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.70\",\n    doi = \"10.18653/v1/2024.acl-long.70\",\n    pages = \"1280--1297\",\n    abstract = \"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 $\\times$ expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which sets the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with DeepSeek 7B and LLaMA2 7B, with only about 40{\\%} of computations.\",\n}\n",
    "authors": [
        "Damai Dai",
        "Chengqi Deng",
        "Chenggang Zhao",
        "R.x. Xu",
        "Huazuo Gao",
        "Deli Chen",
        "Jiashi Li",
        "Wangding Zeng",
        "Xingkai Yu",
        "Y. Wu",
        "Zhenda Xie",
        "Y.k. Li",
        "Panpan Huang",
        "Fuli Luo",
        "Chong Ruan",
        "Zhifang Sui",
        "Wenfeng Liang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.70.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/08bc4e58-65ef-5df5-86e6-7a0197e8ea55.pdf",
    "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating Ks experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 Ã— expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which sets the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with DeepSeek 7B and LLaMA2 7B, with only about 40% of computations.",
    "num_pages": 18
}