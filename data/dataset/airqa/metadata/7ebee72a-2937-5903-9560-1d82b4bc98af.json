{
    "uuid": "7ebee72a-2937-5903-9560-1d82b4bc98af",
    "title": "How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhang-etal-2023-many,\n    title = \"How Many Answers Should {I} Give? An Empirical Study of Multi-Answer Reading Comprehension\",\n    author = \"Zhang, Chen  and\n      Lin, Jiuheng  and\n      Liu, Xiao  and\n      Lai, Yuxuan  and\n      Feng, Yansong  and\n      Zhao, Dongyan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.359\",\n    doi = \"10.18653/v1/2023.findings-acl.359\",\n    pages = \"5811--5827\",\n    abstract = \"The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this phenomenon arises and how to better address it. In this work, we design a taxonomy to categorize commonly-seen multi-answer MRC instances, with which we inspect three multi-answer datasets and analyze where the multi-answer challenge comes from. We further analyze how well different paradigms of current multi-answer MRC models deal with different types of multi-answer instances. We find that some paradigms capture well the key information in the questions while others better model the relation between questions and contexts. We thus explore strategies to make the best of the strengths of different paradigms. Experiments show that generation models can be a promising platform to incorporate different paradigms. Our annotations and code are released for further research.\",\n}\n",
    "authors": [
        "Chen Zhang",
        "Jiuheng Lin",
        "Xiao Liu",
        "Yuxuan Lai",
        "Yansong Feng",
        "Dongyan Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.359.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7ebee72a-2937-5903-9560-1d82b4bc98af.pdf",
    "abstract": "The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this phenomenon arises and how to better address it. In this work, we design a taxonomy to categorize commonly-seen multi-answer MRC instances, with which we inspect three multi-answer datasets and analyze where the multi-answer challenge comes from. We further analyze how well different paradigms of current multi-answer MRC models deal with different types of multi-answer instances. We find that some paradigms capture well the key information in the questions while others better model the relation between questions and contexts. We thus explore strategies to make the best of the strengths of different paradigms. Experiments show that generation models can be a promising platform to incorporate different paradigms. Our annotations and code are released for further research.",
    "num_pages": 17
}