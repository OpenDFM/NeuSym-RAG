{
    "uuid": "6435f055-a064-504a-b636-d3c71c51a6e8",
    "title": "Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{kumar-etal-2024-ranking,\n    title = \"Ranking Entities along Conceptual Space Dimensions with {LLM}s: An Analysis of Fine-Tuning Strategies\",\n    author = \"Kumar, Nitesh  and\n      Chatterjee, Usashi  and\n      Schockaert, Steven\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.474\",\n    doi = \"10.18653/v1/2024.findings-acl.474\",\n    pages = \"7974--7989\",\n    abstract = \"Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results.\",\n}\n",
    "authors": [
        "Nitesh Kumar",
        "Usashi Chatterjee",
        "Steven Schockaert"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.474.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6435f055-a064-504a-b636-d3c71c51a6e8.pdf",
    "abstract": "Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy, but existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having at least some perceptual and subjective features in the training data seems essential for achieving the best results.",
    "num_pages": 16
}