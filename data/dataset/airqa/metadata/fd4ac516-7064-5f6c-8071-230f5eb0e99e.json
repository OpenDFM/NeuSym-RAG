{
    "uuid": "fd4ac516-7064-5f6c-8071-230f5eb0e99e",
    "title": "QCon at SemEval-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{feely-etal-2023-qcon,\n    title = \"{QC}on at {S}em{E}val-2023 Task 10: Data Augmentation and Model Ensembling for Detection of Online Sexism\",\n    author = \"Feely, Weston  and\n      Gupta, Prabhakar  and\n      Mohanty, Manas Ranjan  and\n      Chon, Timothy  and\n      Kundu, Tuhin  and\n      Singh, Vijit  and\n      Atluri, Sandeep  and\n      Roosta, Tanya  and\n      Ghaderi, Viviane  and\n      Schulam, Peter\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.175\",\n    doi = \"10.18653/v1/2023.semeval-1.175\",\n    pages = \"1260--1270\",\n    abstract = \"The web contains an abundance of user- generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F1 score of 0.84 for the binary classification task aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.\",\n}\n",
    "authors": [
        "Weston Feely",
        "Prabhakar Gupta",
        "Manas Ranjan Mohanty",
        "Timothy Chon",
        "Tuhin Kundu",
        "Vijit Singh",
        "Sandeep Atluri",
        "Tanya Roosta",
        "Viviane Ghaderi",
        "Peter Schulam"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.175.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fd4ac516-7064-5f6c-8071-230f5eb0e99e.pdf",
    "abstract": "The web contains an abundance of user- generated content. While this content is useful for many applications, it poses many challenges due to the presence of offensive, biased, and overall toxic language. In this work, we present a system that identifies and classifies sexist content at different levels of granularity. Using transformer-based models, we explore the value of data augmentation, use of ensemble methods, and leverage in-context learning using foundation models to tackle the task. We evaluate the different components of our system both quantitatively and qualitatively. Our best systems achieve an F1 score of 0.84 for the binary classification task aiming to identify whether a given content is sexist or not and 0.64 and 0.47 for the two multi-class tasks that aim to identify the coarse and fine-grained types of sexism present in the given content respectively.",
    "num_pages": 11
}