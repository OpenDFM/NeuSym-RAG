{
    "uuid": "46e5462e-c4a7-57c3-9274-1ea354dafb66",
    "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-enhancing-multilingual,\n    title = \"Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages\",\n    author = \"Zhang, Yuanchi  and\n      Wang, Yile  and\n      Liu, Zijun  and\n      Wang, Shuo  and\n      Wang, Xiaolong  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Liu, Yang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.603\",\n    doi = \"10.18653/v1/2024.acl-long.603\",\n    pages = \"11189--11204\",\n    abstract = \"While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.\",\n}\n",
    "authors": [
        "Yuanchi Zhang",
        "Yile Wang",
        "Zijun Liu",
        "Shuo Wang",
        "Xiaolong Wang",
        "Peng Li",
        "Maosong Sun",
        "Yang Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.603.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/46e5462e-c4a7-57c3-9274-1ea354dafb66.pdf",
    "abstract": "While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.",
    "num_pages": 16
}