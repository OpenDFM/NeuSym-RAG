{
    "uuid": "31816643-709f-5e97-b7c9-bfc0700142a2",
    "title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hsu-etal-2024-chime,\n    title = \"{CHIME}: {LLM}-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support\",\n    author = \"Hsu, Chao-Chun  and\n      Bransom, Erin  and\n      Sparks, Jenna  and\n      Kuehl, Bailey  and\n      Tan, Chenhao  and\n      Wadden, David  and\n      Wang, Lucy  and\n      Naik, Aakanksha\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.8\",\n    doi = \"10.18653/v1/2024.findings-acl.8\",\n    pages = \"118--132\",\n    abstract = \"Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.\",\n}\n",
    "authors": [
        "Chao-Chun Hsu",
        "Erin Bransom",
        "Jenna Sparks",
        "Bailey Kuehl",
        "Chenhao Tan",
        "David Wadden",
        "Lucy Wang",
        "Aakanksha Naik"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.8.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/31816643-709f-5e97-b7c9-bfc0700142a2.pdf",
    "abstract": "Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review.",
    "num_pages": 15
}