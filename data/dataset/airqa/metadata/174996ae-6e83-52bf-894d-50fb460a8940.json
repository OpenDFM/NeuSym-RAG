{
    "uuid": "174996ae-6e83-52bf-894d-50fb460a8940",
    "title": "VISPool: Enhancing Transformer Encoders with Vector Visibility Graph Neural Networks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{alikasifoglu-etal-2024-vispool,\n    title = \"{VISP}ool: Enhancing Transformer Encoders with Vector Visibility Graph Neural Networks\",\n    author = \"Alika{\\c{s}}ifo{\\u{g}}lu, Tuna  and\n      Aras, Arda  and\n      Koc, Aykut\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.149\",\n    doi = \"10.18653/v1/2024.findings-acl.149\",\n    pages = \"2547--2556\",\n    abstract = \"The emergence of transformers has revolutionized natural language processing (NLP), as evidenced in various NLP tasks. While graph neural networks (GNNs) show recent promise in NLP, they are not standalone replacements for transformers. Rather, recent research explores combining transformers and GNNs. Existing GNN-based approaches rely on static graph construction methods requiring excessive text processing, and most of them are not scalable with the increasing document and word counts. We address these limitations by proposing a novel dynamic graph construction method for text documents based on vector visibility graphs (VVGs) generated from transformer output. Then, we introduce visibility pooler (VISPool), a scalable model architecture that seamlessly integrates VVG convolutional networks into transformer pipelines. We evaluate the proposed model on the General Language Understanding Evaluation (GLUE) benchmark datasets. VISPool outperforms the baselines with less trainable parameters, demonstrating the viability of the visibility-based graph construction method for enhancing transformers with GNNs.\",\n}\n",
    "authors": [
        "Tuna Alikaşifoğlu",
        "Arda Aras",
        "Aykut Koc"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.149.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/174996ae-6e83-52bf-894d-50fb460a8940.pdf",
    "abstract": "The emergence of transformers has revolutionized natural language processing (NLP), as evidenced in various NLP tasks. While graph neural networks (GNNs) show recent promise in NLP, they are not standalone replacements for transformers. Rather, recent research explores combining transformers and GNNs. Existing GNN-based approaches rely on static graph construction methods requiring excessive text processing, and most of them are not scalable with the increasing document and word counts. We address these limitations by proposing a novel dynamic graph construction method for text documents based on vector visibility graphs (VVGs) generated from transformer output. Then, we introduce visibility pooler (VISPool), a scalable model architecture that seamlessly integrates VVG convolutional networks into transformer pipelines. We evaluate the proposed model on the General Language Understanding Evaluation (GLUE) benchmark datasets. VISPool outperforms the baselines with less trainable parameters, demonstrating the viability of the visibility-based graph construction method for enhancing transformers with GNNs.",
    "num_pages": 10
}