{
    "uuid": "49d3ece7-dccb-5b37-ac3a-628352d104a2",
    "title": "Speculative Contrastive Decoding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{yuan-etal-2024-speculative,\n    title = \"Speculative Contrastive Decoding\",\n    author = \"Yuan, Hongyi  and\n      Lu, Keming  and\n      Huang, Fei  and\n      Yuan, Zheng  and\n      Zhou, Chang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.5\",\n    doi = \"10.18653/v1/2024.acl-short.5\",\n    pages = \"56--64\",\n    abstract = \"Large language models (LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding (SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models (LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.\",\n}\n",
    "authors": [
        "Hongyi Yuan",
        "Keming Lu",
        "Fei Huang",
        "Zheng Yuan",
        "Chang Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/49d3ece7-dccb-5b37-ac3a-628352d104a2.pdf",
    "abstract": "Large language models (LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding (SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models (LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.",
    "num_pages": 9
}