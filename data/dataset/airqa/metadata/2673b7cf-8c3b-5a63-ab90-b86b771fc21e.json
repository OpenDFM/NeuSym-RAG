{
    "uuid": "2673b7cf-8c3b-5a63-ab90-b86b771fc21e",
    "title": "Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wein-etal-2023-follow,\n    title = \"Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance\",\n    author = \"Wein, Shira  and\n      Homan, Christopher  and\n      Aroyo, Lora  and\n      Welty, Chris\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.196\",\n    doi = \"10.18653/v1/2023.findings-acl.196\",\n    pages = \"3138--3161\",\n    abstract = \"Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one is actually better (i.e. ranked higher) than the other. What has made this difficult in the past is that each system must itself be evaluated by comparison to a gold standard. We define a null hypothesis that each system{'}s metric scores are drawn from the same distribution, using variance found naturally (though rarely reported) in test set items and individual labels on an item (responses) to produce the metric distributions. We create a test set that evenly mixes the responses of the two systems under the assumption the null hypothesis is true. Exploring how to best estimate the true p-value from a single test set under different metrics, tests, and sampling methods, we find that the presence of response variance (from multiple raters or multiple model versions) has a profound impact on p-value estimates for model comparison, and that choice of metric and sampling method is critical to providing statistical guarantees on model comparisons.\",\n}\n",
    "authors": [
        "Shira Wein",
        "Christopher Homan",
        "Lora Aroyo",
        "Chris Welty"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.196.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2673b7cf-8c3b-5a63-ab90-b86b771fc21e.pdf",
    "abstract": "Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one is actually better (i.e. ranked higher) than the other. What has made this difficult in the past is that each system must itself be evaluated by comparison to a gold standard. We define a null hypothesis that each systemâ€™s metric scores are drawn from the same distribution, using variance found naturally (though rarely reported) in test set items and individual labels on an item (responses) to produce the metric distributions. We create a test set that evenly mixes the responses of the two systems under the assumption the null hypothesis is true. Exploring how to best estimate the true p-value from a single test set under different metrics, tests, and sampling methods, we find that the presence of response variance (from multiple raters or multiple model versions) has a profound impact on p-value estimates for model comparison, and that choice of metric and sampling method is critical to providing statistical guarantees on model comparisons.",
    "num_pages": 24
}