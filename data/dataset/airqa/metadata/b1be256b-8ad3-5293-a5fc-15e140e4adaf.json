{
    "uuid": "b1be256b-8ad3-5293-a5fc-15e140e4adaf",
    "title": "RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{han-etal-2023-robustqa,\n    title = \"{R}obust{QA}: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering\",\n    author = \"Han, Rujun  and\n      Qi, Peng  and\n      Zhang, Yuhao  and\n      Liu, Lan  and\n      Burger, Juliette  and\n      Wang, William Yang  and\n      Huang, Zhiheng  and\n      Xiang, Bing  and\n      Roth, Dan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.263\",\n    doi = \"10.18653/v1/2023.findings-acl.263\",\n    pages = \"4294--4311\",\n    abstract = \"Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models{'} generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose **RobustQA**, a novel benchmark consisting of datasets from 8 different domains, which facilitates the evaluation of ODQA{'}s domain robustness. To build **RobustQA**, we annotate QA pairs in retrieval datasets with rigorous quality control. We further examine improving QA performances by incorporating unsupervised learning methods with target-domain corpus and adopting large generative language models. These methods can effectively improve model performances on **RobustQA**. However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.\",\n}\n",
    "authors": [
        "Rujun Han",
        "Peng Qi",
        "Yuhao Zhang",
        "Lan Liu",
        "Juliette Burger",
        "William Yang Wang",
        "Zhiheng Huang",
        "Bing Xiang",
        "Dan Roth"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.263.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b1be256b-8ad3-5293-a5fc-15e140e4adaf.pdf",
    "abstract": "Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models’ generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose **RobustQA**, a novel benchmark consisting of datasets from 8 different domains, which facilitates the evaluation of ODQA’s domain robustness. To build **RobustQA**, we annotate QA pairs in retrieval datasets with rigorous quality control. We further examine improving QA performances by incorporating unsupervised learning methods with target-domain corpus and adopting large generative language models. These methods can effectively improve model performances on **RobustQA**. However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness.",
    "num_pages": 18
}