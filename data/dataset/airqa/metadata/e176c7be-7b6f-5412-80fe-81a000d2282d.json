{
    "uuid": "e176c7be-7b6f-5412-80fe-81a000d2282d",
    "title": "StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cao-etal-2024-structeval,\n    title = \"{S}truct{E}val: Deepen and Broaden Large Language Model Assessment via Structured Evaluation\",\n    author = \"Cao, Boxi  and\n      Ren, Mengjie  and\n      Lin, Hongyu  and\n      Han, Xianpei  and\n      Zhang, Feng  and\n      Zhan, Junfeng  and\n      Sun, Le\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.314\",\n    doi = \"10.18653/v1/2024.findings-acl.314\",\n    pages = \"5300--5318\",\n    abstract = \"Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggle to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, this paper proposes a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluations for large language models. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination, and reducing the interference of potential biases, thereby providing a more reliable and consistent conclusion regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.\",\n}\n",
    "authors": [
        "Boxi Cao",
        "Mengjie Ren",
        "Hongyu Lin",
        "Xianpei Han",
        "Feng Zhang",
        "Junfeng Zhan",
        "Le Sun"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.314.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e176c7be-7b6f-5412-80fe-81a000d2282d.pdf",
    "abstract": "Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggle to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, this paper proposes a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluations for large language models. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination, and reducing the interference of potential biases, thereby providing a more reliable and consistent conclusion regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.",
    "num_pages": 19
}