{
    "uuid": "75daf8c3-77d7-5fa4-ac5e-14ce406a4e58",
    "title": "Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction (ACE) Annotations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{cai-oconnor-2023-evaluating,\n    title = \"Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction ({ACE}) Annotations\",\n    author = \"Cai, Erica  and\n      O{'}Connor, Brendan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.142\",\n    doi = \"10.18653/v1/2023.acl-short.142\",\n    pages = \"1651--1665\",\n    abstract = \"Zero-shot event extraction (EE) methods infer richly structured event records from text, based only on a minimal user specification and no training examples, which enables flexibility in exploring and developing applications. Most event extraction research uses the Automatic Content Extraction (ACE) annotated dataset to evaluate supervised EE methods, but can it be used to evaluate zero-shot and other low-supervision EE? We describe ACE{'}s event structures and identify significant ambiguities and issues in current evaluation practice, including (1) coreferent argument mentions, (2) conflicting argument head conventions, and (3) ignorance of modality and event class details. By sometimes mishandling these subtleties, current work may dramatically understate the actual performance of zero-shot and other low-supervision EE, considering up to 32{\\%} of correctly identified arguments and 25{\\%} of correctly ignored event mentions as false negatives. For each issue, we propose recommendations for future evaluations so the research community can better utilize ACE as an event evaluation resource.\",\n}\n",
    "authors": [
        "Erica Cai",
        "Brendan O’Connor"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.142.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/75daf8c3-77d7-5fa4-ac5e-14ce406a4e58.pdf",
    "abstract": "Zero-shot event extraction (EE) methods infer richly structured event records from text, based only on a minimal user specification and no training examples, which enables flexibility in exploring and developing applications. Most event extraction research uses the Automatic Content Extraction (ACE) annotated dataset to evaluate supervised EE methods, but can it be used to evaluate zero-shot and other low-supervision EE? We describe ACE’s event structures and identify significant ambiguities and issues in current evaluation practice, including (1) coreferent argument mentions, (2) conflicting argument head conventions, and (3) ignorance of modality and event class details. By sometimes mishandling these subtleties, current work may dramatically understate the actual performance of zero-shot and other low-supervision EE, considering up to 32% of correctly identified arguments and 25% of correctly ignored event mentions as false negatives. For each issue, we propose recommendations for future evaluations so the research community can better utilize ACE as an event evaluation resource.",
    "num_pages": 15
}