{
    "uuid": "e856b23e-8524-5bac-9fd4-f6a2117f93ee",
    "title": "Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{park-etal-2023-cross,\n    title = \"Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification\",\n    author = \"Park, Seongmin  and\n      Kim, Kyungho  and\n      Lee, Jihwa\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.328\",\n    doi = \"10.18653/v1/2023.findings-acl.328\",\n    pages = \"5329--5341\",\n    abstract = \"Text classification with extremely weak supervision (EWS) imposes stricter supervision constraints compared to regular weakly supervise classification. Absolutely no labeled training samples or hand-crafted rules specific to the evaluation data are allowed. Such restrictions limit state-of-the-art EWS classification methods to indirect weak labeling techniques that assign unnatural label uncertainty estimates. We present PLAT, a framework that creates weak labels by leveraging recent developments in zero-shot text classification. PLAT employs models trained for sub-tasks other than classification to label documents. Most importantly, PLAT refrains from assigning overly confident weak labels and improves soft-label training performance for downstream classifiers. Classifiers trained with PLAT significantly outperform those trained on weak labels generated by the previous state-of-the-art in extremely weakly supervised text classification.\",\n}\n",
    "authors": [
        "Seongmin Park",
        "Kyungho Kim",
        "Jihwa Lee"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.328.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e856b23e-8524-5bac-9fd4-f6a2117f93ee.pdf",
    "abstract": "Text classification with extremely weak supervision (EWS) imposes stricter supervision constraints compared to regular weakly supervise classification. Absolutely no labeled training samples or hand-crafted rules specific to the evaluation data are allowed. Such restrictions limit state-of-the-art EWS classification methods to indirect weak labeling techniques that assign unnatural label uncertainty estimates. We present PLAT, a framework that creates weak labels by leveraging recent developments in zero-shot text classification. PLAT employs models trained for sub-tasks other than classification to label documents. Most importantly, PLAT refrains from assigning overly confident weak labels and improves soft-label training performance for downstream classifiers. Classifiers trained with PLAT significantly outperform those trained on weak labels generated by the previous state-of-the-art in extremely weakly supervised text classification.",
    "num_pages": 13
}