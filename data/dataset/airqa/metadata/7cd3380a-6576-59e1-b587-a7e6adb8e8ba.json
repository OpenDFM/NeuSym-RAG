{
    "uuid": "7cd3380a-6576-59e1-b587-a7e6adb8e8ba",
    "title": "Confounders in Instance Variation for the Analysis of Data Contamination",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Data Contamination (CONDA)",
    "bibtex": "@inproceedings{mehrbakhsh-etal-2024-confounders,\n    title = \"Confounders in Instance Variation for the Analysis of Data Contamination\",\n    author = \"Mehrbakhsh, Behzad  and\n      Garigliotti, Dario  and\n      Mart{\\'\\i}nez-Plumed, Fernando  and\n      Hernandez-Orallo, Jose\",\n    editor = \"Sainz, Oscar  and\n      Garc{\\'\\i}a Ferrero, Iker  and\n      Agirre, Eneko  and\n      Ander Campos, Jon  and\n      Jacovi, Alon  and\n      Elazar, Yanai  and\n      Goldberg, Yoav\",\n    booktitle = \"Proceedings of the 1st Workshop on Data Contamination (CONDA)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.conda-1.2\",\n    doi = \"10.18653/v1/2024.conda-1.2\",\n    pages = \"13--21\",\n    abstract = \"Test contamination is a serious problem for the evaluation of large language models (LLMs) because it leads to the overestimation of their performance and a quick saturation of benchmarks, even before the actual capability is achieved. One strategy to address this issue is the (adversarial) generation of variations, by including different exemplars and different rephrasings of the questions. However, these two interventions can lead to instances that can be more difficult (accumulating on the expected loss of performance by partly removing the contamination) but also to instances that can be less difficult (cancelling the expected loss of performance), which would make contamination undetectable. Understanding these two phenomena in terms of instance difficulty is critical to determine and measure contamination. In this paper we conduct a comprehensive analysis of these two interventions on an addition task with fine-tuned LLAMA-2 models.\",\n}\n",
    "authors": [
        "Behzad Mehrbakhsh",
        "Dario Garigliotti",
        "Fernando Mart√≠nez-Plumed",
        "Jose Hernandez-Orallo"
    ],
    "pdf_url": "https://aclanthology.org/2024.conda-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7cd3380a-6576-59e1-b587-a7e6adb8e8ba.pdf",
    "abstract": "Test contamination is a serious problem for the evaluation of large language models (LLMs) because it leads to the overestimation of their performance and a quick saturation of benchmarks, even before the actual capability is achieved. One strategy to address this issue is the (adversarial) generation of variations, by including different exemplars and different rephrasings of the questions. However, these two interventions can lead to instances that can be more difficult (accumulating on the expected loss of performance by partly removing the contamination) but also to instances that can be less difficult (cancelling the expected loss of performance), which would make contamination undetectable. Understanding these two phenomena in terms of instance difficulty is critical to determine and measure contamination. In this paper we conduct a comprehensive analysis of these two interventions on an addition task with fine-tuned LLAMA-2 models.",
    "num_pages": 9
}