{
    "uuid": "0fd9a48f-6fac-5060-979a-73307799289e",
    "title": "SemanticCuetSync at AraFinNLP2024: Classification of Cross-Dialect Intent in the Banking Domain using Transformers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{paran-etal-2024-semanticcuetsync,\n    title = \"{S}emantic{C}uet{S}ync at {A}ra{F}in{NLP}2024: Classification of Cross-Dialect Intent in the Banking Domain using Transformers\",\n    author = \"Paran, Ashraful  and\n      Shohan, Symom  and\n      Hossain, Md.  and\n      Hossain, Jawad  and\n      Ahsan, Shawly  and\n      Hoque, Mohammed Moshiul\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.38\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.38\",\n    pages = \"422--427\",\n    abstract = \"Intention detection is a crucial aspect of natural language understanding (NLU), focusing on identifying the primary objective underlying user input. In this work, we present a transformer-based method that excels in determining the intent of Arabic text within the banking domain. We explored several machine learning (ML), deep learning (DL), and transformer-based models on an Arabic banking dataset for intent detection. Our findings underscore the challenges that traditional ML and DL models face in understanding the nuances of various Arabic dialects, leading to subpar performance in intent detection. However, the transformer-based methods, designed to tackle such complexities, significantly outperformed the other models in classifying intent across different Arabic dialects. Notably, the AraBERTv2 model achieved the highest micro F1 score of 82.08{\\%} in ArBanking77 dataset, a testament to its effectiveness in this context. This achievement, which contributed to our work being ranked 5$^{th}$ in the shared task, AraFinNLP2024, highlights the importance of developing models that can effectively handle the intricacies of Arabic language processing and intent detection.\",\n}\n",
    "authors": [
        "Ashraful Paran",
        "Symom Shohan",
        "Md. Hossain",
        "Jawad Hossain",
        "Shawly Ahsan",
        "Mohammed Moshiul Hoque"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.38.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0fd9a48f-6fac-5060-979a-73307799289e.pdf",
    "abstract": "Intention detection is a crucial aspect of natural language understanding (NLU), focusing on identifying the primary objective underlying user input. In this work, we present a transformer-based method that excels in determining the intent of Arabic text within the banking domain. We explored several machine learning (ML), deep learning (DL), and transformer-based models on an Arabic banking dataset for intent detection. Our findings underscore the challenges that traditional ML and DL models face in understanding the nuances of various Arabic dialects, leading to subpar performance in intent detection. However, the transformer-based methods, designed to tackle such complexities, significantly outperformed the other models in classifying intent across different Arabic dialects. Notably, the AraBERTv2 model achieved the highest micro F1 score of 82.08% in ArBanking77 dataset, a testament to its effectiveness in this context. This achievement, which contributed to our work being ranked 5th in the shared task, AraFinNLP2024, highlights the importance of developing models that can effectively handle the intricacies of Arabic language processing and intent detection.",
    "num_pages": 6
}