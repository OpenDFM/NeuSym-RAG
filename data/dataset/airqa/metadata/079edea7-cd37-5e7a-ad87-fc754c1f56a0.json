{
    "uuid": "079edea7-cd37-5e7a-ad87-fc754c1f56a0",
    "title": "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{mangaokar-etal-2024-prp,\n    title = \"{PRP}: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails\",\n    author = \"Mangaokar, Neal  and\n      Hooda, Ashish  and\n      Choi, Jihye  and\n      Chandrashekaran, Shreyas  and\n      Fawaz, Kassem  and\n      Jha, Somesh  and\n      Prakash, Atul\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.591\",\n    doi = \"10.18653/v1/2024.acl-long.591\",\n    pages = \"10960--10976\",\n    abstract = \"Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective. Code at https://github.com/AshishHoodaIITD/prp-llm-guard-rail-attack.\",\n}\n",
    "authors": [
        "Neal Mangaokar",
        "Ashish Hooda",
        "Jihye Choi",
        "Shreyas Chandrashekaran",
        "Kassem Fawaz",
        "Somesh Jha",
        "Atul Prakash"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.591.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/079edea7-cd37-5e7a-ad87-fc754c1f56a0.pdf",
    "abstract": "Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective. Code at https://github.com/AshishHoodaIITD/prp-llm-guard-rail-attack.",
    "num_pages": 17
}