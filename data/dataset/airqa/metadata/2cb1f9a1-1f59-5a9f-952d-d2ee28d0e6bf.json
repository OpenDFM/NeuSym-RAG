{
    "uuid": "2cb1f9a1-1f59-5a9f-952d-d2ee28d0e6bf",
    "title": "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{sonkar-etal-2023-maner,\n    title = \"{MANER}: Mask Augmented Named Entity Recognition for Extreme Low-Resource Languages\",\n    author = \"Sonkar, Shashank  and\n      Wang, Zichao  and\n      Baraniuk, Richard\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.16\",\n    doi = \"10.18653/v1/2023.sustainlp-1.16\",\n    pages = \"219--226\",\n}\n",
    "authors": [
        "Shashank Sonkar",
        "Zichao Wang",
        "Richard Baraniuk"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2cb1f9a1-1f59-5a9f-952d-d2ee28d0e6bf.pdf",
    "abstract": "This paper investigates the problem of Named Entity Recognition (NER) for extreme lowresource languages with only a few hundred tagged data samples. A critical enabler of most of the progress in NER is the readily available, large-scale training data for languages such as English and French. However, NER for lowresource languages remains relatively underexplored, leaving much room for improvement. We propose Mask Augmented Named Entity Recognition (MANER), a simple yet effective method that leverages the distributional hypothesis of pre-trained masked language models (MLMs) to improve NER performance for lowresource languages significantly. MANER repurposes the [mask] token in MLMs, which encodes valuable semantic contextual information, for NER prediction. Specifically, we prepend a [mask] token to every word in a sentence and predict the named entity for each word from its preceding [mask] token. We demonstrate that MANER is well-suited for NER in low-resource languages; our experiments show that for 100 languages with as few as 100 training examples, it improves on the state-of-the-art by up to 48% and by 12% on average on F1 score. We also perform detailed analyses and ablation studies to understand the scenarios that are best suited to MANER.",
    "num_pages": 8
}