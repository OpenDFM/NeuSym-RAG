{
    "uuid": "1deab857-74ea-5b63-903c-c9dfcfd1f8dc",
    "title": "Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{liang-etal-2024-controlled,\n    title = \"Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs\",\n    author = \"Liang, Xun  and\n      Wang, Hanyu  and\n      Song, Shichao  and\n      Hu, Mengting  and\n      Wang, Xunzhi  and\n      Li, Zhiyu  and\n      Xiong, Feiyu  and\n      Tang, Bo\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.345\",\n    doi = \"10.18653/v1/2024.findings-acl.345\",\n    pages = \"5797--5814\",\n    abstract = \"Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29{\\%} over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.\",\n}\n",
    "authors": [
        "Xun Liang",
        "Hanyu Wang",
        "Shichao Song",
        "Mengting Hu",
        "Xunzhi Wang",
        "Zhiyu Li",
        "Feiyu Xiong",
        "Bo Tang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.345.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1deab857-74ea-5b63-903c-c9dfcfd1f8dc.pdf",
    "abstract": "Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency.",
    "num_pages": 18
}