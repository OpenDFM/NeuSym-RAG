{
    "uuid": "64a62ad3-f583-54b0-a554-75441219c15d",
    "title": "Instruction-tuned Language Models are Better Knowledge Learners",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jiang-etal-2024-instruction,\n    title = \"Instruction-tuned Language Models are Better Knowledge Learners\",\n    author = \"Jiang, Zhengbao  and\n      Sun, Zhiqing  and\n      Shi, Weijia  and\n      Rodriguez, Pedro  and\n      Zhou, Chunting  and\n      Neubig, Graham  and\n      Lin, Xi  and\n      Yih, Wen-tau  and\n      Iyer, Srini\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.296\",\n    doi = \"10.18653/v1/2024.acl-long.296\",\n    pages = \"5421--5434\",\n    abstract = \"In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8{\\%}.\",\n}\n",
    "authors": [
        "Zhengbao Jiang",
        "Zhiqing Sun",
        "Weijia Shi",
        "Pedro Rodriguez",
        "Chunting Zhou",
        "Graham Neubig",
        "Xi Lin",
        "Wen-tau Yih",
        "Srini Iyer"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.296.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/64a62ad3-f583-54b0-a554-75441219c15d.pdf",
    "abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.",
    "num_pages": 14
}