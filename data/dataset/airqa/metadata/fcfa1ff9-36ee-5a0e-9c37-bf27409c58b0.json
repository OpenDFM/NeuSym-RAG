{
    "uuid": "fcfa1ff9-36ee-5a0e-9c37-bf27409c58b0",
    "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-cmmlu,\n    title = \"{CMMLU}: Measuring massive multitask language understanding in {C}hinese\",\n    author = \"Li, Haonan  and\n      Zhang, Yixuan  and\n      Koto, Fajri  and\n      Yang, Yifei  and\n      Zhao, Hai  and\n      Gong, Yeyun  and\n      Duan, Nan  and\n      Baldwin, Timothy\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.671\",\n    doi = \"10.18653/v1/2024.findings-acl.671\",\n    pages = \"11260--11285\",\n    abstract = \"As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of even 60{\\%}, which is the pass mark for Chinese exams. This highlights that there is substantial room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models{'} performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models for Chinese.\",\n}\n",
    "authors": [
        "Haonan Li",
        "Yixuan Zhang",
        "Fajri Koto",
        "Yifei Yang",
        "Hai Zhao",
        "Yeyun Gong",
        "Nan Duan",
        "Timothy Baldwin"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.671.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/fcfa1ff9-36ee-5a0e-9c37-bf27409c58b0.pdf",
    "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of even 60%, which is the pass mark for Chinese exams. This highlights that there is substantial room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the modelsâ€™ performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models for Chinese.",
    "num_pages": 26
}