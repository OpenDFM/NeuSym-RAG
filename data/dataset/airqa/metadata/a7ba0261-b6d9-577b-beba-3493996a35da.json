{
    "uuid": "a7ba0261-b6d9-577b-beba-3493996a35da",
    "title": "Revisiting Non-Autoregressive Translation at Scale",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-revisiting,\n    title = \"Revisiting Non-Autoregressive Translation at Scale\",\n    author = \"Wang, Zhihao  and\n      Wang, Longyue  and\n      Su, Jinsong  and\n      Yao, Junfeng  and\n      Tu, Zhaopeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.763\",\n    doi = \"10.18653/v1/2023.findings-acl.763\",\n    pages = \"12051--12065\",\n    abstract = \"In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on the scaled dataset, which can be regarded as a strong baseline for future works. We release code and system outputs at \\url{https://github.com/DeepLearnXMU/Scaling4NAT}.\",\n}\n",
    "authors": [
        "Zhihao Wang",
        "Longyue Wang",
        "Jinsong Su",
        "Junfeng Yao",
        "Zhaopeng Tu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.763.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a7ba0261-b6d9-577b-beba-3493996a35da.pdf",
    "abstract": "In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on the scaled dataset, which can be regarded as a strong baseline for future works. We release code and system outputs at https://github.com/DeepLearnXMU/Scaling4NAT.",
    "num_pages": 15
}