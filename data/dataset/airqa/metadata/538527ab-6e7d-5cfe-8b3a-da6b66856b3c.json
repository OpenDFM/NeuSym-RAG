{
    "uuid": "538527ab-6e7d-5cfe-8b3a-da6b66856b3c",
    "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lucy-etal-2024-aboutme,\n    title = \"{A}bout{M}e: Using Self-Descriptions in Webpages to Document the Effects of {E}nglish Pretraining Data Filters\",\n    author = \"Lucy, Li  and\n      Gururangan, Suchin  and\n      Soldaini, Luca  and\n      Strubell, Emma  and\n      Bamman, David  and\n      Klein, Lauren  and\n      Dodge, Jesse\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.400\",\n    doi = \"10.18653/v1/2024.acl-long.400\",\n    pages = \"7393--7420\",\n    abstract = \"Large language models{'} (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten {``}quality{''} and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.\",\n}\n",
    "authors": [
        "Li Lucy",
        "Suchin Gururangan",
        "Luca Soldaini",
        "Emma Strubell",
        "David Bamman",
        "Lauren Klein",
        "Jesse Dodge"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.400.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/538527ab-6e7d-5cfe-8b3a-da6b66856b3c.pdf",
    "abstract": "Large language models’ (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten “quality” and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.",
    "num_pages": 28
}