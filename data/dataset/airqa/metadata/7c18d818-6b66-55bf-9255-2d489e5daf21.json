{
    "uuid": "7c18d818-6b66-55bf-9255-2d489e5daf21",
    "title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2023-ad,\n    title = \"{AD}-{KD}: Attribution-Driven Knowledge Distillation for Language Model Compression\",\n    author = \"Wu, Siyue  and\n      Chen, Hongzhan  and\n      Quan, Xiaojun  and\n      Wang, Qifan  and\n      Wang, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.471\",\n    doi = \"10.18653/v1/2023.acl-long.471\",\n    pages = \"8449--8465\",\n    abstract = \"Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher{'}s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.\",\n}\n",
    "authors": [
        "Siyue Wu",
        "Hongzhan Chen",
        "Xiaojun Quan",
        "Qifan Wang",
        "Rui Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.471.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7c18d818-6b66-55bf-9255-2d489e5daf21.pdf",
    "abstract": "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacherâ€™s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.",
    "num_pages": 17
}