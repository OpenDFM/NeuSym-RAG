{
    "uuid": "f4fdfbaf-959b-54f2-95c9-c200483991d3",
    "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{joshi-etal-2023-machine,\n    title = \"Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales\",\n    author = \"Joshi, Brihi  and\n      Liu, Ziyi  and\n      Ramnath, Sahana  and\n      Chan, Aaron  and\n      Tong, Zhewei  and\n      Nie, Shaoliang  and\n      Wang, Qifan  and\n      Choi, Yejin  and\n      Ren, Xiang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.392\",\n    doi = \"10.18653/v1/2023.acl-long.392\",\n    pages = \"7103--7128\",\n    abstract = \"Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale{'}s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs{'} ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.\",\n}\n",
    "authors": [
        "Brihi Joshi",
        "Ziyi Liu",
        "Sahana Ramnath",
        "Aaron Chan",
        "Zhewei Tong",
        "Shaoliang Nie",
        "Qifan Wang",
        "Yejin Choi",
        "Xiang Ren"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.392.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f4fdfbaf-959b-54f2-95c9-c200483991d3.pdf",
    "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.",
    "num_pages": 26
}