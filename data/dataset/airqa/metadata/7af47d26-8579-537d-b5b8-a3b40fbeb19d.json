{
    "uuid": "7af47d26-8579-537d-b5b8-a3b40fbeb19d",
    "title": "DiscoFlan: Instruction Fine-tuning and Refined Text Generation for Discourse Relation Label Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 3rd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2023)",
    "bibtex": "@inproceedings{anuranjana-2023-discoflan,\n    title = \"{D}isco{F}lan: Instruction Fine-tuning and Refined Text Generation for Discourse Relation Label Classification\",\n    author = \"Anuranjana, Kaveri\",\n    editor = \"Braud, Chlo{\\'e}  and\n      Liu, Yang Janet  and\n      Metheniti, Eleni  and\n      Muller, Philippe  and\n      Rivi{\\`e}re, Laura  and\n      Rutherford, Attapol  and\n      Zeldes, Amir\",\n    booktitle = \"Proceedings of the 3rd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"The Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.disrpt-1.2\",\n    doi = \"10.18653/v1/2023.disrpt-1.2\",\n    pages = \"22--28\",\n    abstract = \"This paper introduces DiscoFlan, a multilingual discourse relation classifier submitted for DISRPT 2023. Our submission represents the first attempt at building a multilingual discourse relation classifier for the DISRPT 2023 shared task. By our model addresses the issue to mismatches caused by hallucination in a seq2seq model by utilizing the label distribution information for label generation. In contrast to the previous state-of-the-art model, our approach eliminates the need for hand-crafted features in computing the discourse relation classes. Furthermore, we propose a novel label generation mechanism that anchors the labels to a fixed set by selectively enhancing training on the decoder model. Our experimental results demonstrate that our model surpasses the current state-of-the-art performance in 11 out of the 26 datasets considered, however the submitted model compatible with provided evaluation scripts is better in 7 out of 26 considered datasets, while demonstrating competitive results in the rest. Overall, DiscoFlan showcases promising advancements in multilingual discourse relation classification for the DISRPT 2023 shared task.\",\n}\n",
    "authors": [
        "Kaveri Anuranjana"
    ],
    "pdf_url": "https://aclanthology.org/2023.disrpt-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7af47d26-8579-537d-b5b8-a3b40fbeb19d.pdf",
    "abstract": "This paper introduces DiscoFlan, a multilingual discourse relation classifier submitted for DISRPT 2023. Our submission represents the first attempt at building a multilingual discourse relation classifier for the DISRPT 2023 shared task. By our model addresses the issue to mismatches caused by hallucination in a seq2seq model by utilizing the label distribution information for label generation. In contrast to the previous state-of-the-art model, our approach eliminates the need for hand-crafted features in computing the discourse relation classes. Furthermore, we propose a novel label generation mechanism that anchors the labels to a fixed set by selectively enhancing training on the decoder model. Our experimental results demonstrate that our model surpasses the current state-of-the-art performance in 11 out of the 26 datasets considered, however the submitted model compatible with provided evaluation scripts is better in 7 out of 26 considered datasets, while demonstrating competitive results in the rest. Overall, DiscoFlan showcases promising advancements in multilingual discourse relation classification for the DISRPT 2023 shared task.",
    "num_pages": 7
}