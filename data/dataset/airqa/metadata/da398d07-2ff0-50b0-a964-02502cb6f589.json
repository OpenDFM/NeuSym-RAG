{
    "uuid": "da398d07-2ff0-50b0-a964-02502cb6f589",
    "title": "English-to-Japanese Multimodal Machine Translation Based on Image-Text Matching of Lecture Videos",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)",
    "bibtex": "@inproceedings{teramen-etal-2024-english,\n    title = \"{E}nglish-to-{J}apanese Multimodal Machine Translation Based on Image-Text Matching of Lecture Videos\",\n    author = \"Teramen, Ayu  and\n      Ohtsuka, Takumi  and\n      Kondo, Risa  and\n      Kajiwara, Tomoyuki  and\n      Ninomiya, Takashi\",\n    editor = \"Gu, Jing  and\n      Fu, Tsu-Jui (Ray)  and\n      Hudson, Drew  and\n      Celikyilmaz, Asli  and\n      Wang, William\",\n    booktitle = \"Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.alvr-1.7\",\n    doi = \"10.18653/v1/2024.alvr-1.7\",\n    pages = \"86--91\",\n    abstract = \"We work on a multimodal machine translation of the audio contained in English lecture videos to generate Japanese subtitles. Image-guided multimodal machine translation is promising for error correction in speech recognition and for text disambiguation. In our situation, lecture videos provide a variety of images. Images of presentation materials can complement information not available from audio and may help improve translation quality. However, images of speakers or audiences would not directly affect the translation quality. We construct a multimodal parallel corpus with automatic speech recognition text and multiple images for a transcribed parallel corpus of lecture videos, and propose a method to select the most relevant ones from the multiple images with the speech text for improving the performance of image-guided multimodal machine translation. Experimental results on translating automatic speech recognition or transcribed English text into Japanese show the effectiveness of our method to select a relevant image.\",\n}\n",
    "authors": [
        "Ayu Teramen",
        "Takumi Ohtsuka",
        "Risa Kondo",
        "Tomoyuki Kajiwara",
        "Takashi Ninomiya"
    ],
    "pdf_url": "https://aclanthology.org/2024.alvr-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/da398d07-2ff0-50b0-a964-02502cb6f589.pdf",
    "abstract": "We work on a multimodal machine translation of the audio contained in English lecture videos to generate Japanese subtitles. Image-guided multimodal machine translation is promising for error correction in speech recognition and for text disambiguation. In our situation, lecture videos provide a variety of images. Images of presentation materials can complement information not available from audio and may help improve translation quality. However, images of speakers or audiences would not directly affect the translation quality. We construct a multimodal parallel corpus with automatic speech recognition text and multiple images for a transcribed parallel corpus of lecture videos, and propose a method to select the most relevant ones from the multiple images with the speech text for improving the performance of image-guided multimodal machine translation. Experimental results on translating automatic speech recognition or transcribed English text into Japanese show the effectiveness of our method to select a relevant image.",
    "num_pages": 6
}