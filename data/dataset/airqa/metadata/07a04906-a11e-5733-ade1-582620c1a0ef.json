{
    "uuid": "07a04906-a11e-5733-ade1-582620c1a0ef",
    "title": "How to Engage your Readers? Generating Guiding Questions to Promote Active Reading",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{cui-etal-2024-engage,\n    title = \"How to Engage your Readers? Generating Guiding Questions to Promote Active Reading\",\n    author = \"Cui, Peng  and\n      Zouhar, Vil{\\'e}m  and\n      Zhang, Xiaoyu  and\n      Sachan, Mrinmaya\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.632\",\n    doi = \"10.18653/v1/2024.acl-long.632\",\n    pages = \"11749--11765\",\n    abstract = \"Using questions in written text is an effective strategy to enhance readability. However, what makes an active reading question good, what the linguistic role of these questions is, and what is their impact on human reading remains understudied. We introduce GuidingQ, a dataset of 10K in-text questions from textbooks and scientific articles. By analyzing the dataset, we present a comprehensive understanding of the use, distribution, and linguistic characteristics of these questions. Then, we explore various approaches to generate such questions using language models. Our results highlight the importance of capturing inter-question relationships and the challenge of question position identification in generating these questions. Finally, we conduct a human study to understand the implication of such questions on reading comprehension. We find that the generated questions are of high quality and are almost as effective as human-written questions in terms of improving readers{'} memorization and comprehension.\",\n}\n",
    "authors": [
        "Peng Cui",
        "Vilém Zouhar",
        "Xiaoyu Zhang",
        "Mrinmaya Sachan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.632.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/07a04906-a11e-5733-ade1-582620c1a0ef.pdf",
    "abstract": "Using questions in written text is an effective strategy to enhance readability. However, what makes an active reading question good, what the linguistic role of these questions is, and what is their impact on human reading remains understudied. We introduce GuidingQ, a dataset of 10K in-text questions from textbooks and scientific articles. By analyzing the dataset, we present a comprehensive understanding of the use, distribution, and linguistic characteristics of these questions. Then, we explore various approaches to generate such questions using language models. Our results highlight the importance of capturing inter-question relationships and the challenge of question position identification in generating these questions. Finally, we conduct a human study to understand the implication of such questions on reading comprehension. We find that the generated questions are of high quality and are almost as effective as human-written questions in terms of improving readers’ memorization and comprehension.",
    "num_pages": 17
}