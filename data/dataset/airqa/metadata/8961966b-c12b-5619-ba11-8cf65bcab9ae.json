{
    "uuid": "8961966b-c12b-5619-ba11-8cf65bcab9ae",
    "title": "GraphReason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)",
    "bibtex": "@inproceedings{cao-2024-graphreason,\n    title = \"{G}raph{R}eason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach\",\n    author = \"Cao, Lang\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Lipkin, Ben  and\n      Neves Ribeiro, Danilo  and\n      Wong, Lionel  and\n      Ye, Xi  and\n      Zhao, Wenting\",\n    booktitle = \"Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlrse-1.1\",\n    pages = \"1--12\",\n    abstract = \"Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these models{'} reasoning performance.\",\n}\n",
    "authors": [
        "Lang Cao"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlrse-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8961966b-c12b-5619-ba11-8cf65bcab9ae.pdf",
    "abstract": "Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these modelsâ€™ reasoning performance.",
    "num_pages": 12
}