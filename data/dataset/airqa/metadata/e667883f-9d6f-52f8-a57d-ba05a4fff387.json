{
    "uuid": "e667883f-9d6f-52f8-a57d-ba05a4fff387",
    "title": "GNN-SL: Sequence Labeling Based on Nearest Examples via GNN",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-gnn,\n    title = \"{GNN}-{SL}: Sequence Labeling Based on Nearest Examples via {GNN}\",\n    author = \"Wang, Shuhe  and\n      Meng, Yuxian  and\n      Ouyang, Rongbin  and\n      Li, Jiwei  and\n      Zhang, Tianwei  and\n      Lyu, Lingjuan  and\n      Wang, Guoyin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.803\",\n    doi = \"10.18653/v1/2023.findings-acl.803\",\n    pages = \"12679--12692\",\n    abstract = \"To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and resultscomparable to SOTA performances on NER datasets, and POS datasets.\",\n}\n",
    "authors": [
        "Shuhe Wang",
        "Yuxian Meng",
        "Rongbin Ouyang",
        "Jiwei Li",
        "Tianwei Zhang",
        "Lingjuan Lyu",
        "Guoyin Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.803.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e667883f-9d6f-52f8-a57d-ba05a4fff387.pdf",
    "abstract": "To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and resultscomparable to SOTA performances on NER datasets, and POS datasets.",
    "num_pages": 14
}