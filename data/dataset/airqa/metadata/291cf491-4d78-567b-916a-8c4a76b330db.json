{
    "uuid": "291cf491-4d78-567b-916a-8c4a76b330db",
    "title": "Constructing Multilingual Code Search Dataset Using Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{sekizawa-etal-2023-constructing,\n    title = \"Constructing Multilingual Code Search Dataset Using Neural Machine Translation\",\n    author = \"Sekizawa, Ryo  and\n      Duan, Nan  and\n      Lu, Shuai  and\n      Yanaka, Hitomi\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.10\",\n    doi = \"10.18653/v1/2023.acl-srw.10\",\n    pages = \"69--75\",\n    abstract = \"Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate them on multiple code search test sets. Our results show that the model pre-trained with all natural and programming language data has performed best in most cases. By applying back-translation data filtering to our dataset, we demonstrate that the translation quality affects the model{'}s performance to a certain extent, but the data size matters more.\",\n}\n",
    "authors": [
        "Ryo Sekizawa",
        "Nan Duan",
        "Shuai Lu",
        "Hitomi Yanaka"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/291cf491-4d78-567b-916a-8c4a76b330db.pdf",
    "abstract": "Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate them on multiple code search test sets. Our results show that the model pre-trained with all natural and programming language data has performed best in most cases. By applying back-translation data filtering to our dataset, we demonstrate that the translation quality affects the modelâ€™s performance to a certain extent, but the data size matters more.",
    "num_pages": 7
}