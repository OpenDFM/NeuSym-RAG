{
    "uuid": "5fce4778-5aaf-5e4d-95bd-fc4997ddb807",
    "title": "Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{cho-etal-2023-probing,\n    title = \"Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning\",\n    author = \"Cho, Hyunsoo  and\n      Park, Choonghyun  and\n      Kim, Junyeob  and\n      Kim, Hyuhng Joon  and\n      Yoo, Kang Min  and\n      Lee, Sang-goo\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.21\",\n    doi = \"10.18653/v1/2023.starsem-1.21\",\n    pages = \"225--235\",\n    abstract = \"As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the high cost of fine-tuning. While large PLMs and various PETL methods have achieved impressive results on various benchmarks, it is uncertain whether they can effectively handle inputs that have been distributionally shifted. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, with various language models with different scales.\",\n}\n",
    "authors": [
        "Hyunsoo Cho",
        "Choonghyun Park",
        "Junyeob Kim",
        "Hyuhng Joon Kim",
        "Kang Min Yoo",
        "Sang-goo Lee"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.21.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5fce4778-5aaf-5e4d-95bd-fc4997ddb807.pdf",
    "abstract": "As the size of the pre-trained language model (PLM) continues to increase, numerous parameter-efficient transfer learning methods have been proposed recently to compensate for the high cost of fine-tuning. While large PLMs and various PETL methods have achieved impressive results on various benchmarks, it is uncertain whether they can effectively handle inputs that have been distributionally shifted. In this study, we systematically explore how the ability to detect out-of-distribution (OOD) changes as the size of the PLM grows or the transfer methods are altered. Specifically, we evaluated various PETL techniques, including fine-tuning, Adapter, LoRA, and prefix-tuning, with various language models with different scales.",
    "num_pages": 11
}