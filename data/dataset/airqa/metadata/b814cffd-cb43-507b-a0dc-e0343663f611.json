{
    "uuid": "b814cffd-cb43-507b-a0dc-e0343663f611",
    "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{monea-etal-2024-glitch,\n    title = \"A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia\",\n    author = \"Monea, Giovanni  and\n      Peyrard, Maxime  and\n      Josifoski, Martin  and\n      Chaudhary, Vishrav  and\n      Eisner, Jason  and\n      Kiciman, Emre  and\n      Palangi, Hamid  and\n      Patra, Barun  and\n      West, Robert\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.369\",\n    doi = \"10.18653/v1/2024.acl-long.369\",\n    pages = \"6828--6844\",\n    abstract = \"Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model{'}s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.\",\n}\n",
    "authors": [
        "Giovanni Monea",
        "Maxime Peyrard",
        "Martin Josifoski",
        "Vishrav Chaudhary",
        "Jason Eisner",
        "Emre Kiciman",
        "Hamid Palangi",
        "Barun Patra",
        "Robert West"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.369.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b814cffd-cb43-507b-a0dc-e0343663f611.pdf",
    "abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a modelâ€™s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.",
    "num_pages": 17
}