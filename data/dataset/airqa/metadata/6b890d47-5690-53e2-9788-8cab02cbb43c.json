{
    "uuid": "6b890d47-5690-53e2-9788-8cab02cbb43c",
    "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{simmons-2023-moral,\n    title = \"Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity\",\n    author = \"Simmons, Gabriel\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.40\",\n    doi = \"10.18653/v1/2023.acl-srw.40\",\n    pages = \"282--297\",\n    abstract = \"Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.\",\n}\n",
    "authors": [
        "Gabriel Simmons"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.40.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6b890d47-5690-53e2-9788-8cab02cbb43c.pdf",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.",
    "num_pages": 16
}