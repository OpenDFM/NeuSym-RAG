{
    "uuid": "d1ac6d1f-1e5f-5c5d-94a3-27a36b6b5f2f",
    "title": "QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{li-etal-2023-qap,\n    title = \"{QAP}: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition\",\n    author = \"Li, Ziming  and\n      Zhou, Yan  and\n      Liu, Yaxin  and\n      Zhu, Fuqing  and\n      Yang, Chuanpeng  and\n      Hu, Songlin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.772\",\n    doi = \"10.18653/v1/2023.findings-acl.772\",\n    pages = \"12191--12204\",\n    abstract = \"Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (\\textit{i.e.,} textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results.\",\n}\n",
    "authors": [
        "Ziming Li",
        "Yan Zhou",
        "Yaxin Liu",
        "Fuqing Zhu",
        "Chuanpeng Yang",
        "Songlin Hu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.772.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d1ac6d1f-1e5f-5c5d-94a3-27a36b6b5f2f.pdf",
    "abstract": "Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (i.e., textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results.",
    "num_pages": 14
}