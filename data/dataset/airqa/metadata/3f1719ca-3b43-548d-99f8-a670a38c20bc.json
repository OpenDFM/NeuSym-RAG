{
    "uuid": "3f1719ca-3b43-548d-99f8-a670a38c20bc",
    "title": "CREPE: Open-Domain Question Answering with False Presuppositions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yu-etal-2023-crepe,\n    title = \"{CREPE}: Open-Domain Question Answering with False Presuppositions\",\n    author = \"Yu, Xinyan  and\n      Min, Sewon  and\n      Zettlemoyer, Luke  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.583\",\n    doi = \"10.18653/v1/2023.acl-long.583\",\n    pages = \"10457--10480\",\n    abstract = \"When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25{\\%} of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task.\",\n}\n",
    "authors": [
        "Xinyan Yu",
        "Sewon Min",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.583.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3f1719ca-3b43-548d-99f8-a670a38c20bc.pdf",
    "abstract": "When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25% of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task.",
    "num_pages": 24
}