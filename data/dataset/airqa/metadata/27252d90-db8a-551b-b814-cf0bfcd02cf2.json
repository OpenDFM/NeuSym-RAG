{
    "uuid": "27252d90-db8a-551b-b814-cf0bfcd02cf2",
    "title": "Towards Better Entity Linking with Multi-View Enhanced Distillation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-towards-better,\n    title = \"Towards Better Entity Linking with Multi-View Enhanced Distillation\",\n    author = \"Liu, Yi  and\n      Tian, Yuan  and\n      Lian, Jianxun  and\n      Wang, Xinlong  and\n      Cao, Yanan  and\n      Fang, Fang  and\n      Zhang, Wen  and\n      Huang, Haizhen  and\n      Deng, Weiwei  and\n      Zhang, Qi\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.542\",\n    doi = \"10.18653/v1/2023.acl-long.542\",\n    pages = \"9729--9743\",\n    abstract = \"Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes a Multi-View Enhanced Distillation (MVD) framework, which can effectively transfer knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. Each entity is split into multiple views to avoid irrelevant information being over-squashed into the mention-relevant view. We further design cross-alignment and self-alignment mechanisms for this framework to facilitate fine-grained knowledge distillation from the teacher model to the student model. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information. Experiments show our method achieves state-of-the-art performance on several entity linking benchmarks.\",\n}\n",
    "authors": [
        "Yi Liu",
        "Yuan Tian",
        "Jianxun Lian",
        "Xinlong Wang",
        "Yanan Cao",
        "Fang Fang",
        "Wen Zhang",
        "Haizhen Huang",
        "Weiwei Deng",
        "Qi Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.542.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/27252d90-db8a-551b-b814-cf0bfcd02cf2.pdf",
    "abstract": "Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes a Multi-View Enhanced Distillation (MVD) framework, which can effectively transfer knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. Each entity is split into multiple views to avoid irrelevant information being over-squashed into the mention-relevant view. We further design cross-alignment and self-alignment mechanisms for this framework to facilitate fine-grained knowledge distillation from the teacher model to the student model. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information. Experiments show our method achieves state-of-the-art performance on several entity linking benchmarks.",
    "num_pages": 15
}