{
    "uuid": "345da35a-35c5-5207-b85d-a1c14b80f28f",
    "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lin-etal-2024-interpretable,\n    title = \"Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models\",\n    author = \"Lin, Ying-Chun  and\n      Neville, Jennifer  and\n      Stokes, Jack  and\n      Yang, Longqi  and\n      Safavi, Tara  and\n      Wan, Mengting  and\n      Counts, Scott  and\n      Suri, Siddharth  and\n      Andersen, Reid  and\n      Xu, Xiaofeng  and\n      Gupta, Deepak  and\n      Jauhar, Sujay Kumar  and\n      Song, Xia  and\n      Buscher, Georg  and\n      Tiwary, Saurabh  and\n      Hecht, Brent  and\n      Teevan, Jaime\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.598\",\n    doi = \"10.18653/v1/2024.acl-long.598\",\n    pages = \"11100--11115\",\n    abstract = \"Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.\",\n}\n",
    "authors": [
        "Ying-Chun Lin",
        "Jennifer Neville",
        "Jack Stokes",
        "Longqi Yang",
        "Tara Safavi",
        "Mengting Wan",
        "Scott Counts",
        "Siddharth Suri",
        "Reid Andersen",
        "Xiaofeng Xu",
        "Deepak Gupta",
        "Sujay Kumar Jauhar",
        "Xia Song",
        "Georg Buscher",
        "Saurabh Tiwary",
        "Brent Hecht",
        "Jaime Teevan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.598.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/345da35a-35c5-5207-b85d-a1c14b80f28f.pdf",
    "abstract": "Accurate and interpretable user satisfaction estimation (USE) is critical for understanding, evaluating, and continuously improving conversational systems. Users express their satisfaction or dissatisfaction with diverse conversational patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented (customer service chatbot) conversational systems. Existing approaches based on featurized ML models or text embeddings fall short in extracting generalizable patterns and are hard to interpret. In this work, we show that LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches. Moreover, an LLM can be tailored for USE via an iterative prompting framework using supervision from labeled examples. Our proposed method, Supervised Prompting for User satisfaction Rubrics (SPUR), not only has higher accuracy but is more interpretable as it scores user satisfaction via learned rubrics with a detailed breakdown.",
    "num_pages": 16
}