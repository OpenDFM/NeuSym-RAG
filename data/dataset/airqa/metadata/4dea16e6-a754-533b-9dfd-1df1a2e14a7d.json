{
    "uuid": "4dea16e6-a754-533b-9dfd-1df1a2e14a7d",
    "title": "ChatGPT is not a good indigenous translator",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    "bibtex": "@inproceedings{stap-araabi-2023-chatgpt,\n    title = \"{C}hat{GPT} is not a good indigenous translator\",\n    author = \"Stap, David  and\n      Araabi, Ali\",\n    editor = \"Mager, Manuel  and\n      Ebrahimi, Abteen  and\n      Oncevay, Arturo  and\n      Rice, Enora  and\n      Rijhwani, Shruti  and\n      Palmer, Alexis  and\n      Kann, Katharina\",\n    booktitle = \"Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.americasnlp-1.17\",\n    doi = \"10.18653/v1/2023.americasnlp-1.17\",\n    pages = \"163--167\",\n    abstract = \"This report investigates the continuous challenges of Machine Translation (MT) systems on indigenous and extremely low-resource language pairs. Despite the notable achievements of Large Language Models (LLMs) that excel in various tasks, their applicability to low-resource languages remains questionable. In this study, we leveraged the AmericasNLP competition to evaluate the translation performance of different systems for Spanish to 11 indigenous languages from South America. Our team, LTLAmsterdam, submitted a total of four systems including GPT-4, a bilingual model, fine-tuned M2M100, and a combination of fine-tuned M2M100 with {\\$}k{\\$}NN-MT. We found that even large language models like GPT-4 are not well-suited for extremely low-resource languages. Our results suggest that fine-tuning M2M100 models can offer significantly better performance for extremely low-resource translation.\",\n}\n",
    "authors": [
        "David Stap",
        "Ali Araabi"
    ],
    "pdf_url": "https://aclanthology.org/2023.americasnlp-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4dea16e6-a754-533b-9dfd-1df1a2e14a7d.pdf",
    "abstract": "This report investigates the continuous challenges of Machine Translation (MT) systems on indigenous and extremely low-resource language pairs. Despite the notable achievements of Large Language Models (LLMs) that excel in various tasks, their applicability to low-resource languages remains questionable. In this study, we leveraged the AmericasNLP competition to evaluate the translation performance of different systems for Spanish to 11 indigenous languages from South America. Our team, LTLAmsterdam, submitted a total of four systems including GPT-4, a bilingual model, fine-tuned M2M100, and a combination of fine-tuned M2M100 with $k$NN-MT. We found that even large language models like GPT-4 are not well-suited for extremely low-resource languages. Our results suggest that fine-tuning M2M100 models can offer significantly better performance for extremely low-resource translation.",
    "num_pages": 5
}