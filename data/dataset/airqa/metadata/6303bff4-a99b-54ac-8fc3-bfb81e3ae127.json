{
    "uuid": "6303bff4-a99b-54ac-8fc3-bfb81e3ae127",
    "title": "CoELM: Construction-Enhanced Language Modeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xu-etal-2024-coelm,\n    title = \"{C}o{ELM}: Construction-Enhanced Language Modeling\",\n    author = \"Xu, Lvxiaowei  and\n      Gong, Zhilin  and\n      Dai, Jianhua  and\n      Wang, Tianxiang  and\n      Cai, Ming  and\n      Peng, Jiawei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.542\",\n    doi = \"10.18653/v1/2024.acl-long.542\",\n    pages = \"10061--10081\",\n    abstract = \"Recent studies have shown that integrating constructional information can improve the performance of pre-trained language models (PLMs) in natural language understanding. However, exploration into leveraging constructional information to enhance generative language models for natural language generation has been limited. Additionally, probing studies indicate that PLMs primarily grasp the syntactic structure of constructions but struggle to capture their semantics. In this work, we encode constructions as inductive biases to explicitly embed constructional semantics and guide the generation process. We begin by presenting a construction grammar induction framework designed to automatically identify constructions from corpora. Subsequently, we propose the Construction-Enhanced Language Model (CoELM). It introduces a construction-guided language modeling approach that employs a dynamic sequence reassembly strategy during pre-training. Extensive experiments have demonstrated the superiority of CoELM across various benchmarks.\",\n}\n",
    "authors": [
        "Lvxiaowei Xu",
        "Zhilin Gong",
        "Jianhua Dai",
        "Tianxiang Wang",
        "Ming Cai",
        "Jiawei Peng"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.542.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6303bff4-a99b-54ac-8fc3-bfb81e3ae127.pdf",
    "abstract": "Recent studies have shown that integrating constructional information can improve the performance of pre-trained language models (PLMs) in natural language understanding. However, exploration into leveraging constructional information to enhance generative language models for natural language generation has been limited. Additionally, probing studies indicate that PLMs primarily grasp the syntactic structure of constructions but struggle to capture their semantics. In this work, we encode constructions as inductive biases to explicitly embed constructional semantics and guide the generation process. We begin by presenting a construction grammar induction framework designed to automatically identify constructions from corpora. Subsequently, we propose the Construction-Enhanced Language Model (CoELM). It introduces a construction-guided language modeling approach that employs a dynamic sequence reassembly strategy during pre-training. Extensive experiments have demonstrated the superiority of CoELM across various benchmarks.",
    "num_pages": 21
}