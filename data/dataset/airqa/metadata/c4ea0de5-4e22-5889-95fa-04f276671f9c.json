{
    "uuid": "c4ea0de5-4e22-5889-95fa-04f276671f9c",
    "title": "Generating Visual Spatial Description via Holistic 3D Scene Understanding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2023-generating-visual,\n    title = \"Generating Visual Spatial Description via Holistic 3{D} Scene Understanding\",\n    author = \"Zhao, Yu  and\n      Fei, Hao  and\n      Ji, Wei  and\n      Wei, Jianguo  and\n      Zhang, Meishan  and\n      Zhang, Min  and\n      Chua, Tat-Seng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.442\",\n    doi = \"10.18653/v1/2023.acl-long.442\",\n    pages = \"7960--7977\",\n    abstract = \"Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation.\",\n}\n",
    "authors": [
        "Yu Zhao",
        "Hao Fei",
        "Wei Ji",
        "Jianguo Wei",
        "Meishan Zhang",
        "Min Zhang",
        "Tat-Seng Chua"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.442.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c4ea0de5-4e22-5889-95fa-04f276671f9c.pdf",
    "abstract": "Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation.",
    "num_pages": 18
}