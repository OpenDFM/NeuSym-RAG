{
    "uuid": "063ec1a5-6b1c-52a0-96bd-24ce24658fe7",
    "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{yang-etal-2023-domain,\n    title = \"In and Out-of-Domain Text Adversarial Robustness via Label Smoothing\",\n    author = \"Yang, Yahan  and\n      Dan, Soham  and\n      Roth, Dan  and\n      Lee, Insup\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.58\",\n    doi = \"10.18653/v1/2023.acl-short.58\",\n    pages = \"657--669\",\n    abstract = \"Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.\",\n}\n",
    "authors": [
        "Yahan Yang",
        "Soham Dan",
        "Dan Roth",
        "Insup Lee"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.58.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/063ec1a5-6b1c-52a0-96bd-24ce24658fe7.pdf",
    "abstract": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.",
    "num_pages": 13
}