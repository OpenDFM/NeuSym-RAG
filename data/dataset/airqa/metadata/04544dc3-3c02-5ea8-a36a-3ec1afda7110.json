{
    "uuid": "04544dc3-3c02-5ea8-a36a-3ec1afda7110",
    "title": "Reinforcement Learning-Driven LLM Agent for Automated Attacks on LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Fifth Workshop on Privacy in Natural Language Processing",
    "bibtex": "@inproceedings{wang-etal-2024-reinforcement-learning,\n    title = \"Reinforcement Learning-Driven {LLM} Agent for Automated Attacks on {LLM}s\",\n    author = \"Wang, Xiangwen  and\n      Peng, Jie  and\n      Xu, Kaidi  and\n      Yao, Huaxiu  and\n      Chen, Tianlong\",\n    editor = \"Habernal, Ivan  and\n      Ghanavati, Sepideh  and\n      Ravichander, Abhilasha  and\n      Jain, Vijayanta  and\n      Thaine, Patricia  and\n      Igamberdiev, Timour  and\n      Mireshghallah, Niloofar  and\n      Feyisetan, Oluwaseyi\",\n    booktitle = \"Proceedings of the Fifth Workshop on Privacy in Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.privatenlp-1.17\",\n    pages = \"170--177\",\n    abstract = \"Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMs{'} safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful information without controlling the specific content of their output. Exactly control of the LLM output can produce more inconspicuous attacks which could reveal a new page for LLM security. To achieve this, we propose RLTA: the Reinforcement Learning Targeted Attack, a framework that is designed for attacking language models (LLMs) and is adaptable to both white box (weight accessible) and black box (weight inaccessible) scenarios. It is capable of automatically generating malicious prompts that trigger target LLMs to produce specific outputs. We demonstrate RLTA in two different scenarios: LLM trojan detection and jailbreaking. The comprehensive experimental results show the potential of RLTA in enhancing the security measures surrounding contemporary LLMs.\",\n}\n",
    "authors": [
        "Xiangwen Wang",
        "Jie Peng",
        "Kaidi Xu",
        "Huaxiu Yao",
        "Tianlong Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.privatenlp-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/04544dc3-3c02-5ea8-a36a-3ec1afda7110.pdf",
    "abstract": "Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMsâ€™ safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful information without controlling the specific content of their output. Exactly control of the LLM output can produce more inconspicuous attacks which could reveal a new page for LLM security. To achieve this, we propose RLTA: the Reinforcement Learning Targeted Attack, a framework that is designed for attacking language models (LLMs) and is adaptable to both white box (weight accessible) and black box (weight inaccessible) scenarios. It is capable of automatically generating malicious prompts that trigger target LLMs to produce specific outputs. We demonstrate RLTA in two different scenarios: LLM trojan detection and jailbreaking. The comprehensive experimental results show the potential of RLTA in enhancing the security measures surrounding contemporary LLMs.",
    "num_pages": 8
}