{
    "uuid": "a52e5101-8303-5b0f-a639-e15d1af5d873",
    "title": "TeleChat: An Open-source Billingual Large Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)",
    "bibtex": "@inproceedings{wang-etal-2024-telechat,\n    title = \"{T}ele{C}hat: An Open-source Billingual Large Language Model\",\n    author = \"Wang, Zihan  and\n      Liuxz2@chinatelecom.cn, Liuxz2@chinatelecom.cn  and\n      Liusx14@chinatelecom.cn, Liusx14@chinatelecom.cn  and\n      Yao, Yitong  and\n      Huangyy121@chinatelecom.cn, Huangyy121@chinatelecom.cn  and\n      Mengxiang, Li  and\n      He, Zhongjiang  and\n      Liyx25@chinatelecom.cn, Liyx25@chinatelecom.cn  and\n      Pulw@chinatelecom.cn, Pulw@chinatelecom.cn  and\n      Xuhn@chinatelecom.cn, Xuhn@chinatelecom.cn  and\n      Wang, Chao  and\n      Song, Shuangyong\",\n    editor = \"Wong, Kam-Fai  and\n      Zhang, Min  and\n      Xu, Ruifeng  and\n      Li, Jing  and\n      Wei, Zhongyu  and\n      Gui, Lin  and\n      Liang, Bin  and\n      Zhao, Runcong\",\n    booktitle = \"Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sighan-1.2\",\n    pages = \"10--20\",\n    abstract = \"In this paper, we present \\textbf{TeleChat}, a collection of large language models (LLMs) with parameters of 7 billion and 12 billion. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, encompassing trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including general dialogue generation, language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves state-of-the-art performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat-7B and TeleChat-12B, along with code and a portion of our filtered high-quality pretraining data, to the public community.\",\n}\n",
    "authors": [
        "Zihan Wang",
        "Liuxz2@chinatelecom.cn Liuxz2@chinatelecom.cn",
        "Liusx14@chinatelecom.cn Liusx14@chinatelecom.cn",
        "Yitong Yao",
        "Huangyy121@chinatelecom.cn Huangyy121@chinatelecom.cn",
        "Li Mengxiang",
        "Zhongjiang He",
        "Liyx25@chinatelecom.cn Liyx25@chinatelecom.cn",
        "Pulw@chinatelecom.cn Pulw@chinatelecom.cn",
        "Xuhn@chinatelecom.cn Xuhn@chinatelecom.cn",
        "Chao Wang",
        "Shuangyong Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.sighan-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a52e5101-8303-5b0f-a639-e15d1af5d873.pdf",
    "abstract": "In this paper, we present TeleChat, a collection of large language models (LLMs) with parameters of 7 billion and 12 billion. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, encompassing trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including general dialogue generation, language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves state-of-the-art performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing LLMs, we release the fine-tuned model checkpoints of TeleChat-7B and TeleChat-12B, along with code and a portion of our filtered high-quality pretraining data, to the public community.",
    "num_pages": 11
}