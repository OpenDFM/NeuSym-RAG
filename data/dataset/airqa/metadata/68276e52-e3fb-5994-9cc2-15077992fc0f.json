{
    "uuid": "68276e52-e3fb-5994-9cc2-15077992fc0f",
    "title": "DIMSIM: Distilled Multilingual Critics for Indic Text Simplification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{mondal-etal-2024-dimsim,\n    title = \"{DIMSIM}: Distilled Multilingual Critics for {I}ndic Text Simplification\",\n    author = \"Mondal, Sneha  and\n      Ritika, Ritika  and\n      Agrawal, Ashish  and\n      Jyothi, Preethi  and\n      Raghuveer, Aravindan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.952\",\n    doi = \"10.18653/v1/2024.findings-acl.952\",\n    pages = \"16093--16109\",\n    abstract = \"Self-correction techniques have recently emerged as a promising framework to improve the quality of responses generated by large language models (LLMs). Few-shot prompted LLMs act as critics to produce feedback for an input, which is further fed to a refiner (also an LLM) to produce an output. However, these critique-refine steps require multiple expensive LLM calls. To circumvent this large inference cost, we borrow inspiration from prior work on knowledge distillation and propose the use of critique distillation to train critic models. These are smaller sequence-to-sequence models that are trained on input-critique pairs generated by an LLM. We focus on the problem of text simplification for three Indian languages: Hindi, Bengali and Marathi. This task is a good fit for self-correction style techniques. It also hasn{'}t been systematically explored for Indian languages before. We train two separate critics that focus on lexical and structure complexity, and show that it is surprisingly more effective than using an LLM directly as a critic in both 0-shot and few-shot settings. We also show the benefits of training multilingual critics, as opposed to monolingual critics. Extensive human evaluations show that on average, raters find 80{\\%} of DIMSIM{'}s output to be simple and easy to read.\",\n}\n",
    "authors": [
        "Sneha Mondal",
        "Ritika Ritika",
        "Ashish Agrawal",
        "Preethi Jyothi",
        "Aravindan Raghuveer"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.952.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/68276e52-e3fb-5994-9cc2-15077992fc0f.pdf",
    "abstract": "Self-correction techniques have recently emerged as a promising framework to improve the quality of responses generated by large language models (LLMs). Few-shot prompted LLMs act as critics to produce feedback for an input, which is further fed to a refiner (also an LLM) to produce an output. However, these critique-refine steps require multiple expensive LLM calls. To circumvent this large inference cost, we borrow inspiration from prior work on knowledge distillation and propose the use of critique distillation to train critic models. These are smaller sequence-to-sequence models that are trained on input-critique pairs generated by an LLM. We focus on the problem of text simplification for three Indian languages: Hindi, Bengali and Marathi. This task is a good fit for self-correction style techniques. It also hasn’t been systematically explored for Indian languages before. We train two separate critics that focus on lexical and structure complexity, and show that it is surprisingly more effective than using an LLM directly as a critic in both 0-shot and few-shot settings. We also show the benefits of training multilingual critics, as opposed to monolingual critics. Extensive human evaluations show that on average, raters find 80% of DIMSIM’s output to be simple and easy to read.",
    "num_pages": 17
}