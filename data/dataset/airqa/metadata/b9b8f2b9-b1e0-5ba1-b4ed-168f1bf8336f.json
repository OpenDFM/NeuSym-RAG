{
    "uuid": "b9b8f2b9-b1e0-5ba1-b4ed-168f1bf8336f",
    "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{yang-etal-2024-revisiting,\n    title = \"Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration\",\n    author = \"Yang, Kejuan  and\n      Liu, Xiao  and\n      Men, Kaiwen  and\n      Zeng, Aohan  and\n      Dong, Yuxiao  and\n      Tang, Jie\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.523\",\n    doi = \"10.18653/v1/2024.findings-acl.523\",\n    pages = \"8841--8852\",\n    abstract = \"We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing window-wise attention and positional embedding techniques. We first show that a simple yet strong baseline, weighted sum ensemble, is missing for the in-context few-shot classification. Moreover, on more challenging Chain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected deterioration regarding question miscomprehension and false inference. Based on our findings, we suggest that the existing PCW design may not guarantee sufficient improvement and practicality in handling lengthy documents in real-world applications. More community efforts on enabling language models{'} long context understanding ability should be paid.\",\n}\n",
    "authors": [
        "Kejuan Yang",
        "Xiao Liu",
        "Kaiwen Men",
        "Aohan Zeng",
        "Yuxiao Dong",
        "Jie Tang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.523.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b9b8f2b9-b1e0-5ba1-b4ed-168f1bf8336f.pdf",
    "abstract": "We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing window-wise attention and positional embedding techniques. We first show that a simple yet strong baseline, weighted sum ensemble, is missing for the in-context few-shot classification. Moreover, on more challenging Chain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected deterioration regarding question miscomprehension and false inference. Based on our findings, we suggest that the existing PCW design may not guarantee sufficient improvement and practicality in handling lengthy documents in real-world applications. More community efforts on enabling language modelsâ€™ long context understanding ability should be paid.",
    "num_pages": 12
}