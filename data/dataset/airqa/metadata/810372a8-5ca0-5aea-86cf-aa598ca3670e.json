{
    "uuid": "810372a8-5ca0-5aea-86cf-aa598ca3670e",
    "title": "Cross-lingual AMR Aligner: Paying Attention to Cross-Attention",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{martinez-lorenzo-etal-2023-cross,\n    title = \"Cross-lingual {AMR} Aligner: Paying Attention to Cross-Attention\",\n    author = \"Mart{\\'\\i}nez Lorenzo, Abelardo Carlos  and\n      Huguet Cabot, Pere Llu{\\'\\i}s  and\n      Navigli, Roberto\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.109\",\n    doi = \"10.18653/v1/2023.findings-acl.109\",\n    pages = \"1726--1742\",\n    abstract = \"This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner{'}s ability to obtain them across multiple languages. Our code will be available at [\\url{https://www.github.com/babelscape/AMR-alignment}](\\url{https://www.github.com/babelscape/AMR-alignment}).\",\n}\n",
    "authors": [
        "Abelardo Carlos Martínez Lorenzo",
        "Pere Lluís Huguet Cabot",
        "Roberto Navigli"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.109.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/810372a8-5ca0-5aea-86cf-aa598ca3670e.pdf",
    "abstract": "This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner’s ability to obtain them across multiple languages. Our code will be available at [https://www.github.com/babelscape/AMR-alignment](https://www.github.com/babelscape/AMR-alignment).",
    "num_pages": 17
}