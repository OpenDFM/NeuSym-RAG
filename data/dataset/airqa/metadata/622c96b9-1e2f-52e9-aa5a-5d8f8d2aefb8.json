{
    "uuid": "622c96b9-1e2f-52e9-aa5a-5d8f8d2aefb8",
    "title": "Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
    "bibtex": "@inproceedings{liu-etal-2024-modeling,\n    title = \"Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with {LLM}s\",\n    author = \"Liu, Ye  and\n      Meng, Rui  and\n      Bhat, Meghana Moorthy  and\n      Joty, Shafiq  and\n      Xiong, Caiming  and\n      Zhou, Yingbo  and\n      Yavuz, Semih\",\n    editor = \"Li, Sha  and\n      Li, Manling  and\n      Zhang, Michael JQ  and\n      Choi, Eunsol  and\n      Geva, Mor  and\n      Hase, Peter  and\n      Ji, Heng\",\n    booktitle = \"Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.knowllm-1.7\",\n    doi = \"10.18653/v1/2024.knowllm-1.7\",\n    pages = \"69--82\",\n    abstract = \"The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating {``}unknown{''} outputs, even when the correct document is among the top-$k$ retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs. On three open-domain question answering datesets, NQ, TriviaQA and SQuAD, our multi-round approaches outperform traditional concatenation approach, achieving over a 10{\\%} improvement in answer EM.\",\n}\n",
    "authors": [
        "Ye Liu",
        "Rui Meng",
        "Meghana Moorthy Bhat",
        "Shafiq Joty",
        "Caiming Xiong",
        "Yingbo Zhou",
        "Semih Yavuz"
    ],
    "pdf_url": "https://aclanthology.org/2024.knowllm-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/622c96b9-1e2f-52e9-aa5a-5d8f8d2aefb8.pdf",
    "abstract": "The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating “unknown” outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs. On three open-domain question answering datesets, NQ, TriviaQA and SQuAD, our multi-round approaches outperform traditional concatenation approach, achieving over a 10% improvement in answer EM.",
    "num_pages": 14
}