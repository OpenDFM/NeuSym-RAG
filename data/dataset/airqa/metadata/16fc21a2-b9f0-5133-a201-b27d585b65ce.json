{
    "uuid": "16fc21a2-b9f0-5133-a201-b27d585b65ce",
    "title": "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{chen-etal-2024-dual,\n    title = \"{DUAL}-{REFLECT}: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms\",\n    author = \"Chen, Andong  and\n      Lou, Lianzhang  and\n      Chen, Kehai  and\n      Bai, Xuefeng  and\n      Xiang, Yang  and\n      Yang, Muyun  and\n      Zhao, Tiejun  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.64\",\n    doi = \"10.18653/v1/2024.acl-short.64\",\n    pages = \"693--704\",\n    abstract = \"Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine transla004 tion. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models{'} self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.\",\n}\n",
    "authors": [
        "Andong Chen",
        "Lianzhang Lou",
        "Kehai Chen",
        "Xuefeng Bai",
        "Yang Xiang",
        "Muyun Yang",
        "Tiejun Zhao",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.64.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/16fc21a2-b9f0-5133-a201-b27d585b65ce.pdf",
    "abstract": "Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine transla004 tion. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the modelsâ€™ self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs.",
    "num_pages": 12
}