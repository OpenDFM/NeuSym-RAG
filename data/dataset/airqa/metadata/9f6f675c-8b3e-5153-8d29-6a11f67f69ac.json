{
    "uuid": "9f6f675c-8b3e-5153-8d29-6a11f67f69ac",
    "title": "Referring to Screen Texts with Voice Assistants",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{bhargava-etal-2023-referring,\n    title = \"Referring to Screen Texts with Voice Assistants\",\n    author = \"Bhargava, Shruti  and\n      Dhoot, Anand  and\n      Jonsson, Ing-marie  and\n      Nguyen, Hoang Long  and\n      Patel, Alkesh  and\n      Yu, Hong  and\n      Renkens, Vincent\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.72\",\n    doi = \"10.18653/v1/2023.acl-industry.72\",\n    pages = \"752--762\",\n    abstract = \"Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their users{'} context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory.\",\n}\n",
    "authors": [
        "Shruti Bhargava",
        "Anand Dhoot",
        "Ing-marie Jonsson",
        "Hoang Long Nguyen",
        "Alkesh Patel",
        "Hong Yu",
        "Vincent Renkens"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.72.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9f6f675c-8b3e-5153-8d29-6a11f67f69ac.pdf",
    "abstract": "Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their usersâ€™ context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory.",
    "num_pages": 11
}