{
    "uuid": "73e82150-1e76-570c-808e-75f72fd332b3",
    "title": "STAGE: Simplified Text-Attributed Graph Embeddings using Pre-trained LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
    "bibtex": "@inproceedings{zolnai-lucas-etal-2024-stage,\n    title = \"{STAGE}: Simplified Text-Attributed Graph Embeddings using Pre-trained {LLM}s\",\n    author = \"Zolnai-Lucas, Aaron  and\n      Boylan, Jack  and\n      Hokamp, Chris  and\n      Ghaffari, Parsa\",\n    editor = \"Biswas, Russa  and\n      Kaffee, Lucie-Aim{\\'e}e  and\n      Agarwal, Oshin  and\n      Minervini, Pasquale  and\n      Singh, Sameer  and\n      de Melo, Gerard\",\n    booktitle = \"Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.kallm-1.10\",\n    doi = \"10.18653/v1/2024.kallm-1.10\",\n    pages = \"92--104\",\n    abstract = \"We present STAGE, a straightforward yet effective method for enhancing node features in Graph Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our approach leverages Large-Language Models (LLMs) to generate embeddings for textual attributes. STAGE achieves competitive results on various node classification benchmarks while also maintaining a simplicity in implementation relative to current state-of-the-art (SoTA) techniques. We show that utilizing pre-trained LLMs as embedding generators provides robust features for ensemble GNN training, enabling pipelines that are simpler than current SoTA approaches which require multiple expensive training and prompting stages. We also implement diffusion-pattern GNNs in an effort to make this pipeline scalable to graphs beyond academic benchmarks.\",\n}\n",
    "authors": [
        "Aaron Zolnai-Lucas",
        "Jack Boylan",
        "Chris Hokamp",
        "Parsa Ghaffari"
    ],
    "pdf_url": "https://aclanthology.org/2024.kallm-1.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/73e82150-1e76-570c-808e-75f72fd332b3.pdf",
    "abstract": "We present STAGE, a straightforward yet effective method for enhancing node features in Graph Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our approach leverages Large-Language Models (LLMs) to generate embeddings for textual attributes. STAGE achieves competitive results on various node classification benchmarks while also maintaining a simplicity in implementation relative to current state-of-the-art (SoTA) techniques. We show that utilizing pre-trained LLMs as embedding generators provides robust features for ensemble GNN training, enabling pipelines that are simpler than current SoTA approaches which require multiple expensive training and prompting stages. We also implement diffusion-pattern GNNs in an effort to make this pipeline scalable to graphs beyond academic benchmarks.",
    "num_pages": 13
}