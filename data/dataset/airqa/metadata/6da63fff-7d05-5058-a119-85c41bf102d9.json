{
    "uuid": "6da63fff-7d05-5058-a119-85c41bf102d9",
    "title": "Human-Centered Design Recommendations for LLM-as-a-judge",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
    "bibtex": "@inproceedings{pan-etal-2024-human,\n    title = \"Human-Centered Design Recommendations for {LLM}-as-a-judge\",\n    author = \"Pan, Qian  and\n      Ashktorab, Zahra  and\n      Desmond, Michael  and\n      Santill{\\'a}n Cooper, Mart{\\'\\i}n  and\n      Johnson, James  and\n      Nair, Rahul  and\n      Daly, Elizabeth  and\n      Geyer, Werner\",\n    editor = \"Soni, Nikita  and\n      Flek, Lucie  and\n      Sharma, Ashish  and\n      Yang, Diyi  and\n      Hooker, Sara  and\n      Schwartz, H. Andrew\",\n    booktitle = \"Proceedings of the 1st Human-Centered Large Language Modeling Workshop\",\n    month = aug,\n    year = \"2024\",\n    address = \"TBD\",\n    publisher = \"ACL\",\n    url = \"https://aclanthology.org/2024.hucllm-1.2\",\n    doi = \"10.18653/v1/2024.hucllm-1.2\",\n    pages = \"16--29\",\n    abstract = \"Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human{'}s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners{'} preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.\",\n}\n",
    "authors": [
        "Qian Pan",
        "Zahra Ashktorab",
        "Michael Desmond",
        "Martín Santillán Cooper",
        "James Johnson",
        "Rahul Nair",
        "Elizabeth Daly",
        "Werner Geyer"
    ],
    "pdf_url": "https://aclanthology.org/2024.hucllm-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6da63fff-7d05-5058-a119-85c41bf102d9.pdf",
    "abstract": "Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human’s intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners’ preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.",
    "num_pages": 14
}