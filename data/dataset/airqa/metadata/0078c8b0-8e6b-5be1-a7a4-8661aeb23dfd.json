{
    "uuid": "0078c8b0-8e6b-5be1-a7a4-8661aeb23dfd",
    "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{varshney-etal-2024-art,\n    title = \"The Art of Defending: A Systematic Evaluation and Analysis of {LLM} Defense Strategies on Safety and Over-Defensiveness\",\n    author = \"Varshney, Neeraj  and\n      Dolin, Pavel  and\n      Seth, Agastya  and\n      Baral, Chitta\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.776\",\n    doi = \"10.18653/v1/2024.findings-acl.776\",\n    pages = \"13111--13128\",\n    abstract = \"As Large Language Models (LLMs) play an increasingly pivotal role in natural language processing applications, their safety concerns become critical areas of NLP research. This has resulted in the development of various LLM defense strategies. Unfortunately, despite the shared goal of improving the safety of LLMs, the evaluation suites across various research works are disjoint and lack diverse inputs to ensure accurate and precise evaluation estimates. Furthermore, the important factor of {`}over-defensiveness{'} on the safe inputs has largely remained overlooked. Addressing these limitations, this paper presents a systematic evaluation, comparison, and analysis of various LLM defense strategies over both {`}safety{'} and {`}over-defensiveness{'}. To this end, we compile a large and diverse collection of safe and unsafe prompts, design precise evaluation methodology, and study the efficacy of various LLM defense strategies on multiple state-of-the-art LLMs. Our work reveals a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the safety of LLMs.\",\n}\n",
    "authors": [
        "Neeraj Varshney",
        "Pavel Dolin",
        "Agastya Seth",
        "Chitta Baral"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.776.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0078c8b0-8e6b-5be1-a7a4-8661aeb23dfd.pdf",
    "abstract": "As Large Language Models (LLMs) play an increasingly pivotal role in natural language processing applications, their safety concerns become critical areas of NLP research. This has resulted in the development of various LLM defense strategies. Unfortunately, despite the shared goal of improving the safety of LLMs, the evaluation suites across various research works are disjoint and lack diverse inputs to ensure accurate and precise evaluation estimates. Furthermore, the important factor of ‘over-defensiveness’ on the safe inputs has largely remained overlooked. Addressing these limitations, this paper presents a systematic evaluation, comparison, and analysis of various LLM defense strategies over both ‘safety’ and ‘over-defensiveness’. To this end, we compile a large and diverse collection of safe and unsafe prompts, design precise evaluation methodology, and study the efficacy of various LLM defense strategies on multiple state-of-the-art LLMs. Our work reveals a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the safety of LLMs.",
    "num_pages": 18
}