{
    "uuid": "474f727e-2d57-53b3-8338-55d5e81d5909",
    "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{kementchedjhieva-chalkidis-2023-exploration,\n    title = \"An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text\",\n    author = \"Kementchedjhieva, Yova  and\n      Chalkidis, Ilias\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.360\",\n    doi = \"10.18653/v1/2023.findings-acl.360\",\n    pages = \"5828--5843\",\n    abstract = \"Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets{---}two in the legal domain and two in the biomedical domain, each with two levels of label granularity{---} and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.\",\n}\n",
    "authors": [
        "Yova Kementchedjhieva",
        "Ilias Chalkidis"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.360.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/474f727e-2d57-53b3-8338-55d5e81d5909.pdf",
    "abstract": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets—two in the legal domain and two in the biomedical domain, each with two levels of label granularity— and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.",
    "num_pages": 16
}