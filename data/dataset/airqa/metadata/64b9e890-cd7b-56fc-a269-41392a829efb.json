{
    "uuid": "64b9e890-cd7b-56fc-a269-41392a829efb",
    "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2024-analysing,\n    title = \"Analysing The Impact of Sequence Composition on Language Model Pre-Training\",\n    author = \"Zhao, Yu  and\n      Qu, Yuanbin  and\n      Staniszewski, Konrad  and\n      Tworkowski, Szymon  and\n      Liu, Wei  and\n      Mi{\\l}o{\\'s}, Piotr  and\n      Wu, Yuxiang  and\n      Minervini, Pasquale\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.427\",\n    doi = \"10.18653/v1/2024.acl-long.427\",\n    pages = \"7897--7912\",\n    abstract = \"Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use \\textit{causal masking} to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In \\textit{intra-document causal masking}, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, Bm25Chunk, can improve in-context learning (+11.6{\\%}), knowledge memorisation (+9.8{\\%}), and context utilisation (+7.2{\\%}) abilities of language models without sacrificing efficiency.\",\n}\n",
    "authors": [
        "Yu Zhao",
        "Yuanbin Qu",
        "Konrad Staniszewski",
        "Szymon Tworkowski",
        "Wei Liu",
        "Piotr Miłoś",
        "Yuxiang Wu",
        "Pasquale Minervini"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.427.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/64b9e890-cd7b-56fc-a269-41392a829efb.pdf",
    "abstract": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, Bm25Chunk, can improve in-context learning (+11.6%), knowledge memorisation (+9.8%), and context utilisation (+7.2%) abilities of language models without sacrificing efficiency.",
    "num_pages": 16
}