{
    "uuid": "63361855-939e-5abd-b0f3-cbac907d0a8d",
    "title": "Benchmarking Diverse-Modal Entity Linking with Generative Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-benchmarking,\n    title = \"Benchmarking Diverse-Modal Entity Linking with Generative Models\",\n    author = \"Wang, Sijia  and\n      Li, Alexander Hanbo  and\n      Zhu, Henghui  and\n      Zhang, Sheng  and\n      Perera, Pramuditha  and\n      Hang, Chung-Wei  and\n      Ma, Jie  and\n      Wang, William Yang  and\n      Wang, Zhiguo  and\n      Castelli, Vittorio  and\n      Xiang, Bing  and\n      Ng, Patrick\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.497\",\n    doi = \"10.18653/v1/2023.findings-acl.497\",\n    pages = \"7841--7857\",\n    abstract = \"Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training GDMM with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenge of DMEL, facilitating future researches on this task.\",\n}\n",
    "authors": [
        "Sijia Wang",
        "Alexander Hanbo Li",
        "Henghui Zhu",
        "Sheng Zhang",
        "Pramuditha Perera",
        "Chung-Wei Hang",
        "Jie Ma",
        "William Yang Wang",
        "Zhiguo Wang",
        "Vittorio Castelli",
        "Bing Xiang",
        "Patrick Ng"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.497.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/63361855-939e-5abd-b0f3-cbac907d0a8d.pdf",
    "abstract": "Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training GDMM with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenge of DMEL, facilitating future researches on this task.",
    "num_pages": 17
}