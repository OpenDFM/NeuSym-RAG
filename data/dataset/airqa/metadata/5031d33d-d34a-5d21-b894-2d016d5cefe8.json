{
    "uuid": "5031d33d-d34a-5d21-b894-2d016d5cefe8",
    "title": "Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{tan-etal-2023-intermediate,\n    title = \"Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method\",\n    author = \"Tan, Shicheng  and\n      Tam, Weng Lam  and\n      Wang, Yuanchun  and\n      Gong, Wenwen  and\n      Zhao, Shu  and\n      Zhang, Peng  and\n      Tang, Jie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.614\",\n    doi = \"10.18653/v1/2023.findings-acl.614\",\n    pages = \"9678--9696\",\n    abstract = \"The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the model{'}s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively. Moreover, the parameters of vocabulary are usually neglected in existing methods. To address these problems, we propose a general language model distillation (GLMD) method that performs two-stage word prediction distillation and vocabulary compression, which is simple and surprisingly shows extremely strong performance. Specifically, GLMD supports more general application scenarios by eliminating the constraints of dimension and structure between models and the need for labeled datasets through the absence of intermediate layers and golden labels. Meanwhile, based on the long-tailed distribution of word frequencies in the data, GLMD designs a strategy of vocabulary compression through decreasing vocabulary size instead of dimensionality. Experimental results show that our method outperforms 25 state-of-the-art methods on the SuperGLUE benchmark, achieving an average score that surpasses the best method by 3{\\%}.\",\n}\n",
    "authors": [
        "Shicheng Tan",
        "Weng Lam Tam",
        "Yuanchun Wang",
        "Wenwen Gong",
        "Shu Zhao",
        "Peng Zhang",
        "Jie Tang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.614.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5031d33d-d34a-5d21-b894-2d016d5cefe8.pdf",
    "abstract": "The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the modelâ€™s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively. Moreover, the parameters of vocabulary are usually neglected in existing methods. To address these problems, we propose a general language model distillation (GLMD) method that performs two-stage word prediction distillation and vocabulary compression, which is simple and surprisingly shows extremely strong performance. Specifically, GLMD supports more general application scenarios by eliminating the constraints of dimension and structure between models and the need for labeled datasets through the absence of intermediate layers and golden labels. Meanwhile, based on the long-tailed distribution of word frequencies in the data, GLMD designs a strategy of vocabulary compression through decreasing vocabulary size instead of dimensionality. Experimental results show that our method outperforms 25 state-of-the-art methods on the SuperGLUE benchmark, achieving an average score that surpasses the best method by 3%.",
    "num_pages": 19
}