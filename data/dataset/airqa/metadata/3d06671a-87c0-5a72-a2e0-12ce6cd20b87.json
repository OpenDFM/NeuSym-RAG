{
    "uuid": "3d06671a-87c0-5a72-a2e0-12ce6cd20b87",
    "title": "Hierarchical syntactic structure in human-like language models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    "bibtex": "@inproceedings{wolfman-etal-2024-hierarchical,\n    title = \"Hierarchical syntactic structure in human-like language models\",\n    author = \"Wolfman, Michael  and\n      Dunagan, Donald  and\n      Brennan, Jonathan  and\n      Hale, John\",\n    editor = \"Kuribayashi, Tatsuki  and\n      Rambelli, Giulia  and\n      Takmaz, Ece  and\n      Wicke, Philipp  and\n      Oseki, Yohei\",\n    booktitle = \"Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.cmcl-1.6\",\n    doi = \"10.18653/v1/2024.cmcl-1.6\",\n    pages = \"72--80\",\n    abstract = \"Language models (LMs) are a meeting point for cognitive modeling and computational linguistics. How should they be designed to serve as adequate cognitive models? To address this question, this study contrasts two Transformer-based LMs that share the same architecture. Only one of them analyzes sentences in terms of explicit hierarchical structure. Evaluating the two LMs against fMRI time series via the surprisal complexity metric, the results implicate the superior temporal gyrus. These findings underline the need for hierarchical sentence structures in word-by-word models of human language comprehension.\",\n}\n",
    "authors": [
        "Michael Wolfman",
        "Donald Dunagan",
        "Jonathan Brennan",
        "John Hale"
    ],
    "pdf_url": "https://aclanthology.org/2024.cmcl-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3d06671a-87c0-5a72-a2e0-12ce6cd20b87.pdf",
    "abstract": "Language models (LMs) are a meeting point for cognitive modeling and computational linguistics. How should they be designed to serve as adequate cognitive models? To address this question, this study contrasts two Transformer-based LMs that share the same architecture. Only one of them analyzes sentences in terms of explicit hierarchical structure. Evaluating the two LMs against fMRI time series via the surprisal complexity metric, the results implicate the superior temporal gyrus. These findings underline the need for hierarchical sentence structures in word-by-word models of human language comprehension.",
    "num_pages": 9
}