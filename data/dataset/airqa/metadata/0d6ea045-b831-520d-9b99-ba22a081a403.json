{
    "uuid": "0d6ea045-b831-520d-9b99-ba22a081a403",
    "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{karoui-etal-2023-stop,\n    title = \"Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages\",\n    author = \"Karoui, Yasmine  and\n      Lebret, R{\\'e}mi  and\n      Foroutan Eghlidi, Negar  and\n      Aberer, Karl\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.32\",\n    doi = \"10.18653/v1/2023.acl-short.32\",\n    pages = \"366--375\",\n    abstract = \"Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at \\url{https://github.com/Yasminekaroui/CliCoTea}.\",\n}\n",
    "authors": [
        "Yasmine Karoui",
        "RÃ©mi Lebret",
        "Negar Foroutan Eghlidi",
        "Karl Aberer"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/0d6ea045-b831-520d-9b99-ba22a081a403.pdf",
    "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at https://github.com/Yasminekaroui/CliCoTea.",
    "num_pages": 10
}