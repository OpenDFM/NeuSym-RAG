{
    "uuid": "c81cc613-5c7b-574b-8e39-929bd3f42a33",
    "title": "DRU at WojoodNER 2024: A Multi-level Method Approach",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{hamoud-etal-2024-dru,\n    title = \"{DRU} at {W}ojood{NER} 2024: A Multi-level Method Approach\",\n    author = \"Hamoud, Hadi  and\n      Chakra, Chadi  and\n      Hamdan, Nancy  and\n      Mraikhat, Osama  and\n      Albared, Doha  and\n      Zaraket, Fadi\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.104\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.104\",\n    pages = \"874--879\",\n    abstract = \"In this paper, we present our submission for the WojoodNER 2024 Shared Tasks addressing flat and nested sub-tasks (1, 2). We experiment with three different approaches. We train (i) an Arabic fine-tuned version of BLOOMZ-7b-mt, GEMMA-7b, and AraBERTv2 on multi-label token classifications task; (ii) two AraBERTv2 models, on main types and sub-types respectively; and (iii) one model for main types and four for the four sub-types. Based on the Wojood NER 2024 test set results, the three fine-tuned models performed similarly with AraBERTv2 favored (F1: Flat=.8780 Nested=.9040). The five model approach performed slightly better (F1: Flat=.8782 Nested=.9043).\",\n}\n",
    "authors": [
        "Hadi Hamoud",
        "Chadi Chakra",
        "Nancy Hamdan",
        "Osama Mraikhat",
        "Doha Albared",
        "Fadi Zaraket"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.104.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c81cc613-5c7b-574b-8e39-929bd3f42a33.pdf",
    "abstract": "In this paper, we present our submission for the WojoodNER 2024 Shared Tasks addressing flat and nested sub-tasks (1, 2). We experiment with three different approaches. We train (i) an Arabic fine-tuned version of BLOOMZ-7b-mt, GEMMA-7b, and AraBERTv2 on multi-label token classifications task; (ii) two AraBERTv2 models, on main types and sub-types respectively; and (iii) one model for main types and four for the four sub-types. Based on the Wojood NER 2024 test set results, the three fine-tuned models performed similarly with AraBERTv2 favored (F1: Flat=.8780 Nested=.9040). The five model approach performed slightly better (F1: Flat=.8782 Nested=.9043).",
    "num_pages": 6
}