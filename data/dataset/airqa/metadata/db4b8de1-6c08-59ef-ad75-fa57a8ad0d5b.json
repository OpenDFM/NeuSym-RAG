{
    "uuid": "db4b8de1-6c08-59ef-ad75-fa57a8ad0d5b",
    "title": "What Evidence Do Language Models Find Convincing?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wan-etal-2024-evidence,\n    title = \"What Evidence Do Language Models Find Convincing?\",\n    author = \"Wan, Alexander  and\n      Wallace, Eric  and\n      Klein, Dan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.403\",\n    doi = \"10.18653/v1/2024.acl-long.403\",\n    pages = \"7468--7484\",\n    abstract = \"Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as {``}is aspartame linked to cancer{''}. To resolve these ambiguous queries, one must search through a large range of websites and consider {``}which, if any, of this evidence do I find convincing?{''}. In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.\",\n}\n",
    "authors": [
        "Alexander Wan",
        "Eric Wallace",
        "Dan Klein"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.403.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/db4b8de1-6c08-59ef-ad75-fa57a8ad0d5b.pdf",
    "abstract": "Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as “is aspartame linked to cancer”. To resolve these ambiguous queries, one must search through a large range of websites and consider “which, if any, of this evidence do I find convincing?”. In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.",
    "num_pages": 17
}