{
    "uuid": "80b0a1f4-452e-5c22-a2f3-3c241c91433f",
    "title": "ELiRF-VRAIN at BioLaySumm: Boosting Lay Summarization Systems Performance with Ranking Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{ahuir-etal-2024-elirf,\n    title = \"{EL}i{RF}-{VRAIN} at {B}io{L}ay{S}umm: Boosting Lay Summarization Systems Performance with Ranking Models\",\n    author = \"Ahuir, Vicent  and\n      Torres, Diego  and\n      Segarra, Encarna  and\n      Hurtado, Llu{\\'\\i}s-F.\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.68\",\n    doi = \"10.18653/v1/2024.bionlp-1.68\",\n    pages = \"755--761\",\n    abstract = \"This paper presents our contribution to the BioLaySumm 2024 shared task of the 23rd BioNLP Workshop. The task is to create a lay summary, given a biomedical research article and its technical summary. As the input to the system could be large, a Longformer Encoder-Decoder (LED) has been used. We continuously pre-trained a general domain LED model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks were aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. Since the distribution of samples between the two datasets, eLife and PLOS, is unbalanced, we fine-tuned two models: one for eLife and another for PLOS. To increase the quality of the lay summaries of the system, we developed a regression model that helps us rank the summaries generated by the summarization models. This regression model predicts the quality of the summary in three different aspects: Relevance, Readability, and Factuality. We present the results of our models and a study to measure the ranking capabilities of the regression model.\",\n}\n",
    "authors": [
        "Vicent Ahuir",
        "Diego Torres",
        "Encarna Segarra",
        "Llu√≠s-F. Hurtado"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.68.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/80b0a1f4-452e-5c22-a2f3-3c241c91433f.pdf",
    "abstract": "This paper presents our contribution to the BioLaySumm 2024 shared task of the 23rd BioNLP Workshop. The task is to create a lay summary, given a biomedical research article and its technical summary. As the input to the system could be large, a Longformer Encoder-Decoder (LED) has been used. We continuously pre-trained a general domain LED model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks were aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. Since the distribution of samples between the two datasets, eLife and PLOS, is unbalanced, we fine-tuned two models: one for eLife and another for PLOS. To increase the quality of the lay summaries of the system, we developed a regression model that helps us rank the summaries generated by the summarization models. This regression model predicts the quality of the summary in three different aspects: Relevance, Readability, and Factuality. We present the results of our models and a study to measure the ranking capabilities of the regression model.",
    "num_pages": 7
}