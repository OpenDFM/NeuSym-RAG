{
    "uuid": "03588f6c-205d-5f47-be8f-3b5ec99ffdd5",
    "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{mo-etal-2023-convgqr,\n    title = \"{C}onv{GQR}: Generative Query Reformulation for Conversational Search\",\n    author = \"Mo, Fengran  and\n      Mao, Kelong  and\n      Zhu, Yutao  and\n      Wu, Yihong  and\n      Huang, Kaiyu  and\n      Nie, Jian-Yun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.274\",\n    doi = \"10.18653/v1/2023.acl-long.274\",\n    pages = \"4998--5012\",\n    abstract = \"In conversational search, the user{'}s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.\",\n}\n",
    "authors": [
        "Fengran Mo",
        "Kelong Mao",
        "Yutao Zhu",
        "Yihong Wu",
        "Kaiyu Huang",
        "Jian-Yun Nie"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.274.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/03588f6c-205d-5f47-be8f-3b5ec99ffdd5.pdf",
    "abstract": "In conversational search, the userâ€™s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR.",
    "num_pages": 15
}