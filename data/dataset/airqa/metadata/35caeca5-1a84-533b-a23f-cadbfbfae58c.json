{
    "uuid": "35caeca5-1a84-533b-a23f-cadbfbfae58c",
    "title": "MEEL: Multi-Modal Event Evolution Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{tao-etal-2024-meel,\n    title = \"{MEEL}: Multi-Modal Event Evolution Learning\",\n    author = \"Tao, Zhengwei  and\n      Jin, Zhi  and\n      Huang, Junqiang  and\n      Chen, Xiancai  and\n      Bai, Xiaoying  and\n      Zhang, Yifan  and\n      Tao, Chongyang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.528\",\n    doi = \"10.18653/v1/2024.findings-acl.528\",\n    pages = \"8912--8925\",\n    abstract = \"Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER is fundamental and underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans. Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution. In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction. We collect and curate a benchmark M-EV2 for MMER. Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs.\",\n}\n",
    "authors": [
        "Zhengwei Tao",
        "Zhi Jin",
        "Junqiang Huang",
        "Xiancai Chen",
        "Xiaoying Bai",
        "Yifan Zhang",
        "Chongyang Tao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.528.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/35caeca5-1a84-533b-a23f-cadbfbfae58c.pdf",
    "abstract": "Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER is fundamental and underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that existing models are insufficient to capture underlying principles governing event evolution in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the event evolution mechanism yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose an instruction encapsulation process that formulates the evolving graphs into instruction-tuning data, aligning the comprehension of event reasoning to humans. Finally, we observe that models trained in this way are still struggling to fully comprehend event evolution. In such a case, we propose the guiding discrimination strategy, in which models are trained to discriminate the improper evolution direction. We collect and curate a benchmark M-EV2 for MMER. Extensive experiments on M-EV2 validate the effectiveness of our approach, showcasing competitive performance in open-source multi-modal LLMs.",
    "num_pages": 14
}