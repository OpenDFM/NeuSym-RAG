{
    "uuid": "15154422-db1c-5fe2-895b-eaeabca6eef3",
    "title": "imapScore: Medical Fact Evaluation Made Easy",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wang-etal-2024-imapscore,\n    title = \"imap{S}core: Medical Fact Evaluation Made Easy\",\n    author = \"Wang, Huimin  and\n      Zhao, Yutian  and\n      Wu, Xian  and\n      Zheng, Yefeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.610\",\n    doi = \"10.18653/v1/2024.findings-acl.610\",\n    pages = \"10242--10257\",\n    abstract = \"Automatic evaluation of natural language generation (NLG) tasks has gained extensive research interests, since it can rapidly assess the performance of large language models (LLMs). However, automatic NLG evaluation struggles with medical QA because it fails to focus on the crucial correctness of medical facts throughout the generated text. To address this, this paper introduces a new data structure, \\textit{imap}, designed to capture key information in questions and answers, enabling evaluators to focus on essential details. The \\textit{imap} comprises three components: Query, Constraint, and Inform, each of which is in the form of term-value pairs to represent medical facts in a structural manner. We then introduce \\textit{imap}Score, which compares the corresponding medical term-value pairs in the \\textit{imap} to score generated texts. We utilize GPT-4 to extract \\textit{imap} from questions, human-annotated answers, and generated responses. To mitigate the diversity in medical terminology for fair term-value pairs comparison, we use a medical knowledge graph to assist GPT-4 in determining matches. To compare \\textit{imap}Score with existing NLG metrics, we establish a new benchmark dataset. The experimental results show that \\textit{imap}Score consistently outperforms state-of-the-art metrics, demonstrating an average improvement of 79.8{\\%} in correlation with human scores. Furthermore, incorporating \\textit{imap} into n-gram, embedding, and LLM metrics boosts the base versions, increasing correlation with human scores by averages of 89.9{\\%}, 81.7{\\%}, and 32.6{\\%}, respectively.\",\n}\n",
    "authors": [
        "Huimin Wang",
        "Yutian Zhao",
        "Xian Wu",
        "Yefeng Zheng"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.610.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/15154422-db1c-5fe2-895b-eaeabca6eef3.pdf",
    "abstract": "Automatic evaluation of natural language generation (NLG) tasks has gained extensive research interests, since it can rapidly assess the performance of large language models (LLMs). However, automatic NLG evaluation struggles with medical QA because it fails to focus on the crucial correctness of medical facts throughout the generated text. To address this, this paper introduces a new data structure, imap, designed to capture key information in questions and answers, enabling evaluators to focus on essential details. The imap comprises three components: Query, Constraint, and Inform, each of which is in the form of term-value pairs to represent medical facts in a structural manner. We then introduce imapScore, which compares the corresponding medical term-value pairs in the imap to score generated texts. We utilize GPT-4 to extract imap from questions, human-annotated answers, and generated responses. To mitigate the diversity in medical terminology for fair term-value pairs comparison, we use a medical knowledge graph to assist GPT-4 in determining matches. To compare imapScore with existing NLG metrics, we establish a new benchmark dataset. The experimental results show that imapScore consistently outperforms state-of-the-art metrics, demonstrating an average improvement of 79.8% in correlation with human scores. Furthermore, incorporating imap into n-gram, embedding, and LLM metrics boosts the base versions, increasing correlation with human scores by averages of 89.9%, 81.7%, and 32.6%, respectively.",
    "num_pages": 16
}