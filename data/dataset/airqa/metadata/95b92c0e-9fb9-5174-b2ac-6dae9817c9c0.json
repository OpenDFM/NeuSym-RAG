{
    "uuid": "95b92c0e-9fb9-5174-b2ac-6dae9817c9c0",
    "title": "KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{jang-etal-2023-know,\n    title = \"{KNOW} How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations\",\n    author = \"Jang, Myeongjun  and\n      Majumder, Bodhisattwa Prasad  and\n      McAuley, Julian  and\n      Lukasiewicz, Thomas  and\n      Camburu, Oana-Maria\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.47\",\n    doi = \"10.18653/v1/2023.acl-short.47\",\n    pages = \"540--553\",\n    abstract = \"While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not necessarily generate fewer inconsistencies. Moreover, we propose an off-the-shelf mitigation method to alleviate inconsistencies by grounding the model into external background knowledge. Our method decreases the inconsistencies of previous high-performing NLE models as detected by our attack.\",\n}\n",
    "authors": [
        "Myeongjun Jang",
        "Bodhisattwa Prasad Majumder",
        "Julian McAuley",
        "Thomas Lukasiewicz",
        "Oana-Maria Camburu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.47.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/95b92c0e-9fb9-5174-b2ac-6dae9817c9c0.pdf",
    "abstract": "While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not necessarily generate fewer inconsistencies. Moreover, we propose an off-the-shelf mitigation method to alleviate inconsistencies by grounding the model into external background knowledge. Our method decreases the inconsistencies of previous high-performing NLE models as detected by our attack.",
    "num_pages": 14
}