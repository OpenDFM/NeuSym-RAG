{
    "uuid": "c3b612a8-6fbb-5180-a177-716702c84fc4",
    "title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{lin-etal-2023-linear,\n    title = \"Linear Classifier: An Often-Forgotten Baseline for Text Classification\",\n    author = \"Lin, Yu-Chen  and\n      Chen, Si-An  and\n      Liu, Jie-Jyun  and\n      Lin, Chih-Jen\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.160\",\n    doi = \"10.18653/v1/2023.acl-short.160\",\n    pages = \"1876--1888\",\n    abstract = \"Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.\",\n}\n",
    "authors": [
        "Yu-Chen Lin",
        "Si-An Chen",
        "Jie-Jyun Liu",
        "Chih-Jen Lin"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.160.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c3b612a8-6fbb-5180-a177-716702c84fc4.pdf",
    "abstract": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points.",
    "num_pages": 13
}