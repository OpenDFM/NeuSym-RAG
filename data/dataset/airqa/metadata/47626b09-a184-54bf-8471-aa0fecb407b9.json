{
    "uuid": "47626b09-a184-54bf-8471-aa0fecb407b9",
    "title": "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2023-needle,\n    title = \"A Needle in a Haystack: An Analysis of High-Agreement Workers on {MT}urk for Summarization\",\n    author = \"Zhang, Lining  and\n      Mille, Simon  and\n      Hou, Yufang  and\n      Deutsch, Daniel  and\n      Clark, Elizabeth  and\n      Liu, Yixin  and\n      Mahamood, Saad  and\n      Gehrmann, Sebastian  and\n      Clinciu, Miruna  and\n      Chandu, Khyathi Raghavi  and\n      Sedoc, Jo{\\~a}o\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.835\",\n    doi = \"10.18653/v1/2023.acl-long.835\",\n    pages = \"14944--14982\",\n    abstract = \"To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.\",\n}\n",
    "authors": [
        "Lining Zhang",
        "Simon Mille",
        "Yufang Hou",
        "Daniel Deutsch",
        "Elizabeth Clark",
        "Yixin Liu",
        "Saad Mahamood",
        "Sebastian Gehrmann",
        "Miruna Clinciu",
        "Khyathi Raghavi Chandu",
        "Jo√£o Sedoc"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.835.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/47626b09-a184-54bf-8471-aa0fecb407b9.pdf",
    "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
    "num_pages": 39
}