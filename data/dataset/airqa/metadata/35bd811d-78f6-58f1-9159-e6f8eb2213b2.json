{
    "uuid": "35bd811d-78f6-58f1-9159-e6f8eb2213b2",
    "title": "It is a Bird Therefore it is a Robin: On BERTâ€™s Internal Consistency Between Hypernym Knowledge and Logical Words",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{guerin-chemla-2023-bird,\n    title = \"It is a Bird Therefore it is a Robin: On {BERT}{'}s Internal Consistency Between Hypernym Knowledge and Logical Words\",\n    author = \"Guerin, Nicolas  and\n      Chemla, Emmanuel\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.560\",\n    doi = \"10.18653/v1/2023.findings-acl.560\",\n    pages = \"8807--8817\",\n    abstract = \"The lexical knowledge of NLP systems shouldbe tested (i) for their internal consistency(avoiding groundedness issues) and (ii) bothfor content words and logical words. In thispaper we propose a new method to test the understandingof the hypernymy relationship bymeasuring its antisymmetry according to themodels. Previous studies often rely only on thedirect question (e.g., A robin is a ...), where weargue a correct answer could only rely on collocationalcues, rather than hierarchical cues. We show how to control for this, and how it isimportant. We develop a method to ask similarquestions about logical words that encode anentailment-like relation (e.g., because or therefore).Our results show important weaknessesof BERT-like models on these semantic tasks.\",\n}\n",
    "authors": [
        "Nicolas Guerin",
        "Emmanuel Chemla"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.560.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/35bd811d-78f6-58f1-9159-e6f8eb2213b2.pdf",
    "abstract": "The lexical knowledge of NLP systems shouldbe tested (i) for their internal consistency(avoiding groundedness issues) and (ii) bothfor content words and logical words. In thispaper we propose a new method to test the understandingof the hypernymy relationship bymeasuring its antisymmetry according to themodels. Previous studies often rely only on thedirect question (e.g., A robin is a ...), where weargue a correct answer could only rely on collocationalcues, rather than hierarchical cues. We show how to control for this, and how it isimportant. We develop a method to ask similarquestions about logical words that encode anentailment-like relation (e.g., because or therefore).Our results show important weaknessesof BERT-like models on these semantic tasks.",
    "num_pages": 11
}