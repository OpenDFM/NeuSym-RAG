{
    "uuid": "0351bd03-d094-5be7-88c7-e3302ba69e40",
    "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wang-etal-2024-instructgraph,\n    title = \"{I}nstruct{G}raph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment\",\n    author = \"Wang, Jianing  and\n      Wu, Junda  and\n      Hou, Yupeng  and\n      Liu, Yao  and\n      Gao, Ming  and\n      McAuley, Julian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.801\",\n    doi = \"10.18653/v1/2024.findings-acl.801\",\n    pages = \"13492--13510\",\n    abstract = \"Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose \\textbf{InstructGraph}, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output{'}s reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13{\\%} and 38{\\%}, respectively.\",\n}\n",
    "authors": [
        "Jianing Wang",
        "Junda Wu",
        "Yupeng Hou",
        "Yao Liu",
        "Ming Gao",
        "Julian McAuley"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.801.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0351bd03-d094-5be7-88c7-e3302ba69e40.pdf",
    "abstract": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the outputâ€™s reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13% and 38%, respectively.",
    "num_pages": 19
}