{
    "uuid": "73d25630-7787-5852-9bee-f4c6e5408c26",
    "title": "Improving Long Context Document-Level Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)",
    "bibtex": "@inproceedings{herold-ney-2023-improving,\n    title = \"Improving Long Context Document-Level Machine Translation\",\n    author = \"Herold, Christian  and\n      Ney, Hermann\",\n    editor = \"Strube, Michael  and\n      Braud, Chloe  and\n      Hardmeier, Christian  and\n      Li, Junyi Jessy  and\n      Loaiciga, Sharid  and\n      Zeldes, Amir\",\n    booktitle = \"Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.codi-1.15\",\n    doi = \"10.18653/v1/2023.codi-1.15\",\n    pages = \"112--125\",\n    abstract = \"Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneously reducing the memory consumption. For evaluation, we utilize targeted test sets in combination with novel evaluation techniques to analyze the translations in regards to specific discourse-related phenomena. We find that our approach is a good compromise between sentence-level NMT vs attending to the full context, especially in low resource scenarios.\",\n}\n",
    "authors": [
        "Christian Herold",
        "Hermann Ney"
    ],
    "pdf_url": "https://aclanthology.org/2023.codi-1.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/73d25630-7787-5852-9bee-f4c6e5408c26.pdf",
    "abstract": "Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneously reducing the memory consumption. For evaluation, we utilize targeted test sets in combination with novel evaluation techniques to analyze the translations in regards to specific discourse-related phenomena. We find that our approach is a good compromise between sentence-level NMT vs attending to the full context, especially in low resource scenarios.",
    "num_pages": 14
}