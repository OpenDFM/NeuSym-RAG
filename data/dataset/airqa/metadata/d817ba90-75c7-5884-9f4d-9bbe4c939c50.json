{
    "uuid": "d817ba90-75c7-5884-9f4d-9bbe4c939c50",
    "title": "Aligning Speech Segments Beyond Pure Semantics",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{heffernan-etal-2024-aligning,\n    title = \"Aligning Speech Segments Beyond Pure Semantics\",\n    author = \"Heffernan, Kevin  and\n      Kozhevnikov, Artyom  and\n      Barrault, Loic  and\n      Mourachko, Alexandre  and\n      Schwenk, Holger\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.216\",\n    doi = \"10.18653/v1/2024.findings-acl.216\",\n    pages = \"3626--3635\",\n    abstract = \"Multilingual parallel data for speech-to-speech translation is scarce and expensive to create from scratch. This is all the more true for expressive speech translation, which aims at preserving not only the semantics, but also the overall prosody (e.g. style, emotion, rate-of-speech). Existing corpora contain speech utterances with the same meaning, yet the overall prosody is typically different, as human annotators are not tasked with reproducing these aspects, or crowed-sourced efforts do not specifically target this kind of alignment in priority. In this paper, we propose a novel alignment algorithm, which automatically forms pairs of speech segments aligned not only in meaning, but also in expressivity. In order to validate our approach, we train an expressive multilingual speech-to-speech translation system on the automatically aligned data. Our experiments show that in comparison to semantic-only approaches, expressively aligned data yields large improvements in source expressivity preservation (e.g. 43{\\%} uplift in speech rate preservation on average), while still maintaining content translation quality. In some scenarios, results also indicate that this alignment algorithm can outperform standard, semantic-focused approaches even on content translation quality.\",\n}\n",
    "authors": [
        "Kevin Heffernan",
        "Artyom Kozhevnikov",
        "Loic Barrault",
        "Alexandre Mourachko",
        "Holger Schwenk"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.216.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d817ba90-75c7-5884-9f4d-9bbe4c939c50.pdf",
    "abstract": "Multilingual parallel data for speech-to-speech translation is scarce and expensive to create from scratch. This is all the more true for expressive speech translation, which aims at preserving not only the semantics, but also the overall prosody (e.g. style, emotion, rate-of-speech). Existing corpora contain speech utterances with the same meaning, yet the overall prosody is typically different, as human annotators are not tasked with reproducing these aspects, or crowed-sourced efforts do not specifically target this kind of alignment in priority. In this paper, we propose a novel alignment algorithm, which automatically forms pairs of speech segments aligned not only in meaning, but also in expressivity. In order to validate our approach, we train an expressive multilingual speech-to-speech translation system on the automatically aligned data. Our experiments show that in comparison to semantic-only approaches, expressively aligned data yields large improvements in source expressivity preservation (e.g. 43% uplift in speech rate preservation on average), while still maintaining content translation quality. In some scenarios, results also indicate that this alignment algorithm can outperform standard, semantic-focused approaches even on content translation quality.",
    "num_pages": 10
}