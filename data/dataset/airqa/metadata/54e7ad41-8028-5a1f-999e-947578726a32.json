{
    "uuid": "54e7ad41-8028-5a1f-999e-947578726a32",
    "title": "Connectivity Patterns are Task Embeddings",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{xi-etal-2023-connectivity,\n    title = \"Connectivity Patterns are Task Embeddings\",\n    author = \"Xi, Zhiheng  and\n      Zheng, Rui  and\n      Zhang, Yuansen  and\n      Huang, Xuanjing  and\n      Wei, Zhongyu  and\n      Peng, Minlong  and\n      Sun, Mingming  and\n      Zhang, Qi  and\n      Gui, Tao\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.759\",\n    doi = \"10.18653/v1/2023.findings-acl.759\",\n    pages = \"11993--12013\",\n    abstract = \"Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.\",\n}\n",
    "authors": [
        "Zhiheng Xi",
        "Rui Zheng",
        "Yuansen Zhang",
        "Xuanjing Huang",
        "Zhongyu Wei",
        "Minlong Peng",
        "Mingming Sun",
        "Qi Zhang",
        "Tao Gui"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.759.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/54e7ad41-8028-5a1f-999e-947578726a32.pdf",
    "abstract": "Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.",
    "num_pages": 21
}