{
    "uuid": "68219846-ab78-5b51-8284-a5f367b47b10",
    "title": "Prototypical Reward Network for Data-Efficient RLHF",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-prototypical,\n    title = \"Prototypical Reward Network for Data-Efficient RLHF\",\n    author = \"Zhang, Jinghan  and\n      Wang, Xiting  and\n      Jin, Yiqiao  and\n      Chen, Changyu  and\n      Zhang, Xinhao  and\n      Liu, Kunpeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.748\",\n    doi = \"10.18653/v1/2024.acl-long.748\",\n    pages = \"13871--13884\",\n    abstract = \"The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions.\",\n}\n",
    "authors": [
        "Jinghan Zhang",
        "Xiting Wang",
        "Yiqiao Jin",
        "Changyu Chen",
        "Xinhao Zhang",
        "Kunpeng Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.748.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/68219846-ab78-5b51-8284-a5f367b47b10.pdf",
    "abstract": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). Notably, collecting human feedback for RLHF can be resource-intensive and lead to scalability issues for LLMs and complex tasks. Our proposed framework Proto-RM leverages prototypical networks to enhance reward models under limited human feedback. By enabling stable and reliable structural learning from fewer samples, Proto-RM significantly enhances LLMs' adaptability and accuracy in interpreting human preferences. Extensive experiments on various datasets demonstrate that Proto-RM significantly improves the performance of reward models and LLMs in human feedback tasks, achieving comparable and usually better results than traditional methods, while requiring significantly less data in data-limited scenarios. This research offers a promising direction for enhancing the efficiency of reward models and optimizing the fine-tuning of language models under restricted feedback conditions.",
    "num_pages": 14
}