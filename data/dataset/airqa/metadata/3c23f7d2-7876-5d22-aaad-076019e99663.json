{
    "uuid": "3c23f7d2-7876-5d22-aaad-076019e99663",
    "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{tang-etal-2023-large,\n    title = \"Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning\",\n    author = \"Tang, Ruixiang  and\n      Kong, Dehan  and\n      Huang, Longtao  and\n      Xue, Hui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.284\",\n    doi = \"10.18653/v1/2023.findings-acl.284\",\n    pages = \"4645--4657\",\n    abstract = \"Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are {``}lazy learners{''} that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.\",\n}\n",
    "authors": [
        "Ruixiang Tang",
        "Dehan Kong",
        "Longtao Huang",
        "Hui Xue"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.284.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3c23f7d2-7876-5d22-aaad-076019e99663.pdf",
    "abstract": "Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.",
    "num_pages": 13
}