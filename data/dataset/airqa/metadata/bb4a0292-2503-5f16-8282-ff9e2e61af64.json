{
    "uuid": "bb4a0292-2503-5f16-8282-ff9e2e61af64",
    "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{gao-etal-2024-evaluating-large,\n    title = \"Evaluating Large Language Models on {W}ikipedia-Style Survey Generation\",\n    author = \"Gao, Fan  and\n      Jiang, Hang  and\n      Yang, Rui  and\n      Zeng, Qingcheng  and\n      Lu, Jinghui  and\n      Blum, Moritz  and\n      She, Tianwei  and\n      Jiang, Yuang  and\n      Li, Irene\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.321\",\n    doi = \"10.18653/v1/2024.findings-acl.321\",\n    pages = \"5405--5418\",\n    abstract = \"Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2{\\%} to 20{\\%} in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.\",\n}\n",
    "authors": [
        "Fan Gao",
        "Hang Jiang",
        "Rui Yang",
        "Qingcheng Zeng",
        "Jinghui Lu",
        "Moritz Blum",
        "Tianwei She",
        "Yuang Jiang",
        "Irene Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.321.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bb4a0292-2503-5f16-8282-ff9e2e61af64.pdf",
    "abstract": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",
    "num_pages": 14
}