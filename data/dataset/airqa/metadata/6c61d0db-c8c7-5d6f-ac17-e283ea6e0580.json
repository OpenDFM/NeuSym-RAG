{
    "uuid": "6c61d0db-c8c7-5d6f-ac17-e283ea6e0580",
    "title": "MANNER: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fang-etal-2023-manner,\n    title = \"{MANNER}: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition\",\n    author = \"Fang, Jinyuan  and\n      Wang, Xiaobin  and\n      Meng, Zaiqiao  and\n      Xie, Pengjun  and\n      Huang, Fei  and\n      Jiang, Yong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.234\",\n    doi = \"10.18653/v1/2023.acl-long.234\",\n    pages = \"4261--4276\",\n    abstract = \"This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task in the target domain. In order to effectively utilize the information from memory, MANNER uses optimal transport to retrieve and process information from memory, which can explicitly adapt the retrieved information from source domain to target domain and improve the performance in the cross domain few-shot setting. We conduct experiments on English and Chinese cross domain few-shot NER datasets, and the experimental results demonstrate that MANNER can achieve superior performance.\",\n}\n",
    "authors": [
        "Jinyuan Fang",
        "Xiaobin Wang",
        "Zaiqiao Meng",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.234.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6c61d0db-c8c7-5d6f-ac17-e283ea6e0580.pdf",
    "abstract": "This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task in the target domain. In order to effectively utilize the information from memory, MANNER uses optimal transport to retrieve and process information from memory, which can explicitly adapt the retrieved information from source domain to target domain and improve the performance in the cross domain few-shot setting. We conduct experiments on English and Chinese cross domain few-shot NER datasets, and the experimental results demonstrate that MANNER can achieve superior performance.",
    "num_pages": 16
}