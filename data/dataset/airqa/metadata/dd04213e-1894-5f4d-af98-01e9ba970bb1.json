{
    "uuid": "dd04213e-1894-5f4d-af98-01e9ba970bb1",
    "title": "CIDAR: Culturally Relevant Instruction Dataset For Arabic",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{alyafeai-etal-2024-cidar,\n    title = \"{CIDAR}: Culturally Relevant Instruction Dataset For {A}rabic\",\n    author = \"Alyafeai, Zaid  and\n      Almubarak, Khalid  and\n      Ashraf, Ahmed  and\n      Alnuhait, Deema  and\n      Alshahrani, Saied  and\n      Abdulrahman, Gubran  and\n      Ahmed, Gamil  and\n      Gawah, Qais  and\n      Saleh, Zead  and\n      Ghaleb, Mustafa  and\n      Ali, Yousef  and\n      Al-shaibani, Maged\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.764\",\n    doi = \"10.18653/v1/2024.findings-acl.764\",\n    pages = \"12878--12901\",\n    abstract = \"Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, leading to inherent biases toward Western culture. This bias negatively impacts non-English languages such as Arabic and the unique culture of the Arab region. This paper addresses this limitation by introducing CIDAR, the first open Arabic instruction-tuning dataset culturally aligned by native Arabic speakers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to a few models fine-tuned on other datasets. Our experiments indicate that models fine-tuned on CIDAR achieve better cultural alignment compared to those fine-tuned on 30x more data.\",\n}\n",
    "authors": [
        "Zaid Alyafeai",
        "Khalid Almubarak",
        "Ahmed Ashraf",
        "Deema Alnuhait",
        "Saied Alshahrani",
        "Gubran Abdulrahman",
        "Gamil Ahmed",
        "Qais Gawah",
        "Zead Saleh",
        "Mustafa Ghaleb",
        "Yousef Ali",
        "Maged Al-shaibani"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.764.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/dd04213e-1894-5f4d-af98-01e9ba970bb1.pdf",
    "abstract": "Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, leading to inherent biases toward Western culture. This bias negatively impacts non-English languages such as Arabic and the unique culture of the Arab region. This paper addresses this limitation by introducing CIDAR, the first open Arabic instruction-tuning dataset culturally aligned by native Arabic speakers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to a few models fine-tuned on other datasets. Our experiments indicate that models fine-tuned on CIDAR achieve better cultural alignment compared to those fine-tuned on 30x more data.",
    "num_pages": 24
}