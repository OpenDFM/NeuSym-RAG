{
    "uuid": "ff2218b6-3154-502e-a288-3e798603b500",
    "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-prolora,\n    title = \"{PR}o{L}o{RA}: Partial Rotation Empowers More Parameter-Efficient {L}o{RA}\",\n    author = \"Wang, Sheng  and\n      Xue, Boyang  and\n      Ye, Jiacheng  and\n      Jiang, Jiyue  and\n      Chen, Liheng  and\n      Kong, Lingpeng  and\n      Wu, Chuan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.156\",\n    doi = \"10.18653/v1/2024.acl-long.156\",\n    pages = \"2829--2841\",\n    abstract = \"With the rapid scaling of large language models (LLMs), serving numerouslow-rank adaptations (LoRAs) concurrently has become increasingly impractical,leading to unaffordable costs and necessitating more parameter-efficientfinetuning methods. In this work, we introduce $\\text{\\textbf{P}artially \\textbf{Ro}tation-enhanced \\textbf{Lo}w-\\textbf{R}ank \\textbf{A}daptation (PRoLoRA)}$, an intra-layer sharing mechanism comprising fouressential components: broadcast reduction, rotation enhancement,partially-sharing refinement, and rectified initialization strategy. As asuperset of LoRA, PRoLoRA retains its advantages, and effectively circumventthe drawbacks of peer parameter-sharing methods with superior model capacity,practical feasibility, and broad applicability. Empirical experimentsdemonstrate the remarkably higher parameter efficiency of PRoLoRA in bothspecific parameter budget and performance target scenarios, and its scalabilityto larger LLMs. Notably, with one time less trainable parameters, PRoLoRA stilloutperforms LoRA on multiple instruction tuning datasets. Subsequently, anablation study is conducted to validate the necessity of individual componentsand highlight the superiority of PRoLoRA over three potential variants.Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRAas a resource-friendly alternative to LoRA.\",\n}\n",
    "authors": [
        "Sheng Wang",
        "Boyang Xue",
        "Jiacheng Ye",
        "Jiyue Jiang",
        "Liheng Chen",
        "Lingpeng Kong",
        "Chuan Wu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.156.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ff2218b6-3154-502e-a288-3e798603b500.pdf",
    "abstract": "With the rapid scaling of large language models (LLMs), serving numerouslow-rank adaptations (LoRAs) concurrently has become increasingly impractical,leading to unaffordable costs and necessitating more parameter-efficientfinetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising fouressential components: broadcast reduction, rotation enhancement,partially-sharing refinement, and rectified initialization strategy. As asuperset of LoRA, PRoLoRA retains its advantages, and effectively circumventthe drawbacks of peer parameter-sharing methods with superior model capacity,practical feasibility, and broad applicability. Empirical experimentsdemonstrate the remarkably higher parameter efficiency of PRoLoRA in bothspecific parameter budget and performance target scenarios, and its scalabilityto larger LLMs. Notably, with one time less trainable parameters, PRoLoRA stilloutperforms LoRA on multiple instruction tuning datasets. Subsequently, anablation study is conducted to validate the necessity of individual componentsand highlight the superiority of PRoLoRA over three potential variants.Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRAas a resource-friendly alternative to LoRA.",
    "num_pages": 13
}