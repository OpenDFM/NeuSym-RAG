{
    "uuid": "010bd550-61f3-528c-955a-7ed1bf9401f8",
    "title": "Unsupervised Domain Adaptation using Lexical Transformations and Label Injection for Twitter Data",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{gupta-etal-2023-unsupervised,\n    title = \"Unsupervised Domain Adaptation using Lexical Transformations and Label Injection for {T}witter Data\",\n    author = \"Gupta, Akshat  and\n      Liu, Xiaomo  and\n      Shah, Sameena\",\n    editor = \"Barnes, Jeremy  and\n      De Clercq, Orph{\\'e}e  and\n      Klinger, Roman\",\n    booktitle = \"Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.wassa-1.17\",\n    doi = \"10.18653/v1/2023.wassa-1.17\",\n    pages = \"184--193\",\n    abstract = \"Domain adaptation is an important and widely studied problem in natural language processing. A large body of literature tries to solve this problem by adapting models trained on the source domain to the target domain. In this paper, we instead solve this problem from a dataset perspective. We modify the source domain dataset with simple lexical transformations to reduce the domain shift between the source dataset distribution and the target dataset distribution. We find that models trained on the transformed source domain dataset performs significantly better than zero-shot models. Using our proposed transformations to convert standard English to tweets, we reach an unsupervised part-of-speech (POS) tagging accuracy of 92.14{\\%} (from 81.54{\\%} zero shot accuracy), which is only slightly below the supervised performance of 94.45{\\%}. We also use our proposed transformations to synthetically generate tweets and augment the Twitter dataset to achieve state-of-the-art performance for POS tagging.\",\n}\n",
    "authors": [
        "Akshat Gupta",
        "Xiaomo Liu",
        "Sameena Shah"
    ],
    "pdf_url": "https://aclanthology.org/2023.wassa-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/010bd550-61f3-528c-955a-7ed1bf9401f8.pdf",
    "abstract": "Domain adaptation is an important and widely studied problem in natural language processing. A large body of literature tries to solve this problem by adapting models trained on the source domain to the target domain. In this paper, we instead solve this problem from a dataset perspective. We modify the source domain dataset with simple lexical transformations to reduce the domain shift between the source dataset distribution and the target dataset distribution. We find that models trained on the transformed source domain dataset performs significantly better than zero-shot models. Using our proposed transformations to convert standard English to tweets, we reach an unsupervised part-of-speech (POS) tagging accuracy of 92.14% (from 81.54% zero shot accuracy), which is only slightly below the supervised performance of 94.45%. We also use our proposed transformations to synthetically generate tweets and augment the Twitter dataset to achieve state-of-the-art performance for POS tagging.",
    "num_pages": 10
}