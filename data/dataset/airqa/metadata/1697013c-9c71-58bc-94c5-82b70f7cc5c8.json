{
    "uuid": "1697013c-9c71-58bc-94c5-82b70f7cc5c8",
    "title": "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2024-hypermoe,\n    title = \"{H}yper{M}o{E}: Towards Better Mixture of Experts via Transferring Among Experts\",\n    author = \"Zhao, Hao  and\n      Qiu, Zihan  and\n      Wu, Huijia  and\n      Wang, Zili  and\n      He, Zhaofeng  and\n      Fu, Jie\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.571\",\n    doi = \"10.18653/v1/2024.acl-long.571\",\n    pages = \"10605--10618\",\n    abstract = \"The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at https://github.com/Bumble666/Hyper{\\_}MoE\",\n}\n",
    "authors": [
        "Hao Zhao",
        "Zihan Qiu",
        "Huijia Wu",
        "Zili Wang",
        "Zhaofeng He",
        "Jie Fu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.571.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1697013c-9c71-58bc-94c5-82b70f7cc5c8.pdf",
    "abstract": "The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. Our code is publicly available at https://github.com/Bumble666/Hyper_MoE",
    "num_pages": 14
}