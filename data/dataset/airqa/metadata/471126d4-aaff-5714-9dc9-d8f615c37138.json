{
    "uuid": "471126d4-aaff-5714-9dc9-d8f615c37138",
    "title": "Harder Task Needs More Experts: Dynamic Routing in MoE Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{huang-etal-2024-harder,\n    title = \"Harder Task Needs More Experts: Dynamic Routing in {M}o{E} Models\",\n    author = \"Huang, Quzhe  and\n      An, Zhenwei  and\n      Zhuang, Nan  and\n      Tao, Mingxu  and\n      Zhang, Chen  and\n      Jin, Yang  and\n      Xu, Kun  and\n      Xu, Kun  and\n      Chen, Liwei  and\n      Huang, Songfang  and\n      Feng, Yansong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.696\",\n    doi = \"10.18653/v1/2024.acl-long.696\",\n    pages = \"12883--12895\",\n    abstract = \"In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike existing MoE approaches that rely on fixed TopK Routing, which activates a predetermined number of experts regardless of the input{'}s complexity, our method dynamically allocates experts based on the confidence level in expert selection for each input. This allows for more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over Top2 Routing across various benchmarks, achieving an average improvement of 0.7{\\%} with less than 90{\\%} activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input{'}s complexity.Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic{\\_}MoE.\",\n}\n",
    "authors": [
        "Quzhe Huang",
        "Zhenwei An",
        "Nan Zhuang",
        "Mingxu Tao",
        "Chen Zhang",
        "Yang Jin",
        "Kun Xu",
        "Kun Xu",
        "Liwei Chen",
        "Songfang Huang",
        "Yansong Feng"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.696.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/471126d4-aaff-5714-9dc9-d8f615c37138.pdf",
    "abstract": "In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike existing MoE approaches that rely on fixed TopK Routing, which activates a predetermined number of experts regardless of the input’s complexity, our method dynamically allocates experts based on the confidence level in expert selection for each input. This allows for more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates substantial improvements over Top2 Routing across various benchmarks, achieving an average improvement of 0.7% with less than 90% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input’s complexity.Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. The code and models are available at https://github.com/ZhenweiAn/Dynamic_MoE.",
    "num_pages": 13
}