{
    "uuid": "44f3c40d-1952-5c04-9543-4a9e2e305ed6",
    "title": "Distilling Calibrated Knowledge for Stance Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{li-caragea-2023-distilling,\n    title = \"Distilling Calibrated Knowledge for Stance Detection\",\n    author = \"Li, Yingjie  and\n      Caragea, Cornelia\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.393\",\n    doi = \"10.18653/v1/2023.findings-acl.393\",\n    pages = \"6316--6329\",\n    abstract = \"Stance detection aims to determine the position of an author toward a target and provides insights into people{'}s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which ignores meaningful signals among categories offered by soft labels. In this work, we explore knowledge distillation for stance detection and present a comprehensive analysis. Our contributions are: 1) we propose to use knowledge distillation over multiple generations in which a student is taken as a new teacher to transfer knowledge to a new fresh student; 2) we propose a novel dynamic temperature scaling for knowledge distillation to calibrate teacher predictions in each generation step. Extensive results on three stance detection datasets show that knowledge distillation benefits stance detection and a teacher is able to transfer knowledge to a student more smoothly via calibrated guiding signals. We publicly release our code to facilitate future research.\",\n}\n",
    "authors": [
        "Yingjie Li",
        "Cornelia Caragea"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.393.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/44f3c40d-1952-5c04-9543-4a9e2e305ed6.pdf",
    "abstract": "Stance detection aims to determine the position of an author toward a target and provides insights into peopleâ€™s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which ignores meaningful signals among categories offered by soft labels. In this work, we explore knowledge distillation for stance detection and present a comprehensive analysis. Our contributions are: 1) we propose to use knowledge distillation over multiple generations in which a student is taken as a new teacher to transfer knowledge to a new fresh student; 2) we propose a novel dynamic temperature scaling for knowledge distillation to calibrate teacher predictions in each generation step. Extensive results on three stance detection datasets show that knowledge distillation benefits stance detection and a teacher is able to transfer knowledge to a student more smoothly via calibrated guiding signals. We publicly release our code to facilitate future research.",
    "num_pages": 14
}