{
    "uuid": "8736e2d4-c48e-5ac3-ac76-30aa51c3ac98",
    "title": "Challenges to Evaluating the Generalization of Coreference Resolution Models: A Measurement Modeling Perspective",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{porada-etal-2024-challenges,\n    title = \"Challenges to Evaluating the Generalization of Coreference Resolution Models: A Measurement Modeling Perspective\",\n    author = \"Porada, Ian  and\n      Olteanu, Alexandra  and\n      Suleman, Kaheer  and\n      Trischler, Adam  and\n      Cheung, Jackie\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.909\",\n    doi = \"10.18653/v1/2024.findings-acl.909\",\n    pages = \"15380--15395\",\n    abstract = \"It is increasingly common to evaluate the same coreference resolution (CR) model on multiple datasets. Do these multi-dataset evaluations allow us to draw meaningful conclusions about model generalization? Or, do they rather reflect the idiosyncrasies of a particular experimental setup (e.g., the specific datasets used)? To study this, we view evaluation through the lens of measurement modeling, a framework commonly used in the social sciences for analyzing the validity of measurements. By taking this perspective, we show how multi-dataset evaluations risk conflating different factors concerning what, precisely, is being measured. This in turn makes it difficult to draw more generalizable conclusions from these evaluations. For instance, we show that across seven datasets, measurements intended to reflect CR model generalization are often correlated with differences in both how coreference is defined and how it is operationalized; this limits our ability to draw conclusions regarding the ability of CR models to generalize across any singular dimension. We believe the measurement modeling framework provides the needed vocabulary for discussing challenges surrounding what is actually being measured by CR evaluations.\",\n}\n",
    "authors": [
        "Ian Porada",
        "Alexandra Olteanu",
        "Kaheer Suleman",
        "Adam Trischler",
        "Jackie Cheung"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.909.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8736e2d4-c48e-5ac3-ac76-30aa51c3ac98.pdf",
    "abstract": "It is increasingly common to evaluate the same coreference resolution (CR) model on multiple datasets. Do these multi-dataset evaluations allow us to draw meaningful conclusions about model generalization? Or, do they rather reflect the idiosyncrasies of a particular experimental setup (e.g., the specific datasets used)? To study this, we view evaluation through the lens of measurement modeling, a framework commonly used in the social sciences for analyzing the validity of measurements. By taking this perspective, we show how multi-dataset evaluations risk conflating different factors concerning what, precisely, is being measured. This in turn makes it difficult to draw more generalizable conclusions from these evaluations. For instance, we show that across seven datasets, measurements intended to reflect CR model generalization are often correlated with differences in both how coreference is defined and how it is operationalized; this limits our ability to draw conclusions regarding the ability of CR models to generalize across any singular dimension. We believe the measurement modeling framework provides the needed vocabulary for discussing challenges surrounding what is actually being measured by CR evaluations.",
    "num_pages": 16
}