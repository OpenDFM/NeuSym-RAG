{
    "uuid": "2d09e6fb-e268-5d5d-a362-9effcfe79d9f",
    "title": "Machine Unlearning of Pre-trained Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yao-etal-2024-machine,\n    title = \"Machine Unlearning of Pre-trained Large Language Models\",\n    author = \"Yao, Jin  and\n      Chien, Eli  and\n      Du, Minxin  and\n      Niu, Xinyao  and\n      Wang, Tianhao  and\n      Cheng, Zezhou  and\n      Yue, Xiang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.457\",\n    doi = \"10.18653/v1/2024.acl-long.457\",\n    pages = \"8403--8419\",\n    abstract = \"This study investigates the concept of the {`}right to be forgotten{'} within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models{--}a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.\",\n}\n",
    "authors": [
        "Jin Yao",
        "Eli Chien",
        "Minxin Du",
        "Xinyao Niu",
        "Tianhao Wang",
        "Zezhou Cheng",
        "Xiang Yue"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.457.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2d09e6fb-e268-5d5d-a362-9effcfe79d9f.pdf",
    "abstract": "This study investigates the concept of the ‘right to be forgotten’ within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models–a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over 105 times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.",
    "num_pages": 17
}