{
    "uuid": "621d42a1-dbab-5003-b7c5-625335653001",
    "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{su-etal-2024-unsupervised,\n    title = \"Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models\",\n    author = \"Su, Weihang  and\n      Wang, Changyue  and\n      Ai, Qingyao  and\n      Hu, Yiran  and\n      Wu, Zhijing  and\n      Zhou, Yujia  and\n      Liu, Yiqun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.854\",\n    doi = \"10.18653/v1/2024.findings-acl.854\",\n    pages = \"14379--14391\",\n    abstract = \"Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM{'}s inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.\",\n}\n",
    "authors": [
        "Weihang Su",
        "Changyue Wang",
        "Qingyao Ai",
        "Yiran Hu",
        "Zhijing Wu",
        "Yujia Zhou",
        "Yiqun Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.854.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/621d42a1-dbab-5003-b7c5-625335653001.pdf",
    "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLMâ€™s inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.",
    "num_pages": 13
}