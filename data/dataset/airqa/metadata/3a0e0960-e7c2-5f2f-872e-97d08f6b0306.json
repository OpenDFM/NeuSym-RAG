{
    "uuid": "3a0e0960-e7c2-5f2f-872e-97d08f6b0306",
    "title": "Unlearning Traces the Influential Training Data of Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{isonuma-titov-2024-unlearning,\n    title = \"Unlearning Traces the Influential Training Data of Language Models\",\n    author = \"Isonuma, Masaru  and\n      Titov, Ivan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.343\",\n    doi = \"10.18653/v1/2024.acl-long.343\",\n    pages = \"6312--6325\",\n    abstract = \"Identifying the training datasets that influence a language model{'}s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model{'}s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model{'}s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.\",\n}\n",
    "authors": [
        "Masaru Isonuma",
        "Ivan Titov"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.343.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3a0e0960-e7c2-5f2f-872e-97d08f6b0306.pdf",
    "abstract": "Identifying the training datasets that influence a language model’s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model’s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model’s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.",
    "num_pages": 14
}