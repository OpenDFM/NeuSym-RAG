{
    "uuid": "a0688b09-70b4-58a2-96bd-ae79a98a2b5a",
    "title": "Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2023-revisiting,\n    title = \"Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation\",\n    author = \"Chen, Yulong  and\n      Zhang, Huajian  and\n      Zhou, Yijie  and\n      Bai, Xuefeng  and\n      Wang, Yueguan  and\n      Zhong, Ming  and\n      Yan, Jianhao  and\n      Li, Yafu  and\n      Li, Judy  and\n      Zhu, Xianchao  and\n      Zhang, Yue\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.519\",\n    doi = \"10.18653/v1/2023.acl-long.519\",\n    pages = \"9332--9351\",\n    abstract = \"Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries.\",\n}\n",
    "authors": [
        "Yulong Chen",
        "Huajian Zhang",
        "Yijie Zhou",
        "Xuefeng Bai",
        "Yueguan Wang",
        "Ming Zhong",
        "Jianhao Yan",
        "Yafu Li",
        "Judy Li",
        "Xianchao Zhu",
        "Yue Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.519.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a0688b09-70b4-58a2-96bd-ae79a98a2b5a.pdf",
    "abstract": "Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries.",
    "num_pages": 18
}