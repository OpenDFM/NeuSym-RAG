{
    "uuid": "f270db10-f9d6-541e-9837-b471e8d6a9b5",
    "title": "On the Role of Parallel Data in Cross-lingual Transfer Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{reid-artetxe-2023-role,\n    title = \"On the Role of Parallel Data in Cross-lingual Transfer Learning\",\n    author = \"Reid, Machel  and\n      Artetxe, Mikel\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.372\",\n    doi = \"10.18653/v1/2023.findings-acl.372\",\n    pages = \"5999--6006\",\n    abstract = \"While prior work has established that the use of parallel data is conducive for cross-lingual learning, it is unclear if the improvements come from the data itself, or if it is the modeling of parallel interactions that matters. Exploring this, we examine the usage of unsupervised machine translation to generate synthetic parallel data, and compare it to supervised machine translation and gold parallel data. We find that even model generated parallel data can be useful for downstream tasks, in both a general setting (continued pretraining) as well as the task-specific setting (translate-train), although our best results are still obtained using real parallel data. Our findings suggest that existing multilingual models do not exploit the full potential of monolingual data, and prompt the community to reconsider the traditional categorization of cross-lingual learning approaches.\",\n}\n",
    "authors": [
        "Machel Reid",
        "Mikel Artetxe"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.372.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f270db10-f9d6-541e-9837-b471e8d6a9b5.pdf",
    "abstract": "While prior work has established that the use of parallel data is conducive for cross-lingual learning, it is unclear if the improvements come from the data itself, or if it is the modeling of parallel interactions that matters. Exploring this, we examine the usage of unsupervised machine translation to generate synthetic parallel data, and compare it to supervised machine translation and gold parallel data. We find that even model generated parallel data can be useful for downstream tasks, in both a general setting (continued pretraining) as well as the task-specific setting (translate-train), although our best results are still obtained using real parallel data. Our findings suggest that existing multilingual models do not exploit the full potential of monolingual data, and prompt the community to reconsider the traditional categorization of cross-lingual learning approaches.",
    "num_pages": 8
}