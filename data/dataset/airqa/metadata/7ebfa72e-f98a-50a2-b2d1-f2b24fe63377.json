{
    "uuid": "7ebfa72e-f98a-50a2-b2d1-f2b24fe63377",
    "title": "Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cheng-etal-2023-exploring,\n    title = \"Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization\",\n    author = \"Cheng, Luyao  and\n      Zheng, Siqi  and\n      Qinglin, Zhang  and\n      Wang, Hui  and\n      Chen, Yafeng  and\n      Chen, Qian\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.884\",\n    doi = \"10.18653/v1/2023.findings-acl.884\",\n    pages = \"14068--14077\",\n    abstract = \"Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic environment. In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and Speaker-Turn Detection, in which we effectively extract speaker information from conversational semantics. We also propose a simple yet effective algorithm to jointly model acoustic and semantic information and obtain speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets show that our method achieves consistent improvements over acoustic-only speaker diarization systems.\",\n}\n",
    "authors": [
        "Luyao Cheng",
        "Siqi Zheng",
        "Zhang Qinglin",
        "Hui Wang",
        "Yafeng Chen",
        "Qian Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.884.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7ebfa72e-f98a-50a2-b2d1-f2b24fe63377.pdf",
    "abstract": "Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic environment. In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and Speaker-Turn Detection, in which we effectively extract speaker information from conversational semantics. We also propose a simple yet effective algorithm to jointly model acoustic and semantic information and obtain speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets show that our method achieves consistent improvements over acoustic-only speaker diarization systems.",
    "num_pages": 10
}