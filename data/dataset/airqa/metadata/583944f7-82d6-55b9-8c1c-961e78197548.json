{
    "uuid": "583944f7-82d6-55b9-8c1c-961e78197548",
    "title": "Arizonans at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis with XLM-T",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{bozdag-etal-2023-arizonans,\n    title = \"Arizonans at {S}em{E}val-2023 Task 9: Multilingual Tweet Intimacy Analysis with {XLM}-{T}\",\n    author = \"Bozdag, Nimet Beyza  and\n      Bilgis, Tugay  and\n      Bethard, Steven\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.230\",\n    doi = \"10.18653/v1/2023.semeval-1.230\",\n    pages = \"1656--1659\",\n    abstract = \"This paper presents the systems and approaches of the Arizonans team for the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. We finetune the Multilingual RoBERTa model trained with about 200M tweets, XLM-T. Our final model ranked 9th out of 45 overall, 13th in seen languages, and 8th in unseen languages.\",\n}\n",
    "authors": [
        "Nimet Beyza Bozdag",
        "Tugay Bilgis",
        "Steven Bethard"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.230.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/583944f7-82d6-55b9-8c1c-961e78197548.pdf",
    "abstract": "This paper presents the systems and approaches of the Arizonans team for the SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. We finetune the Multilingual RoBERTa model trained with about 200M tweets, XLM-T. Our final model ranked 9th out of 45 overall, 13th in seen languages, and 8th in unseen languages.",
    "num_pages": 4
}