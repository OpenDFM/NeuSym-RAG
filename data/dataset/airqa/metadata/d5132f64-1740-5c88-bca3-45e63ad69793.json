{
    "uuid": "d5132f64-1740-5c88-bca3-45e63ad69793",
    "title": "Unified Language Representation for Question Answering over Text, Tables, and Images",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yu-etal-2023-unified,\n    title = \"Unified Language Representation for Question Answering over Text, Tables, and Images\",\n    author = \"Yu, Bowen  and\n      Fu, Cheng  and\n      Yu, Haiyang  and\n      Huang, Fei  and\n      Li, Yongbin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.292\",\n    doi = \"10.18653/v1/2023.findings-acl.292\",\n    pages = \"4756--4765\",\n    abstract = \"When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard.\",\n}\n",
    "authors": [
        "Bowen Yu",
        "Cheng Fu",
        "Haiyang Yu",
        "Fei Huang",
        "Yongbin Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.292.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d5132f64-1740-5c88-bca3-45e63ad69793.pdf",
    "abstract": "When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard.",
    "num_pages": 10
}