{
    "uuid": "9cd1c530-267d-56df-8fb1-6ba942ad3c2c",
    "title": "Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    "bibtex": "@inproceedings{chiang-etal-2023-revealing,\n    title = \"Revealing the Blind Spot of Sentence Encoder Evaluation by {HEROS}\",\n    author = \"Chiang, Cheng-Han  and\n      Lee, Hung-yi  and\n      Chuang, Yung-Sung  and\n      Glass, James\",\n    editor = \"Can, Burcu  and\n      Mozes, Maximilian  and\n      Cahyawijaya, Samuel  and\n      Saphra, Naomi  and\n      Kassner, Nora  and\n      Ravfogel, Shauli  and\n      Ravichander, Abhilasha  and\n      Zhao, Chen  and\n      Augenstein, Isabelle  and\n      Rogers, Anna  and\n      Cho, Kyunghyun  and\n      Grefenstette, Edward  and\n      Voita, Lena\",\n    booktitle = \"Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.repl4nlp-1.24\",\n    doi = \"10.18653/v1/2023.repl4nlp-1.24\",\n    pages = \"289--302\",\n}\n",
    "authors": [
        "Cheng-Han Chiang",
        "Hung-yi Lee",
        "Yung-Sung Chuang",
        "James Glass"
    ],
    "pdf_url": "https://aclanthology.org/2023.repl4nlp-1.24.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9cd1c530-267d-56df-8fb1-6ba942ad3c2c.pdf",
    "abstract": "Existing sentence textual similarity benchmark datasets only use a single number to summarize how similar the sentence encoder’s decision is to humans’. However, it is unclear what kind of sentence pairs a sentence encoder (SE) would consider similar. Moreover, existing SE benchmarks mainly consider sentence pairs with low lexical overlap, so it is unclear how the SEs behave when two sentences have high lexical overlap. We introduce a high-quality SE diagnostic dataset, HEROS. HEROS is constructed by transforming an original sentence into a new sentence based on certain rules to form a minimal pair, and the minimal pair has high lexical overlaps. The rules include replacing a word with a synonym, an antonym, a typo, a random word, and converting the original sentence into its negation. Different rules yield different subsets of HEROS. By systematically comparing the performance of over 60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised sentence encoders are insensitive to negation. We find the datasets used to train the SE are the main determinants of what kind of sentence pairs an SE considers similar. We also show that even if two SEs have similar performance on STS benchmarks, they can have very different behavior on HEROS. Our result reveals the blind spot of traditional STS benchmarks when evaluating SEs.1",
    "num_pages": 14
}