{
    "uuid": "2821c282-ab1e-583c-8bf1-e1f54d60986f",
    "title": "Shimo Lab at “Discharge Me!”: Discharge Summarization by Prompt-Driven Concatenation of Electronic Health Record Sections",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{he-etal-2024-shimo,\n    title = \"Shimo Lab at {``}Discharge Me!{''}: Discharge Summarization by Prompt-Driven Concatenation of Electronic Health Record Sections\",\n    author = \"He, Yunzhen  and\n      Yamagiwa, Hiroaki  and\n      Shimodaira, Hidetoshi\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.56\",\n    doi = \"10.18653/v1/2024.bionlp-1.56\",\n    pages = \"645--657\",\n    abstract = \"In this paper, we present our approach to the shared task {``}Discharge Me!{''} at the BioNLP Workshop 2024. The primary goal of this task is to reduce the time and effort clinicians spend on writing detailed notes in the electronic health record (EHR). Participants develop a pipeline to generate the {``}Brief Hospital Course{''} and {``}Discharge Instructions{''} sections from the EHR. Our approach involves a first step of extracting the relevant sections from the EHR. We then add explanatory prompts to these sections and concatenate them with separate tokens to create the input text. To train a text generation model, we perform LoRA fine-tuning on the ClinicalT5-large model. On the final test data, our approach achieved a ROUGE-1 of 0.394, which is comparable to the top solutions.\",\n}\n",
    "authors": [
        "Yunzhen He",
        "Hiroaki Yamagiwa",
        "Hidetoshi Shimodaira"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.56.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2821c282-ab1e-583c-8bf1-e1f54d60986f.pdf",
    "abstract": "In this paper, we present our approach to the shared task “Discharge Me!” at the BioNLP Workshop 2024. The primary goal of this task is to reduce the time and effort clinicians spend on writing detailed notes in the electronic health record (EHR). Participants develop a pipeline to generate the “Brief Hospital Course” and “Discharge Instructions” sections from the EHR. Our approach involves a first step of extracting the relevant sections from the EHR. We then add explanatory prompts to these sections and concatenate them with separate tokens to create the input text. To train a text generation model, we perform LoRA fine-tuning on the ClinicalT5-large model. On the final test data, our approach achieved a ROUGE-1 of 0.394, which is comparable to the top solutions.",
    "num_pages": 13
}