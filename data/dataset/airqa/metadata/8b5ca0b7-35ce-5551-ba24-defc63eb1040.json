{
    "uuid": "8b5ca0b7-35ce-5551-ba24-defc63eb1040",
    "title": "TOME: A Two-stage Approach for Model-based Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ren-etal-2023-tome,\n    title = \"{TOME}: A Two-stage Approach for Model-based Retrieval\",\n    author = \"Ren, Ruiyang  and\n      Zhao, Wayne Xin  and\n      Liu, Jing  and\n      Wu, Hua  and\n      Wen, Ji-Rong  and\n      Wang, Haifeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.336\",\n    doi = \"10.18653/v1/2023.acl-long.336\",\n    pages = \"6102--6114\",\n    abstract = \"Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors.\",\n}\n",
    "authors": [
        "Ruiyang Ren",
        "Wayne Xin Zhao",
        "Jing Liu",
        "Hua Wu",
        "Ji-Rong Wen",
        "Haifeng Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.336.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8b5ca0b7-35ce-5551-ba24-defc63eb1040.pdf",
    "abstract": "Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors.",
    "num_pages": 13
}