{
    "uuid": "fb0d9a8b-3c4f-5228-8984-39257717882b",
    "title": "SPC: Soft Prompt Construction for Cross Domain Generalization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    "bibtex": "@inproceedings{zhao-etal-2023-spc,\n    title = \"{SPC}: Soft Prompt Construction for Cross Domain Generalization\",\n    author = \"Zhao, Wenbo  and\n      Gupta, Arpit  and\n      Chung, Tagyoung  and\n      Huang, Jing\",\n    editor = \"Can, Burcu  and\n      Mozes, Maximilian  and\n      Cahyawijaya, Samuel  and\n      Saphra, Naomi  and\n      Kassner, Nora  and\n      Ravfogel, Shauli  and\n      Ravichander, Abhilasha  and\n      Zhao, Chen  and\n      Augenstein, Isabelle  and\n      Rogers, Anna  and\n      Cho, Kyunghyun  and\n      Grefenstette, Edward  and\n      Voita, Lena\",\n    booktitle = \"Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.repl4nlp-1.10\",\n    doi = \"10.18653/v1/2023.repl4nlp-1.10\",\n    pages = \"118--130\",\n    abstract = \"Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is unstable, task-specific, and bias-prone. This paper proposes a principled learning framework{---}soft prompt construction (SPC){---}to facilitate learning domain-adaptable soft prompts. Derived from the SPC framework is a simple loss that can plug into various models and tuning approaches to improve their cross-domain performance. We show SPC can improve upon SOTA for contextual query rewriting, summarization, and paraphrase detection by up to 5{\\%}, 19{\\%}, and 16{\\%}, respectively.\",\n}\n",
    "authors": [
        "Wenbo Zhao",
        "Arpit Gupta",
        "Tagyoung Chung",
        "Jing Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.repl4nlp-1.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fb0d9a8b-3c4f-5228-8984-39257717882b.pdf",
    "abstract": "Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is unstable, task-specific, and bias-prone. This paper proposes a principled learning framework—soft prompt construction (SPC)—to facilitate learning domain-adaptable soft prompts. Derived from the SPC framework is a simple loss that can plug into various models and tuning approaches to improve their cross-domain performance. We show SPC can improve upon SOTA for contextual query rewriting, summarization, and paraphrase detection by up to 5%, 19%, and 16%, respectively.",
    "num_pages": 13
}