{
    "uuid": "4245eb7b-7cee-5724-a391-406a848bb20b",
    "title": "Aligning Large Language Models for Controllable Recommendations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lu-etal-2024-aligning,\n    title = \"Aligning Large Language Models for Controllable Recommendations\",\n    author = \"Lu, Wensheng  and\n      Lian, Jianxun  and\n      Zhang, Wei  and\n      Li, Guanghua  and\n      Zhou, Mingyang  and\n      Liao, Hao  and\n      Xie, Xing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.443\",\n    doi = \"10.18653/v1/2024.acl-long.443\",\n    pages = \"8159--8172\",\n    abstract = \"Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems {---} systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy using a fixed task template, often overlooking the diversity of recommendation tasks and the ability of LLMs to follow recommendation-specific instructions. To address this gap, we first introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs{'} proficiency in adhering to recommendation-specific instructions. Next, we propose a reinforcement learning-based alignment procedure to enhance LLMs{'} generalization ability. Extensive experiments on two real-world datasets demonstrate that our approach significantly improves the capability of LLMs to respond to instructions within recommender systems, reducing formatting errors while maintaining a high level of accuracy.\",\n}\n",
    "authors": [
        "Wensheng Lu",
        "Jianxun Lian",
        "Wei Zhang",
        "Guanghua Li",
        "Mingyang Zhou",
        "Hao Liao",
        "Xing Xie"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.443.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4245eb7b-7cee-5724-a391-406a848bb20b.pdf",
    "abstract": "Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems — systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy using a fixed task template, often overlooking the diversity of recommendation tasks and the ability of LLMs to follow recommendation-specific instructions. To address this gap, we first introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs’ proficiency in adhering to recommendation-specific instructions. Next, we propose a reinforcement learning-based alignment procedure to enhance LLMs’ generalization ability. Extensive experiments on two real-world datasets demonstrate that our approach significantly improves the capability of LLMs to respond to instructions within recommender systems, reducing formatting errors while maintaining a high level of accuracy.",
    "num_pages": 14
}