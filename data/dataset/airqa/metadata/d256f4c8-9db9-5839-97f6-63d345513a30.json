{
    "uuid": "d256f4c8-9db9-5839-97f6-63d345513a30",
    "title": "Efficient Continual Pre-training for Building Domain Specific Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{xie-etal-2024-efficient,\n    title = \"Efficient Continual Pre-training for Building Domain Specific Large Language Models\",\n    author = \"Xie, Yong  and\n      Aggarwal, Karan  and\n      Ahmad, Aitzaz\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.606\",\n    doi = \"10.18653/v1/2024.findings-acl.606\",\n    pages = \"10184--10201\",\n    abstract = \"Large language models (LLMs) have demonstrated remarkable open-domain capabilities. LLMs tailored for a domain are typically trained entirely on domain corpus to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs over an existing open-domain LLM. We introduce \\textit{FinPythia-6.9B}, developed through domain-adaptive continual pre-training on the financial domain.Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperform vanilla continual pre-training{'}s performance with just 10{\\%} of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs cost-effectively.\",\n}\n",
    "authors": [
        "Yong Xie",
        "Karan Aggarwal",
        "Aitzaz Ahmad"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.606.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d256f4c8-9db9-5839-97f6-63d345513a30.pdf",
    "abstract": "Large language models (LLMs) have demonstrated remarkable open-domain capabilities. LLMs tailored for a domain are typically trained entirely on domain corpus to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs over an existing open-domain LLM. We introduce FinPythia-6.9B, developed through domain-adaptive continual pre-training on the financial domain.Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperform vanilla continual pre-trainingâ€™s performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs cost-effectively.",
    "num_pages": 18
}