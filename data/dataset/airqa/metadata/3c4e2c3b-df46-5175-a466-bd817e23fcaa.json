{
    "uuid": "3c4e2c3b-df46-5175-a466-bd817e23fcaa",
    "title": "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{park-etal-2024-picturing,\n    title = \"Picturing Ambiguity: A Visual Twist on the {W}inograd Schema Challenge\",\n    author = \"Park, Brendan  and\n      Janecek, Madeline  and\n      Ezzati-Jivan, Naser  and\n      Li, Yifeng  and\n      Emami, Ali\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.22\",\n    doi = \"10.18653/v1/2024.acl-long.22\",\n    pages = \"355--374\",\n    abstract = \"Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models{'} ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7{\\%} on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.\",\n}\n",
    "authors": [
        "Brendan Park",
        "Madeline Janecek",
        "Naser Ezzati-Jivan",
        "Yifeng Li",
        "Ali Emami"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.22.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3c4e2c3b-df46-5175-a466-bd817e23fcaa.pdf",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the modelsâ€™ ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.",
    "num_pages": 20
}