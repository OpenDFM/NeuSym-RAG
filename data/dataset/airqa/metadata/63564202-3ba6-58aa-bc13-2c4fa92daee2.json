{
    "uuid": "63564202-3ba6-58aa-bc13-2c4fa92daee2",
    "title": "ECNU_MIV at SemEval-2023 Task 1: CTIM - Contrastive Text-Image Model for Multilingual Visual Word Sense Disambiguation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{li-etal-2023-ecnu,\n    title = \"{ECNU}{\\_}{MIV} at {S}em{E}val-2023 Task 1: {CTIM} - Contrastive Text-Image Model for Multilingual Visual Word Sense Disambiguation\",\n    author = \"Li, Zhenghui  and\n      Zhang, Qi  and\n      Xia, Xueyin  and\n      Ye, Yinxiang  and\n      Zhang, Qi  and\n      Huang, Cong\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.13\",\n    doi = \"10.18653/v1/2023.semeval-1.13\",\n    pages = \"101--107\",\n    abstract = \"Our team focuses on the multimodal domain of images and texts, we propose a model that can learn the matching relationship between text-image pairs by contrastive learning. More specifically, We train the model from the labeled data provided by the official organizer, after pre-training, texts are used to reference learned visual concepts enabling visual word sense disambiguation tasks. In addition, the top results our teams get have been released showing the effectiveness of our solution.\",\n}\n",
    "authors": [
        "Zhenghui Li",
        "Qi Zhang",
        "Xueyin Xia",
        "Yinxiang Ye",
        "Qi Zhang",
        "Cong Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/63564202-3ba6-58aa-bc13-2c4fa92daee2.pdf",
    "abstract": "Our team focuses on the multimodal domain of images and texts, we propose a model that can learn the matching relationship between text-image pairs by contrastive learning. More specifically, We train the model from the labeled data provided by the official organizer, after pre-training, texts are used to reference learned visual concepts enabling visual word sense disambiguation tasks. In addition, the top results our teams get have been released showing the effectiveness of our solution.",
    "num_pages": 7
}