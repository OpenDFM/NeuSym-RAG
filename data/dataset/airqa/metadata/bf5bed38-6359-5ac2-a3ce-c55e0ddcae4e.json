{
    "uuid": "bf5bed38-6359-5ac2-a3ce-c55e0ddcae4e",
    "title": "IgnitionInnovators at “Discharge Me!”: Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{tang-etal-2024-ignitioninnovators,\n    title = \"{I}gnition{I}nnovators at {``}Discharge Me!{''}: Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries\",\n    author = \"Tang, An Quang  and\n      Zhang, Xiuzhen  and\n      Dinh, Minh Ngoc\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.65\",\n    doi = \"10.18653/v1/2024.bionlp-1.65\",\n    pages = \"731--739\",\n    abstract = \"This paper presents our proposed approach to the Discharge Me! shared task, collocated with the 23th Workshop on Biomedical Natural Language Processing (BioNLP). In this work, we develop an LLM-based framework for solving the Discharge Summary Documentation (DSD) task, i.e., generating the two critical target sections {`}Brief Hospital Course{'} and {`}Discharge Instructions{'} in the discharge summary. By streamlining the recent instruction-finetuning process on LLMs, we explore several prompting strategies for optimally adapting LLMs to specific generation task of DSD. Experimental results show that providing a clear output structure, complimented by a set of comprehensive Chain-of-Thoughts (CoT) questions, effectively improves the model{'}s reasoning capability, and thereby, enhancing the structural correctness and faithfulness of clinical information in the generated text. Source code is available at: https://anonymous.4open.science/r/Discharge{\\_}LLM-A233\",\n}\n",
    "authors": [
        "An Quang Tang",
        "Xiuzhen Zhang",
        "Minh Ngoc Dinh"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.65.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bf5bed38-6359-5ac2-a3ce-c55e0ddcae4e.pdf",
    "abstract": "This paper presents our proposed approach to the Discharge Me! shared task, collocated with the 23th Workshop on Biomedical Natural Language Processing (BioNLP). In this work, we develop an LLM-based framework for solving the Discharge Summary Documentation (DSD) task, i.e., generating the two critical target sections ‘Brief Hospital Course’ and ‘Discharge Instructions’ in the discharge summary. By streamlining the recent instruction-finetuning process on LLMs, we explore several prompting strategies for optimally adapting LLMs to specific generation task of DSD. Experimental results show that providing a clear output structure, complimented by a set of comprehensive Chain-of-Thoughts (CoT) questions, effectively improves the model’s reasoning capability, and thereby, enhancing the structural correctness and faithfulness of clinical information in the generated text. Source code is available at: https://anonymous.4open.science/r/Discharge_LLM-A233",
    "num_pages": 9
}