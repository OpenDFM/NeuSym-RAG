{
    "uuid": "3099f1d6-e63b-5acf-8e43-937058d1fee1",
    "title": "Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{pandey-etal-2023-cross,\n    title = \"Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment\",\n    author = \"Pandey, Rohan  and\n      Shao, Rulin  and\n      Liang, Paul Pu  and\n      Salakhutdinov, Ruslan  and\n      Morency, Louis-Philippe\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.298\",\n    doi = \"10.18653/v1/2023.acl-long.298\",\n    pages = \"5444--5455\",\n    abstract = \"Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., {`}mug in grass{'}) with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the language attention from {`}mug{'} to {`}grass{'} (capturing the semantic relation {`}in{'}) to match the visual attention from the mug to the grass (capturing the corresponding physical relation). Tokens and their corresponding objects are softly identified using a weighted mean of cross-modal attention. We prove that this notion of soft cross-modal equivalence is equivalent to enforcing congruence between vision and language attention matrices under a {`}change of basis{'} provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to fine-tune UNITER and improve its Winoground Group score by 5.75 points.\",\n}\n",
    "authors": [
        "Rohan Pandey",
        "Rulin Shao",
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.298.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3099f1d6-e63b-5acf-8e43-937058d1fee1.pdf",
    "abstract": "Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., ‘mug in grass’) with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the language attention from ‘mug’ to ‘grass’ (capturing the semantic relation ‘in’) to match the visual attention from the mug to the grass (capturing the corresponding physical relation). Tokens and their corresponding objects are softly identified using a weighted mean of cross-modal attention. We prove that this notion of soft cross-modal equivalence is equivalent to enforcing congruence between vision and language attention matrices under a ‘change of basis’ provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to fine-tune UNITER and improve its Winoground Group score by 5.75 points.",
    "num_pages": 12
}