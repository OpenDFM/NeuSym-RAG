{
    "uuid": "a29f2bea-e602-5c62-afd2-1770a610d8fa",
    "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-dawn,\n    title = \"The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models\",\n    author = \"Li, Junyi  and\n      Chen, Jie  and\n      Ren, Ruiyang  and\n      Cheng, Xiaoxue  and\n      Zhao, Xin  and\n      Nie, Jian-Yun  and\n      Wen, Ji-Rong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.586\",\n    doi = \"10.18653/v1/2024.acl-long.586\",\n    pages = \"10879--10899\",\n    abstract = \"In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.\",\n}\n",
    "authors": [
        "Junyi Li",
        "Jie Chen",
        "Ruiyang Ren",
        "Xiaoxue Cheng",
        "Xin Zhao",
        "Jian-Yun Nie",
        "Ji-Rong Wen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.586.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a29f2bea-e602-5c62-afd2-1770a610d8fa.pdf",
    "abstract": "In the era of large language models (LLMs), hallucination (the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs.",
    "num_pages": 21
}