{
    "uuid": "65a155e7-2741-5b43-b32e-7057a71f0cdd",
    "title": "Are U a Joke Master? Pun Generation via Multi-Stage Curriculum Learning towards a Humor LLM",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chen-etal-2024-u,\n    title = \"Are {U} a Joke Master? Pun Generation via Multi-Stage Curriculum Learning towards a Humor {LLM}\",\n    author = \"Chen, Yang  and\n      Yang, Chong  and\n      Hu, Tu  and\n      Chen, Xinhao  and\n      Lan, Man  and\n      Cai, Li  and\n      Zhuang, Xinlin  and\n      Lin, Xuan  and\n      Lu, Xin  and\n      Zhou, Aimin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.51\",\n    doi = \"10.18653/v1/2024.findings-acl.51\",\n    pages = \"878--890\",\n    abstract = \"Although large language models (LLMs) acquire extensive world knowledge and some reasoning abilities, their proficiency in generating humorous sentences remains a challenge. Previous research has demonstrated that the humor generation capabilities of ChatGPT are confined to producing merely 25 unique jokes. In this work, we concentrate on endowing LLMs with the ability of generating puns, a particular category of humor by preference learning method. We propose a multi-stage curriculum preference learning framework to optimize both pun structure preferences and humor preferences. Specifically, we improve the Direct Preference Optimization (DPO) algorithm to address the challenge of multi-objective alignment problem. Besides, to facilitate further advancement in this field, we collect a Chinese Pun (ChinesePun) dataset, containing 2.1k puns and corresponding annotations. Experimental results on both Chinese and English benchmark datasets demonstrate that our method significantly outperforms all the baseline models.\",\n}\n",
    "authors": [
        "Yang Chen",
        "Chong Yang",
        "Tu Hu",
        "Xinhao Chen",
        "Man Lan",
        "Li Cai",
        "Xinlin Zhuang",
        "Xuan Lin",
        "Xin Lu",
        "Aimin Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.51.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/65a155e7-2741-5b43-b32e-7057a71f0cdd.pdf",
    "abstract": "Although large language models (LLMs) acquire extensive world knowledge and some reasoning abilities, their proficiency in generating humorous sentences remains a challenge. Previous research has demonstrated that the humor generation capabilities of ChatGPT are confined to producing merely 25 unique jokes. In this work, we concentrate on endowing LLMs with the ability of generating puns, a particular category of humor by preference learning method. We propose a multi-stage curriculum preference learning framework to optimize both pun structure preferences and humor preferences. Specifically, we improve the Direct Preference Optimization (DPO) algorithm to address the challenge of multi-objective alignment problem. Besides, to facilitate further advancement in this field, we collect a Chinese Pun (ChinesePun) dataset, containing 2.1k puns and corresponding annotations. Experimental results on both Chinese and English benchmark datasets demonstrate that our method significantly outperforms all the baseline models.",
    "num_pages": 13
}