{
    "uuid": "f8ff3d3c-d305-5a9d-b6b7-7848ce1aea06",
    "title": "Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{anagnostopoulou-etal-2023-towards,\n    title = \"Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory\",\n    author = \"Anagnostopoulou, Aliki  and\n      Hartmann, Mareike  and\n      Sonntag, Daniel\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.19\",\n    doi = \"10.18653/v1/2023.sustainlp-1.19\",\n    pages = \"245--256\",\n}\n",
    "authors": [
        "Aliki Anagnostopoulou",
        "Mareike Hartmann",
        "Daniel Sonntag"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.19.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f8ff3d3c-d305-5a9d-b6b7-7848ce1aea06.pdf",
    "abstract": "Interactive machine learning (IML) is a beneficial learning paradigm in cases of limited data availability, as human feedback is incrementally integrated into the training process. In this paper, we present an IML pipeline for image captioning which allows us to incrementally adapt a pre-trained image captioning model to a new data distribution based on user input. In order to incorporate user input into the model, we explore the use of a combination of simple data augmentation methods to obtain larger data batches for each newly annotated data instance and implement continual learning methods to prevent catastrophic forgetting from repeated updates. For our experiments, we split a domain-specific image captioning dataset, namely VizWiz, into non-overlapping parts to simulate an incremental input flow for continually adapting the model to new data. We find that, while data augmentation worsens results, even when relatively small amounts of data are available, episodic memory is an effective strategy to retain knowledge from previously seen clusters.",
    "num_pages": 12
}