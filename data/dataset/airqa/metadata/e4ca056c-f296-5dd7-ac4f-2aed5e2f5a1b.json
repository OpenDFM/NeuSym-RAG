{
    "uuid": "e4ca056c-f296-5dd7-ac4f-2aed5e2f5a1b",
    "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{imani-etal-2023-mathprompter,\n    title = \"{M}ath{P}rompter: Mathematical Reasoning using Large Language Models\",\n    author = \"Imani, Shima  and\n      Du, Liang  and\n      Shrivastava, Harsh\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.4\",\n    doi = \"10.18653/v1/2023.acl-industry.4\",\n    pages = \"37--42\",\n    abstract = \"Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose {`}MathPrompter{'}, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the {`}MultiArith{'} dataset (78.7{\\%} - 92.5{\\%}) evaluated using 175B parameter GPT-based LLM.\",\n}\n",
    "authors": [
        "Shima Imani",
        "Liang Du",
        "Harsh Shrivastava"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.4.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e4ca056c-f296-5dd7-ac4f-2aed5e2f5a1b.pdf",
    "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
    "num_pages": 6
}