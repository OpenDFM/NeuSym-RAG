{
    "uuid": "dfb666f2-2f34-5d68-b047-f72dea658dee",
    "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{hsieh-etal-2023-distilling,\n    title = \"Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\",\n    author = \"Hsieh, Cheng-Yu  and\n      Li, Chun-Liang  and\n      Yeh, Chih-kuan  and\n      Nakhost, Hootan  and\n      Fujii, Yasuhisa  and\n      Ratner, Alex  and\n      Krishna, Ranjay  and\n      Lee, Chen-Yu  and\n      Pfister, Tomas\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.507\",\n    doi = \"10.18653/v1/2023.findings-acl.507\",\n    pages = \"8003--8017\",\n    abstract = \"Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80{\\%} of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100{\\%} of the dataset.\",\n}\n",
    "authors": [
        "Cheng-Yu Hsieh",
        "Chun-Liang Li",
        "Chih-kuan Yeh",
        "Hootan Nakhost",
        "Yasuhisa Fujii",
        "Alex Ratner",
        "Ranjay Krishna",
        "Chen-Yu Lee",
        "Tomas Pfister"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.507.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dfb666f2-2f34-5d68-b047-f72dea658dee.pdf",
    "abstract": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset.",
    "num_pages": 15
}