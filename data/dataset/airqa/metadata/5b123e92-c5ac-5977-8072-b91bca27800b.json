{
    "uuid": "5b123e92-c5ac-5977-8072-b91bca27800b",
    "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-knowledge-alignment,\n    title = \"The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models\",\n    author = \"Zhang, Shuo  and\n      Pan, Liangming  and\n      Zhao, Junzhou  and\n      Wang, William Yang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.121\",\n    doi = \"10.18653/v1/2024.findings-acl.121\",\n    pages = \"2025--2038\",\n    abstract = \"Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2{\\%} and 27.1{\\%} respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.\",\n}\n",
    "authors": [
        "Shuo Zhang",
        "Liangming Pan",
        "Junzhou Zhao",
        "William Yang Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.121.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5b123e92-c5ac-5977-8072-b91bca27800b.pdf",
    "abstract": "Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.",
    "num_pages": 14
}