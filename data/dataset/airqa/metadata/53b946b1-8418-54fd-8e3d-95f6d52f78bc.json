{
    "uuid": "53b946b1-8418-54fd-8e3d-95f6d52f78bc",
    "title": "Counterfactual Active Learning for Out-of-Distribution Generalization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{deng-etal-2023-counterfactual,\n    title = \"Counterfactual Active Learning for Out-of-Distribution Generalization\",\n    author = \"Deng, Xun  and\n      Wang, Wenjie  and\n      Feng, Fuli  and\n      Zhang, Hanwang  and\n      He, Xiangnan  and\n      Liao, Yong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.636\",\n    doi = \"10.18653/v1/2023.acl-long.636\",\n    pages = \"11362--11377\",\n    abstract = \"We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance.\",\n}\n",
    "authors": [
        "Xun Deng",
        "Wenjie Wang",
        "Fuli Feng",
        "Hanwang Zhang",
        "Xiangnan He",
        "Yong Liao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.636.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/53b946b1-8418-54fd-8e3d-95f6d52f78bc.pdf",
    "abstract": "We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance.",
    "num_pages": 16
}