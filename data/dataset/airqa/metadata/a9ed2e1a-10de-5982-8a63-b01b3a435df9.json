{
    "uuid": "a9ed2e1a-10de-5982-8a63-b01b3a435df9",
    "title": "Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{komma-etal-2023-toward,\n    title = \"Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs\",\n    author = \"Komma, Abishek  and\n      Panyam Chandrasekarasastry, Nagesh  and\n      Leffel, Timothy  and\n      Goyal, Anuj  and\n      Metallinou, Angeliki  and\n      Matsoukas, Spyros  and\n      Galstyan, Aram\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.19\",\n    doi = \"10.18653/v1/2023.acl-industry.19\",\n    pages = \"186--195\",\n    abstract = \"Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.\",\n}\n",
    "authors": [
        "Abishek Komma",
        "Nagesh Panyam Chandrasekarasastry",
        "Timothy Leffel",
        "Anuj Goyal",
        "Angeliki Metallinou",
        "Spyros Matsoukas",
        "Aram Galstyan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.19.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a9ed2e1a-10de-5982-8a63-b01b3a435df9.pdf",
    "abstract": "Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.",
    "num_pages": 10
}