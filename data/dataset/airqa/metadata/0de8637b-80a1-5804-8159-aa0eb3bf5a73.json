{
    "uuid": "0de8637b-80a1-5804-8159-aa0eb3bf5a73",
    "title": "John vs. Ahmed: Debate-Induced Bias in Multilingual LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{demidova-etal-2024-john,\n    title = \"John vs. Ahmed: Debate-Induced Bias in Multilingual {LLM}s\",\n    author = \"Demidova, Anastasiia  and\n      Atwany, Hanin  and\n      Rabih, Nour  and\n      Sha{'}ban, Sanad  and\n      Abdul-Mageed, Muhammad\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.18\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.18\",\n    pages = \"193--209\",\n    abstract = \"Large language models (LLMs) play a crucial role in a wide range of real world applications. However, concerns about their safety and ethical implications are growing. While research on LLM safety is expanding, there is a noticeable gap in evaluating safety across multiple languages, especially in Arabic and Russian. We address this gap by exploring biases in LLMs across different languages and contexts, focusing on GPT-3.5 and Gemini. Through carefully designed argument-based prompts and scenarios in Arabic, English, and Russian, we examine biases in cultural, political, racial, religious, and gender domains. Our findings reveal biases in these domains. In particular, our investigation uncovers subtle biases where each model tends to present winners as those speaking the primary language the model is prompted with. Our study contributes to ongoing efforts to ensure justice and equality in LLM development and emphasizes the importance of further research towards responsible progress in this field.\",\n}\n",
    "authors": [
        "Anastasiia Demidova",
        "Hanin Atwany",
        "Nour Rabih",
        "Sanad Shaâ€™ban",
        "Muhammad Abdul-Mageed"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0de8637b-80a1-5804-8159-aa0eb3bf5a73.pdf",
    "abstract": "Large language models (LLMs) play a crucial role in a wide range of real world applications. However, concerns about their safety and ethical implications are growing. While research on LLM safety is expanding, there is a noticeable gap in evaluating safety across multiple languages, especially in Arabic and Russian. We address this gap by exploring biases in LLMs across different languages and contexts, focusing on GPT-3.5 and Gemini. Through carefully designed argument-based prompts and scenarios in Arabic, English, and Russian, we examine biases in cultural, political, racial, religious, and gender domains. Our findings reveal biases in these domains. In particular, our investigation uncovers subtle biases where each model tends to present winners as those speaking the primary language the model is prompted with. Our study contributes to ongoing efforts to ensure justice and equality in LLM development and emphasizes the importance of further research towards responsible progress in this field.",
    "num_pages": 17
}