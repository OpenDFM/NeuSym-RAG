{
    "uuid": "7fdef00e-ba25-5b45-9625-8c1ff052f874",
    "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jang-etal-2023-knowledge,\n    title = \"Knowledge Unlearning for Mitigating Privacy Risks in Language Models\",\n    author = \"Jang, Joel  and\n      Yoon, Dongkeun  and\n      Yang, Sohee  and\n      Cha, Sungmin  and\n      Lee, Moontae  and\n      Logeswaran, Lajanugen  and\n      Seo, Minjoon\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.805\",\n    doi = \"10.18653/v1/2023.acl-long.805\",\n    pages = \"14389--14408\",\n    abstract = \"Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust.\",\n}\n",
    "authors": [
        "Joel Jang",
        "Dongkeun Yoon",
        "Sohee Yang",
        "Sungmin Cha",
        "Moontae Lee",
        "Lajanugen Logeswaran",
        "Minjoon Seo"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.805.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7fdef00e-ba25-5b45-9625-8c1ff052f874.pdf",
    "abstract": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust.",
    "num_pages": 20
}