{
    "uuid": "360cd423-fa37-5e05-be0f-cf06291c8fac",
    "title": "Effective Contrastive Weighting for Dense Query Expansion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2023-effective,\n    title = \"Effective Contrastive Weighting for Dense Query Expansion\",\n    author = \"Wang, Xiao  and\n      MacAvaney, Sean  and\n      Macdonald, Craig  and\n      Ounis, Iadh\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.710\",\n    doi = \"10.18653/v1/2023.acl-long.710\",\n    pages = \"12688--12704\",\n    abstract = \"Verbatim queries submitted to search engines often do not sufficiently describe the user{'}s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query{'}srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1{\\%} compared to an existing PRF approach for ColBERT while maintaining its efficiency.\",\n}\n",
    "authors": [
        "Xiao Wang",
        "Sean MacAvaney",
        "Craig Macdonald",
        "Iadh Ounis"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.710.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/360cd423-fa37-5e05-be0f-cf06291c8fac.pdf",
    "abstract": "Verbatim queries submitted to search engines often do not sufficiently describe the user’s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query’srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency.",
    "num_pages": 17
}