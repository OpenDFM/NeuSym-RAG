{
    "uuid": "0c74a6c5-bf88-586b-a29d-31f3f3e8e307",
    "title": "Benchmarking Low-Resource Machine Translation Systems",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)",
    "bibtex": "@inproceedings{silva-etal-2024-benchmarking,\n    title = \"Benchmarking Low-Resource Machine Translation Systems\",\n    author = {Silva, Ana  and\n      Srivastava, Nikit  and\n      Moteu Ngoli, Tatiana  and\n      R{\\\"o}der, Michael  and\n      Moussallem, Diego  and\n      Ngonga Ngomo, Axel-Cyrille},\n    editor = \"Ojha, Atul Kr.  and\n      Liu, Chao-hong  and\n      Vylomova, Ekaterina  and\n      Pirinen, Flammie  and\n      Abbott, Jade  and\n      Washington, Jonathan  and\n      Oco, Nathaniel  and\n      Malykh, Valentin  and\n      Logacheva, Varvara  and\n      Zhao, Xiaobing\",\n    booktitle = \"Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.loresmt-1.18\",\n    doi = \"10.18653/v1/2024.loresmt-1.18\",\n    pages = \"175--185\",\n    abstract = \"Assessing the performance of machine translation systems is of critical value, especially to languages with lower resource availability.Due to the large evaluation effort required by the translation task, studies often compare new systems against single systems or commercial solutions. Consequently, determining the best-performing system for specific languages is often unclear. This work benchmarks publicly available translation systems across 4 datasets and 26 languages, including low-resource languages. We consider both effectiveness and efficiency in our evaluation.Our results are made public through BENG{---}a FAIR benchmarking platform for Natural Language Generation tasks.\",\n}\n",
    "authors": [
        "Ana Silva",
        "Nikit Srivastava",
        "Tatiana Moteu Ngoli",
        "Michael Röder",
        "Diego Moussallem",
        "Axel-Cyrille Ngonga Ngomo"
    ],
    "pdf_url": "https://aclanthology.org/2024.loresmt-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0c74a6c5-bf88-586b-a29d-31f3f3e8e307.pdf",
    "abstract": "Assessing the performance of machine translation systems is of critical value, especially to languages with lower resource availability.Due to the large evaluation effort required by the translation task, studies often compare new systems against single systems or commercial solutions. Consequently, determining the best-performing system for specific languages is often unclear. This work benchmarks publicly available translation systems across 4 datasets and 26 languages, including low-resource languages. We consider both effectiveness and efficiency in our evaluation.Our results are made public through BENG—a FAIR benchmarking platform for Natural Language Generation tasks.",
    "num_pages": 11
}