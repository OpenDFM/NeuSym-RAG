{
    "uuid": "238086a2-5ee8-58da-94f7-742038812d48",
    "title": "Parallel Corpus for Indigenous Language Translation: Spanish-Mazatec and Spanish-Mixtec",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    "bibtex": "@inproceedings{tonja-etal-2023-parallel,\n    title = \"Parallel Corpus for Indigenous Language Translation: {S}panish-Mazatec and {S}panish-{M}ixtec\",\n    author = \"Tonja, Atnafu Lambebo  and\n      Maldonado-sifuentes, Christian  and\n      Mendoza Castillo, David Alejandro  and\n      Kolesnikova, Olga  and\n      Castro-S{\\'a}nchez, No{\\'e}  and\n      Sidorov, Grigori  and\n      Gelbukh, Alexander\",\n    editor = \"Mager, Manuel  and\n      Ebrahimi, Abteen  and\n      Oncevay, Arturo  and\n      Rice, Enora  and\n      Rijhwani, Shruti  and\n      Palmer, Alexis  and\n      Kann, Katharina\",\n    booktitle = \"Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.americasnlp-1.11\",\n    doi = \"10.18653/v1/2023.americasnlp-1.11\",\n    pages = \"94--102\",\n    abstract = \"In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous Mexican languages. We evaluated the usability of the collected corpus using three different approaches: transformer, transfer learning, and fine-tuning pre-trained multilingual MT models. Fine-tuning the Facebook m2m100-48 model outperformed the other approaches, with BLEU scores of 12.09 and 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively, and 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations, respectively. The results indicate that translation performance is influenced by the dataset size (9,799 sentences in Mazatec and 13,235 sentences in Mixtec) and is more effective when indigenous languages are used as target languages. The findings emphasize the importance of creating parallel corpora for indigenous languages and fine-tuning models for low-resource translation tasks. Future research will investigate zero-shot and few-shot learning approaches to further improve translation performance in low-resource settings.\",\n}\n",
    "authors": [
        "Atnafu Lambebo Tonja",
        "Christian Maldonado-sifuentes",
        "David Alejandro Mendoza Castillo",
        "Olga Kolesnikova",
        "Noé Castro-Sánchez",
        "Grigori Sidorov",
        "Alexander Gelbukh"
    ],
    "pdf_url": "https://aclanthology.org/2023.americasnlp-1.11.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/238086a2-5ee8-58da-94f7-742038812d48.pdf",
    "abstract": "In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous Mexican languages. We evaluated the usability of the collected corpus using three different approaches: transformer, transfer learning, and fine-tuning pre-trained multilingual MT models. Fine-tuning the Facebook m2m100-48 model outperformed the other approaches, with BLEU scores of 12.09 and 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively, and 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations, respectively. The results indicate that translation performance is influenced by the dataset size (9,799 sentences in Mazatec and 13,235 sentences in Mixtec) and is more effective when indigenous languages are used as target languages. The findings emphasize the importance of creating parallel corpora for indigenous languages and fine-tuning models for low-resource translation tasks. Future research will investigate zero-shot and few-shot learning approaches to further improve translation performance in low-resource settings.",
    "num_pages": 9
}