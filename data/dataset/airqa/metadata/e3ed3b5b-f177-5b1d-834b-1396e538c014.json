{
    "uuid": "e3ed3b5b-f177-5b1d-834b-1396e538c014",
    "title": "Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2023-denoising,\n    title = \"Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\",\n    author = \"Wu, Shaoxiang  and\n      Dai, Damai  and\n      Qin, Ziwei  and\n      Liu, Tianyu  and\n      Lin, Binghuai  and\n      Cao, Yunbo  and\n      Sui, Zhifang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.124\",\n    doi = \"10.18653/v1/2023.acl-long.124\",\n    pages = \"2231--2243\",\n    abstract = \"Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper will be publicly available at \\url{https://github.com/WSXRHFG/DBF}\",\n}\n",
    "authors": [
        "Shaoxiang Wu",
        "Damai Dai",
        "Ziwei Qin",
        "Tianyu Liu",
        "Binghuai Lin",
        "Yunbo Cao",
        "Zhifang Sui"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.124.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e3ed3b5b-f177-5b1d-834b-1396e538c014.pdf",
    "abstract": "Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper will be publicly available at https://github.com/WSXRHFG/DBF",
    "num_pages": 13
}