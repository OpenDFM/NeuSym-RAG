{
    "uuid": "7a6d2ac4-4e4b-5351-aed8-3b086d88ce16",
    "title": "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{trivedi-etal-2024-appworld,\n    title = \"{A}pp{W}orld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents\",\n    author = \"Trivedi, Harsh  and\n      Khot, Tushar  and\n      Hartmann, Mareike  and\n      Manku, Ruskin  and\n      Dong, Vinty  and\n      Li, Edward  and\n      Gupta, Shashank  and\n      Sabharwal, Ashish  and\n      Balasubramanian, Niranjan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.850\",\n    doi = \"10.18653/v1/2024.acl-long.850\",\n    pages = \"16022--16076\",\n    abstract = \"Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of {\\textasciitilde}100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only {\\textasciitilde}49{\\%} of our {`}normal{'} tasks and {\\textasciitilde}30{\\%} of {`}challenge{'} tasks, while other models solve at least 16{\\%} fewer. This highlights the benchmark{'}s difficulty and AppWorld{'}s potential to push the frontiers of interactive coding agents.\",\n}\n",
    "authors": [
        "Harsh Trivedi",
        "Tushar Khot",
        "Mareike Hartmann",
        "Ruskin Manku",
        "Vinty Dong",
        "Edward Li",
        "Shashank Gupta",
        "Ashish Sabharwal",
        "Niranjan Balasubramanian"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.850.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7a6d2ac4-4e4b-5351-aed8-3b086d88ce16.pdf",
    "abstract": "Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls. To remedy this gap, we built AppWorld Engine, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created AppWorld Benchmark (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT4O, solves only ~49% of our ‘normal’ tasks and ~30% of ‘challenge’ tasks, while other models solve at least 16% fewer. This highlights the benchmark’s difficulty and AppWorld’s potential to push the frontiers of interactive coding agents.",
    "num_pages": 55
}