{
    "uuid": "867a56d5-ed86-584b-ae6c-4d18b4c8feb3",
    "title": "Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chen-etal-2024-overcoming,\n    title = \"Overcoming Catastrophic Forgetting by Exemplar Selection in Task-oriented Dialogue System\",\n    author = \"Chen, Chen  and\n      Li, Ruizhe  and\n      Hu, Yuchen  and\n      Chen, Yuanyuan  and\n      Qin, Chengwei  and\n      Zhang, Qiang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.5\",\n    doi = \"10.18653/v1/2024.findings-acl.5\",\n    pages = \"48--61\",\n    abstract = \"Intelligent task-oriented dialogue systems (ToDs) are expected to continuously acquire new knowledge, also known as Continual Learning (CL), which is crucial to fit ever-changing user needs. However, catastrophic forgetting dramatically degrades the model performance in face of a long streamed curriculum. In this paper, we aim to overcome the forgetting problem in ToDs and propose a method (HESIT) with hyper-gradient-based exemplar strategy, which samples influential exemplars for periodic retraining. Instead of unilaterally observing data or models, HESIT adopts a profound exemplar selection strategy that considers the general performance of the trained model when selecting exemplars for each task domain. Specifically, HESIT analyzes the training data influence by tracing their hyper-gradient in the optimization process. Furthermore, HESIT avoids estimating Hessian to make it compatible for ToDs with a large pre-trained model. Experimental results show that HESIT effectively alleviates catastrophic forgetting by exemplar selection, and achieves state-of-the-art performance on the largest CL benchmark of ToDs in terms of all metrics.\",\n}\n",
    "authors": [
        "Chen Chen",
        "Ruizhe Li",
        "Yuchen Hu",
        "Yuanyuan Chen",
        "Chengwei Qin",
        "Qiang Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/867a56d5-ed86-584b-ae6c-4d18b4c8feb3.pdf",
    "abstract": "Intelligent task-oriented dialogue systems (ToDs) are expected to continuously acquire new knowledge, also known as Continual Learning (CL), which is crucial to fit ever-changing user needs. However, catastrophic forgetting dramatically degrades the model performance in face of a long streamed curriculum. In this paper, we aim to overcome the forgetting problem in ToDs and propose a method (HESIT) with hyper-gradient-based exemplar strategy, which samples influential exemplars for periodic retraining. Instead of unilaterally observing data or models, HESIT adopts a profound exemplar selection strategy that considers the general performance of the trained model when selecting exemplars for each task domain. Specifically, HESIT analyzes the training data influence by tracing their hyper-gradient in the optimization process. Furthermore, HESIT avoids estimating Hessian to make it compatible for ToDs with a large pre-trained model. Experimental results show that HESIT effectively alleviates catastrophic forgetting by exemplar selection, and achieves state-of-the-art performance on the largest CL benchmark of ToDs in terms of all metrics.",
    "num_pages": 14
}