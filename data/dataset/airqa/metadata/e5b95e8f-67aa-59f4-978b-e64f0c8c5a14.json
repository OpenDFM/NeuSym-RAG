{
    "uuid": "e5b95e8f-67aa-59f4-978b-e64f0c8c5a14",
    "title": "Patton: Language Model Pretraining on Text-Rich Networks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jin-etal-2023-patton,\n    title = \"Patton: Language Model Pretraining on Text-Rich Networks\",\n    author = \"Jin, Bowen  and\n      Zhang, Wentao  and\n      Zhang, Yu  and\n      Meng, Yu  and\n      Zhang, Xinyang  and\n      Zhu, Qi  and\n      Han, Jiawei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.387\",\n    doi = \"10.18653/v1/2023.acl-long.387\",\n    pages = \"7005--7020\",\n    abstract = \"A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.\",\n}\n",
    "authors": [
        "Bowen Jin",
        "Wentao Zhang",
        "Yu Zhang",
        "Yu Meng",
        "Xinyang Zhang",
        "Qi Zhu",
        "Jiawei Han"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.387.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e5b95e8f-67aa-59f4-978b-e64f0c8c5a14.pdf",
    "abstract": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
    "num_pages": 16
}