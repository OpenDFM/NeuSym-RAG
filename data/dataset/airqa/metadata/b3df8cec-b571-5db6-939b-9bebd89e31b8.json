{
    "uuid": "b3df8cec-b571-5db6-939b-9bebd89e31b8",
    "title": "Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xu-etal-2023-learning,\n    title = \"Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback\",\n    author = \"Xu, Jing  and\n      Ung, Megan  and\n      Komeili, Mojtaba  and\n      Arora, Kushal  and\n      Boureau, Y-Lan  and\n      Weston, Jason\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.758\",\n    doi = \"10.18653/v1/2023.acl-long.758\",\n    pages = \"13557--13572\",\n    abstract = \"Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback {--} including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feed- back and algorithms work best. We find the recently introduced DIRECTOR model (Arora et al., 2022) shows significant improvements over other existing approaches.\",\n}\n",
    "authors": [
        "Jing Xu",
        "Megan Ung",
        "Mojtaba Komeili",
        "Kushal Arora",
        "Y-Lan Boureau",
        "Jason Weston"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.758.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b3df8cec-b571-5db6-939b-9bebd89e31b8.pdf",
    "abstract": "Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback â€“ including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feed- back and algorithms work best. We find the recently introduced DIRECTOR model (Arora et al., 2022) shows significant improvements over other existing approaches.",
    "num_pages": 16
}