{
    "uuid": "bad2d189-c96b-55b8-ad13-728475876276",
    "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{singh-etal-2024-flowvqa,\n    title = \"{F}low{VQA}: Mapping Multimodal Logic in Visual Question Answering with Flowcharts\",\n    author = \"Singh, Shubhankar  and\n      Chaurasia, Purvi  and\n      Varun, Yerram  and\n      Pandya, Pranshu  and\n      Gupta, Vatsal  and\n      Gupta, Vivek  and\n      Roth, Dan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.78\",\n    doi = \"10.18653/v1/2024.findings-acl.78\",\n    pages = \"1330--1350\",\n    abstract = \"Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmark{'}s potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks.\",\n}\n",
    "authors": [
        "Shubhankar Singh",
        "Purvi Chaurasia",
        "Yerram Varun",
        "Pranshu Pandya",
        "Vatsal Gupta",
        "Vivek Gupta",
        "Dan Roth"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.78.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bad2d189-c96b-55b8-ad13-728475876276.pdf",
    "abstract": "Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmarkâ€™s potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks.",
    "num_pages": 21
}