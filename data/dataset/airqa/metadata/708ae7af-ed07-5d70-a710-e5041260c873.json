{
    "uuid": "708ae7af-ed07-5d70-a710-e5041260c873",
    "title": "An Active Learning Pipeline for NLU Error Detection in Conversational Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    "bibtex": "@inproceedings{pascual-etal-2023-active,\n    title = \"An Active Learning Pipeline for {NLU} Error Detection in Conversational Agents\",\n    author = \"Pascual, Damian  and\n      Bercher, Aritz  and\n      Bhardwaj, Akansha  and\n      Cui, Mingbo  and\n      Kohler, Dominic  and\n      Van Der Poel, Liam  and\n      Rosso, Paolo\",\n    editor = \"Prange, Jakob  and\n      Friedrich, Annemarie\",\n    booktitle = \"Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.law-1.6\",\n    doi = \"10.18653/v1/2023.law-1.6\",\n    pages = \"55--60\",\n    abstract = \"High-quality labeled data is paramount to the performance of modern machine learning models. However, annotating data is a time-consuming and costly process that requires human experts to examine large collections of raw data. For conversational agents in production settings with access to large amounts of user-agent conversations, the challenge is to decide what data should be annotated first. We consider the Natural Language Understanding (NLU) component of a conversational agent deployed in a real-world setup with limited resources. We present an active learning pipeline for offline detection of classification errors that leverages two strong classifiers. Then, we perform topic modeling on the potentially mis-classified samples to ease data analysis and to reveal error patterns. In our experiments, we show on a real-world dataset that by using our method to prioritize data annotation we reach 100{\\%} of the performance annotating only 36{\\%} of the data. Finally, we present an analysis of some of the error patterns revealed and argue that our pipeline is a valuable tool to detect critical errors and reduce the workload of annotators.\",\n}\n",
    "authors": [
        "Damian Pascual",
        "Aritz Bercher",
        "Akansha Bhardwaj",
        "Mingbo Cui",
        "Dominic Kohler",
        "Liam Van Der Poel",
        "Paolo Rosso"
    ],
    "pdf_url": "https://aclanthology.org/2023.law-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/708ae7af-ed07-5d70-a710-e5041260c873.pdf",
    "abstract": "High-quality labeled data is paramount to the performance of modern machine learning models. However, annotating data is a time-consuming and costly process that requires human experts to examine large collections of raw data. For conversational agents in production settings with access to large amounts of user-agent conversations, the challenge is to decide what data should be annotated first. We consider the Natural Language Understanding (NLU) component of a conversational agent deployed in a real-world setup with limited resources. We present an active learning pipeline for offline detection of classification errors that leverages two strong classifiers. Then, we perform topic modeling on the potentially mis-classified samples to ease data analysis and to reveal error patterns. In our experiments, we show on a real-world dataset that by using our method to prioritize data annotation we reach 100% of the performance annotating only 36% of the data. Finally, we present an analysis of some of the error patterns revealed and argue that our pipeline is a valuable tool to detect critical errors and reduce the workload of annotators.",
    "num_pages": 6
}