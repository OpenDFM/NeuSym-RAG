{
    "uuid": "81583b36-d14b-5dbd-bf81-4e81cd8866aa",
    "title": "Topic and Style-aware Transformer for Multimodal Emotion Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{qiu-etal-2023-topic,\n    title = \"Topic and Style-aware Transformer for Multimodal Emotion Recognition\",\n    author = \"Qiu, Shuwen  and\n      Sekhar, Nitesh  and\n      Singhal, Prateek\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.130\",\n    doi = \"10.18653/v1/2023.findings-acl.130\",\n    pages = \"2074--2082\",\n    abstract = \"Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we propose content-oriented features Topic and Speaking style on top of it to approach the subjectivity issues. Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues by serving as content {``}normalization{''}.\",\n}\n",
    "authors": [
        "Shuwen Qiu",
        "Nitesh Sekhar",
        "Prateek Singhal"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.130.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/81583b36-d14b-5dbd-bf81-4e81cd8866aa.pdf",
    "abstract": "Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we propose content-oriented features Topic and Speaking style on top of it to approach the subjectivity issues. Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues by serving as content “normalization”.",
    "num_pages": 9
}