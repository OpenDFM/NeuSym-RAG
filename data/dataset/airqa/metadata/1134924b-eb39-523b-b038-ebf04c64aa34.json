{
    "uuid": "1134924b-eb39-523b-b038-ebf04c64aa34",
    "title": "Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{phukan-etal-2024-peering,\n    title = \"Peering into the Mind of Language Models: An Approach for Attribution in Contextual Question Answering\",\n    author = \"Phukan, Anirudh  and\n      Somasundaram, Shwetha  and\n      Saxena, Apoorv  and\n      Goswami, Koustava  and\n      Srinivasan, Balaji Vasan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.682\",\n    doi = \"10.18653/v1/2024.findings-acl.682\",\n    pages = \"11481--11495\",\n    abstract = \"With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with {``}glue text{''} generated by the LLM. Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM. We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs. Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers. Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source. Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability. Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup.\",\n}\n",
    "authors": [
        "Anirudh Phukan",
        "Shwetha Somasundaram",
        "Apoorv Saxena",
        "Koustava Goswami",
        "Balaji Vasan Srinivasan"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.682.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1134924b-eb39-523b-b038-ebf04c64aa34.pdf",
    "abstract": "With the enhancement in the field of generative artificial intelligence (AI), contextual question answering has become extremely relevant. Attributing model generations to the input source document is essential to ensure trustworthiness and reliability. We observe that when large language models (LLMs) are used for contextual question answering, the output answer often consists of text copied verbatim from the input prompt which is linked together with “glue text” generated by the LLM. Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM. We introduce a novel method for attribution in contextual question answering, leveraging the hidden state representations of LLMs. Our approach bypasses the need for extensive model retraining and retrieval model overhead, offering granular attributions and preserving the quality of generated answers. Our experimental results demonstrate that our method performs on par or better than GPT-4 at identifying verbatim copied segments in LLM generations and in attributing these segments to their source. Importantly, our method shows robust performance across various LLM architectures, highlighting its broad applicability. Additionally, we present Verifiability-granular, an attribution dataset which has token level annotations for LLM generations in the contextual question answering setup.",
    "num_pages": 15
}