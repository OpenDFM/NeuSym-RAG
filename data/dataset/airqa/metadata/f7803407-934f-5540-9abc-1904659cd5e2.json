{
    "uuid": "f7803407-934f-5540-9abc-1904659cd5e2",
    "title": "Prompted Opinion Summarization with GPT-3.5",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{bhaskar-etal-2023-prompted,\n    title = \"Prompted Opinion Summarization with {GPT}-3.5\",\n    author = \"Bhaskar, Adithya  and\n      Fabbri, Alex  and\n      Durrett, Greg\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.591\",\n    doi = \"10.18653/v1/2023.findings-acl.591\",\n    pages = \"9282--9300\",\n    abstract = \"Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.\",\n}\n",
    "authors": [
        "Adithya Bhaskar",
        "Alex Fabbri",
        "Greg Durrett"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.591.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f7803407-934f-5540-9abc-1904659cd5e2.pdf",
    "abstract": "Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.",
    "num_pages": 19
}