{
    "uuid": "1ad1ab0f-95c8-51e0-8553-3dd107b443ed",
    "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-matcha,\n    title = \"{M}at{C}ha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering\",\n    author = \"Liu, Fangyu  and\n      Piccinno, Francesco  and\n      Krichene, Syrine  and\n      Pang, Chenxi  and\n      Lee, Kenton  and\n      Joshi, Mandar  and\n      Altun, Yasemin  and\n      Collier, Nigel  and\n      Eisenschlos, Julian\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.714\",\n    doi = \"10.18653/v1/2023.acl-long.714\",\n    pages = \"12756--12770\",\n    abstract = \"Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models{'} capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20{\\%}. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.\",\n}\n",
    "authors": [
        "Fangyu Liu",
        "Francesco Piccinno",
        "Syrine Krichene",
        "Chenxi Pang",
        "Kenton Lee",
        "Mandar Joshi",
        "Yasemin Altun",
        "Nigel Collier",
        "Julian Eisenschlos"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.714.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1ad1ab0f-95c8-51e0-8553-3dd107b443ed.pdf",
    "abstract": "Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language modelsâ€™ capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.",
    "num_pages": 15
}