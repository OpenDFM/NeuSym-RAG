{
    "uuid": "07d40d6e-1b67-5e52-9294-ca4d0e1c9526",
    "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{yuan-etal-2024-chatmusician,\n    title = \"{C}hat{M}usician: Understanding and Generating Music Intrinsically with {LLM}\",\n    author = \"Yuan, Ruibin  and\n      Lin, Hanfeng  and\n      Wang, Yi  and\n      Tian, Zeyue  and\n      Wu, Shangda  and\n      Shen, Tianhao  and\n      Zhang, Ge  and\n      Wu, Yuhang  and\n      Liu, Cong  and\n      Zhou, Ziya  and\n      Xue, Liumeng  and\n      Ma, Ziyang  and\n      Liu, Qin  and\n      Zheng, Tianyu  and\n      Li, Yizhi  and\n      Ma, Yinghao  and\n      Liang, Yiming  and\n      Chi, Xiaowei  and\n      Liu, Ruibo  and\n      Wang, Zili  and\n      Lin, Chenghua  and\n      Liu, Qifeng  and\n      Jiang, Tao  and\n      Huang, Wenhao  and\n      Chen, Wenhu  and\n      Fu, Jie  and\n      Benetos, Emmanouil  and\n      Xia, Gus  and\n      Dannenberg, Roger  and\n      Xue, Wei  and\n      Kang, Shiyin  and\n      Guo, Yike\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.373\",\n    doi = \"10.18653/v1/2024.findings-acl.373\",\n    pages = \"6252--6271\",\n    abstract = \"While LLMs demonstrate impressive capabilities in musical knowledge, we find that music reasoning is still an unsolved task.We introduce ChatMusician, an open-source large language model (LLM) that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language.ChatMusician can understand and generate music with a pure text tokenizer without external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score.ChatMusician is capable of composing well-structured, full-length music, condition on texts, chords, melodies, motifs, musical forms, etc.On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 by a noticeable margin. We show that ChatMusician preserves or even surpasses the original LLaMA2 7B{'}s language abilities by evaluating on MMLU benchmark.Our work reveals that LLMs can be an excellent compressor for music, which can be seen as humanity{'}s creative language, but there remains significant territory to be conquered.We release our 5B token music-language corpora MusicPiles, the collected MusicTheoryBench, code, model and demo.\",\n}\n",
    "authors": [
        "Ruibin Yuan",
        "Hanfeng Lin",
        "Yi Wang",
        "Zeyue Tian",
        "Shangda Wu",
        "Tianhao Shen",
        "Ge Zhang",
        "Yuhang Wu",
        "Cong Liu",
        "Ziya Zhou",
        "Liumeng Xue",
        "Ziyang Ma",
        "Qin Liu",
        "Tianyu Zheng",
        "Yizhi Li",
        "Yinghao Ma",
        "Yiming Liang",
        "Xiaowei Chi",
        "Ruibo Liu",
        "Zili Wang",
        "Chenghua Lin",
        "Qifeng Liu",
        "Tao Jiang",
        "Wenhao Huang",
        "Wenhu Chen",
        "Jie Fu",
        "Emmanouil Benetos",
        "Gus Xia",
        "Roger Dannenberg",
        "Wei Xue",
        "Shiyin Kang",
        "Yike Guo"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.373.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/07d40d6e-1b67-5e52-9294-ca4d0e1c9526.pdf",
    "abstract": "While LLMs demonstrate impressive capabilities in musical knowledge, we find that music reasoning is still an unsolved task.We introduce ChatMusician, an open-source large language model (LLM) that integrates intrinsic musical abilities. It is based on continual pre-training and finetuning LLaMA2 on a text-compatible music representation, ABC notation, and the music is treated as a second language.ChatMusician can understand and generate music with a pure text tokenizer without external multi-modal neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher MMLU score.ChatMusician is capable of composing well-structured, full-length music, condition on texts, chords, melodies, motifs, musical forms, etc.On our meticulously curated college-level music understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and GPT-3.5 by a noticeable margin. We show that ChatMusician preserves or even surpasses the original LLaMA2 7B’s language abilities by evaluating on MMLU benchmark.Our work reveals that LLMs can be an excellent compressor for music, which can be seen as humanity’s creative language, but there remains significant territory to be conquered.We release our 5B token music-language corpora MusicPiles, the collected MusicTheoryBench, code, model and demo.",
    "num_pages": 20
}