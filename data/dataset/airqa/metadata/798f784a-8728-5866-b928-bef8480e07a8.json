{
    "uuid": "798f784a-8728-5866-b928-bef8480e07a8",
    "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{du-etal-2024-bitdistiller,\n    title = \"{B}it{D}istiller: Unleashing the Potential of Sub-4-Bit {LLM}s via Self-Distillation\",\n    author = \"Du, DaYou  and\n      Zhang, Yijia  and\n      Cao, Shijie  and\n      Guo, Jiaqi  and\n      Cao, Ting  and\n      Chu, Xiaowen  and\n      Xu, Ningyi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.7\",\n    doi = \"10.18653/v1/2024.acl-long.7\",\n    pages = \"102--116\",\n    abstract = \"The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.\",\n}\n",
    "authors": [
        "DaYou Du",
        "Yijia Zhang",
        "Shijie Cao",
        "Jiaqi Guo",
        "Ting Cao",
        "Xiaowen Chu",
        "Ningyi Xu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/798f784a-8728-5866-b928-bef8480e07a8.pdf",
    "abstract": "The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.",
    "num_pages": 15
}