{
    "uuid": "acc2bbb3-7f2a-52f8-8e47-a6ceb8654332",
    "title": "Ask Again, Then Fail: Large Language Modelsâ€™ Vacillations in Judgment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xie-etal-2024-ask,\n    title = \"Ask Again, Then Fail: Large Language Models{'} Vacillations in Judgment\",\n    author = \"Xie, Qiming  and\n      Wang, Zengzhi  and\n      Feng, Yi  and\n      Xia, Rui\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.577\",\n    doi = \"10.18653/v1/2024.acl-long.577\",\n    pages = \"10709--10745\",\n    abstract = \"We observe that current large language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current large language models. Furthermore, to mitigate this issue, we explore various prompting strategies for closed-source models, and develop a training-based framework Unwavering-FQ that teaches large language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of large language models.\",\n}\n",
    "authors": [
        "Qiming Xie",
        "Zengzhi Wang",
        "Yi Feng",
        "Rui Xia"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.577.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/acc2bbb3-7f2a-52f8-8e47-a6ceb8654332.pdf",
    "abstract": "We observe that current large language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current large language models. Furthermore, to mitigate this issue, we explore various prompting strategies for closed-source models, and develop a training-based framework Unwavering-FQ that teaches large language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of large language models.",
    "num_pages": 37
}