{
    "uuid": "9961d28e-baec-5488-9efb-92f13f767981",
    "title": "Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ko-etal-2023-discourse,\n    title = \"Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion\",\n    author = \"Ko, Wei-Jen  and\n      Wu, Yating  and\n      Dalton, Cutter  and\n      Srinivas, Dananjay  and\n      Durrett, Greg  and\n      Li, Junyi Jessy\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.710\",\n    doi = \"10.18653/v1/2023.findings-acl.710\",\n    pages = \"11181--11195\",\n    abstract = \"Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.\",\n}\n",
    "authors": [
        "Wei-Jen Ko",
        "Yating Wu",
        "Cutter Dalton",
        "Dananjay Srinivas",
        "Greg Durrett",
        "Junyi Jessy Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.710.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9961d28e-baec-5488-9efb-92f13f767981.pdf",
    "abstract": "Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.",
    "num_pages": 15
}