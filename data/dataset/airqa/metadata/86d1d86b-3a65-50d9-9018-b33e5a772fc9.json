{
    "uuid": "86d1d86b-3a65-50d9-9018-b33e5a772fc9",
    "title": "YANMTT: Yet Another Neural Machine Translation Toolkit",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{dabre-etal-2023-yanmtt,\n    title = \"{YANMTT}: Yet Another Neural Machine Translation Toolkit\",\n    author = \"Dabre, Raj  and\n      Kanojia, Diptesh  and\n      Sawant, Chinmay  and\n      Sumita, Eiichiro\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.24\",\n    doi = \"10.18653/v1/2023.acl-demo.24\",\n    pages = \"257--263\",\n    abstract = \"In this paper, we present our open-source neural machine translation (NMT) toolkit called {``}Yet Another Neural Machine Translation Toolkit{''} abbreviated as YANMTT - \\url{https://github.com/prajdabre/yanmtt}, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.\",\n}\n",
    "authors": [
        "Raj Dabre",
        "Diptesh Kanojia",
        "Chinmay Sawant",
        "Eiichiro Sumita"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.24.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/86d1d86b-3a65-50d9-9018-b33e5a772fc9.pdf",
    "abstract": "In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.",
    "num_pages": 7
}