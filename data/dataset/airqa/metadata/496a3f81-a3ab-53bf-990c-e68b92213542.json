{
    "uuid": "496a3f81-a3ab-53bf-990c-e68b92213542",
    "title": "Improving Language Model Integration for Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{herold-etal-2023-improving,\n    title = \"Improving Language Model Integration for Neural Machine Translation\",\n    author = \"Herold, Christian  and\n      Gao, Yingbo  and\n      Zeineldeen, Mohammad  and\n      Ney, Hermann\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.444\",\n    doi = \"10.18653/v1/2023.findings-acl.444\",\n    pages = \"7114--7123\",\n    abstract = \"The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation.\",\n}\n",
    "authors": [
        "Christian Herold",
        "Yingbo Gao",
        "Mohammad Zeineldeen",
        "Hermann Ney"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.444.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/496a3f81-a3ab-53bf-990c-e68b92213542.pdf",
    "abstract": "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation.",
    "num_pages": 10
}