{
    "uuid": "2ef0e482-3195-57f3-b77b-494cfd0807be",
    "title": "Decouple knowledge from paramters for plug-and-play language modeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cheng-etal-2023-decouple,\n    title = \"Decouple knowledge from paramters for plug-and-play language modeling\",\n    author = \"Cheng, Xin  and\n      Lin, Yankai  and\n      Chen, Xiuying  and\n      Zhao, Dongyan  and\n      Yan, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.901\",\n    doi = \"10.18653/v1/2023.findings-acl.901\",\n    pages = \"14288--14308\",\n    abstract = \"Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro {`}MODEL{'}}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro {`}MEMORY{'}}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro {`}MODEL{'}} could be easily adapted to different domains with pluggable in-domain memory{---}obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro {`}MODEL{'}} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro {`}MEMORY{'}} without re-training. Finally, we show that by incorporating training samples into {pasted macro {`}MEMORY{'}} with knowledge prompting, {pasted macro {`}MODEL{'}} could further be improved by the instruction of in-task knowledge.\",\n}\n",
    "authors": [
        "Xin Cheng",
        "Yankai Lin",
        "Xiuying Chen",
        "Dongyan Zhao",
        "Rui Yan"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.901.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2ef0e482-3195-57f3-b77b-494cfd0807be.pdf",
    "abstract": "Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro ‘MODEL’}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro ‘MEMORY’}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro ‘MODEL’} could be easily adapted to different domains with pluggable in-domain memory—obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro ‘MODEL’} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro ‘MEMORY’} without re-training. Finally, we show that by incorporating training samples into {pasted macro ‘MEMORY’} with knowledge prompting, {pasted macro ‘MODEL’} could further be improved by the instruction of in-task knowledge.",
    "num_pages": 21
}