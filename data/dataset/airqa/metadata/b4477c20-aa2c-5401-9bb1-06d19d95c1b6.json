{
    "uuid": "b4477c20-aa2c-5401-9bb1-06d19d95c1b6",
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-detoxifying,\n    title = \"Detoxifying Large Language Models via Knowledge Editing\",\n    author = \"Wang, Mengru  and\n      Zhang, Ningyu  and\n      Xu, Ziwen  and\n      Xi, Zekun  and\n      Deng, Shumin  and\n      Yao, Yunzhi  and\n      Zhang, Qishen  and\n      Yang, Linyi  and\n      Wang, Jindong  and\n      Chen, Huajun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.171\",\n    doi = \"10.18653/v1/2024.acl-long.171\",\n    pages = \"3093--3118\",\n    abstract = \"This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.\",\n}\n",
    "authors": [
        "Mengru Wang",
        "Ningyu Zhang",
        "Ziwen Xu",
        "Zekun Xi",
        "Shumin Deng",
        "Yunzhi Yao",
        "Qishen Zhang",
        "Linyi Yang",
        "Jindong Wang",
        "Huajun Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.171.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b4477c20-aa2c-5401-9bb1-06d19d95c1b6.pdf",
    "abstract": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.",
    "num_pages": 26
}