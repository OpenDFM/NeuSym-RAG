{
    "uuid": "6970b4f9-2701-5364-b962-242c1894389c",
    "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hu-etal-2024-wilke,\n    title = \"{W}il{KE}: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing\",\n    author = \"Hu, Chenhui  and\n      Cao, Pengfei  and\n      Chen, Yubo  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.207\",\n    doi = \"10.18653/v1/2024.findings-acl.207\",\n    pages = \"3476--3503\",\n    abstract = \"Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2{\\%} and 67.8{\\%} on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.\",\n}\n",
    "authors": [
        "Chenhui Hu",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.207.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6970b4f9-2701-5364-b962-242c1894389c.pdf",
    "abstract": "Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named Wise-Layer Knowledge Editor (WilKE), which selects editing layer based on the pattern matching degree of editing knowledge across different layers in language models. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.",
    "num_pages": 28
}