{
    "uuid": "1137b8a0-bfd5-52f4-bba3-fcb0a406f931",
    "title": "Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fan-etal-2024-muffin,\n    title = \"Muffin or {C}hihuahua? Challenging Multimodal Large Language Models with Multipanel {VQA}\",\n    author = \"Fan, Yue  and\n      Gu, Jing  and\n      Zhou, Kaiwen  and\n      Yan, Qianqi  and\n      Jiang, Shan  and\n      Kuo, Ching-Chen  and\n      Zhao, Yang  and\n      Guan, Xinze  and\n      Wang, Xin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.370\",\n    doi = \"10.18653/v1/2024.acl-long.370\",\n    pages = \"6845--6863\",\n    abstract = \"Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99{\\%} accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs{'} multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs{'} performance with synthetic data and offer insights for enhancement.\",\n}\n",
    "authors": [
        "Yue Fan",
        "Jing Gu",
        "Kaiwen Zhou",
        "Qianqi Yan",
        "Shan Jiang",
        "Ching-Chen Kuo",
        "Yang Zhao",
        "Xinze Guan",
        "Xin Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.370.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1137b8a0-bfd5-52f4-bba3-fcb0a406f931.pdf",
    "abstract": "Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs’ multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs’ performance with synthetic data and offer insights for enhancement.",
    "num_pages": 19
}