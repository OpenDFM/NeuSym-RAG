{
    "uuid": "3cbdcadf-a063-527e-be75-511755ee666b",
    "title": "Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{mela-etal-2024-mass,\n    title = \"Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge\",\n    author = \"Mela, Daniel  and\n      Gonzalez-Agirre, Aitor  and\n      Hernando, Javier  and\n      Villegas, Marta\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.347\",\n    doi = \"10.18653/v1/2024.findings-acl.347\",\n    pages = \"5831--5847\",\n    abstract = \"Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10{\\%} increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.\",\n}\n",
    "authors": [
        "Daniel Mela",
        "Aitor Gonzalez-Agirre",
        "Javier Hernando",
        "Marta Villegas"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.347.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3cbdcadf-a063-527e-be75-511755ee666b.pdf",
    "abstract": "Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at https://github.com/dtamayo-nlp/MEMAT.",
    "num_pages": 17
}