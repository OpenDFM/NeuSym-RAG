{
    "uuid": "743c7279-ebee-5ec8-bb1e-f23f38fe16e8",
    "title": "IEPile: Unearthing Large Scale Schema-Conditioned Information Extraction Corpus",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{gui-etal-2024-iepile,\n    title = \"{IEP}ile: Unearthing Large Scale Schema-Conditioned Information Extraction Corpus\",\n    author = \"Gui, Honghao  and\n      Yuan, Lin  and\n      Ye, Hongbin  and\n      Zhang, Ningyu  and\n      Sun, Mengshu  and\n      Liang, Lei  and\n      Chen, Huajun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.13\",\n    doi = \"10.18653/v1/2024.acl-short.13\",\n    pages = \"127--146\",\n    abstract = \"Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPile enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.\",\n}\n",
    "authors": [
        "Honghao Gui",
        "Lin Yuan",
        "Hongbin Ye",
        "Ningyu Zhang",
        "Mengshu Sun",
        "Lei Liang",
        "Huajun Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/743c7279-ebee-5ec8-bb1e-f23f38fe16e8.pdf",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimentally, IEPile enhance the performance of LLMs for IE, with notable improvements in zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.",
    "num_pages": 20
}