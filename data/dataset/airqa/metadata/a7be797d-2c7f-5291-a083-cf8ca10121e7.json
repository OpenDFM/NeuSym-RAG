{
    "uuid": "a7be797d-2c7f-5291-a083-cf8ca10121e7",
    "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
    "bibtex": "@inproceedings{singh-etal-2024-learning,\n    title = \"Learning New Tasks from a Few Examples with Soft-Label Prototypes\",\n    author = \"Singh, Avyav  and\n      Shutova, Ekaterina  and\n      Yannakoudakis, Helen\",\n    editor = \"Zhao, Chen  and\n      Mosbach, Marius  and\n      Atanasova, Pepa  and\n      Goldfarb-Tarrent, Seraphina  and\n      Hase, Peter  and\n      Hosseini, Arian  and\n      Elbayad, Maha  and\n      Pezzelle, Sandro  and\n      Mozes, Maximilian\",\n    booktitle = \"Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.repl4nlp-1.16\",\n    pages = \"215--236\",\n    abstract = \"Existing approaches to few-shot learning in NLP rely on large language models (LLMs) and/or fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a novel few-shot learning approach based on soft-label prototypes (SLPs) designed to collectively capture the distribution of different classes across the input domain space. We focus on learning previously unseen NLP tasks from very few examples (4, 8, 16) per class and experimentally demonstrate that our approach achieves superior performance on the majority of tested tasks in this data-lean setting while being highly parameter efficient. We also show that our few-shot adaptation method can be integrated into more generalised learning settings, primarily meta-learning, to yield superior performance against strong baselines.\",\n}\n",
    "authors": [
        "Avyav Singh",
        "Ekaterina Shutova",
        "Helen Yannakoudakis"
    ],
    "pdf_url": "https://aclanthology.org/2024.repl4nlp-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a7be797d-2c7f-5291-a083-cf8ca10121e7.pdf",
    "abstract": "Existing approaches to few-shot learning in NLP rely on large language models (LLMs) and/or fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a novel few-shot learning approach based on soft-label prototypes (SLPs) designed to collectively capture the distribution of different classes across the input domain space. We focus on learning previously unseen NLP tasks from very few examples (4, 8, 16) per class and experimentally demonstrate that our approach achieves superior performance on the majority of tested tasks in this data-lean setting while being highly parameter efficient. We also show that our few-shot adaptation method can be integrated into more generalised learning settings, primarily meta-learning, to yield superior performance against strong baselines.",
    "num_pages": 22
}