{
    "uuid": "74868ea3-e562-5826-a089-bf78ca87efc8",
    "title": "Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{meyer-buys-2023-subword,\n    title = \"Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation\",\n    author = \"Meyer, Francois  and\n      Buys, Jan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.175\",\n    doi = \"10.18653/v1/2023.findings-acl.175\",\n    pages = \"2795--2809\",\n    abstract = \"Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation.\",\n}\n",
    "authors": [
        "Francois Meyer",
        "Jan Buys"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.175.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/74868ea3-e562-5826-a089-bf78ca87efc8.pdf",
    "abstract": "Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation.",
    "num_pages": 15
}