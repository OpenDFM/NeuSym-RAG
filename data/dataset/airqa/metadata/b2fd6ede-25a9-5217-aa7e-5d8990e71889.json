{
    "uuid": "b2fd6ede-25a9-5217-aa7e-5d8990e71889",
    "title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{duquenne-etal-2023-speechmatrix,\n    title = \"{S}peech{M}atrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations\",\n    author = \"Duquenne, Paul-Ambroise  and\n      Gong, Hongyu  and\n      Dong, Ning  and\n      Du, Jingfei  and\n      Lee, Ann  and\n      Goswami, Vedanuj  and\n      Wang, Changhan  and\n      Pino, Juan  and\n      Sagot, Beno{\\^\\i}t  and\n      Schwenk, Holger\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.899\",\n    doi = \"10.18653/v1/2023.acl-long.899\",\n    pages = \"16251--16269\",\n    abstract = \"We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models will be publicly released\",\n}\n",
    "authors": [
        "Paul-Ambroise Duquenne",
        "Hongyu Gong",
        "Ning Dong",
        "Jingfei Du",
        "Ann Lee",
        "Vedanuj Goswami",
        "Changhan Wang",
        "Juan Pino",
        "Beno√Æt Sagot",
        "Holger Schwenk"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.899.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b2fd6ede-25a9-5217-aa7e-5d8990e71889.pdf",
    "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models will be publicly released",
    "num_pages": 19
}