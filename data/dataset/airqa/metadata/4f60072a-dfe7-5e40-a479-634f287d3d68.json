{
    "uuid": "4f60072a-dfe7-5e40-a479-634f287d3d68",
    "title": "UIO at SemEval-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{ronningstad-2023-uio,\n    title = \"{UIO} at {S}em{E}val-2023 Task 12: Multilingual fine-tuning for sentiment classification in low-resource Languages\",\n    author = \"R{\\o}nningstad, Egil\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.144\",\n    doi = \"10.18653/v1/2023.semeval-1.144\",\n    pages = \"1054--1060\",\n    abstract = \"Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment Analysis for African Languages, provides insight into how a multilingual large language model can be a resource for sentiment analysis in languages not seen during pretraining. The shared task provides datasets of a variety of African languages from different language families. The languages are to various degrees related to languages used during pretraining, and the language data contain various degrees of code-switching. We experiment with both monolingual and multilingual datasets for the final fine-tuning, and find that with the provided datasets that contain samples in the thousands, monolingual fine-tuning yields the best results.\",\n}\n",
    "authors": [
        "Egil RÃ¸nningstad"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.144.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4f60072a-dfe7-5e40-a479-634f287d3d68.pdf",
    "abstract": "Our contribution to the 2023 AfriSenti-SemEval shared task 12: Sentiment Analysis for African Languages, provides insight into how a multilingual large language model can be a resource for sentiment analysis in languages not seen during pretraining. The shared task provides datasets of a variety of African languages from different language families. The languages are to various degrees related to languages used during pretraining, and the language data contain various degrees of code-switching. We experiment with both monolingual and multilingual datasets for the final fine-tuning, and find that with the provided datasets that contain samples in the thousands, monolingual fine-tuning yields the best results.",
    "num_pages": 7
}