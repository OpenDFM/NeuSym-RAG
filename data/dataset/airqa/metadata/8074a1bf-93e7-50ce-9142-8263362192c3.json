{
    "uuid": "8074a1bf-93e7-50ce-9142-8263362192c3",
    "title": "Measuring and Addressing Indexical Bias in Information Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ziems-etal-2024-measuring,\n    title = \"Measuring and Addressing Indexical Bias in Information Retrieval\",\n    author = \"Ziems, Caleb  and\n      Held, William  and\n      Dwivedi-Yu, Jane  and\n      Yang, Diyi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.763\",\n    doi = \"10.18653/v1/2024.findings-acl.763\",\n    pages = \"12860--12877\",\n    abstract = \"Information Retrieval (IR) systems are designed to deliver relevant content, but traditional systems may not optimize rankings for fairness, neutrality, or the balance of ideas. Consequently, IR can often introduce indexical biases, or biases in the positional order of documents. Although indexical bias can demonstrably affect people{'}s opinion, voting patterns, and other behaviors, these issues remain understudied as the field lacks reliable metrics and procedures for automatically measuring indexical bias. Towards this end, we introduce the PAIR framework, which supports automatic bias audits for ranked documents or entire IR systems. After introducing DUO, the first general-purpose automatic bias metric, we run an extensive evaluation of 8 IR systems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k queries spanning 1.4k controversial issue topics. A human behavioral study validates our approach, showing that our bias metric can help predict when and how indexical bias will shift a reader{'}s opinion.\",\n}\n",
    "authors": [
        "Caleb Ziems",
        "William Held",
        "Jane Dwivedi-Yu",
        "Diyi Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.763.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8074a1bf-93e7-50ce-9142-8263362192c3.pdf",
    "abstract": "Information Retrieval (IR) systems are designed to deliver relevant content, but traditional systems may not optimize rankings for fairness, neutrality, or the balance of ideas. Consequently, IR can often introduce indexical biases, or biases in the positional order of documents. Although indexical bias can demonstrably affect people’s opinion, voting patterns, and other behaviors, these issues remain understudied as the field lacks reliable metrics and procedures for automatically measuring indexical bias. Towards this end, we introduce the PAIR framework, which supports automatic bias audits for ranked documents or entire IR systems. After introducing DUO, the first general-purpose automatic bias metric, we run an extensive evaluation of 8 IR systems on a new corpus of 32k synthetic and 4.7k natural documents, with 4k queries spanning 1.4k controversial issue topics. A human behavioral study validates our approach, showing that our bias metric can help predict when and how indexical bias will shift a reader’s opinion.",
    "num_pages": 18
}