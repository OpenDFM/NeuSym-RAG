{
    "uuid": "3278211d-039c-558c-b81a-4ea0025860a8",
    "title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2024-docmath,\n    title = \"{D}oc{M}ath-Eval: Evaluating Math Reasoning Capabilities of {LLM}s in Understanding Long and Specialized Documents\",\n    author = \"Zhao, Yilun  and\n      Long, Yitao  and\n      Liu, Hongjun  and\n      Kamoi, Ryo  and\n      Nan, Linyong  and\n      Chen, Lyuhao  and\n      Liu, Yixin  and\n      Tang, Xiangru  and\n      Zhang, Rui  and\n      Cohan, Arman\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.852\",\n    doi = \"10.18653/v1/2024.acl-long.852\",\n    pages = \"16103--16120\",\n    abstract = \"Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.\",\n}\n",
    "authors": [
        "Yilun Zhao",
        "Yitao Long",
        "Hongjun Liu",
        "Ryo Kamoi",
        "Linyong Nan",
        "Lyuhao Chen",
        "Yixin Liu",
        "Xiangru Tang",
        "Rui Zhang",
        "Arman Cohan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.852.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3278211d-039c-558c-b81a-4ea0025860a8.pdf",
    "abstract": "Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. We conduct an extensive evaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting methods, aiming to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that even the current best-performing system (i.e., GPT-4o) still significantly lags behind human experts in solving complex numerical reasoning problems grounded in long contexts. We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.",
    "num_pages": 13
}