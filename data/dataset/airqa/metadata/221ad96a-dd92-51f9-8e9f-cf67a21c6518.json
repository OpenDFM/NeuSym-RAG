{
    "uuid": "221ad96a-dd92-51f9-8e9f-cf67a21c6518",
    "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-agent,\n    title = \"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization\",\n    author = \"Zhang, Wenqi  and\n      Tang, Ke  and\n      Wu, Hai  and\n      Wang, Mengna  and\n      Shen, Yongliang  and\n      Hou, Guiyang  and\n      Tan, Zeqi  and\n      Li, Peng  and\n      Zhuang, Yueting  and\n      Lu, Weiming\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.292\",\n    doi = \"10.18653/v1/2024.acl-long.292\",\n    pages = \"5348--5375\",\n    abstract = \"Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, {``}fine-tuning{''} its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold{'}em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.\",\n}\n",
    "authors": [
        "Wenqi Zhang",
        "Ke Tang",
        "Hai Wu",
        "Mengna Wang",
        "Yongliang Shen",
        "Guiyang Hou",
        "Zeqi Tan",
        "Peng Li",
        "Yueting Zhuang",
        "Weiming Lu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.292.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/221ad96a-dd92-51f9-8e9f-cf67a21c6518.pdf",
    "abstract": "Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, “fine-tuning” its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.",
    "num_pages": 28
}