{
    "uuid": "8652def9-1a99-5c03-8b67-ca5bb45d3474",
    "title": "Auto-ACE: An Automatic Answer Correctness Evaluation Method for Conversational Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)",
    "bibtex": "@inproceedings{bai-etal-2024-auto,\n    title = \"Auto-{ACE}: An Automatic Answer Correctness Evaluation Method for Conversational Question Answering\",\n    author = \"Bai, Zhixin  and\n      Wang, Bingbing  and\n      Liang, Bin  and\n      Xu, Ruifeng\",\n    editor = \"Wong, Kam-Fai  and\n      Zhang, Min  and\n      Xu, Ruifeng  and\n      Li, Jing  and\n      Wei, Zhongyu  and\n      Gui, Lin  and\n      Liang, Bin  and\n      Zhao, Runcong\",\n    booktitle = \"Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sighan-1.9\",\n    pages = \"80--87\",\n    abstract = \"Conversational question answering aims to respond to questions based on relevant contexts and previous question-answer history. Existing studies typically use ground-truth answers in history, leading to the inconsistency between the training and inference phases. However, in real-world scenarios, progress in question answering can only be made using predicted answers. Since not all predicted answers are correct, indiscriminately using all predicted answers for training introduces noise into the model. To tackle these challenges, we propose an automatic answer correctness evaluation method named **Auto-ACE**. Specifically, we first construct an Att-BERT model which employs attention weight to the BERT model, so as to bridge the relation between the current question and the question-answer pair in history. Furthermore, to reduce the interference of the irrelevant information in the predicted answer, A-Scorer, an answer scorer is designed to evaluate the confidence of the predicted answer. We conduct a series of experiments on QuAC and CoQA datasets, and the results demonstrate the effectiveness and practicality of our proposed Auto-ACE framework.\",\n}\n",
    "authors": [
        "Zhixin Bai",
        "Bingbing Wang",
        "Bin Liang",
        "Ruifeng Xu"
    ],
    "pdf_url": "https://aclanthology.org/2024.sighan-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8652def9-1a99-5c03-8b67-ca5bb45d3474.pdf",
    "abstract": "Conversational question answering aims to respond to questions based on relevant contexts and previous question-answer history. Existing studies typically use ground-truth answers in history, leading to the inconsistency between the training and inference phases. However, in real-world scenarios, progress in question answering can only be made using predicted answers. Since not all predicted answers are correct, indiscriminately using all predicted answers for training introduces noise into the model. To tackle these challenges, we propose an automatic answer correctness evaluation method named **Auto-ACE**. Specifically, we first construct an Att-BERT model which employs attention weight to the BERT model, so as to bridge the relation between the current question and the question-answer pair in history. Furthermore, to reduce the interference of the irrelevant information in the predicted answer, A-Scorer, an answer scorer is designed to evaluate the confidence of the predicted answer. We conduct a series of experiments on QuAC and CoQA datasets, and the results demonstrate the effectiveness and practicality of our proposed Auto-ACE framework.",
    "num_pages": 8
}