{
    "uuid": "f1ada2d6-c834-52bc-8a8c-7c5f723dc00d",
    "title": "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wieting-etal-2023-beyond,\n    title = \"Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval\",\n    author = \"Wieting, John  and\n      Clark, Jonathan  and\n      Cohen, William  and\n      Neubig, Graham  and\n      Berg-Kirkpatrick, Taylor\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.673\",\n    doi = \"10.18653/v1/2023.acl-long.673\",\n    pages = \"12044--12066\",\n    abstract = \"Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval - the last of which we introduce in this paper. Overall, our model outperforms both a strong contrastive and generative baseline on these tasks.\",\n}\n",
    "authors": [
        "John Wieting",
        "Jonathan Clark",
        "William Cohen",
        "Graham Neubig",
        "Taylor Berg-Kirkpatrick"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.673.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f1ada2d6-c834-52bc-8a8c-7c5f723dc00d.pdf",
    "abstract": "Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval - the last of which we introduce in this paper. Overall, our model outperforms both a strong contrastive and generative baseline on these tasks.",
    "num_pages": 23
}