{
    "uuid": "c28893b8-49b5-58ae-8993-07392530176e",
    "title": "Itâ€™s Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{balepur-etal-2024-easy,\n    title = \"It{'}s Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning\",\n    author = \"Balepur, Nishant  and\n      Palta, Shramay  and\n      Rudinger, Rachel\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.604\",\n    doi = \"10.18653/v1/2024.findings-acl.604\",\n    pages = \"10143--10166\",\n    abstract = \"Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.\",\n}\n",
    "authors": [
        "Nishant Balepur",
        "Shramay Palta",
        "Rachel Rudinger"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.604.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c28893b8-49b5-58ae-8993-07392530176e.pdf",
    "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",
    "num_pages": 24
}