{
    "uuid": "1eb129cc-e013-5c84-a731-34f6a3bb4f02",
    "title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lei-etal-2023-unsupervised,\n    title = \"Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training\",\n    author = \"Lei, Yibin  and\n      Ding, Liang  and\n      Cao, Yu  and\n      Zan, Changtong  and\n      Yates, Andrew  and\n      Tao, Dacheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.695\",\n    doi = \"10.18653/v1/2023.findings-acl.695\",\n    pages = \"10932--10940\",\n    abstract = \"Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at \\url{https://github.com/Yibin-Lei/ReContriever}.\",\n}\n",
    "authors": [
        "Yibin Lei",
        "Liang Ding",
        "Yu Cao",
        "Changtong Zan",
        "Andrew Yates",
        "Dacheng Tao"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.695.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1eb129cc-e013-5c84-a731-34f6a3bb4f02.pdf",
    "abstract": "Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at https://github.com/Yibin-Lei/ReContriever.",
    "num_pages": 9
}