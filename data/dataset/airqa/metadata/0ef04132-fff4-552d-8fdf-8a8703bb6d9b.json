{
    "uuid": "0ef04132-fff4-552d-8fdf-8a8703bb6d9b",
    "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wang-etal-2024-chinese,\n    title = \"A {C}hinese Dataset for Evaluating the Safeguards in Large Language Models\",\n    author = \"Wang, Yuxia  and\n      Zhai, Zenan  and\n      Li, Haonan  and\n      Han, Xudong  and\n      Lin, Shom  and\n      Zhang, Zhenxuan  and\n      Zhao, Angela  and\n      Nakov, Preslav  and\n      Baldwin, Timothy\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.184\",\n    doi = \"10.18653/v1/2024.findings-acl.184\",\n    pages = \"3106--3119\",\n    abstract = \"Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks. Previous studies have proposed comprehensive taxonomies of LLM risks, as well as corresponding prompts that can be used to examine LLM safety. However, the focus has been almost exclusively on English. We aim to broaden LLM safety research by introducing a dataset for the safety evaluation of Chinese LLMs, and extending it to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments over five LLMs show that region-specific risks are the prevalent risk type. Warning: this paper contains example data that may be offensive, harmful, or biased. Our data is available at https://github.com/Libr-AI/do-not-answer.\",\n}\n",
    "authors": [
        "Yuxia Wang",
        "Zenan Zhai",
        "Haonan Li",
        "Xudong Han",
        "Shom Lin",
        "Zhenxuan Zhang",
        "Angela Zhao",
        "Preslav Nakov",
        "Timothy Baldwin"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.184.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0ef04132-fff4-552d-8fdf-8a8703bb6d9b.pdf",
    "abstract": "Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks. Previous studies have proposed comprehensive taxonomies of LLM risks, as well as corresponding prompts that can be used to examine LLM safety. However, the focus has been almost exclusively on English. We aim to broaden LLM safety research by introducing a dataset for the safety evaluation of Chinese LLMs, and extending it to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments over five LLMs show that region-specific risks are the prevalent risk type. Warning: this paper contains example data that may be offensive, harmful, or biased. Our data is available at https://github.com/Libr-AI/do-not-answer.",
    "num_pages": 14
}