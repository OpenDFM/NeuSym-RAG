{
    "uuid": "a71ecd57-9b39-5b5d-99b8-b9a2a0914cd2",
    "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chen-etal-2024-agent,\n    title = \"Agent-{FLAN}: Designing Data and Methods of Effective Agent Tuning for Large Language Models\",\n    author = \"Chen, Zehui  and\n      Liu, Kuikun  and\n      Wang, Qiuchen  and\n      Zhang, Wenwei  and\n      Liu, Jiangning  and\n      Lin, Dahua  and\n      Chen, Kai  and\n      Zhao, Feng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.557\",\n    doi = \"10.18653/v1/2024.findings-acl.557\",\n    pages = \"9354--9366\",\n    abstract = \"Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem.This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5{\\%} across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code and models are available at https://github.com/InternLM/Agent-FLAN.\",\n}\n",
    "authors": [
        "Zehui Chen",
        "Kuikun Liu",
        "Qiuchen Wang",
        "Wenwei Zhang",
        "Jiangning Liu",
        "Dahua Lin",
        "Kai Chen",
        "Feng Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.557.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a71ecd57-9b39-5b5d-99b8-b9a2a0914cd2.pdf",
    "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem.This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents.Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code and models are available at https://github.com/InternLM/Agent-FLAN.",
    "num_pages": 13
}