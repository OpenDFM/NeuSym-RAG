{
    "uuid": "fda8d68b-3e4c-5a59-a29f-3241a81a2f20",
    "title": "M3AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2024-m3av,\n    title = \"{M}$^3${AV}: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset\",\n    author = \"Chen, Zhe  and\n      Liu, Heyang  and\n      Yu, Wenyi  and\n      Sun, Guangzhi  and\n      Liu, Hongcheng  and\n      Wu, Ji  and\n      Zhang, Chao  and\n      Wang, Yu  and\n      Wang, Yanfeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.489\",\n    doi = \"10.18653/v1/2024.acl-long.489\",\n    pages = \"9041--9060\",\n    abstract = \"Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M$^3$AV makes it a challenging dataset.\",\n}\n",
    "authors": [
        "Zhe Chen",
        "Heyang Liu",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Hongcheng Liu",
        "Ji Wu",
        "Chao Zhang",
        "Yu Wang",
        "Yanfeng Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.489.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/fda8d68b-3e4c-5a59-a29f-3241a81a2f20.pdf",
    "abstract": "Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M3AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M3AV makes it a challenging dataset.",
    "num_pages": 20
}