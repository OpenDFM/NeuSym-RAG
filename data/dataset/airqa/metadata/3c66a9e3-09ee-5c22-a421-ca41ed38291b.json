{
    "uuid": "3c66a9e3-09ee-5c22-a421-ca41ed38291b",
    "title": "muNERa at WojoodNER 2024: Multi-tasking NER Approach",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{alotaibi-etal-2024-munera,\n    title = \"mu{NER}a at {W}ojood{NER} 2024: Multi-tasking {NER} Approach\",\n    author = \"Alotaibi, Nouf  and\n      Alhomoud, Haneen  and\n      Murayshid, Hanan  and\n      Alshammari, Waad  and\n      Alshalawi, Nouf  and\n      Alkhereyf, Sakhar\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.102\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.102\",\n    pages = \"858--866\",\n    abstract = \"This paper presents our system {``}muNERa{''}, submitted to the WojoodNER 2024 shared task at the second ArabicNLP conference. We participated in two subtasks, the flat and nested fine-grained NER sub-tasks (1 and 2). muNERa achieved first place in the nested NER sub-task and second place in the flat NER sub-task. The system is based on the TANL framework (CITATION),by using a sequence-to-sequence structured language translation approach to model both tasks. We utilize the pre-trained AraT5v2-base model as the base model for the TANL framework. The best-performing muNERa model achieves 91.07{\\%} and 90.26{\\%} for the F-1 scores on the test sets for the nested and flat subtasks, respectively.\",\n}\n",
    "authors": [
        "Nouf Alotaibi",
        "Haneen Alhomoud",
        "Hanan Murayshid",
        "Waad Alshammari",
        "Nouf Alshalawi",
        "Sakhar Alkhereyf"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.102.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3c66a9e3-09ee-5c22-a421-ca41ed38291b.pdf",
    "abstract": "This paper presents our system “muNERa”, submitted to the WojoodNER 2024 shared task at the second ArabicNLP conference. We participated in two subtasks, the flat and nested fine-grained NER sub-tasks (1 and 2). muNERa achieved first place in the nested NER sub-task and second place in the flat NER sub-task. The system is based on the TANL framework (CITATION),by using a sequence-to-sequence structured language translation approach to model both tasks. We utilize the pre-trained AraT5v2-base model as the base model for the TANL framework. The best-performing muNERa model achieves 91.07% and 90.26% for the F-1 scores on the test sets for the nested and flat subtasks, respectively.",
    "num_pages": 9
}