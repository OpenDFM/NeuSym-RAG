{
    "uuid": "aeee3ffd-1c19-5aee-88ca-002df0da832a",
    "title": "A Hyperparameter Optimization Toolkit for Neural Machine Translation Research",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{zhang-etal-2023-hyperparameter,\n    title = \"A Hyperparameter Optimization Toolkit for Neural Machine Translation Research\",\n    author = \"Zhang, Xuan  and\n      Duh, Kevin  and\n      McNamee, Paul\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.15\",\n    doi = \"10.18653/v1/2023.acl-demo.15\",\n    pages = \"161--168\",\n    abstract = \"Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort. Code: \\url{https://github.com/kevinduh/sockeye-recipes3Video} demo: \\url{https://cs.jhu.edu/kevinduh/j/demo.mp4}\",\n}\n",
    "authors": [
        "Xuan Zhang",
        "Kevin Duh",
        "Paul McNamee"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/aeee3ffd-1c19-5aee-88ca-002df0da832a.pdf",
    "abstract": "Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort. Code: https://github.com/kevinduh/sockeye-recipes3Video demo: https://cs.jhu.edu/kevinduh/j/demo.mp4",
    "num_pages": 8
}