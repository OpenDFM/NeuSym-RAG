{
    "uuid": "118fd3d8-e13e-591b-8c50-6a7d48d38cd1",
    "title": "UZH_CLyp at SemEval-2023 Task 9: Head-First Fine-Tuning and ChatGPT Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{michail-etal-2023-uzh,\n    title = \"{UZH}{\\_}{CL}yp at {S}em{E}val-2023 Task 9: Head-First Fine-Tuning and {C}hat{GPT} Data Generation for Cross-Lingual Learning in Tweet Intimacy Prediction\",\n    author = \"Michail, Andrianos  and\n      Konstantinou, Stefanos  and\n      Clematide, Simon\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.140\",\n    doi = \"10.18653/v1/2023.semeval-1.140\",\n    pages = \"1021--1029\",\n    abstract = \"This paper describes the submission of UZH{\\_}CLyp for the SemEval 2023 Task 9 {``}Multilingual Tweet Intimacy Analysis. We achieved second-best results in all 10 languages according to the official Pearson{'}s correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text generation systems to improve zeroshot baseline results. Finally, we examine how possible inconsistencies in the annotated data contribute to cross-lingual interference issues.\",\n}\n",
    "authors": [
        "Andrianos Michail",
        "Stefanos Konstantinou",
        "Simon Clematide"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.140.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/118fd3d8-e13e-591b-8c50-6a7d48d38cd1.pdf",
    "abstract": "This paper describes the submission of UZH_CLyp for the SemEval 2023 Task 9 “Multilingual Tweet Intimacy Analysis. We achieved second-best results in all 10 languages according to the official Pearson’s correlation regression evaluation measure. Our cross-lingual transfer learning approach explores the benefits of using a Head-First Fine-Tuning method (HeFiT) that first updates only the regression head parameters and then also updates the pre-trained transformer encoder parameters at a reduced learning rate. Additionally, we study the impact of using a small set of automatically generated examples (in our case, from ChatGPT) for low-resource settings where no human-labeled data is available. Our study shows that HeFiT stabilizes training and consistently improves results for pre-trained models that lack domain adaptation to tweets. Our study also shows a noticeable performance increase in cross-lingual learning when synthetic data is used, confirming the usefulness of current text generation systems to improve zeroshot baseline results. Finally, we examine how possible inconsistencies in the annotated data contribute to cross-lingual interference issues.",
    "num_pages": 9
}