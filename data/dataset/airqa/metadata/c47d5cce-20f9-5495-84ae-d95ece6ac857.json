{
    "uuid": "c47d5cce-20f9-5495-84ae-d95ece6ac857",
    "title": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2024-easygen,\n    title = \"{E}asy{G}en: Easing Multimodal Generation with {B}i{D}iffuser and {LLM}s\",\n    author = \"Zhao, Xiangyu  and\n      Liu, Bo  and\n      Liu, Qijiong  and\n      Shi, Guangyuan  and\n      Wu, Xiao-Ming\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.74\",\n    doi = \"10.18653/v1/2024.acl-long.74\",\n    pages = \"1351--1370\",\n    abstract = \"We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM{'}s text space with the BiDiffuser{'}s image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.\",\n}\n",
    "authors": [
        "Xiangyu Zhao",
        "Bo Liu",
        "Qijiong Liu",
        "Guangyuan Shi",
        "Xiao-Ming Wu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.74.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c47d5cce-20f9-5495-84ae-d95ece6ac857.pdf",
    "abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM’s text space with the BiDiffuser’s image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.",
    "num_pages": 20
}