{
    "uuid": "1b4bad96-960f-54dc-8430-d060c68ae216",
    "title": "Plug-and-Play Document Modules for Pre-trained Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xiao-etal-2023-plug,\n    title = \"Plug-and-Play Document Modules for Pre-trained Models\",\n    author = \"Xiao, Chaojun  and\n      Zhang, Zhengyan  and\n      Han, Xu  and\n      Chan, Chi-Min  and\n      Lin, Yankai  and\n      Liu, Zhiyuan  and\n      Li, Xiangyang  and\n      Li, Zhonghua  and\n      Cao, Zhao  and\n      Sun, Maosong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.875\",\n    doi = \"10.18653/v1/2023.acl-long.875\",\n    pages = \"15713--15729\",\n    abstract = \"Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally inefficient. To this end, we target to decouple document encoding from downstream tasks, and propose to represent each document as a plug-and-play document module, i.e., a document plugin, for PTMs (PlugD). By inserting document plugins into the backbone PTM for downstream tasks, we can encode a document one time to handle multiple tasks, which is more efficient than conventional encoding-task coupling methods that simultaneously encode documents and input queries using task-specific encoders. Extensive experiments on 8 datasets of 4 typical NLP tasks show that PlugD enables models to encode documents once and for all across different scenarios. Especially, PlugD can save 69{\\%} computational costs while achieving comparable performance to state-of-the-art encoding-task coupling methods. Additionally, we show that PlugD can serve as an effective post-processing way to inject knowledge into task-specific models, improving model performance without any additional model training. Our code and checkpoints can be found in \\url{https://github.com/thunlp/Document-Plugin}.\",\n}\n",
    "authors": [
        "Chaojun Xiao",
        "Zhengyan Zhang",
        "Xu Han",
        "Chi-Min Chan",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Xiangyang Li",
        "Zhonghua Li",
        "Zhao Cao",
        "Maosong Sun"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.875.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1b4bad96-960f-54dc-8430-d060c68ae216.pdf",
    "abstract": "Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally inefficient. To this end, we target to decouple document encoding from downstream tasks, and propose to represent each document as a plug-and-play document module, i.e., a document plugin, for PTMs (PlugD). By inserting document plugins into the backbone PTM for downstream tasks, we can encode a document one time to handle multiple tasks, which is more efficient than conventional encoding-task coupling methods that simultaneously encode documents and input queries using task-specific encoders. Extensive experiments on 8 datasets of 4 typical NLP tasks show that PlugD enables models to encode documents once and for all across different scenarios. Especially, PlugD can save 69% computational costs while achieving comparable performance to state-of-the-art encoding-task coupling methods. Additionally, we show that PlugD can serve as an effective post-processing way to inject knowledge into task-specific models, improving model performance without any additional model training. Our code and checkpoints can be found in https://github.com/thunlp/Document-Plugin.",
    "num_pages": 17
}