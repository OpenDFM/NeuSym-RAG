{
    "uuid": "4540b92e-5657-5866-90bf-77372d98fecb",
    "title": "Open foundation models for Azerbaijani language",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the First Workshop on Natural Language Processing for Turkic Languages (SIGTURK 2024)",
    "bibtex": "@inproceedings{isbarov-etal-2024-open,\n    title = \"Open foundation models for {A}zerbaijani language\",\n    author = \"Isbarov, Jafar  and\n      Huseynova, Kavsar  and\n      Mammadov, Elvin  and\n      Hajili, Mammad  and\n      Ataman, Duygu\",\n    editor = {Ataman, Duygu  and\n      Derin, Mehmet Oguz  and\n      Ivanova, Sardana  and\n      K{\\\"o}ksal, Abdullatif  and\n      S{\\\"a}lev{\\\"a}, Jonne  and\n      Zeyrek, Deniz},\n    booktitle = \"Proceedings of the First Workshop on Natural Language Processing for Turkic Languages (SIGTURK 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand and Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sigturk-1.2\",\n    pages = \"18--28\",\n    abstract = \"The emergence of multilingual large language models has enabled the development of language understanding and generation systems in Azerbaijani. However, most of the production-grade systems rely on cloud solutions, such as GPT-4. While there have been several attempts to develop open foundation models for Azerbaijani, these works have not found their way into common use due to a lack of systemic benchmarking. This paper encompasses several lines of work that promote open-source foundation models for Azerbaijani. We introduce (1) a large text corpus for Azerbaijani, (2) a family of encoder-only language models trained on this dataset, (3) labeled datasets for evaluating these models, and (4) extensive evaluation that covers all major open-source models with Azerbaijani support.\",\n}\n",
    "authors": [
        "Jafar Isbarov",
        "Kavsar Huseynova",
        "Elvin Mammadov",
        "Mammad Hajili",
        "Duygu Ataman"
    ],
    "pdf_url": "https://aclanthology.org/2024.sigturk-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4540b92e-5657-5866-90bf-77372d98fecb.pdf",
    "abstract": "The emergence of multilingual large language models has enabled the development of language understanding and generation systems in Azerbaijani. However, most of the production-grade systems rely on cloud solutions, such as GPT-4. While there have been several attempts to develop open foundation models for Azerbaijani, these works have not found their way into common use due to a lack of systemic benchmarking. This paper encompasses several lines of work that promote open-source foundation models for Azerbaijani. We introduce (1) a large text corpus for Azerbaijani, (2) a family of encoder-only language models trained on this dataset, (3) labeled datasets for evaluating these models, and (4) extensive evaluation that covers all major open-source models with Azerbaijani support.",
    "num_pages": 11
}