{
    "uuid": "4e7bc0ee-4537-5676-95d2-f2573931bb56",
    "title": "SciMind: A Multimodal Mixture-of-Experts Model for Advancing Pharmaceutical Sciences",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)",
    "bibtex": "@inproceedings{xiong-etal-2024-scimind,\n    title = \"{S}ci{M}ind: A Multimodal Mixture-of-Experts Model for Advancing Pharmaceutical Sciences\",\n    author = \"Xiong, Zhaoping  and\n      Fang, Xintao  and\n      Chu, Haotian  and\n      Wan, Xiaozhe  and\n      Liu, Liwei  and\n      Li, Yameng  and\n      Xiang, Wenkai  and\n      Zheng, Mingyue\",\n    editor = \"Edwards, Carl  and\n      Wang, Qingyun  and\n      Li, Manling  and\n      Zhao, Lawrence  and\n      Hope, Tom  and\n      Ji, Heng\",\n    booktitle = \"Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.langmol-1.8\",\n    doi = \"10.18653/v1/2024.langmol-1.8\",\n    pages = \"66--73\",\n    abstract = \"Large language models (LLMs) have made substantial strides, but their use in reliably tackling issues within specialized domains, particularly in interdisciplinary areas like pharmaceutical sciences, is hindered by data heterogeneity, knowledge complexity, unique objectives, and a spectrum of constraint conditions. In this area, diverse modalities such as nucleic acids, proteins, molecular structures, and natural language are often involved. We designed a specialized token set and introduced a new Mixture-of-Experts (MoEs) pre-training and fine-tuning strategy to unify these modalities in one model. With this strategy, we{'}ve created a multi-modal mixture-of-experts foundational model for pharmaceutical sciences, named SciMind. This model has undergone extensive pre-training on publicly accessible datasets including nucleic acid sequences, protein sequences, molecular structure strings, and biomedical texts, and delivers good performance on biomedical text comprehension, promoter prediction, protein function prediction, molecular description, and molecular generation.\",\n}\n",
    "authors": [
        "Zhaoping Xiong",
        "Xintao Fang",
        "Haotian Chu",
        "Xiaozhe Wan",
        "Liwei Liu",
        "Yameng Li",
        "Wenkai Xiang",
        "Mingyue Zheng"
    ],
    "pdf_url": "https://aclanthology.org/2024.langmol-1.8.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4e7bc0ee-4537-5676-95d2-f2573931bb56.pdf",
    "abstract": "Large language models (LLMs) have made substantial strides, but their use in reliably tackling issues within specialized domains, particularly in interdisciplinary areas like pharmaceutical sciences, is hindered by data heterogeneity, knowledge complexity, unique objectives, and a spectrum of constraint conditions. In this area, diverse modalities such as nucleic acids, proteins, molecular structures, and natural language are often involved. We designed a specialized token set and introduced a new Mixture-of-Experts (MoEs) pre-training and fine-tuning strategy to unify these modalities in one model. With this strategy, weâ€™ve created a multi-modal mixture-of-experts foundational model for pharmaceutical sciences, named SciMind. This model has undergone extensive pre-training on publicly accessible datasets including nucleic acid sequences, protein sequences, molecular structure strings, and biomedical texts, and delivers good performance on biomedical text comprehension, promoter prediction, protein function prediction, molecular description, and molecular generation.",
    "num_pages": 8
}