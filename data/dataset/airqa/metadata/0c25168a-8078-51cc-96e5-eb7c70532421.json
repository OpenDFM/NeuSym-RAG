{
    "uuid": "0c25168a-8078-51cc-96e5-eb7c70532421",
    "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{han-etal-2024-xlavs,\n    title = \"{XLAVS}-{R}: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception\",\n    author = \"Han, HyoJung  and\n      Anwar, Mohamed  and\n      Pino, Juan  and\n      Hsu, Wei-Ning  and\n      Carpuat, Marine  and\n      Shi, Bowen  and\n      Wang, Changhan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.697\",\n    doi = \"10.18653/v1/2024.acl-long.697\",\n    pages = \"12896--12911\",\n    abstract = \"Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources.To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5{\\%} WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.\",\n}\n",
    "authors": [
        "HyoJung Han",
        "Mohamed Anwar",
        "Juan Pino",
        "Wei-Ning Hsu",
        "Marine Carpuat",
        "Bowen Shi",
        "Changhan Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.697.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0c25168a-8078-51cc-96e5-eb7c70532421.pdf",
    "abstract": "Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources.To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.",
    "num_pages": 16
}