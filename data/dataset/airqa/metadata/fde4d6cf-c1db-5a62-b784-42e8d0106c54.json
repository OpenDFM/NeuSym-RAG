{
    "uuid": "fde4d6cf-c1db-5a62-b784-42e8d0106c54",
    "title": "Probing neural language models for understanding of words of estimative probability",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{sileo-moens-2023-probing,\n    title = \"Probing neural language models for understanding of words of estimative probability\",\n    author = \"Sileo, Damien  and\n      Moens, Marie-francine\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.41\",\n    doi = \"10.18653/v1/2023.starsem-1.41\",\n    pages = \"469--476\",\n    abstract = \"Words of Estimative Probability (WEP) are phrases used to express the plausibility of a statement. Examples include terms like {\\textbackslash}textit{probably, maybe, likely, doubt, unlikely}, and {\\textbackslash}textit{impossible}. Surveys have shown that human evaluators tend to agree when assigning numerical probability levels to these WEPs. For instance, the term {\\textbackslash}textit{highly likely} equates to a median probability of {\\$}0.90{{\\textbackslash}pm}0.08{\\$} according to a survey by {\\textbackslash}citet{fagen-ulmschneider}.In this study, our focus is to gauge the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP. Our first approach is utilizing the UNLI dataset {\\textbackslash}cite{chen-etal-2020-uncertain}, which links premises and hypotheses with their perceived joint probability {\\$}p{\\$}. From this, we craft prompts in the form: ''[{\\textbackslash}textsc{Premise}]. [{\\textbackslash}textsc{Wep}], [{\\textbackslash}textsc{Hypothesis}].{''} This allows us to evaluate whether language models can predict if the consensual probability level of a WEP aligns closely with {\\$}p{\\$}.In our second approach, we develop a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions. For example, given the prompt ''[{\\textbackslash}textsc{EventA}] {\\textbackslash}textit{is likely}. [{\\textbackslash}textsc{EventB}] {\\textbackslash}textit{is impossible}.{''}, a well-functioning language model should not conclude that [{\\textbackslash}textsc{EventA{\\$}{\\textbackslash}{\\&}amp;{\\$}B}] is likely. Through our study, we observe that both tasks present challenges to out-of-the-box English language models. However, we also demonstrate that fine-tuning these models can lead to significant and transferable improvements.\",\n}\n",
    "authors": [
        "Damien Sileo",
        "Marie-francine Moens"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.41.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fde4d6cf-c1db-5a62-b784-42e8d0106c54.pdf",
    "abstract": "Words of Estimative Probability (WEP) are phrases used to express the plausibility of a statement. Examples include terms like \\textit{probably, maybe, likely, doubt, unlikely}, and \\textit{impossible}. Surveys have shown that human evaluators tend to agree when assigning numerical probability levels to these WEPs. For instance, the term \\textit{highly likely} equates to a median probability of $0.90{\\pm}0.08$ according to a survey by \\citet{fagen-ulmschneider}.In this study, our focus is to gauge the competency of neural language processing models in accurately capturing the consensual probability level associated with each WEP. Our first approach is utilizing the UNLI dataset \\cite{chen-etal-2020-uncertain}, which links premises and hypotheses with their perceived joint probability $p$. From this, we craft prompts in the form: \"[\\textsc{Premise}]. [\\textsc{Wep}], [\\textsc{Hypothesis}].” This allows us to evaluate whether language models can predict if the consensual probability level of a WEP aligns closely with $p$.In our second approach, we develop a dataset based on WEP-focused probabilistic reasoning to assess if language models can logically process WEP compositions. For example, given the prompt \"[\\textsc{EventA}] \\textit{is likely}. [\\textsc{EventB}] \\textit{is impossible}.”, a well-functioning language model should not conclude that [\\textsc{EventA$\\&amp;$B}] is likely. Through our study, we observe that both tasks present challenges to out-of-the-box English language models. However, we also demonstrate that fine-tuning these models can lead to significant and transferable improvements.",
    "num_pages": 8
}