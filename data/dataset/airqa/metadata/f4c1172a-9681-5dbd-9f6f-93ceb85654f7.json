{
    "uuid": "f4c1172a-9681-5dbd-9f6f-93ceb85654f7",
    "title": "Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{futeral-etal-2023-tackling,\n    title = \"Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation\",\n    author = \"Futeral, Matthieu  and\n      Schmid, Cordelia  and\n      Laptev, Ivan  and\n      Sagot, Beno{\\^\\i}t  and\n      Bawden, Rachel\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.295\",\n    doi = \"10.18653/v1/2023.acl-long.295\",\n    pages = \"5394--5413\",\n    abstract = \"One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard EnglishâFrench, EnglishâGerman and EnglishâCzech benchmarks and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set. Our code and CoMMuTE are freely available.\",\n}\n",
    "authors": [
        "Matthieu Futeral",
        "Cordelia Schmid",
        "Ivan Laptev",
        "Benoît Sagot",
        "Rachel Bawden"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.295.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f4c1172a-9681-5dbd-9f6f-93ceb85654f7.pdf",
    "abstract": "One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard English→French, English→German and English→Czech benchmarks and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set. Our code and CoMMuTE are freely available.",
    "num_pages": 20
}