{
    "uuid": "9fd0d127-aadd-5059-8d25-16a2a831e3eb",
    "title": "ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{tan-etal-2024-proxyqa,\n    title = \"{P}roxy{QA}: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models\",\n    author = \"Tan, Haochen  and\n      Guo, Zhijiang  and\n      Shi, Zhan  and\n      Xu, Lu  and\n      Liu, Zhili  and\n      Feng, Yunlong  and\n      Li, Xiaoguang  and\n      Wang, Yasheng  and\n      Shang, Lifeng  and\n      Liu, Qun  and\n      Song, Linqi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.368\",\n    doi = \"10.18653/v1/2024.acl-long.368\",\n    pages = \"6806--6827\",\n    abstract = \"Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content{'}s quality through the evaluator{'}s accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA{'}s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at \\url{https://proxy-qa.com}.\",\n}\n",
    "authors": [
        "Haochen Tan",
        "Zhijiang Guo",
        "Zhan Shi",
        "Lu Xu",
        "Zhili Liu",
        "Yunlong Feng",
        "Xiaoguang Li",
        "Yasheng Wang",
        "Lifeng Shang",
        "Qun Liu",
        "Linqi Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.368.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9fd0d127-aadd-5059-8d25-16a2a831e3eb.pdf",
    "abstract": "Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content’s quality through the evaluator’s accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA’s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at https://proxy-qa.com.",
    "num_pages": 22
}