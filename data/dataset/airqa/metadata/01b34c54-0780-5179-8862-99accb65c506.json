{
    "uuid": "01b34c54-0780-5179-8862-99accb65c506",
    "title": "Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{du-etal-2024-bi,\n    title = \"Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model\",\n    author = \"Du, Haowei  and\n      Li, Chen  and\n      Zhang, Dinghao  and\n      Zhao, Dongyan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.14\",\n    doi = \"10.18653/v1/2024.acl-short.14\",\n    pages = \"147--152\",\n    abstract = \"The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. Existing methods generate the whole target text based on all KG triples at once and may incorporate incorrect KG triples for each sentence. To this end, we propose the bi-directional multi-granularity generation framework. Instead of generating the whole text at a time, we construct the sentence level generation based on the corresponding triples and generate the graph-level text as a result. Moreover, we design a backward relation extraction task to enhance the correctness of relational information. Our method achieves the new state-of-the-art in benchmark dataset WebNLG and further analysis shows the efficiency of different modules.\",\n}\n",
    "authors": [
        "Haowei Du",
        "Chen Li",
        "Dinghao Zhang",
        "Dongyan Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.14.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/01b34c54-0780-5179-8862-99accb65c506.pdf",
    "abstract": "The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. Existing methods generate the whole target text based on all KG triples at once and may incorporate incorrect KG triples for each sentence. To this end, we propose the bi-directional multi-granularity generation framework. Instead of generating the whole text at a time, we construct the sentence level generation based on the corresponding triples and generate the graph-level text as a result. Moreover, we design a backward relation extraction task to enhance the correctness of relational information. Our method achieves the new state-of-the-art in benchmark dataset WebNLG and further analysis shows the efficiency of different modules.",
    "num_pages": 6
}