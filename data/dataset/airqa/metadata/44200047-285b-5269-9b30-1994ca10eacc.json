{
    "uuid": "44200047-285b-5269-9b30-1994ca10eacc",
    "title": "CFSum Coarse-to-Fine Contribution Network for Multimodal Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xiao-etal-2023-cfsum,\n    title = \"{CFS}um Coarse-to-Fine Contribution Network for Multimodal Summarization\",\n    author = \"Xiao, Min  and\n      Zhu, Junnan  and\n      Lin, Haitao  and\n      Zhou, Yu  and\n      Zong, Chengqing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.476\",\n    doi = \"10.18653/v1/2023.acl-long.476\",\n    pages = \"8538--8553\",\n    abstract = \"Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies that useful images can even help generate non-visual words which are implicitly represented in the image.\",\n}\n",
    "authors": [
        "Min Xiao",
        "Junnan Zhu",
        "Haitao Lin",
        "Yu Zhou",
        "Chengqing Zong"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.476.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/44200047-285b-5269-9b30-1994ca10eacc.pdf",
    "abstract": "Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies that useful images can even help generate non-visual words which are implicitly represented in the image.",
    "num_pages": 16
}