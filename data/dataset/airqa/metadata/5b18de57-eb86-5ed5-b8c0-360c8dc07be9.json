{
    "uuid": "5b18de57-eb86-5ed5-b8c0-360c8dc07be9",
    "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    "bibtex": "@inproceedings{van-veen-etal-2023-radadapt,\n    title = \"{R}ad{A}dapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models\",\n    author = \"Van Veen, Dave  and\n      Van Uden, Cara  and\n      Attias, Maayane  and\n      Pareek, Anuj  and\n      Bluethgen, Christian  and\n      Polacin, Malgorzata  and\n      Chiu, Wah  and\n      Delbrouck, Jean-Benoit  and\n      Zambrano Chaves, Juan  and\n      Langlotz, Curtis  and\n      Chaudhari, Akshay  and\n      Pauly, John\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.42\",\n    doi = \"10.18653/v1/2023.bionlp-1.42\",\n    pages = \"449--460\",\n    abstract = \"We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32{\\%} of parameters throughout the model, in contrast to end-to-end fine-tuning (100{\\%} of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.\",\n}\n",
    "authors": [
        "Dave Van Veen",
        "Cara Van Uden",
        "Maayane Attias",
        "Anuj Pareek",
        "Christian Bluethgen",
        "Malgorzata Polacin",
        "Wah Chiu",
        "Jean-Benoit Delbrouck",
        "Juan Zambrano Chaves",
        "Curtis Langlotz",
        "Akshay Chaudhari",
        "John Pauly"
    ],
    "pdf_url": "https://aclanthology.org/2023.bionlp-1.42.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5b18de57-eb86-5ed5-b8c0-360c8dc07be9.pdf",
    "abstract": "We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.",
    "num_pages": 12
}