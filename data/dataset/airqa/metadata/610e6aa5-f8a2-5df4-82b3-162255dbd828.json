{
    "uuid": "610e6aa5-f8a2-5df4-82b3-162255dbd828",
    "title": "Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{sharma-etal-2024-speech,\n    title = \"Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?\",\n    author = \"Sharma, Roshan  and\n      Shon, Suwon  and\n      Lindsey, Mark  and\n      Dhamyal, Hira  and\n      Raj, Bhiksha\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.790\",\n    doi = \"10.18653/v1/2024.acl-long.790\",\n    pages = \"14779--14797\",\n    abstract = \"Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area.\",\n}\n",
    "authors": [
        "Roshan Sharma",
        "Suwon Shon",
        "Mark Lindsey",
        "Hira Dhamyal",
        "Bhiksha Raj"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.790.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/610e6aa5-f8a2-5df4-82b3-162255dbd828.pdf",
    "abstract": "Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area.",
    "num_pages": 19
}