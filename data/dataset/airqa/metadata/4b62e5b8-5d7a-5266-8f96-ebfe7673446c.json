{
    "uuid": "4b62e5b8-5d7a-5266-8f96-ebfe7673446c",
    "title": "From Tarzan to Tolkien: Controlling the Language Proficiency Level of LLMs for Content Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{malik-etal-2024-tarzan,\n    title = \"From Tarzan to {T}olkien: Controlling the Language Proficiency Level of {LLM}s for Content Generation\",\n    author = \"Malik, Ali  and\n      Mayhew, Stephen  and\n      Piech, Christopher  and\n      Bicknell, Klinton\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.926\",\n    doi = \"10.18653/v1/2024.findings-acl.926\",\n    pages = \"15670--15693\",\n    abstract = \"We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.\",\n}\n",
    "authors": [
        "Ali Malik",
        "Stephen Mayhew",
        "Christopher Piech",
        "Klinton Bicknell"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.926.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4b62e5b8-5d7a-5266-8f96-ebfe7673446c.pdf",
    "abstract": "We study the problem of controlling the difficulty level of text generated by Large Language Models (LLMs) for contexts where end-users are not fully proficient, such as language learners. Using a novel framework, we evaluate the effectiveness of several key approaches for this task, including few-shot prompting, supervised finetuning, and reinforcement learning (RL), utilising both GPT-4 and open source alternatives like LLama2-7B and Mistral-7B.Our findings reveal a large performance gap between GPT-4 and the open source models when using prompt-based strategies. However, we show how to bridge this gap with a careful combination of finetuning and RL alignment. Our best model, CALM (CEFR-Aligned Language Model), surpasses the performance of GPT-4 and other strategies, at only a fraction of the cost. We further validate the quality of our results through a small-scale human study.",
    "num_pages": 24
}