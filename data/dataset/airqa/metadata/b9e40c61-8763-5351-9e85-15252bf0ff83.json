{
    "uuid": "b9e40c61-8763-5351-9e85-15252bf0ff83",
    "title": "Exploring the Impact of Transliteration on NLP Performance: Treating Maltese as an Arabic Dialect",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Computation and Written Language (CAWL 2023)",
    "bibtex": "@inproceedings{micallef-etal-2023-exploring,\n    title = \"Exploring the Impact of Transliteration on {NLP} Performance: Treating {M}altese as an {A}rabic Dialect\",\n    author = \"Micallef, Kurt  and\n      Eryani, Fadhl  and\n      Habash, Nizar  and\n      Bouamor, Houda  and\n      Borg, Claudia\",\n    editor = \"Gorman, Kyle  and\n      Sproat, Richard  and\n      Roark, Brian\",\n    booktitle = \"Proceedings of the Workshop on Computation and Written Language (CAWL 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.cawl-1.4\",\n    doi = \"10.18653/v1/2023.cawl-1.4\",\n    pages = \"22--32\",\n    abstract = \"Multilingual models such as mBERT have been demonstrated to exhibit impressive crosslingual transfer for a number of languages. Despite this, the performance drops for lowerresourced languages, especially when they are not part of the pre-training setup and when there are script differences. In this work we consider Maltese, a low-resource language of Arabic and Romance origins written in Latin script. Specifically, we investigate the impact of transliterating Maltese into Arabic scipt on a number of downstream tasks: Part-of-Speech Tagging, Dependency Parsing, and Sentiment Analysis. We compare multiple transliteration pipelines ranging from deterministic character maps to more sophisticated alternatives, including manually annotated word mappings and non-deterministic character mappings. For the latter, we show that selection techniques using n-gram language models of Tunisian Arabic, the dialect with the highest degree of mutual intelligibility to Maltese, yield better results on downstream tasks. Moreover, our experiments highlight that the use of an Arabic pre-trained model paired with transliteration outperforms mBERT. Overall, our results show that transliterating Maltese can be considered an option to improve the cross-lingual transfer capabilities.\",\n}\n",
    "authors": [
        "Kurt Micallef",
        "Fadhl Eryani",
        "Nizar Habash",
        "Houda Bouamor",
        "Claudia Borg"
    ],
    "pdf_url": "https://aclanthology.org/2023.cawl-1.4.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b9e40c61-8763-5351-9e85-15252bf0ff83.pdf",
    "abstract": "Multilingual models such as mBERT have been demonstrated to exhibit impressive crosslingual transfer for a number of languages. Despite this, the performance drops for lowerresourced languages, especially when they are not part of the pre-training setup and when there are script differences. In this work we consider Maltese, a low-resource language of Arabic and Romance origins written in Latin script. Specifically, we investigate the impact of transliterating Maltese into Arabic scipt on a number of downstream tasks: Part-of-Speech Tagging, Dependency Parsing, and Sentiment Analysis. We compare multiple transliteration pipelines ranging from deterministic character maps to more sophisticated alternatives, including manually annotated word mappings and non-deterministic character mappings. For the latter, we show that selection techniques using n-gram language models of Tunisian Arabic, the dialect with the highest degree of mutual intelligibility to Maltese, yield better results on downstream tasks. Moreover, our experiments highlight that the use of an Arabic pre-trained model paired with transliteration outperforms mBERT. Overall, our results show that transliterating Maltese can be considered an option to improve the cross-lingual transfer capabilities.",
    "num_pages": 11
}