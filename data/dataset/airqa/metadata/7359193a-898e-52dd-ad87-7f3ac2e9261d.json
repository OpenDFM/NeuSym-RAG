{
    "uuid": "7359193a-898e-52dd-ad87-7f3ac2e9261d",
    "title": "Amanda: Adaptively Modality-Balanced Domain Adaptation for Multimodal Emotion Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-amanda,\n    title = \"Amanda: Adaptively Modality-Balanced Domain Adaptation for Multimodal Emotion Recognition\",\n    author = \"Zhang, Xinxin  and\n      Sun, Jun  and\n      Hong, Simin  and\n      Li, Taihao\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.859\",\n    doi = \"10.18653/v1/2024.findings-acl.859\",\n    pages = \"14448--14458\",\n    abstract = \"This paper investigates unsupervised multimodal domain adaptation for multimodal emotion recognition, which is a solution for data scarcity yet remains under studied. Due to the varying distribution discrepancies of different modalities between source and target domains, the primary challenge lies in how to balance the domain alignment across modalities to guarantee they are all well aligned. To achieve this, we first develop our model based on the information bottleneck theory to learn optimal representation for each modality independently. Then, we align the domains via matching the label distributions and the representations. In order to balance the representation alignment, we propose to minimize a surrogate of the alignment losses, which is equivalent to adaptively adjusting the weights of the modalities throughout training, thus achieving balanced domain alignment across modalities. Overall, the proposed approach features \\textbf{A}daptively \\textbf{m}odality-bal\\textbf{an}ced \\textbf{d}omain \\textbf{a}daptation, dubbed \\textbf{Amanda}, for multimodal emotion recognition. Extensive empirical results on commonly used benchmark datasets demonstrate that Amanda significantly outperforms competing approaches. The code is available at \\url{https://github.com/sunjunaimer/Amanda}.\",\n}\n",
    "authors": [
        "Xinxin Zhang",
        "Jun Sun",
        "Simin Hong",
        "Taihao Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.859.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7359193a-898e-52dd-ad87-7f3ac2e9261d.pdf",
    "abstract": "This paper investigates unsupervised multimodal domain adaptation for multimodal emotion recognition, which is a solution for data scarcity yet remains under studied. Due to the varying distribution discrepancies of different modalities between source and target domains, the primary challenge lies in how to balance the domain alignment across modalities to guarantee they are all well aligned. To achieve this, we first develop our model based on the information bottleneck theory to learn optimal representation for each modality independently. Then, we align the domains via matching the label distributions and the representations. In order to balance the representation alignment, we propose to minimize a surrogate of the alignment losses, which is equivalent to adaptively adjusting the weights of the modalities throughout training, thus achieving balanced domain alignment across modalities. Overall, the proposed approach features Adaptively modality-balanced domain adaptation, dubbed Amanda, for multimodal emotion recognition. Extensive empirical results on commonly used benchmark datasets demonstrate that Amanda significantly outperforms competing approaches. The code is available at https://github.com/sunjunaimer/Amanda.",
    "num_pages": 11
}