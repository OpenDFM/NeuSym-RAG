{
    "uuid": "0333f853-0ef3-5a0b-9f42-cd2181011357",
    "title": "SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{kong-etal-2024-swapmoe,\n    title = \"{S}wap{M}o{E}: Serving Off-the-shelf {M}o{E}-based Large Language Models with Tunable Memory Budget\",\n    author = \"Kong, Rui  and\n      Li, Yuanchun  and\n      Feng, Qingtian  and\n      Wang, Weijun  and\n      Ye, Xiaozhou  and\n      Ouyang, Ye  and\n      Kong, Linghe  and\n      Liu, Yunxin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.363\",\n    doi = \"10.18653/v1/2024.acl-long.363\",\n    pages = \"6710--6720\",\n    abstract = \"Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss.In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50{\\%} latency reduction and a slight Rouge-2 score drop of 0.041.\",\n}\n",
    "authors": [
        "Rui Kong",
        "Yuanchun Li",
        "Qingtian Feng",
        "Weijun Wang",
        "Xiaozhou Ye",
        "Ye Ouyang",
        "Linghe Kong",
        "Yunxin Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.363.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0333f853-0ef3-5a0b-9f42-cd2181011357.pdf",
    "abstract": "Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss.In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50% latency reduction and a slight Rouge-2 score drop of 0.041.",
    "num_pages": 11
}