{
    "uuid": "b4c479ac-a3e8-563d-8ff8-aa03a7b5d7e4",
    "title": "Exploiting Target Language Data for Neural Machine Translation Beyond Back Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{reheman-etal-2024-exploiting,\n    title = \"Exploiting Target Language Data for Neural Machine Translation Beyond Back Translation\",\n    author = \"Reheman, Abudurexiti  and\n      Luo, Yingfeng  and\n      Ruan, Junhao  and\n      Zhang, Chunliang  and\n      Ma, Anxiang  and\n      Xiao, Tong  and\n      Zhu, JingBo\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.727\",\n    doi = \"10.18653/v1/2024.findings-acl.727\",\n    pages = \"12216--12228\",\n    abstract = \"Neural Machine Translation (NMT) encounters challenges when translating in new domains and low-resource languages. To address these issues, researchers have proposed methods to integrate additional knowledge into NMT, such as translation memories (TMs). However, finding TMs that closely match the input sentence remains challenging, particularly in specific domains. On the other hand, monolingual data is widely accessible in most languages, and back-translation is seen as a promising approach for utilizing target language data. Nevertheless, it still necessitates additional training. In this paper, we introduce Pseudo-$k$NN-MT, a variant of $k$-nearest neighbor machine translation ($k$NN-MT) that utilizes target language data by constructing a pseudo datastore. Furthermore, we investigate the utility of large language models (LLMs) for the $k$NN component. Experimental results demonstrate that our approach exhibits strong domain adaptation capability in both high-resource and low-resource machine translation. Notably, LLMs are found to be beneficial for robust NMT systems.\",\n}\n",
    "authors": [
        "Abudurexiti Reheman",
        "Yingfeng Luo",
        "Junhao Ruan",
        "Chunliang Zhang",
        "Anxiang Ma",
        "Tong Xiao",
        "JingBo Zhu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.727.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b4c479ac-a3e8-563d-8ff8-aa03a7b5d7e4.pdf",
    "abstract": "Neural Machine Translation (NMT) encounters challenges when translating in new domains and low-resource languages. To address these issues, researchers have proposed methods to integrate additional knowledge into NMT, such as translation memories (TMs). However, finding TMs that closely match the input sentence remains challenging, particularly in specific domains. On the other hand, monolingual data is widely accessible in most languages, and back-translation is seen as a promising approach for utilizing target language data. Nevertheless, it still necessitates additional training. In this paper, we introduce Pseudo-kNN-MT, a variant of k-nearest neighbor machine translation (kNN-MT) that utilizes target language data by constructing a pseudo datastore. Furthermore, we investigate the utility of large language models (LLMs) for the kNN component. Experimental results demonstrate that our approach exhibits strong domain adaptation capability in both high-resource and low-resource machine translation. Notably, LLMs are found to be beneficial for robust NMT systems.",
    "num_pages": 13
}