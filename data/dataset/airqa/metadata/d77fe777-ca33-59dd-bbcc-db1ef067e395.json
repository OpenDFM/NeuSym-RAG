{
    "uuid": "d77fe777-ca33-59dd-bbcc-db1ef067e395",
    "title": "Dolomites@#SMM4H 2024: Helping LLMs “Know The Drill” in Low-Resource Settings - A Study on Social Media Posts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks",
    "bibtex": "@inproceedings{tortoreto-mousavi-2024-dolomites,\n    title = \"Dolomites@{\\#}{SMM}4{H} 2024: Helping {LLM}s {``}Know The Drill{''} in Low-Resource Settings - A Study on Social Media Posts\",\n    author = \"Tortoreto, Giuliano  and\n      Mousavi, Seyed Mahed\",\n    editor = \"Xu, Dongfang  and\n      Gonzalez-Hernandez, Graciela\",\n    booktitle = \"Proceedings of The 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.smm4h-1.5\",\n    pages = \"17--22\",\n    abstract = \"The amount of data to fine-tune LLMs plays a crucial role in the performance of these models in downstream tasks. Consequently, it is not straightforward to deploy these models in low-resource settings. In this work, we investigate two new multi-task learning data augmentation approaches for fine-tuning LLMs when little data is available: {``}In-domain Augmentation{''} of the training data and extracting {``}Drills{''} as smaller tasks from the target dataset. We evaluate the proposed approaches in three natural language processing settings in the context of SMM4H 2024 competition tasks: multi-class classification, entity recognition, and information extraction. The results show that both techniques improve the performance of the models in all three settings, suggesting a positive impact from the knowledge learned in multi-task training to perform the target task.\",\n}\n",
    "authors": [
        "Giuliano Tortoreto",
        "Seyed Mahed Mousavi"
    ],
    "pdf_url": "https://aclanthology.org/2024.smm4h-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d77fe777-ca33-59dd-bbcc-db1ef067e395.pdf",
    "abstract": "The amount of data to fine-tune LLMs plays a crucial role in the performance of these models in downstream tasks. Consequently, it is not straightforward to deploy these models in low-resource settings. In this work, we investigate two new multi-task learning data augmentation approaches for fine-tuning LLMs when little data is available: “In-domain Augmentation” of the training data and extracting “Drills” as smaller tasks from the target dataset. We evaluate the proposed approaches in three natural language processing settings in the context of SMM4H 2024 competition tasks: multi-class classification, entity recognition, and information extraction. The results show that both techniques improve the performance of the models in all three settings, suggesting a positive impact from the knowledge learned in multi-task training to perform the target task.",
    "num_pages": 6
}