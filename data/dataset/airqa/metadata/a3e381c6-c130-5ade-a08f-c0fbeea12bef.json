{
    "uuid": "a3e381c6-c130-5ade-a08f-c0fbeea12bef",
    "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lu-etal-2023-makes,\n    title = \"What Makes Pre-trained Language Models Better Zero-shot Learners?\",\n    author = \"Lu, Jinghui  and\n      Zhu, Dongsheng  and\n      Han, Weidong  and\n      Zhao, Rui  and\n      Mac Namee, Brian  and\n      Tan, Fei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.128\",\n    doi = \"10.18653/v1/2023.acl-long.128\",\n    pages = \"2288--2303\",\n    abstract = \"Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.\",\n}\n",
    "authors": [
        "Jinghui Lu",
        "Dongsheng Zhu",
        "Weidong Han",
        "Rui Zhao",
        "Brian Mac Namee",
        "Fei Tan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.128.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a3e381c6-c130-5ade-a08f-c0fbeea12bef.pdf",
    "abstract": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.",
    "num_pages": 16
}