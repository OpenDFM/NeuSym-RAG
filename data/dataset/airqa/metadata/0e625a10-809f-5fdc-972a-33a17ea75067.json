{
    "uuid": "0e625a10-809f-5fdc-972a-33a17ea75067",
    "title": "Tokenisation in Machine Translation Does Matter: The impact of different tokenisation approaches for Maltese",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)",
    "bibtex": "@inproceedings{abela-etal-2024-tokenisation,\n    title = \"Tokenisation in Machine Translation Does Matter: The impact of different tokenisation approaches for {M}altese\",\n    author = \"Abela, Kurt  and\n      Micallef, Kurt  and\n      Tanti, Marc  and\n      Borg, Claudia\",\n    editor = \"Ojha, Atul Kr.  and\n      Liu, Chao-hong  and\n      Vylomova, Ekaterina  and\n      Pirinen, Flammie  and\n      Abbott, Jade  and\n      Washington, Jonathan  and\n      Oco, Nathaniel  and\n      Malykh, Valentin  and\n      Logacheva, Varvara  and\n      Zhao, Xiaobing\",\n    booktitle = \"Proceedings of the Seventh Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.loresmt-1.11\",\n    doi = \"10.18653/v1/2024.loresmt-1.11\",\n    pages = \"109--120\",\n    abstract = \"In Machine Translation, various tokenisers are used to segment inputs before training a model. Despite tokenisation being mostly considered a solved problem for languages such as English, it is still unclear as to how effective different tokenisers are for morphologically rich languages. This study aims to explore how different approaches to tokenising Maltese impact machine translation results on the English-Maltese language pair.We observed that the OPUS-100 dataset has tokenisation inconsistencies in Maltese. We empirically found that training models on the original OPUS-100 dataset led to the generation of sentences with these issues.We therefore release an updated version of the OPUS-100 parallel English-Maltese dataset, referred to as OPUS-100-Fix, fixing these inconsistencies in Maltese by using the MLRS tokeniser. We show that after fixing the inconsistencies in the dataset, results on the fixed test set increase by 2.49 BLEU points over models trained on the original OPUS-100. We also experiment with different tokenisers, including BPE and SentencePiece to find the ideal tokeniser and vocabulary size for our setup, which was shown to be BPE with a vocabulary size of 8,000. Finally, we train different models in both directions for the ENG-MLT language pair using OPUS-100-Fix by training models from scratch as well as fine-tuning other pre-trained models, namely mBART-50 and NLLB, where a finetuned NLLB model performed the best.\",\n}\n",
    "authors": [
        "Kurt Abela",
        "Kurt Micallef",
        "Marc Tanti",
        "Claudia Borg"
    ],
    "pdf_url": "https://aclanthology.org/2024.loresmt-1.11.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0e625a10-809f-5fdc-972a-33a17ea75067.pdf",
    "abstract": "In Machine Translation, various tokenisers are used to segment inputs before training a model. Despite tokenisation being mostly considered a solved problem for languages such as English, it is still unclear as to how effective different tokenisers are for morphologically rich languages. This study aims to explore how different approaches to tokenising Maltese impact machine translation results on the English-Maltese language pair.We observed that the OPUS-100 dataset has tokenisation inconsistencies in Maltese. We empirically found that training models on the original OPUS-100 dataset led to the generation of sentences with these issues.We therefore release an updated version of the OPUS-100 parallel English-Maltese dataset, referred to as OPUS-100-Fix, fixing these inconsistencies in Maltese by using the MLRS tokeniser. We show that after fixing the inconsistencies in the dataset, results on the fixed test set increase by 2.49 BLEU points over models trained on the original OPUS-100. We also experiment with different tokenisers, including BPE and SentencePiece to find the ideal tokeniser and vocabulary size for our setup, which was shown to be BPE with a vocabulary size of 8,000. Finally, we train different models in both directions for the ENG-MLT language pair using OPUS-100-Fix by training models from scratch as well as fine-tuning other pre-trained models, namely mBART-50 and NLLB, where a finetuned NLLB model performed the best.",
    "num_pages": 12
}