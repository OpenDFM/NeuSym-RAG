{
    "uuid": "b24b2931-007c-5acd-961c-af0de304d1d5",
    "title": "CTC-based Non-autoregressive Speech Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xu-etal-2023-ctc,\n    title = \"{CTC}-based Non-autoregressive Speech Translation\",\n    author = \"Xu, Chen  and\n      Liu, Xiaoqian  and\n      Liu, Xiaowen  and\n      Sun, Qingxuan  and\n      Zhang, Yuhao  and\n      Yang, Murun  and\n      Dong, Qianqian  and\n      Ko, Tom  and\n      Wang, Mingxuan  and\n      Xiao, Tong  and\n      Ma, Anxiang  and\n      Zhu, Jingbo\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.744\",\n    doi = \"10.18653/v1/2023.acl-long.744\",\n    pages = \"13321--13339\",\n    abstract = \"Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for non-autoregressive speech translation (NAST).In particular, we develop a model consisting of two encoders that are guided by CTC to predict the source and target texts, respectively. Introducing CTC into NAST on both language sides has obvious challenges: 1) the conditional independent generation somewhat breaks the interdependency among tokens, and 2) the monotonic alignment assumption in standard CTC does not hold in translation tasks. In response, we develop a prediction-aware encoding approach and a cross-layer attention approach to address these issues. We also use curriculum learning to improve convergence of training. Experiments on the MuST-C ST benchmarks show that our NAST model achieves an average BLEU score of 29.5 with a speed-up of 5.67$\\times$, which is comparable to the autoregressive counterpart and even outperforms the previous best result of 0.9 BLEU points.\",\n}\n",
    "authors": [
        "Chen Xu",
        "Xiaoqian Liu",
        "Xiaowen Liu",
        "Qingxuan Sun",
        "Yuhao Zhang",
        "Murun Yang",
        "Qianqian Dong",
        "Tom Ko",
        "Mingxuan Wang",
        "Tong Xiao",
        "Anxiang Ma",
        "Jingbo Zhu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.744.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b24b2931-007c-5acd-961c-af0de304d1d5.pdf",
    "abstract": "Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for non-autoregressive speech translation (NAST).In particular, we develop a model consisting of two encoders that are guided by CTC to predict the source and target texts, respectively. Introducing CTC into NAST on both language sides has obvious challenges: 1) the conditional independent generation somewhat breaks the interdependency among tokens, and 2) the monotonic alignment assumption in standard CTC does not hold in translation tasks. In response, we develop a prediction-aware encoding approach and a cross-layer attention approach to address these issues. We also use curriculum learning to improve convergence of training. Experiments on the MuST-C ST benchmarks show that our NAST model achieves an average BLEU score of 29.5 with a speed-up of 5.67Ã—, which is comparable to the autoregressive counterpart and even outperforms the previous best result of 0.9 BLEU points.",
    "num_pages": 19
}