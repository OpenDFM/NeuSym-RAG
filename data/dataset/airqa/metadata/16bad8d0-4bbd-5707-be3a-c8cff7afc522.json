{
    "uuid": "16bad8d0-4bbd-5707-be3a-c8cff7afc522",
    "title": "DPDLLM: A Black-box Framework for Detecting Pre-training Data from Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhou-etal-2024-dpdllm,\n    title = \"{DPDLLM}: A Black-box Framework for Detecting Pre-training Data from Large Language Models\",\n    author = \"Zhou, Baohang  and\n      Wang, Zezhong  and\n      Wang, Lingzhi  and\n      Wang, Hongru  and\n      Zhang, Ying  and\n      Song, Kehui  and\n      Sui, Xuhui  and\n      Wong, Kam-Fai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.35\",\n    doi = \"10.18653/v1/2024.findings-acl.35\",\n    pages = \"644--653\",\n    abstract = \"The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the textual data for training LLM can not be confirmed to be legal because they are crawled from different web sites. For example, there are copyrighted articles, personal reviews and information in the pre-training data for LLM which are illegal. To address the above issue and develop legal LLM, we propose to detect the pre-training data from LLM in a pure black-box way because the existing LLM services only return the generated text. The previous most related works are the membership inference attack (MIA) on machine learning models to detect the training data from them. But the existing methods are based on analyzing the output probabilities of models which are unrealistic to LLM services. To tackle the problem, we firstly construct the benchmark datasets by collecting textual data from different domains as the seen and unseen pre-training data for LLMs. Then, we investigate a black-box framework named DPDLLM, with the only access to the generated texts from LLM for detecting textual data whether was used to train it. In the proposed framework, we exploit GPT-2 as the reference model to fit the textual data and feed the generated text from LLM into it to acquire sequence probabilities as the significant feature for detection. The experimental results on the benchmark datasets demonstrate that DPDLLM is effective on different popular LLMs and outperforms the existing methods.\",\n}\n",
    "authors": [
        "Baohang Zhou",
        "Zezhong Wang",
        "Lingzhi Wang",
        "Hongru Wang",
        "Ying Zhang",
        "Kehui Song",
        "Xuhui Sui",
        "Kam-Fai Wong"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.35.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/16bad8d0-4bbd-5707-be3a-c8cff7afc522.pdf",
    "abstract": "The success of large language models (LLM) benefits from large-scale model parameters and large amounts of pre-training data. However, the textual data for training LLM can not be confirmed to be legal because they are crawled from different web sites. For example, there are copyrighted articles, personal reviews and information in the pre-training data for LLM which are illegal. To address the above issue and develop legal LLM, we propose to detect the pre-training data from LLM in a pure black-box way because the existing LLM services only return the generated text. The previous most related works are the membership inference attack (MIA) on machine learning models to detect the training data from them. But the existing methods are based on analyzing the output probabilities of models which are unrealistic to LLM services. To tackle the problem, we firstly construct the benchmark datasets by collecting textual data from different domains as the seen and unseen pre-training data for LLMs. Then, we investigate a black-box framework named DPDLLM, with the only access to the generated texts from LLM for detecting textual data whether was used to train it. In the proposed framework, we exploit GPT-2 as the reference model to fit the textual data and feed the generated text from LLM into it to acquire sequence probabilities as the significant feature for detection. The experimental results on the benchmark datasets demonstrate that DPDLLM is effective on different popular LLMs and outperforms the existing methods.",
    "num_pages": 10
}