{
    "uuid": "85c03737-ba7f-5482-96ed-89b0ffe5de55",
    "title": "SKAM at SemEval-2023 Task 10: Linguistic Feature Integration and Continuous Pretraining for Online Sexism Detection and Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{kondragunta-etal-2023-skam,\n    title = \"{SKAM} at {S}em{E}val-2023 Task 10: Linguistic Feature Integration and Continuous Pretraining for Online Sexism Detection and Classification\",\n    author = \"Kondragunta, Murali Manohar  and\n      Chen, Amber  and\n      Slot, Karlo  and\n      Weering, Sanne  and\n      Caselli, Tommaso\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.250\",\n    doi = \"10.18653/v1/2023.semeval-1.250\",\n    pages = \"1805--1817\",\n    abstract = \"Sexism has been prevalent online. In this paper, we explored the effect of explicit linguistic features and continuous pretraining on the performance of pretrained language models in sexism detection. While adding linguistic features did not improve the performance of the model, continuous pretraining did slightly boost the performance of the model in Task B from a mean macro-F1 score of 0.6156 to 0.6246. The best mean macro-F1 score in Task A was achieved by a finetuned HateBERT model using regular pretraining (0.8331). We observed that the linguistic features did not improve the model{'}s performance. At the same time, continuous pretraining proved beneficial only for nuanced downstream tasks like Task-B.\",\n}\n",
    "authors": [
        "Murali Manohar Kondragunta",
        "Amber Chen",
        "Karlo Slot",
        "Sanne Weering",
        "Tommaso Caselli"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.250.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/85c03737-ba7f-5482-96ed-89b0ffe5de55.pdf",
    "abstract": "Sexism has been prevalent online. In this paper, we explored the effect of explicit linguistic features and continuous pretraining on the performance of pretrained language models in sexism detection. While adding linguistic features did not improve the performance of the model, continuous pretraining did slightly boost the performance of the model in Task B from a mean macro-F1 score of 0.6156 to 0.6246. The best mean macro-F1 score in Task A was achieved by a finetuned HateBERT model using regular pretraining (0.8331). We observed that the linguistic features did not improve the modelâ€™s performance. At the same time, continuous pretraining proved beneficial only for nuanced downstream tasks like Task-B.",
    "num_pages": 13
}