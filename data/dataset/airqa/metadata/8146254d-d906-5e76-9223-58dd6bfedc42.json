{
    "uuid": "8146254d-d906-5e76-9223-58dd6bfedc42",
    "title": "Prompt Discriminative Language Models for Domain Adaptation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{lu-etal-2023-prompt,\n    title = \"Prompt Discriminative Language Models for Domain Adaptation\",\n    author = \"Lu, Keming  and\n      Potash, Peter  and\n      Lin, Xihui  and\n      Sun, Yuwen  and\n      Qian, Zihan  and\n      Yuan, Zheng  and\n      Naumann, Tristan  and\n      Cai, Tianxi  and\n      Lu, Junwei\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.30\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.30\",\n    pages = \"247--258\",\n    abstract = \"Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining. In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.\",\n}\n",
    "authors": [
        "Keming Lu",
        "Peter Potash",
        "Xihui Lin",
        "Yuwen Sun",
        "Zihan Qian",
        "Zheng Yuan",
        "Tristan Naumann",
        "Tianxi Cai",
        "Junwei Lu"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.30.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8146254d-d906-5e76-9223-58dd6bfedc42.pdf",
    "abstract": "Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. However, the potential of discriminative language models in biomedical tasks remains underexplored.To bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. BioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining. In this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.",
    "num_pages": 12
}