{
    "uuid": "fcbfe144-254e-5c9f-942c-6f154c8363e0",
    "title": "CUFE at NADI 2024 shared task: Fine-Tuning Llama-3 To Translate From Arabic Dialects To Modern Standard Arabic",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{ibrahim-2024-cufe,\n    title = \"{CUFE} at {NADI} 2024 shared task: Fine-Tuning Llama-3 To Translate From {A}rabic Dialects To {M}odern {S}tandard {A}rabic\",\n    author = \"Ibrahim, Michael\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.87\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.87\",\n    pages = \"769--773\",\n    abstract = \"LLMs such as GPT-4 and LLaMA excel in multiple natural language processing tasks, however, LLMs face challenges in delivering satisfactory performance on low-resource languages due to limited availability of training data. In this paper, LLaMA-3 with 8 Billion parameters is finetuned to translate among Egyptian, Emirati, Jordanian, Palestinian Arabic dialects, and Modern Standard Arabic (MSA). In the NADI 2024 Task on DA-MSA Machine Translation, the proposed method achieved a BLEU score of 21.44 when it was fine-tuned on thedevelopment dataset of the NADI 2024 Task on DA-MSA and a BLEU score of 16.09 when trained when it was fine-tuned using the OSACT dataset.\",\n}\n",
    "authors": [
        "Michael Ibrahim"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.87.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/fcbfe144-254e-5c9f-942c-6f154c8363e0.pdf",
    "abstract": "LLMs such as GPT-4 and LLaMA excel in multiple natural language processing tasks, however, LLMs face challenges in delivering satisfactory performance on low-resource languages due to limited availability of training data. In this paper, LLaMA-3 with 8 Billion parameters is finetuned to translate among Egyptian, Emirati, Jordanian, Palestinian Arabic dialects, and Modern Standard Arabic (MSA). In the NADI 2024 Task on DA-MSA Machine Translation, the proposed method achieved a BLEU score of 21.44 when it was fine-tuned on thedevelopment dataset of the NADI 2024 Task on DA-MSA and a BLEU score of 16.09 when trained when it was fine-tuned using the OSACT dataset.",
    "num_pages": 5
}