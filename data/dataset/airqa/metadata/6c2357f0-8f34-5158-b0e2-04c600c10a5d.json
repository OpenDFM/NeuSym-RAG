{
    "uuid": "6c2357f0-8f34-5158-b0e2-04c600c10a5d",
    "title": "Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ranaldi-etal-2024-empowering,\n    title = \"Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations\",\n    author = \"Ranaldi, Leonardo  and\n      Pucci, Giulia  and\n      Freitas, Andre\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.473\",\n    doi = \"10.18653/v1/2024.findings-acl.473\",\n    pages = \"7961--7973\",\n    abstract = \"The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs. In this paper, we propose to empower Instruction-tuned LLMs (It-LLMs) in languages other than English by building semantic alignment between them. Hence, we propose \\textit{CrossAlpaca}, an It-LLM with cross-lingual Instruction-following and Translation-following demonstrations to improve semantic alignment between languages. We validate our approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA and adapted versions of MMLU and BBH.Our models, tested over six different languages, outperform the It-LLMs tuned on monolingual data. The final results show that instruction tuning on non-English data is not enough and that semantic alignment can be further improved by Translation-following demonstrations.\",\n}\n",
    "authors": [
        "Leonardo Ranaldi",
        "Giulia Pucci",
        "Andre Freitas"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.473.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6c2357f0-8f34-5158-b0e2-04c600c10a5d.pdf",
    "abstract": "The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs. In this paper, we propose to empower Instruction-tuned LLMs (It-LLMs) in languages other than English by building semantic alignment between them. Hence, we propose CrossAlpaca, an It-LLM with cross-lingual Instruction-following and Translation-following demonstrations to improve semantic alignment between languages. We validate our approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA and adapted versions of MMLU and BBH.Our models, tested over six different languages, outperform the It-LLMs tuned on monolingual data. The final results show that instruction tuning on non-English data is not enough and that semantic alignment can be further improved by Translation-following demonstrations.",
    "num_pages": 13
}