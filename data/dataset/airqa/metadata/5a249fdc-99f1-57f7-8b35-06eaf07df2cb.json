{
    "uuid": "5a249fdc-99f1-57f7-8b35-06eaf07df2cb",
    "title": "CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ye-etal-2023-clapspeech,\n    title = \"{CLAPS}peech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training\",\n    author = \"Ye, Zhenhui  and\n      Huang, Rongjie  and\n      Ren, Yi  and\n      Jiang, Ziyue  and\n      Liu, Jinglin  and\n      He, Jinzheng  and\n      Yin, Xiang  and\n      Zhao, Zhou\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.518\",\n    doi = \"10.18653/v1/2023.acl-long.518\",\n    pages = \"9317--9331\",\n    abstract = \"Improving text representation has attracted much attention to achieve expressive text-to-speech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech, a cross-modal contrastive pre-training framework that learns from the prosody variance of the same text token under different contexts. Specifically, 1) with the design of a text encoder and a prosody encoder, we encourage the model to connect the text context with its corresponding prosody pattern in the joint multi-modal space; 2) we introduce a multi-scale pre-training pipeline to capture prosody patterns in multiple levels. 3) we show how to incorporate CLAPSpeech into existing TTS models for better prosody. Experiments on three datasets not only show that CLAPSpeech could improve the prosody prediction for existing TTS methods, but also demonstrate its generalization ability to adapt to multiple languages and multi-speaker text-to-speech. We also deeply analyze the principle behind the performance of CLAPSpeech. Ablation studies demonstrate the necessity of each component in CLAPSpeech. Source code and audio samples are available at \\url{https://clapspeech.github.io}.\",\n}\n",
    "authors": [
        "Zhenhui Ye",
        "Rongjie Huang",
        "Yi Ren",
        "Ziyue Jiang",
        "Jinglin Liu",
        "Jinzheng He",
        "Xiang Yin",
        "Zhou Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.518.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5a249fdc-99f1-57f7-8b35-06eaf07df2cb.pdf",
    "abstract": "Improving text representation has attracted much attention to achieve expressive text-to-speech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech, a cross-modal contrastive pre-training framework that learns from the prosody variance of the same text token under different contexts. Specifically, 1) with the design of a text encoder and a prosody encoder, we encourage the model to connect the text context with its corresponding prosody pattern in the joint multi-modal space; 2) we introduce a multi-scale pre-training pipeline to capture prosody patterns in multiple levels. 3) we show how to incorporate CLAPSpeech into existing TTS models for better prosody. Experiments on three datasets not only show that CLAPSpeech could improve the prosody prediction for existing TTS methods, but also demonstrate its generalization ability to adapt to multiple languages and multi-speaker text-to-speech. We also deeply analyze the principle behind the performance of CLAPSpeech. Ablation studies demonstrate the necessity of each component in CLAPSpeech. Source code and audio samples are available at https://clapspeech.github.io.",
    "num_pages": 15
}