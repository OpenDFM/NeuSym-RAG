{
    "uuid": "326e6c52-c3d7-5184-b530-b48e8b232837",
    "title": "A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{araujo-etal-2023-memory,\n    title = \"A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information\",\n    author = \"Araujo, Vladimir  and\n      Soto, Alvaro  and\n      Moens, Marie-Francine\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.830\",\n    doi = \"10.18653/v1/2023.findings-acl.830\",\n    pages = \"13124--13138\",\n    abstract = \"Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms{'} importance for memory models.\",\n}\n",
    "authors": [
        "Vladimir Araujo",
        "Alvaro Soto",
        "Marie-Francine Moens"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.830.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/326e6c52-c3d7-5184-b530-b48e8b232837.pdf",
    "abstract": "Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanismsâ€™ importance for memory models.",
    "num_pages": 15
}