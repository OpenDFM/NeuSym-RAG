{
    "uuid": "9d64aa5e-7359-5233-9bbc-48dbf397c6ee",
    "title": "LLMBox: A Comprehensive Library for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{tang-etal-2024-llmbox,\n    title = \"{LLMB}ox: A Comprehensive Library for Large Language Models\",\n    author = \"Tang, Tianyi  and\n      Yiwen, Hu  and\n      Li, Bingqian  and\n      Luo, Wenyang  and\n      Qin, ZiJing  and\n      Sun, Haoxiang  and\n      Wang, Jiapeng  and\n      Xu, Shiyi  and\n      Cheng, Xiaoxue  and\n      Guo, Geyang  and\n      Peng, Han  and\n      Zheng, Bowen  and\n      Tang, Yiru  and\n      Min, Yingqian  and\n      Chen, Yushuo  and\n      Chen, Jie  and\n      Zhao, Ranchi  and\n      Ding, Luran  and\n      Wang, Yuhao  and\n      Dong, Zican  and\n      Chunxuan, Xia  and\n      Li, Junyi  and\n      Zhou, Kun  and\n      Zhao, Xin  and\n      Wen, Ji-Rong\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.37\",\n    doi = \"10.18653/v1/2024.acl-demos.37\",\n    pages = \"388--399\",\n    abstract = \"To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at \\url{https://github.com/RUCAIBox/LLMBox}.\",\n}\n",
    "authors": [
        "Tianyi Tang",
        "Hu Yiwen",
        "Bingqian Li",
        "Wenyang Luo",
        "ZiJing Qin",
        "Haoxiang Sun",
        "Jiapeng Wang",
        "Shiyi Xu",
        "Xiaoxue Cheng",
        "Geyang Guo",
        "Han Peng",
        "Bowen Zheng",
        "Yiru Tang",
        "Yingqian Min",
        "Yushuo Chen",
        "Jie Chen",
        "Ranchi Zhao",
        "Luran Ding",
        "Yuhao Wang",
        "Zican Dong",
        "Xia Chunxuan",
        "Junyi Li",
        "Kun Zhou",
        "Xin Zhao",
        "Ji-Rong Wen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.37.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9d64aa5e-7359-5233-9bbc-48dbf397c6ee.pdf",
    "abstract": "To facilitate the research on large language models (LLMs), this paper presents a comprehensive and unified library, LLMBox, to ease the development, use, and evaluation of LLMs. This library is featured with three main merits: (1) a unified data interface that supports the flexible implementation of various training strategies, (2) a comprehensive evaluation that covers extensive tasks, datasets, and models, and (3) more practical consideration, especially on user-friendliness and efficiency. With our library, users can easily reproduce existing methods, train new models, and conduct comprehensive performance comparisons. To rigorously test LLMBox, we conduct extensive experiments in a diverse coverage of evaluation settings, and experimental results demonstrate the effectiveness and efficiency of our library in supporting various implementations related to LLMs. The detailed introduction and usage guidance can be found at https://github.com/RUCAIBox/LLMBox.",
    "num_pages": 12
}