{
    "uuid": "59d8d5f4-1c38-5baf-a3b0-99ced3637ef1",
    "title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{gao-etal-2024-self-evolving,\n    title = \"Self-Evolving {GPT}: A Lifelong Autonomous Experiential Learner\",\n    author = \"Gao, Jinglong  and\n      Ding, Xiao  and\n      Cui, Yiming  and\n      Zhao, Jianbai  and\n      Wang, Hepeng  and\n      Liu, Ting  and\n      Qin, Bing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.346\",\n    doi = \"10.18653/v1/2024.acl-long.346\",\n    pages = \"6385--6432\",\n    abstract = \"To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions.To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them.Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities, offering a new path worth further exploration for the evolution of machine intelligence. Additionally, we provide a detailed analysis of the behavior of our framework at each step.We will open source codes after the acceptance, fostering open research in the NLP community and beyond.\",\n}\n",
    "authors": [
        "Jinglong Gao",
        "Xiao Ding",
        "Yiming Cui",
        "Jianbai Zhao",
        "Hepeng Wang",
        "Ting Liu",
        "Bing Qin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.346.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/59d8d5f4-1c38-5baf-a3b0-99ced3637ef1.pdf",
    "abstract": "To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions.To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them.Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities, offering a new path worth further exploration for the evolution of machine intelligence. Additionally, we provide a detailed analysis of the behavior of our framework at each step.We will open source codes after the acceptance, fostering open research in the NLP community and beyond.",
    "num_pages": 48
}