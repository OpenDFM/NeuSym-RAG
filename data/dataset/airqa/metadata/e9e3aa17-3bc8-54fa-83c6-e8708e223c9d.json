{
    "uuid": "e9e3aa17-3bc8-54fa-83c6-e8708e223c9d",
    "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{zhang-etal-2023-reaugkd,\n    title = \"{R}e{A}ug{KD}: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models\",\n    author = \"Zhang, Jianyi  and\n      Muhamed, Aashiq  and\n      Anantharaman, Aditya  and\n      Wang, Guoyin  and\n      Chen, Changyou  and\n      Zhong, Kai  and\n      Cui, Qingjun  and\n      Xu, Yi  and\n      Zeng, Belinda  and\n      Chilimbi, Trishul  and\n      Chen, Yiran\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.97\",\n    doi = \"10.18653/v1/2023.acl-short.97\",\n    pages = \"1128--1136\",\n    abstract = \"Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher{'}s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.\",\n}\n",
    "authors": [
        "Jianyi Zhang",
        "Aashiq Muhamed",
        "Aditya Anantharaman",
        "Guoyin Wang",
        "Changyou Chen",
        "Kai Zhong",
        "Qingjun Cui",
        "Yi Xu",
        "Belinda Zeng",
        "Trishul Chilimbi",
        "Yiran Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.97.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e9e3aa17-3bc8-54fa-83c6-e8708e223c9d.pdf",
    "abstract": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacherâ€™s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
    "num_pages": 9
}