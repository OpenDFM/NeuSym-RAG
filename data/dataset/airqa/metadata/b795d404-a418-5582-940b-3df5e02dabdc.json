{
    "uuid": "b795d404-a418-5582-940b-3df5e02dabdc",
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{peng-etal-2023-copying,\n    title = \"Are You Copying My Model? Protecting the Copyright of Large Language Models for {E}aa{S} via Backdoor Watermark\",\n    author = \"Peng, Wenjun  and\n      Yi, Jingwei  and\n      Wu, Fangzhao  and\n      Wu, Shangxi  and\n      Bin Zhu, Bin  and\n      Lyu, Lingjuan  and\n      Jiao, Binxing  and\n      Xu, Tong  and\n      Sun, Guangzhong  and\n      Xie, Xing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.423\",\n    doi = \"10.18653/v1/2023.acl-long.423\",\n    pages = \"7653--7668\",\n    abstract = \"Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro {`}METHOD{'}} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer{'}s model for copyright verification while minimizing the adverse impact on the original embeddings{'} utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at \\url{https://github.com/yjw1029/EmbMarker}.\",\n}\n",
    "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Bin Zhu",
        "Lingjuan Lyu",
        "Binxing Jiao",
        "Tong Xu",
        "Guangzhong Sun",
        "Xing Xie"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.423.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b795d404-a418-5582-940b-3df5e02dabdc.pdf",
    "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro ‘METHOD’} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer’s model for copyright verification while minimizing the adverse impact on the original embeddings’ utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker.",
    "num_pages": 16
}