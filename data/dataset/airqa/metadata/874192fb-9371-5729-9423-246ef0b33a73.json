{
    "uuid": "874192fb-9371-5729-9423-246ef0b33a73",
    "title": "Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{fatahi-bayat-etal-2024-enhanced,\n    title = \"Enhanced Language Model Truthfulness with Learnable Intervention and Uncertainty Expression\",\n    author = \"Fatahi Bayat, Farima  and\n      Liu, Xin  and\n      Jagadish, H.  and\n      Wang, Lu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.737\",\n    doi = \"10.18653/v1/2024.findings-acl.737\",\n    pages = \"12388--12400\",\n    abstract = \"Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the {``}truthful directions{''} previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model{'}s internal knowledge only when it is confident. Our code is available at https://github.com/launchnlp/LITO.\",\n}\n",
    "authors": [
        "Farima Fatahi Bayat",
        "Xin Liu",
        "H. Jagadish",
        "Lu Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.737.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/874192fb-9371-5729-9423-246ef0b33a73.pdf",
    "abstract": "Large language models (LLMs) can generate long-form and coherent text, yet they often hallucinate facts, which undermines their reliability. To mitigate this issue, inference-time methods steer LLM representations toward the “truthful directions” previously learned for truth elicitation. However, applying these truthful directions with the same intensity fails to generalize across different query contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to each specific context. LITO explores a sequence of model generations based on increasing levels of intervention intensities. It selects the most accurate response or refuses to answer when the predictions are highly uncertain. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters the limitations of one-size-fits-all intervention methods, maximizing truthfulness by reflecting the model’s internal knowledge only when it is confident. Our code is available at https://github.com/launchnlp/LITO.",
    "num_pages": 13
}