{
    "uuid": "1ae40dca-0be3-5414-ac35-6958305ae2ba",
    "title": "The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{miceli-barone-etal-2023-larger,\n    title = \"The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python\",\n    author = \"Miceli Barone, Antonio Valerio  and\n      Barez, Fazl  and\n      Cohen, Shay B.  and\n      Konstas, Ioannis\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.19\",\n    doi = \"10.18653/v1/2023.findings-acl.19\",\n    pages = \"272--292\",\n    abstract = \"Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.\",\n}\n",
    "authors": [
        "Antonio Valerio Miceli Barone",
        "Fazl Barez",
        "Shay B. Cohen",
        "Ioannis Konstas"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.19.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1ae40dca-0be3-5414-ac35-6958305ae2ba.pdf",
    "abstract": "Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.",
    "num_pages": 21
}