{
    "uuid": "e53eb390-7bb7-5a6d-8ae3-c3653f37ec83",
    "title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhuang-etal-2024-lexicon,\n    title = \"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling\",\n    author = \"Zhuang, Chengxu  and\n      Fedorenko, Evelina  and\n      Andreas, Jacob\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.15\",\n    doi = \"10.18653/v1/2024.findings-acl.15\",\n    pages = \"231--247\",\n    abstract = \"Today{'}s most accurate language models are trained on orders of magnitude more language data than human language learners receive{---} but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs{'} representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next-token prediction strategy with a contrastive visual grounding objective, focusing on early-layerrepresentations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastiveGrounding not only outperforms standard language-only models in terms of learning efficiency in small and developmentally plausible data regimes, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization.Moreover, LexiContrastive Grounding improves perplexity by around 5{\\%} on multiple language modeling tasks compared to other models trained on the same amount of text data. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.\",\n}\n",
    "authors": [
        "Chengxu Zhuang",
        "Evelina Fedorenko",
        "Jacob Andreas"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e53eb390-7bb7-5a6d-8ae3-c3653f37ec83.pdf",
    "abstract": "Today’s most accurate language models are trained on orders of magnitude more language data than human language learners receive— but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs’ representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next-token prediction strategy with a contrastive visual grounding objective, focusing on early-layerrepresentations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastiveGrounding not only outperforms standard language-only models in terms of learning efficiency in small and developmentally plausible data regimes, but also improves upon vision-and-language learning procedures including CLIP, GIT, Flamingo, and Vokenization.Moreover, LexiContrastive Grounding improves perplexity by around 5% on multiple language modeling tasks compared to other models trained on the same amount of text data. This work underscores the potential of incorporating visual grounding into language models, aligning more closely with the multimodal nature of human language acquisition.",
    "num_pages": 17
}