{
    "uuid": "0e7160dd-a88d-5b20-95f7-bac70045d85b",
    "title": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{hu-etal-2023-uncertainty,\n    title = \"Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction\",\n    author = \"Hu, Mengting  and\n      Bai, Yinhao  and\n      Wu, Yike  and\n      Zhang, Zhen  and\n      Zhang, Liqi  and\n      Gao, Hang  and\n      Zhao, Shiwan  and\n      Huang, Minlie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.851\",\n    doi = \"10.18653/v1/2023.findings-acl.851\",\n    pages = \"13481--13494\",\n    abstract = \"Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public datasets demonstrate the effectiveness of our approach on various generation templates.\",\n}\n",
    "authors": [
        "Mengting Hu",
        "Yinhao Bai",
        "Yike Wu",
        "Zhen Zhang",
        "Liqi Zhang",
        "Hang Gao",
        "Shiwan Zhao",
        "Minlie Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.851.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/0e7160dd-a88d-5b20-95f7-bac70045d85b.pdf",
    "abstract": "Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public datasets demonstrate the effectiveness of our approach on various generation templates.",
    "num_pages": 14
}