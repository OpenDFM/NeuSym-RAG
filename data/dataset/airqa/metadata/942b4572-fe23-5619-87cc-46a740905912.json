{
    "uuid": "942b4572-fe23-5619-87cc-46a740905912",
    "title": "How Do In-Context Examples Affect Compositional Generalization?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{an-etal-2023-context,\n    title = \"How Do In-Context Examples Affect Compositional Generalization?\",\n    author = \"An, Shengnan  and\n      Lin, Zeqi  and\n      Fu, Qiang  and\n      Chen, Bei  and\n      Zheng, Nanning  and\n      Lou, Jian-Guang  and\n      Zhang, Dongmei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.618\",\n    doi = \"10.18653/v1/2023.acl-long.618\",\n    pages = \"11027--11052\",\n    abstract = \"Compositional generalization{--}understanding unseen combinations of seen primitives{--}is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning{--}the prevailing few-shot paradigm based on large language models{--}exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.\",\n}\n",
    "authors": [
        "Shengnan An",
        "Zeqi Lin",
        "Qiang Fu",
        "Bei Chen",
        "Nanning Zheng",
        "Jian-Guang Lou",
        "Dongmei Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.618.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/942b4572-fe23-5619-87cc-46a740905912.pdf",
    "abstract": "Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.",
    "num_pages": 26
}