{
    "uuid": "deee67a7-25e9-5298-9adb-f53f08d62029",
    "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{li-etal-2023-structure,\n    title = \"Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data\",\n    author = \"Li, Xinze  and\n      Liu, Zhenghao  and\n      Xiong, Chenyan  and\n      Yu, Shi  and\n      Gu, Yu  and\n      Liu, Zhiyuan  and\n      Yu, Ge\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.734\",\n    doi = \"10.18653/v1/2023.findings-acl.734\",\n    pages = \"11560--11574\",\n    abstract = \"This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at \\url{https://github.com/OpenMatch/OpenMatch}.\",\n}\n",
    "authors": [
        "Xinze Li",
        "Zhenghao Liu",
        "Chenyan Xiong",
        "Shi Yu",
        "Yu Gu",
        "Zhiyuan Liu",
        "Ge Yu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.734.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/deee67a7-25e9-5298-9adb-f53f08d62029.pdf",
    "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
    "num_pages": 15
}