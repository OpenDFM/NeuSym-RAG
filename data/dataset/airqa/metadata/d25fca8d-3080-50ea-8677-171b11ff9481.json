{
    "uuid": "d25fca8d-3080-50ea-8677-171b11ff9481",
    "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2023-experts,\n    title = \"Are Experts Needed? On Human Evaluation of Counselling Reflection Generation\",\n    author = \"Wu, Zixiu  and\n      Balloccu, Simone  and\n      Reiter, Ehud  and\n      Helaoui, Rim  and\n      Reforgiato Recupero, Diego  and\n      Riboni, Daniele\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.382\",\n    doi = \"10.18653/v1/2023.acl-long.382\",\n    pages = \"6906--6930\",\n    abstract = \"Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.\",\n}\n",
    "authors": [
        "Zixiu Wu",
        "Simone Balloccu",
        "Ehud Reiter",
        "Rim Helaoui",
        "Diego Reforgiato Recupero",
        "Daniele Riboni"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.382.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d25fca8d-3080-50ea-8677-171b11ff9481.pdf",
    "abstract": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.",
    "num_pages": 25
}