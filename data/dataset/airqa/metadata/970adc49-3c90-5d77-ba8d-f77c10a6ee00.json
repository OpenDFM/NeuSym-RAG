{
    "uuid": "970adc49-3c90-5d77-ba8d-f77c10a6ee00",
    "title": "Large Language Models Can Learn Representation in Natural Language",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{guo-etal-2024-large,\n    title = \"Large Language Models Can Learn Representation in Natural Language\",\n    author = \"Guo, Yiduo  and\n      Liang, Yaobo  and\n      Zhao, Dongyan  and\n      Duan, Nan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.542\",\n    doi = \"10.18653/v1/2024.findings-acl.542\",\n    pages = \"9145--9154\",\n    abstract = \"One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is obtaining accurate natural language representations for each entity to aid in retriever precision. In this paper, we propose the Natural Language Representation Optimization Problem, which aims to refine entity descriptions for improved retrieval and LLM utilization. We introduce the Learning to Represent with Natural Language method, which utilizes LLMs to optimize entity representations consisting of text patterns based on environmental feedback. We iteratively prompt LLMs to enhance or adjust patterns based on entity samples and evaluate their effectiveness through environmental feedback. Our method successfully learns human-understandable representations for classification tasks (e.g., instructions and documents) and API call tasks (e.g., APIbench and Virtual Home), significantly improving GPT-4{'}s task performance.\",\n}\n",
    "authors": [
        "Yiduo Guo",
        "Yaobo Liang",
        "Dongyan Zhao",
        "Nan Duan"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.542.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/970adc49-3c90-5d77-ba8d-f77c10a6ee00.pdf",
    "abstract": "One major challenge for Large Language Models (LLMs) is completing complex tasks involving multiple entities, such as tool APIs. To tackle this, one approach is to retrieve relevant entities to enhance LLMs in task completion. A crucial issue here is obtaining accurate natural language representations for each entity to aid in retriever precision. In this paper, we propose the Natural Language Representation Optimization Problem, which aims to refine entity descriptions for improved retrieval and LLM utilization. We introduce the Learning to Represent with Natural Language method, which utilizes LLMs to optimize entity representations consisting of text patterns based on environmental feedback. We iteratively prompt LLMs to enhance or adjust patterns based on entity samples and evaluate their effectiveness through environmental feedback. Our method successfully learns human-understandable representations for classification tasks (e.g., instructions and documents) and API call tasks (e.g., APIbench and Virtual Home), significantly improving GPT-4â€™s task performance.",
    "num_pages": 10
}