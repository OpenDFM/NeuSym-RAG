{
    "uuid": "461347b7-dd42-57af-a0f6-38339a7d3df7",
    "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 6th Workshop on NLP for Conversational AI (NLP4ConvAI 2024)",
    "bibtex": "@inproceedings{jandaghi-etal-2024-faithful,\n    title = \"Faithful Persona-based Conversational Dataset Generation with Large Language Models\",\n    author = \"Jandaghi, Pegah  and\n      Sheng, Xianghai  and\n      Bai, Xinyi  and\n      Pujara, Jay  and\n      Sidahmed, Hakim\",\n    editor = \"Nouri, Elnaz  and\n      Rastogi, Abhinav  and\n      Spithourakis, Georgios  and\n      Liu, Bing  and\n      Chen, Yun-Nung  and\n      Li, Yu  and\n      Albalak, Alon  and\n      Wakaki, Hiromi  and\n      Papangelis, Alexandros\",\n    booktitle = \"Proceedings of the 6th Workshop on NLP for Conversational AI (NLP4ConvAI 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlp4convai-1.8\",\n    pages = \"114--139\",\n    abstract = \"High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user{'}s character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during an AI detection test decreases from 17.2{\\%} to 8.8{\\%} over three iterations.\",\n}\n",
    "authors": [
        "Pegah Jandaghi",
        "Xianghai Sheng",
        "Xinyi Bai",
        "Jay Pujara",
        "Hakim Sidahmed"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlp4convai-1.8.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/461347b7-dd42-57af-a0f6-38339a7d3df7.pdf",
    "abstract": "High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the userâ€™s character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during an AI detection test decreases from 17.2% to 8.8% over three iterations.",
    "num_pages": 26
}