{
    "uuid": "0113511b-eb3d-59a8-92ba-4a23a5197d70",
    "title": "FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{yue-etal-2024-fragrel,\n    title = \"{F}rag{R}el: Exploiting Fragment-level Relations in the External Memory of Large Language Models\",\n    author = \"Yue, Xihang  and\n      Zhu, Linchao  and\n      Yang, Yi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.968\",\n    doi = \"10.18653/v1/2024.findings-acl.968\",\n    pages = \"16348--16361\",\n    abstract = \"To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM{'}s context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.\",\n}\n",
    "authors": [
        "Xihang Yue",
        "Linchao Zhu",
        "Yi Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.968.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0113511b-eb3d-59a8-92ba-4a23a5197d70.pdf",
    "abstract": "To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLMâ€™s context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.",
    "num_pages": 14
}