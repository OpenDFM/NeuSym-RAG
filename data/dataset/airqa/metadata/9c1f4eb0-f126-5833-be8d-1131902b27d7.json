{
    "uuid": "9c1f4eb0-f126-5833-be8d-1131902b27d7",
    "title": "UBC-DLNLP at SemEval-2023 Task 12: Impact of Transfer Learning on African Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{bhatia-etal-2023-ubc,\n    title = \"{UBC}-{DLNLP} at {S}em{E}val-2023 Task 12: Impact of Transfer Learning on {A}frican Sentiment Analysis\",\n    author = \"Bhatia, Gagan  and\n      Adebara, Ife  and\n      Elmadany, Abdelrahim  and\n      Abdul-mageed, Muhammad\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.33\",\n    doi = \"10.18653/v1/2023.semeval-1.33\",\n    pages = \"246--255\",\n    abstract = \"We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared task, where we tackle the task of sentiment analysis in 14 different African languages. We develop both monolingual and multilingual models under a full supervised setting (subtasks A and B). We also develop models for the zero-shot setting (subtask C). Our approach involves experimenting with transfer learning using six language models, including further pretraining of some of these models as well as a final finetuning stage. Our best performing models achieve an F1-score of 70.36 on development data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate the effectiveness of transfer learning and finetuning techniques for sentiment analysis across multiple languages. Our approach can be applied to other sentiment analysis tasks in different languages and domains.\",\n}\n",
    "authors": [
        "Gagan Bhatia",
        "Ife Adebara",
        "Abdelrahim Elmadany",
        "Muhammad Abdul-mageed"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.33.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9c1f4eb0-f126-5833-be8d-1131902b27d7.pdf",
    "abstract": "We describe our contribution to the SemEVAl 2023 AfriSenti-SemEval shared task, where we tackle the task of sentiment analysis in 14 different African languages. We develop both monolingual and multilingual models under a full supervised setting (subtasks A and B). We also develop models for the zero-shot setting (subtask C). Our approach involves experimenting with transfer learning using six language models, including further pretraining of some of these models as well as a final finetuning stage. Our best performing models achieve an F1-score of 70.36 on development data and an F1-score of 66.13 on test data. Unsurprisingly, our results demonstrate the effectiveness of transfer learning and finetuning techniques for sentiment analysis across multiple languages. Our approach can be applied to other sentiment analysis tasks in different languages and domains.",
    "num_pages": 10
}