{
    "uuid": "29853baf-f3fa-5bc5-94d9-1ce7fbc3f000",
    "title": "“My Answer is C”: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wang-etal-2024-answer-c,\n    title = \"{``}My Answer is {C}{''}: First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models\",\n    author = {Wang, Xinpeng  and\n      Ma, Bolei  and\n      Hu, Chengzhi  and\n      Weber-Genzel, Leon  and\n      R{\\\"o}ttger, Paul  and\n      Kreuter, Frauke  and\n      Hovy, Dirk  and\n      Plank, Barbara},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.441\",\n    doi = \"10.18653/v1/2024.findings-acl.441\",\n    pages = \"7407--7416\",\n    abstract = \"The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model{'}s diverse response styles such as starting with {``}Sure{''} or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned \\textit{on all dimensions}, reaching mismatch rates over 60{\\%}. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.\",\n}\n",
    "authors": [
        "Xinpeng Wang",
        "Bolei Ma",
        "Chengzhi Hu",
        "Leon Weber-Genzel",
        "Paul Röttger",
        "Frauke Kreuter",
        "Dirk Hovy",
        "Barbara Plank"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.441.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/29853baf-f3fa-5bc5-94d9-1ce7fbc3f000.pdf",
    "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model’s diverse response styles such as starting with “Sure” or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.",
    "num_pages": 10
}