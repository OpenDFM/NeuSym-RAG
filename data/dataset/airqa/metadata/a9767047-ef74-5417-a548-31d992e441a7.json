{
    "uuid": "a9767047-ef74-5417-a548-31d992e441a7",
    "title": "Sea_and_Wine at SemEval-2023 Task 9: A Regression Model with Data Augmentation for Multilingual Intimacy Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{chen-etal-2023-sea,\n    title = \"{S}ea{\\_}and{\\_}{W}ine at {S}em{E}val-2023 Task 9: A Regression Model with Data Augmentation for Multilingual Intimacy Analysis\",\n    author = \"Chen, Yuxi  and\n      Chang, Yu  and\n      Tao, Yanqing  and\n      Zhang, Yanru\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.9\",\n    doi = \"10.18653/v1/2023.semeval-1.9\",\n    pages = \"77--82\",\n    abstract = \"In Task 9, we are required to analyze the textual intimacy of tweets in 10 languages. We fine-tune XLM-RoBERTa (XLM-R) pre-trained model to adapt to this multilingual regression task. After tentative experiments, severe class imbalance is observed in the official released dataset, which may compromise the convergence and weaken the model effect. To tackle such challenge, we take measures in two aspects. On the one hand, we implement data augmentation through machine translation to enlarge the scale of classes with fewer samples. On the other hand, we introduce focal mean square error (MSE) loss to emphasize the contributions of hard samples to total loss, thus further mitigating the impact of class imbalance on model effect. Extensive experiments demonstrate remarkable effectiveness of our strategies, and our model achieves high performance on the Pearson{'}s correlation coefficient (CC) almost above 0.85 on validation dataset.\",\n}\n",
    "authors": [
        "Yuxi Chen",
        "Yu Chang",
        "Yanqing Tao",
        "Yanru Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a9767047-ef74-5417-a548-31d992e441a7.pdf",
    "abstract": "In Task 9, we are required to analyze the textual intimacy of tweets in 10 languages. We fine-tune XLM-RoBERTa (XLM-R) pre-trained model to adapt to this multilingual regression task. After tentative experiments, severe class imbalance is observed in the official released dataset, which may compromise the convergence and weaken the model effect. To tackle such challenge, we take measures in two aspects. On the one hand, we implement data augmentation through machine translation to enlarge the scale of classes with fewer samples. On the other hand, we introduce focal mean square error (MSE) loss to emphasize the contributions of hard samples to total loss, thus further mitigating the impact of class imbalance on model effect. Extensive experiments demonstrate remarkable effectiveness of our strategies, and our model achieves high performance on the Pearsonâ€™s correlation coefficient (CC) almost above 0.85 on validation dataset.",
    "num_pages": 6
}