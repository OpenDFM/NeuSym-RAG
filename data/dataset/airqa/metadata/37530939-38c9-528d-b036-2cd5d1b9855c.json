{
    "uuid": "37530939-38c9-528d-b036-2cd5d1b9855c",
    "title": "DADA: Distribution-Aware Domain Adaptation of PLMs for Information Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{lee-etal-2024-dada,\n    title = \"{DADA}: Distribution-Aware Domain Adaptation of {PLM}s for Information Retrieval\",\n    author = \"Lee, Dohyeon  and\n      Kim, Jongyoon  and\n      Hwang, Seung-won  and\n      Park, Joonsuk\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.825\",\n    doi = \"10.18653/v1/2024.findings-acl.825\",\n    pages = \"13882--13893\",\n    abstract = \"Pre-trained language models (PLMs) exhibit promise in retrieval tasks but struggle with out-of-domain data due to distribution shifts.Addressing this, generative domain adaptation (DA), known as GPL, tackles distribution shifts by generating pseudo queries and labels to train models for predicting query-document relationships in new domains.However, it overlooks the domain distribution, causing the model to struggle with aligning the distribution in the target domain.We, therefore, propose a Distribution-Aware Domain Adaptation (DADA) to guide the model to consider the domain distribution knowledge at the level of both a single document and the corpus, which is referred to as observation-level feedback and domain-level feedback, respectively.Our method effectively adapts the model to the target domain and expands document representation to unseen gold query terms using domain and observation feedback, as demonstrated by empirical results on the BEIR benchmark.\",\n}\n",
    "authors": [
        "Dohyeon Lee",
        "Jongyoon Kim",
        "Seung-won Hwang",
        "Joonsuk Park"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.825.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/37530939-38c9-528d-b036-2cd5d1b9855c.pdf",
    "abstract": "Pre-trained language models (PLMs) exhibit promise in retrieval tasks but struggle with out-of-domain data due to distribution shifts.Addressing this, generative domain adaptation (DA), known as GPL, tackles distribution shifts by generating pseudo queries and labels to train models for predicting query-document relationships in new domains.However, it overlooks the domain distribution, causing the model to struggle with aligning the distribution in the target domain.We, therefore, propose a Distribution-Aware Domain Adaptation (DADA) to guide the model to consider the domain distribution knowledge at the level of both a single document and the corpus, which is referred to as observation-level feedback and domain-level feedback, respectively.Our method effectively adapts the model to the target domain and expands document representation to unseen gold query terms using domain and observation feedback, as demonstrated by empirical results on the BEIR benchmark.",
    "num_pages": 12
}