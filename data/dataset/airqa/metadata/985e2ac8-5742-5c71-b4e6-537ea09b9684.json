{
    "uuid": "985e2ac8-5742-5c71-b4e6-537ea09b9684",
    "title": "SoMeLVLM: A Large Vision Language Model for Social Media Processing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-somelvlm,\n    title = \"{S}o{M}e{LVLM}: A Large Vision Language Model for Social Media Processing\",\n    author = \"Zhang, Xinnong  and\n      Kuang, Haoyu  and\n      Mou, Xinyi  and\n      Lyu, Hanjia  and\n      Wu, Kun  and\n      Chen, Siming  and\n      Luo, Jiebo  and\n      Huang, Xuanjing  and\n      Wei, Zhongyu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.140\",\n    doi = \"10.18653/v1/2024.findings-acl.140\",\n    pages = \"2366--2389\",\n    abstract = \"The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge {\\&} comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.\",\n}\n",
    "authors": [
        "Xinnong Zhang",
        "Haoyu Kuang",
        "Xinyi Mou",
        "Hanjia Lyu",
        "Kun Wu",
        "Siming Chen",
        "Jiebo Luo",
        "Xuanjing Huang",
        "Zhongyu Wei"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.140.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/985e2ac8-5742-5c71-b4e6-537ea09b9684.pdf",
    "abstract": "The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.",
    "num_pages": 24
}