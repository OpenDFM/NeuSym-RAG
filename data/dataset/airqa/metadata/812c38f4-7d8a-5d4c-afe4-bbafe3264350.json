{
    "uuid": "812c38f4-7d8a-5d4c-afe4-bbafe3264350",
    "title": "ClaimDiff: Comparing and Contrasting Claims on Contentious Issues",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ko-etal-2023-claimdiff,\n    title = \"{C}laim{D}iff: Comparing and Contrasting Claims on Contentious Issues\",\n    author = \"Ko, Miyoung  and\n      Seong, Ingyu  and\n      Lee, Hwaran  and\n      Park, Joonsuk  and\n      Chang, Minsuk  and\n      Seo, Minjoon\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.289\",\n    doi = \"10.18653/v1/2023.findings-acl.289\",\n    pages = \"4711--4731\",\n    abstract = \"With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one{'}s argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19{\\%} absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.\",\n}\n",
    "authors": [
        "Miyoung Ko",
        "Ingyu Seong",
        "Hwaran Lee",
        "Joonsuk Park",
        "Minsuk Chang",
        "Minjoon Seo"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.289.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/812c38f4-7d8a-5d4c-afe4-bbafe3264350.pdf",
    "abstract": "With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, oneâ€™s argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.",
    "num_pages": 21
}