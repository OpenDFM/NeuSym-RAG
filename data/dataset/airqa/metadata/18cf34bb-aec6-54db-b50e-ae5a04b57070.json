{
    "uuid": "18cf34bb-aec6-54db-b50e-ae5a04b57070",
    "title": "DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{held-etal-2023-damp,\n    title = \"{DAMP}: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue\",\n    author = \"Held, William  and\n      Hidey, Christopher  and\n      Liu, Fei  and\n      Zhu, Eric  and\n      Goel, Rahul  and\n      Yang, Diyi  and\n      Shah, Rushin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.199\",\n    doi = \"10.18653/v1/2023.acl-long.199\",\n    pages = \"3586--3604\",\n    abstract = \"Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improves \\textit{both} English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.\",\n}\n",
    "authors": [
        "William Held",
        "Christopher Hidey",
        "Fei Liu",
        "Eric Zhu",
        "Rahul Goel",
        "Diyi Yang",
        "Rushin Shah"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.199.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/18cf34bb-aec6-54db-b50e-ae5a04b57070.pdf",
    "abstract": "Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.",
    "num_pages": 19
}