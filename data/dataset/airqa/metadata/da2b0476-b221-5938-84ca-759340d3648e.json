{
    "uuid": "da2b0476-b221-5938-84ca-759340d3648e",
    "title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ding-etal-2024-data,\n    title = \"Data Augmentation using {LLM}s: Data Perspectives, Learning Paradigms and Challenges\",\n    author = \"Ding, Bosheng  and\n      Qin, Chengwei  and\n      Zhao, Ruochen  and\n      Luo, Tianze  and\n      Li, Xinze  and\n      Chen, Guizhen  and\n      Xia, Wenhan  and\n      Hu, Junjie  and\n      Luu, Anh Tuan  and\n      Joty, Shafiq\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.97\",\n    doi = \"10.18653/v1/2024.findings-acl.97\",\n    pages = \"1679--1705\",\n    abstract = \"In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.\",\n}\n",
    "authors": [
        "Bosheng Ding",
        "Chengwei Qin",
        "Ruochen Zhao",
        "Tianze Luo",
        "Xinze Li",
        "Guizhen Chen",
        "Wenhan Xia",
        "Junjie Hu",
        "Anh Tuan Luu",
        "Shafiq Joty"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.97.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/da2b0476-b221-5938-84ca-759340d3648e.pdf",
    "abstract": "In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.",
    "num_pages": 27
}