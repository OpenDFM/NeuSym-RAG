{
    "uuid": "c9cf3970-43c5-5382-855e-7cdb1c5cf1b3",
    "title": "Chemical Language Understanding Benchmark",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{kim-etal-2023-chemical,\n    title = \"Chemical Language Understanding Benchmark\",\n    author = \"Kim, Yunsoo  and\n      Ko, Hyuk  and\n      Lee, Jane  and\n      Heo, Hyun Young  and\n      Yang, Jinyoung  and\n      Lee, Sungsoo  and\n      Lee, Kyu-hwang\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.39\",\n    doi = \"10.18653/v1/2023.acl-industry.39\",\n    pages = \"404--411\",\n    abstract = \"In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.\",\n}\n",
    "authors": [
        "Yunsoo Kim",
        "Hyuk Ko",
        "Jane Lee",
        "Hyun Young Heo",
        "Jinyoung Yang",
        "Sungsoo Lee",
        "Kyu-hwang Lee"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.39.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c9cf3970-43c5-5382-855e-7cdb1c5cf1b3.pdf",
    "abstract": "In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.",
    "num_pages": 8
}