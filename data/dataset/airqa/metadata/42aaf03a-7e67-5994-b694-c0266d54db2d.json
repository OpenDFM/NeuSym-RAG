{
    "uuid": "42aaf03a-7e67-5994-b694-c0266d54db2d",
    "title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{laban-etal-2023-swipe,\n    title = \"{SW}i{PE}: A Dataset for Document-Level Simplification of {W}ikipedia Pages\",\n    author = \"Laban, Philippe  and\n      Vig, Jesse  and\n      Kryscinski, Wojciech  and\n      Joty, Shafiq  and\n      Xiong, Caiming  and\n      Wu, Chien-Sheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.596\",\n    doi = \"10.18653/v1/2023.acl-long.596\",\n    pages = \"10674--10695\",\n    abstract = \"Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.9, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits.\",\n}\n",
    "authors": [
        "Philippe Laban",
        "Jesse Vig",
        "Wojciech Kryscinski",
        "Shafiq Joty",
        "Caiming Xiong",
        "Chien-Sheng Wu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.596.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/42aaf03a-7e67-5994-b694-c0266d54db2d.pdf",
    "abstract": "Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.9, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits.",
    "num_pages": 22
}