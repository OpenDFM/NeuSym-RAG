{
    "uuid": "8a941705-ed20-5ebb-8ff8-f0ce21dc044c",
    "title": "Improving the Transferability of Clinical Note Section Classification Models with BERT and Large Language Model Ensembles",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{zhou-etal-2023-improving-transferability,\n    title = \"Improving the Transferability of Clinical Note Section Classification Models with {BERT} and Large Language Model Ensembles\",\n    author = \"Zhou, Weipeng  and\n      Afshar, Majid  and\n      Dligach, Dmitriy  and\n      Gao, Yanjun  and\n      Miller, Timothy\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.16\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.16\",\n    pages = \"125--130\",\n    abstract = \"Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised learning models with the world knowledge inside large language models (LLMs). Surprisingly, we find that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain data. We also find that their strengths are synergistic, so that a simple ensemble technique leads to additional performance gains.\",\n}\n",
    "authors": [
        "Weipeng Zhou",
        "Majid Afshar",
        "Dmitriy Dligach",
        "Yanjun Gao",
        "Timothy Miller"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8a941705-ed20-5ebb-8ff8-f0ce21dc044c.pdf",
    "abstract": "Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised learning models with the world knowledge inside large language models (LLMs). Surprisingly, we find that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain data. We also find that their strengths are synergistic, so that a simple ensemble technique leads to additional performance gains.",
    "num_pages": 6
}