{
    "uuid": "d8197a49-c210-5dd6-93a3-0ba2bf561036",
    "title": "What Can Go Wrong in Authorship Profiling: Cross-Domain Analysis of Gender and Age Prediction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    "bibtex": "@inproceedings{chen-etal-2024-go,\n    title = \"What Can Go Wrong in Authorship Profiling: Cross-Domain Analysis of Gender and Age Prediction\",\n    author = \"Chen, Hongyu  and\n      Roth, Michael  and\n      Falenska, Agnieszka\",\n    editor = \"Fale{\\'n}ska, Agnieszka  and\n      Basta, Christine  and\n      Costa-juss{\\`a}, Marta  and\n      Goldfarb-Tarrant, Seraphina  and\n      Nozza, Debora\",\n    booktitle = \"Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.gebnlp-1.9\",\n    doi = \"10.18653/v1/2024.gebnlp-1.9\",\n    pages = \"150--166\",\n    abstract = \"Authorship Profiling (AP) aims to predict the demographic attributes (such as gender and age) of authors based on their writing styles. Ever-improving models mean that this task is gaining interest and application possibilities. However, with greater use also comes the risk that authors are misclassified more frequently, and it remains unclear to what extent the better models can capture the bias and who is affected by the models{'} mistakes. In this paper, we investigate three established datasets for AP as well as classical and neural classifiers for this task. Our analyses show that it is often possible to predict the demographic information of the authors based on textual features. However, some features learned by the models are specific to datasets. Moreover, models are prone to errors based on stereotypes associated with topical bias.\",\n}\n",
    "authors": [
        "Hongyu Chen",
        "Michael Roth",
        "Agnieszka Falenska"
    ],
    "pdf_url": "https://aclanthology.org/2024.gebnlp-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d8197a49-c210-5dd6-93a3-0ba2bf561036.pdf",
    "abstract": "Authorship Profiling (AP) aims to predict the demographic attributes (such as gender and age) of authors based on their writing styles. Ever-improving models mean that this task is gaining interest and application possibilities. However, with greater use also comes the risk that authors are misclassified more frequently, and it remains unclear to what extent the better models can capture the bias and who is affected by the modelsâ€™ mistakes. In this paper, we investigate three established datasets for AP as well as classical and neural classifiers for this task. Our analyses show that it is often possible to predict the demographic information of the authors based on textual features. However, some features learned by the models are specific to datasets. Moreover, models are prone to errors based on stereotypes associated with topical bias.",
    "num_pages": 17
}