{
    "uuid": "d2cd0e46-f62b-5ddd-8374-520d93149754",
    "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yang-etal-2023-oasum,\n    title = \"{OAS}um: Large-Scale Open Domain Aspect-based Summarization\",\n    author = \"Yang, Xianjun  and\n      Song, Kaiqiang  and\n      Cho, Sangwoo  and\n      Wang, Xiaoyang  and\n      Pan, Xiaoman  and\n      Petzold, Linda  and\n      Yu, Dong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.268\",\n    doi = \"10.18653/v1/2023.findings-acl.268\",\n    pages = \"4381--4401\",\n    abstract = \"Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users{'} interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available.\",\n}\n",
    "authors": [
        "Xianjun Yang",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Xiaoman Pan",
        "Linda Petzold",
        "Dong Yu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.268.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d2cd0e46-f62b-5ddd-8374-520d93149754.pdf",
    "abstract": "Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on usersâ€™ interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available.",
    "num_pages": 21
}