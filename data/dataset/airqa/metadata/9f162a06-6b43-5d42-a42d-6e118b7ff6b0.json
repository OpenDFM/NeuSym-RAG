{
    "uuid": "9f162a06-6b43-5d42-a42d-6e118b7ff6b0",
    "title": "Prompt-Based Length Controlled Generation with Multiple Control Types",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{jie-etal-2024-prompt,\n    title = \"Prompt-Based Length Controlled Generation with Multiple Control Types\",\n    author = \"Jie, Renlong  and\n      Meng, Xiaojun  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.63\",\n    doi = \"10.18653/v1/2024.findings-acl.63\",\n    pages = \"1067--1085\",\n    abstract = \"Large language models (LLMs) have attracted great attention given their strong performance on a wide range of NLP tasks. In practice, users often expect generated texts to fall within a specific length range, making length controlled generation an important topic, especially for GPT-style models. Existing length control methods mostly focus on a simple control type of {``}equal to{''} a target length. Different from them, we propose a prompt-based method to achieve length controlled generation under different control types with high accuracy. In particular, we adopt reinforcement learning (RL) and sample filtering with the reward signal given by rule-based reward models, which enhances the length control ability of models by rewarding outputs that follow certain control instructions. In addition, we introduce a standard prompt extractor to parse arbitrary users{'} input into standard control instructions. Experiments show that our method significantly improves the accuracy of prompt-based length control on popular summarization datasets like CNNDM and NYT under multiple control types. Moreover, both the standard prompt extractor and RL-tuned model show strong generalization to unseen control prompt templates.\",\n}\n",
    "authors": [
        "Renlong Jie",
        "Xiaojun Meng",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.63.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9f162a06-6b43-5d42-a42d-6e118b7ff6b0.pdf",
    "abstract": "Large language models (LLMs) have attracted great attention given their strong performance on a wide range of NLP tasks. In practice, users often expect generated texts to fall within a specific length range, making length controlled generation an important topic, especially for GPT-style models. Existing length control methods mostly focus on a simple control type of “equal to” a target length. Different from them, we propose a prompt-based method to achieve length controlled generation under different control types with high accuracy. In particular, we adopt reinforcement learning (RL) and sample filtering with the reward signal given by rule-based reward models, which enhances the length control ability of models by rewarding outputs that follow certain control instructions. In addition, we introduce a standard prompt extractor to parse arbitrary users’ input into standard control instructions. Experiments show that our method significantly improves the accuracy of prompt-based length control on popular summarization datasets like CNNDM and NYT under multiple control types. Moreover, both the standard prompt extractor and RL-tuned model show strong generalization to unseen control prompt templates.",
    "num_pages": 19
}