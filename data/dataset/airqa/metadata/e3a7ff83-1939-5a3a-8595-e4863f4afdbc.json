{
    "uuid": "e3a7ff83-1939-5a3a-8595-e4863f4afdbc",
    "title": "SeaLLMs - Large Language Models for Southeast Asia",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{nguyen-etal-2024-seallms,\n    title = \"{S}ea{LLM}s - Large Language Models for {S}outheast {A}sia\",\n    author = \"Nguyen, Xuan-Phi  and\n      Zhang, Wenxuan  and\n      Li, Xin  and\n      Aljunied, Mahani  and\n      Hu, Zhiqiang  and\n      Shen, Chenhui  and\n      Chia, Yew Ken  and\n      Li, Xingxuan  and\n      Wang, Jianyu  and\n      Tan, Qingyu  and\n      Cheng, Liying  and\n      Chen, Guanzheng  and\n      Deng, Yue  and\n      Yang, Sen  and\n      Liu, Chaoqun  and\n      Zhang, Hang  and\n      Bing, Lidong\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.28\",\n    doi = \"10.18653/v1/2024.acl-demos.28\",\n    pages = \"294--304\",\n    abstract = \"Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon popular English-centric models through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.\",\n}\n",
    "authors": [
        "Xuan-Phi Nguyen",
        "Wenxuan Zhang",
        "Xin Li",
        "Mahani Aljunied",
        "Zhiqiang Hu",
        "Chenhui Shen",
        "Yew Ken Chia",
        "Xingxuan Li",
        "Jianyu Wang",
        "Qingyu Tan",
        "Liying Cheng",
        "Guanzheng Chen",
        "Yue Deng",
        "Sen Yang",
        "Chaoqun Liu",
        "Hang Zhang",
        "Lidong Bing"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.28.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e3a7ff83-1939-5a3a-8595-e4863f4afdbc.pdf",
    "abstract": "Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon popular English-centric models through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate.",
    "num_pages": 11
}