{
    "uuid": "ec77646a-1a1d-58de-a323-400c508a9f64",
    "title": "RAP: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cao-etal-2024-rap,\n    title = \"{RAP}: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter\",\n    author = \"Cao, Meng  and\n      Tang, Haoran  and\n      Huang, Jinfa  and\n      Jin, Peng  and\n      Zhang, Can  and\n      Liu, Ruyang  and\n      Chen, Long  and\n      Liang, Xiaodan  and\n      Yuan, Li  and\n      Li, Ge\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.427\",\n    doi = \"10.18653/v1/2024.findings-acl.427\",\n    pages = \"7160--7174\",\n    abstract = \"Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries. To date, most of the state-of-the-art TVR methods learn image-to-video transfer learning based on the large-scale pre-trained vision-language models (e.g., CLIP). However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation cost. To this end, we propose to conduct efficient text-video Retrieval with a salient-and-correlated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers. To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics including temporal sparsity and correlation. Specifically, we propose a low-rank modulation module to refine the per-image features from frozen CLIP backbone, which accentuates silent frames within the video features while alleviating temporal redundancy. Besides, we introduce an asynchronous self-attention mechanism which firstly selects top responsive visual patch and augments the correlation modeling between them with learnable temporal and patch offsets. Extensive experiments on four TVR datasets demonstrate that our RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient finetuning methods.\",\n}\n",
    "authors": [
        "Meng Cao",
        "Haoran Tang",
        "Jinfa Huang",
        "Peng Jin",
        "Can Zhang",
        "Ruyang Liu",
        "Long Chen",
        "Xiaodan Liang",
        "Li Yuan",
        "Ge Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.427.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ec77646a-1a1d-58de-a323-400c508a9f64.pdf",
    "abstract": "Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries. To date, most of the state-of-the-art TVR methods learn image-to-video transfer learning based on the large-scale pre-trained vision-language models (e.g., CLIP). However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation cost. To this end, we propose to conduct efficient text-video Retrieval with a salient-and-correlated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers. To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics including temporal sparsity and correlation. Specifically, we propose a low-rank modulation module to refine the per-image features from frozen CLIP backbone, which accentuates silent frames within the video features while alleviating temporal redundancy. Besides, we introduce an asynchronous self-attention mechanism which firstly selects top responsive visual patch and augments the correlation modeling between them with learnable temporal and patch offsets. Extensive experiments on four TVR datasets demonstrate that our RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient finetuning methods.",
    "num_pages": 15
}