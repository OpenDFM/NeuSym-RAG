{
    "uuid": "7d25a923-c0ec-5b9b-b67c-3c8be6610e84",
    "title": "Action Inference for Destination Prediction in Vision-and-Language Navigation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{kondapally-etal-2024-action,\n    title = \"Action Inference for Destination Prediction in Vision-and-Language Navigation\",\n    author = \"Kondapally, Anirudh  and\n      Yamada, Kentaro  and\n      Yanaka, Hitomi\",\n    editor = \"Fu, Xiyan  and\n      Fleisig, Eve\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-srw.26\",\n    doi = \"10.18653/v1/2024.acl-srw.26\",\n    pages = \"192--199\",\n    abstract = \"Vision-and-Language Navigation (VLN) encompasses interacting with autonomous vehicles using language and visual input from the perspective of mobility.Most of the previous work in this field focuses on spatial reasoning and the semantic grounding of visual information.However, reasoning based on the actions of pedestrians in the scene is not much considered.In this study, we provide a VLN dataset for destination prediction with action inference to investigate the extent to which current VLN models perform action inference.We introduce a crowd-sourcing process to construct a dataset for this task in two steps: (1) collecting beliefs about the next action for a pedestrian and (2) annotating the destination considering the pedestrian{'}s next action.Our benchmarking results of the models on destination prediction lead us to believe that the models can learn to reason about the effect of the action and the next action on the destination to a certain extent.However, there is still much scope for improvement.\",\n}\n",
    "authors": [
        "Anirudh Kondapally",
        "Kentaro Yamada",
        "Hitomi Yanaka"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-srw.26.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7d25a923-c0ec-5b9b-b67c-3c8be6610e84.pdf",
    "abstract": "Vision-and-Language Navigation (VLN) encompasses interacting with autonomous vehicles using language and visual input from the perspective of mobility.Most of the previous work in this field focuses on spatial reasoning and the semantic grounding of visual information.However, reasoning based on the actions of pedestrians in the scene is not much considered.In this study, we provide a VLN dataset for destination prediction with action inference to investigate the extent to which current VLN models perform action inference.We introduce a crowd-sourcing process to construct a dataset for this task in two steps: (1) collecting beliefs about the next action for a pedestrian and (2) annotating the destination considering the pedestrianâ€™s next action.Our benchmarking results of the models on destination prediction lead us to believe that the models can learn to reason about the effect of the action and the next action on the destination to a certain extent.However, there is still much scope for improvement.",
    "num_pages": 8
}