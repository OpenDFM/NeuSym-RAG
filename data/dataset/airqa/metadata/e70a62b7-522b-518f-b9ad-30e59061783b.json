{
    "uuid": "e70a62b7-522b-518f-b9ad-30e59061783b",
    "title": "Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhou-etal-2023-enhanced,\n    title = \"Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs\",\n    author = \"Zhou, Mingyang  and\n      Fung, Yi  and\n      Chen, Long  and\n      Thomas, Christopher  and\n      Ji, Heng  and\n      Chang, Shih-Fu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.85\",\n    doi = \"10.18653/v1/2023.findings-acl.85\",\n    pages = \"1314--1326\",\n    abstract = \"Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8{\\%} performance gains.\",\n}\n",
    "authors": [
        "Mingyang Zhou",
        "Yi Fung",
        "Long Chen",
        "Christopher Thomas",
        "Heng Ji",
        "Shih-Fu Chang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.85.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e70a62b7-522b-518f-b9ad-30e59061783b.pdf",
    "abstract": "Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains.",
    "num_pages": 13
}