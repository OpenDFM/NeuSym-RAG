{
    "uuid": "bc1923ae-cce6-5286-8e7e-ed306f2352e2",
    "title": "SheffieldVeraAI at SemEval-2023 Task 3: Mono and Multilingual Approaches for News Genre, Topic and Persuasion Technique Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{wu-etal-2023-sheffieldveraai,\n    title = \"{S}heffield{V}era{AI} at {S}em{E}val-2023 Task 3: Mono and Multilingual Approaches for News Genre, Topic and Persuasion Technique Classification\",\n    author = \"Wu, Ben  and\n      Razuvayevskaya, Olesya  and\n      Heppell, Freddy  and\n      Leite, Jo{\\~a}o A.  and\n      Scarton, Carolina  and\n      Bontcheva, Kalina  and\n      Song, Xingyi\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.275\",\n    doi = \"10.18653/v1/2023.semeval-1.275\",\n    pages = \"1995--2008\",\n    abstract = \"This paper describes our approach for SemEval- 2023 Task 3: Detecting the category, the fram- ing, and the persuasion techniques in online news in a multilingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the high- est mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensem- bles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Sub- task 3 (Persuasion Techniques), we trained a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the re- maining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compared monolingual and multilingual approaches, and considered class imbalance techniques.\",\n}\n",
    "authors": [
        "Ben Wu",
        "Olesya Razuvayevskaya",
        "Freddy Heppell",
        "Jo√£o A. Leite",
        "Carolina Scarton",
        "Kalina Bontcheva",
        "Xingyi Song"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.275.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/bc1923ae-cce6-5286-8e7e-ed306f2352e2.pdf",
    "abstract": "This paper describes our approach for SemEval- 2023 Task 3: Detecting the category, the fram- ing, and the persuasion techniques in online news in a multilingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the high- est mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensem- bles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Sub- task 3 (Persuasion Techniques), we trained a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the re- maining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compared monolingual and multilingual approaches, and considered class imbalance techniques.",
    "num_pages": 14
}