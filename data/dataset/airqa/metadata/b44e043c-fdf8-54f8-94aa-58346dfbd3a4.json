{
    "uuid": "b44e043c-fdf8-54f8-94aa-58346dfbd3a4",
    "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{nie-etal-2023-cross,\n    title = \"Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages\",\n    author = {Nie, Ercong  and\n      Liang, Sheng  and\n      Schmid, Helmut  and\n      Sch{\\\"u}tze, Hinrich},\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.528\",\n    doi = \"10.18653/v1/2023.findings-acl.528\",\n    pages = \"8320--8340\",\n    abstract = \"Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1{\\%}) and labeled settings (+16.3{\\%}). PARC also outperforms finetuning by 3.7{\\%}. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.\",\n}\n",
    "authors": [
        "Ercong Nie",
        "Sheng Liang",
        "Helmut Schmid",
        "Hinrich Sch√ºtze"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.528.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b44e043c-fdf8-54f8-94aa-58346dfbd3a4.pdf",
    "abstract": "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1%) and labeled settings (+16.3%). PARC also outperforms finetuning by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.",
    "num_pages": 21
}