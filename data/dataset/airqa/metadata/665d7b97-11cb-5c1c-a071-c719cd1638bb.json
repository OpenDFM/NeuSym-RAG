{
    "uuid": "665d7b97-11cb-5c1c-a071-c719cd1638bb",
    "title": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{gong-etal-2023-prequant,\n    title = \"{P}re{Q}uant: A Task-agnostic Quantization Approach for Pre-trained Language Models\",\n    author = \"Gong, Zhuocheng  and\n      Liu, Jiahao  and\n      Wang, Qifan  and\n      Yang, Yang  and\n      Wang, Jingang  and\n      Wu, Wei  and\n      Xian, Yunsen  and\n      Zhao, Dongyan  and\n      Yan, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.511\",\n    doi = \"10.18653/v1/2023.findings-acl.511\",\n    pages = \"8065--8079\",\n    abstract = \"While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel {``}quantize before fine-tuning{''} framework, PreQuant, that differs from both quantization-aware training and post-training quantization. {pasted macro {`}OUR{'}} is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy.\",\n}\n",
    "authors": [
        "Zhuocheng Gong",
        "Jiahao Liu",
        "Qifan Wang",
        "Yang Yang",
        "Jingang Wang",
        "Wei Wu",
        "Yunsen Xian",
        "Dongyan Zhao",
        "Rui Yan"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.511.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/665d7b97-11cb-5c1c-a071-c719cd1638bb.pdf",
    "abstract": "While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel “quantize before fine-tuning” framework, PreQuant, that differs from both quantization-aware training and post-training quantization. {pasted macro ‘OUR’} is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy.",
    "num_pages": 15
}