{
    "uuid": "8b758f98-ca8d-534e-b225-93979e4a9981",
    "title": "Improving Knowledge Graph Completion with Generative Hard Negative Mining",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{qiao-etal-2023-improving,\n    title = \"Improving Knowledge Graph Completion with Generative Hard Negative Mining\",\n    author = \"Qiao, Zile  and\n      Ye, Wei  and\n      Yu, Dingyao  and\n      Mo, Tong  and\n      Li, Weiping  and\n      Zhang, Shikun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.362\",\n    doi = \"10.18653/v1/2023.findings-acl.362\",\n    pages = \"5866--5878\",\n    abstract = \"Contrastive learning has recently shown great potential to improve text-based knowledge graph completion (KGC). In this paper, we propose to learn a more semantically structured entity representation space in text-based KGC via hard negatives mining. Specifically, we novelly leverage a sequence-to-sequence architecture to generate high-quality hard negatives. These negatives are sampled from the same decoding distributions as the anchor (or correct entity), inherently being semantically close to the anchor and thus enjoying good hardness. A self-information-enhanced contrasting strategy is further incorporated into the Seq2Seq generator to systematically diversify the produced negatives. Extensive experiments on three KGC benchmarks demonstrate the sound hardness and diversity of our generated negatives and the resulting performance superiority on KGC.\",\n}\n",
    "authors": [
        "Zile Qiao",
        "Wei Ye",
        "Dingyao Yu",
        "Tong Mo",
        "Weiping Li",
        "Shikun Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.362.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8b758f98-ca8d-534e-b225-93979e4a9981.pdf",
    "abstract": "Contrastive learning has recently shown great potential to improve text-based knowledge graph completion (KGC). In this paper, we propose to learn a more semantically structured entity representation space in text-based KGC via hard negatives mining. Specifically, we novelly leverage a sequence-to-sequence architecture to generate high-quality hard negatives. These negatives are sampled from the same decoding distributions as the anchor (or correct entity), inherently being semantically close to the anchor and thus enjoying good hardness. A self-information-enhanced contrasting strategy is further incorporated into the Seq2Seq generator to systematically diversify the produced negatives. Extensive experiments on three KGC benchmarks demonstrate the sound hardness and diversity of our generated negatives and the resulting performance superiority on KGC.",
    "num_pages": 13
}