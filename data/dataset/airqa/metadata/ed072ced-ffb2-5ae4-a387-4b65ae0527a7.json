{
    "uuid": "ed072ced-ffb2-5ae4-a387-4b65ae0527a7",
    "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhou-etal-2023-wyweb,\n    title = \"{WYWEB}: A {NLP} Evaluation Benchmark For Classical {C}hinese\",\n    author = \"Zhou, Bo  and\n      Chen, Qianglong  and\n      Wang, Tianyu  and\n      Zhong, Xiaomi  and\n      Zhang, Yin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.204\",\n    doi = \"10.18653/v1/2023.findings-acl.204\",\n    pages = \"3294--3319\",\n    abstract = \"To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as ``wen yan wen (æè¨æ)'', which has a rich history spanning thousands of years and holds significant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is \\url{https://github.com/baudzhou/WYWEB}.\",\n}\n",
    "authors": [
        "Bo Zhou",
        "Qianglong Chen",
        "Tianyu Wang",
        "Xiaomi Zhong",
        "Yin Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.204.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ed072ced-ffb2-5ae4-a387-4b65ae0527a7.pdf",
    "abstract": "To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as \"wen yan wen (文言文)\", which has a rich history spanning thousands of years and holds significant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB.",
    "num_pages": 26
}