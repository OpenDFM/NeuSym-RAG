{
    "uuid": "ab7d8838-6561-5d56-871b-1d28363f7465",
    "title": "Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "The 7th Workshop on Online Abuse and Harms (WOAH)",
    "bibtex": "@inproceedings{goldzycher-etal-2023-evaluating,\n    title = \"Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data\",\n    author = \"Goldzycher, Janis  and\n      Preisig, Moritz  and\n      Amrhein, Chantal  and\n      Schneider, Gerold\",\n    editor = {Chung, Yi-ling  and\n      R{{\\textbackslash}\"ottger}, Paul  and\n      Nozza, Debora  and\n      Talat, Zeerak  and\n      Mostafazadeh Davani, Aida},\n    booktitle = \"The 7th Workshop on Online Abuse and Harms (WOAH)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.woah-1.19\",\n    doi = \"10.18653/v1/2023.woah-1.19\",\n    pages = \"187--201\",\n    abstract = \"Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.\",\n}\n",
    "authors": [
        "Janis Goldzycher",
        "Moritz Preisig",
        "Chantal Amrhein",
        "Gerold Schneider"
    ],
    "pdf_url": "https://aclanthology.org/2023.woah-1.19.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ab7d8838-6561-5d56-871b-1d28363f7465.pdf",
    "abstract": "Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.",
    "num_pages": 15
}