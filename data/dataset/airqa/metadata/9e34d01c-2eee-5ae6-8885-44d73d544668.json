{
    "uuid": "9e34d01c-2eee-5ae6-8885-44d73d544668",
    "title": "Open Grounded Planning: Challenges and Benchmark Construction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{guo-etal-2024-open,\n    title = \"Open Grounded Planning: Challenges and Benchmark Construction\",\n    author = \"Guo, Shiguang  and\n      Deng, Ziliang  and\n      Lin, Hongyu  and\n      Lu, Yaojie  and\n      Han, Xianpei  and\n      Sun, Le\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.272\",\n    doi = \"10.18653/v1/2024.acl-long.272\",\n    pages = \"4982--5003\",\n    abstract = \"The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task{--}open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.\",\n}\n",
    "authors": [
        "Shiguang Guo",
        "Ziliang Deng",
        "Hongyu Lin",
        "Yaojie Lu",
        "Xianpei Han",
        "Le Sun"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.272.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9e34d01c-2eee-5ae6-8885-44d73d544668.pdf",
    "abstract": "The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning taskâ€“open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.",
    "num_pages": 22
}