{
    "uuid": "f4911431-3d21-53cf-98b8-e22ec9f728d9",
    "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhong-etal-2023-revisiting,\n    title = \"Revisiting Token Dropping Strategy in Efficient {BERT} Pretraining\",\n    author = \"Zhong, Qihuang  and\n      Ding, Liang  and\n      Liu, Juhua  and\n      Liu, Xuebo  and\n      Zhang, Min  and\n      Du, Bo  and\n      Tao, Dacheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.579\",\n    doi = \"10.18653/v1/2023.acl-long.579\",\n    pages = \"10391--10405\",\n    abstract = \"Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57{\\%} of pretraining time and brings up to +1.56{\\%} average improvement over the vanilla token dropping.\",\n}\n",
    "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Xuebo Liu",
        "Min Zhang",
        "Bo Du",
        "Dacheng Tao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.579.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f4911431-3d21-53cf-98b8-e22ec9f728d9.pdf",
    "abstract": "Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings up to +1.56% average improvement over the vanilla token dropping.",
    "num_pages": 15
}