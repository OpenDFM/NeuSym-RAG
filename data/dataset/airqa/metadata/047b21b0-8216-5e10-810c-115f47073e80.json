{
    "uuid": "047b21b0-8216-5e10-810c-115f47073e80",
    "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{lee-etal-2024-llm2llm,\n    title = \"{LLM}2{LLM}: Boosting {LLM}s with Novel Iterative Data Enhancement\",\n    author = \"Lee, Nicholas  and\n      Wattanawong, Thanakul  and\n      Kim, Sehoon  and\n      Mangalam, Karttikeya  and\n      Shen, Sheng  and\n      Anumanchipalli, Gopala  and\n      Mahoney, Michael  and\n      Keutzer, Kurt  and\n      Gholami, Amir\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.388\",\n    doi = \"10.18653/v1/2024.findings-acl.388\",\n    pages = \"6498--6526\",\n    abstract = \"Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2{\\%} on the GSM8K dataset, 32.6{\\%} on CaseHOLD, 32.0{\\%} on SNIPS, 52.6{\\%} on TREC and 39.8{\\%} on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https://github.com/SqueezeAILab/LLM2LLM.\",\n}\n",
    "authors": [
        "Nicholas Lee",
        "Thanakul Wattanawong",
        "Sehoon Kim",
        "Karttikeya Mangalam",
        "Sheng Shen",
        "Gopala Anumanchipalli",
        "Michael Mahoney",
        "Kurt Keutzer",
        "Amir Gholami"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.388.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/047b21b0-8216-5e10-810c-115f47073e80.pdf",
    "abstract": "Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them into the dataset to focus on more challenging examples for the LLM. Our results show that LLM2LLM significantly enhances the performance of LLMs in the low-data regime, outperforming both traditional fine-tuning and other data augmentation baselines. LLM2LLM reduces the dependence on labor-intensive data curation and paves the way for more scalable and performant LLM solutions, allowing us to tackle data-constrained domains and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular fine-tuning in the low-data regime using a Llama-2-7B student model. Our code is available at https://github.com/SqueezeAILab/LLM2LLM.",
    "num_pages": 29
}