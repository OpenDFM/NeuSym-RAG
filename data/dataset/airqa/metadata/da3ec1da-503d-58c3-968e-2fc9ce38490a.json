{
    "uuid": "da3ec1da-503d-58c3-968e-2fc9ce38490a",
    "title": "Task-aware Retrieval with Instructions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{asai-etal-2023-task,\n    title = \"Task-aware Retrieval with Instructions\",\n    author = \"Asai, Akari  and\n      Schick, Timo  and\n      Lewis, Patrick  and\n      Chen, Xilun  and\n      Izacard, Gautier  and\n      Riedel, Sebastian  and\n      Hajishirzi, Hannaneh  and\n      Yih, Wen-tau\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.225\",\n    doi = \"10.18653/v1/2023.findings-acl.225\",\n    pages = \"3650--3675\",\n    abstract = \"We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. TART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. TART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions.\",\n}\n",
    "authors": [
        "Akari Asai",
        "Timo Schick",
        "Patrick Lewis",
        "Xilun Chen",
        "Gautier Izacard",
        "Sebastian Riedel",
        "Hannaneh Hajishirzi",
        "Wen-tau Yih"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.225.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/da3ec1da-503d-58c3-968e-2fc9ce38490a.pdf",
    "abstract": "We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. TART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. TART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions.",
    "num_pages": 26
}