{
    "uuid": "ac141d72-cee6-57e3-866d-e9a61481a240",
    "title": "What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhu-etal-2023-knowledge,\n    title = \"What Knowledge Is Needed? Towards Explainable Memory for k{NN}-{MT} Domain Adaptation\",\n    author = \"Zhu, Wenhao  and\n      Huang, Shujian  and\n      Lv, Yunzhe  and\n      Zheng, Xin  and\n      Chen, Jiajun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.177\",\n    doi = \"10.18653/v1/2023.findings-acl.177\",\n    pages = \"2824--2836\",\n    abstract = \"kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single entry and for a given neighborhood. Empirical study shows that our investigation successfully finds the conditions where the NMT model could easily fail and need related knowledge. Experiments on six diverse target domains and two language-pairs show that pruning according to local correctness brings a light and more explainable memory for kNN-MT domain adaptation.\",\n}\n",
    "authors": [
        "Wenhao Zhu",
        "Shujian Huang",
        "Yunzhe Lv",
        "Xin Zheng",
        "Jiajun Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.177.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ac141d72-cee6-57e3-866d-e9a61481a240.pdf",
    "abstract": "kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single entry and for a given neighborhood. Empirical study shows that our investigation successfully finds the conditions where the NMT model could easily fail and need related knowledge. Experiments on six diverse target domains and two language-pairs show that pruning according to local correctness brings a light and more explainable memory for kNN-MT domain adaptation.",
    "num_pages": 13
}