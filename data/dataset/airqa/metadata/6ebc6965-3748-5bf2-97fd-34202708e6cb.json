{
    "uuid": "6ebc6965-3748-5bf2-97fd-34202708e6cb",
    "title": "Representation of Lexical Stylistic Features in Language Modelsâ€™ Embedding Space",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{lyu-etal-2023-representation,\n    title = \"Representation of Lexical Stylistic Features in Language Models{'} Embedding Space\",\n    author = \"Lyu, Qing  and\n      Apidianaki, Marianna  and\n      Callison-burch, Chris\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.32\",\n    doi = \"10.18653/v1/2023.starsem-1.32\",\n    pages = \"370--387\",\n    abstract = \"The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected to some extent using techniques like standardization.\",\n}\n",
    "authors": [
        "Qing Lyu",
        "Marianna Apidianaki",
        "Chris Callison-burch"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/6ebc6965-3748-5bf2-97fd-34202708e6cb.pdf",
    "abstract": "The representation space of pretrained Language Models (LMs) encodes rich information about words and their relationships (e.g., similarity, hypernymy, polysemy) as well as abstract semantic notions (e.g., intensity). In this paper, we demonstrate that lexical stylistic notions such as complexity, formality, and figurativeness, can also be identified in this space. We show that it is possible to derive a vector representation for each of these stylistic notions from only a small number of seed pairs. Using these vectors, we can characterize new texts in terms of these dimensions by performing simple calculations in the corresponding embedding space. We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences. The lower performance of contextualized representations at the word level is partially attributable to the anisotropy of their vector space, which can be corrected to some extent using techniques like standardization.",
    "num_pages": 18
}