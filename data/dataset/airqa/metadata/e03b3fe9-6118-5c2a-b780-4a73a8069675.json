{
    "uuid": "e03b3fe9-6118-5c2a-b780-4a73a8069675",
    "title": "Rethinking Negative Instances for Generative Named Entity Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ding-etal-2024-rethinking,\n    title = \"Rethinking Negative Instances for Generative Named Entity Recognition\",\n    author = \"Ding, Yuyang  and\n      Li, Juntao  and\n      Wang, Pinzheng  and\n      Tang, Zecheng  and\n      Bowen, Yan  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.206\",\n    doi = \"10.18653/v1/2024.findings-acl.206\",\n    pages = \"3461--3475\",\n    abstract = \"Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce an efficient longest common subsequence (LCS) matching algorithm, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation illustrates our system{'}s superiority, surpassing state-of-the-art (SoTA) methods by 9 $F_1$ score in zero-shot evaluation.\",\n}\n",
    "authors": [
        "Yuyang Ding",
        "Juntao Li",
        "Pinzheng Wang",
        "Zecheng Tang",
        "Yan Bowen",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.206.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e03b3fe9-6118-5c2a-b780-4a73a8069675.pdf",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for generalizing in unseen tasks. In the Named Entity Recognition (NER) task, recent advancements have seen the remarkable improvement of LLMs in a broad range of entity domains via instruction tuning, by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce an efficient longest common subsequence (LCS) matching algorithm, which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative NER system that shows improved zero-shot performance across unseen entity domains. Our comprehensive evaluation illustrates our systemâ€™s superiority, surpassing state-of-the-art (SoTA) methods by 9 F1 score in zero-shot evaluation.",
    "num_pages": 15
}