{
    "uuid": "093512cf-0c46-54c2-a50c-0a302e99eff3",
    "title": "AutoRef: Generating Refinements of Reviews Given Guidelines",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)",
    "bibtex": "@inproceedings{chitnis-etal-2024-tt,\n    title = \"AutoRef: Generating Refinements of Reviews Given Guidelines\",\n    author = \"Chitnis, Soham  and\n      Patwardhan, Manasi  and\n      Srinivasan, Ashwin  and\n      Verlekar, Tanmay Tulsidas  and\n      Vig, Lovekesh  and\n      Shroff, Gautam\",\n    editor = \"Ghosal, Tirthankar  and\n      Singh, Amanpreet  and\n      Waard, Anita  and\n      Mayr, Philipp  and\n      Naik, Aakanksha  and\n      Weller, Orion  and\n      Lee, Yoonjoo  and\n      Shen, Shannon  and\n      Qin, Yanxia\",\n    booktitle = \"Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sdp-1.17\",\n    pages = \"175--190\",\n    abstract = \"When examining reviews of research papers, we can distinguish between two hypothetical referees: the maximally lenient referee who accepts any paper with a vacuous review and the maximally strict one who rejects any paper with an overly pedantic review. Clearly, both are of no practical value. Our interest is in a referee who makes a balanced judgement and provides a review abiding by the guidelines. In this paper, we present a case study of automatic correction of an existing machine-generated or human review. The ${\\tt{AutoRef}\\ }$ system implements an iterative approach that progressively {``}refines{''} a review by attempting to make it more compliant with pre-defined requirements of a {``}good{''} review. It implements the following steps: (1) Translate the review requirements into a specification in natural language, of {``}yes/no{''} questions; (2) Given a $(paper,review)$ pair, extract answers to the questions; (3) Use the results in (2) to generate a new review; and (4) Return to Step (2) with the paper and the new review. Here, (2) and (3) are implemented by large language model (LLM) based agents. We present a case study using papers and reviews made available for the International Conference on Learning Representations (ICLR). Our initial empirical results suggest that ${\\tt{AutoRef}\\ }$ progressively improves the compliance of the generated reviews to the specification. Currently designed specification makes ${\\tt{AutoRef}\\ }$ progressively generate reviews which are stricter, making the decisions more inclined towards {``}rejections{''}. This demonstrates the applicability of {\\$}AutoRef {\\$} for: (1) The progressive correction of overly lenient reviews, being useful for referees and meta-reviewers; and (2) The generation of progressively stricter reviews for a paper, starting from a vacuous review ({``}Great paper. Accept.{''}), facilitating authors when trying to assess weaknesses in their papers.\",\n}\n",
    "authors": [
        "Soham Chitnis",
        "Manasi Patwardhan",
        "Ashwin Srinivasan",
        "Tanmay Tulsidas Verlekar",
        "Lovekesh Vig",
        "Gautam Shroff"
    ],
    "pdf_url": "https://aclanthology.org/2024.sdp-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/093512cf-0c46-54c2-a50c-0a302e99eff3.pdf",
    "abstract": "When examining reviews of research papers, we can distinguish between two hypothetical referees: the maximally lenient referee who accepts any paper with a vacuous review and the maximally strict one who rejects any paper with an overly pedantic review. Clearly, both are of no practical value. Our interest is in a referee who makes a balanced judgement and provides a review abiding by the guidelines. In this paper, we present a case study of automatic correction of an existing machine-generated or human review. The \\tt{AutoRef}\\ system implements an iterative approach that progressively “refines” a review by attempting to make it more compliant with pre-defined requirements of a “good” review. It implements the following steps: (1) Translate the review requirements into a specification in natural language, of “yes/no” questions; (2) Given a (paper,review) pair, extract answers to the questions; (3) Use the results in (2) to generate a new review; and (4) Return to Step (2) with the paper and the new review. Here, (2) and (3) are implemented by large language model (LLM) based agents. We present a case study using papers and reviews made available for the International Conference on Learning Representations (ICLR). Our initial empirical results suggest that \\tt{AutoRef}\\ progressively improves the compliance of the generated reviews to the specification. Currently designed specification makes \\tt{AutoRef}\\ progressively generate reviews which are stricter, making the decisions more inclined towards “rejections”. This demonstrates the applicability of $AutoRef $ for: (1) The progressive correction of overly lenient reviews, being useful for referees and meta-reviewers; and (2) The generation of progressively stricter reviews for a paper, starting from a vacuous review (“Great paper. Accept.”), facilitating authors when trying to assess weaknesses in their papers.",
    "num_pages": 16
}