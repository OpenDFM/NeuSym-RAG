{
    "uuid": "103aec29-bab8-5607-a484-c554b3665771",
    "title": "Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{jain-etal-2023-language,\n    title = \"Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation\",\n    author = \"Jain, Aditi  and\n      Kambhatla, Nishant  and\n      Sarkar, Anoop\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.32\",\n    doi = \"10.18653/v1/2023.iwslt-1.32\",\n    pages = \"341--356\",\n    abstract = \"The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).\",\n}\n",
    "authors": [
        "Aditi Jain",
        "Nishant Kambhatla",
        "Anoop Sarkar"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/103aec29-bab8-5607-a484-c554b3665771.pdf",
    "abstract": "The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).",
    "num_pages": 16
}