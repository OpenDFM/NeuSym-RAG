{
    "uuid": "3e3ef280-f483-5adf-b3e0-13ce4a363d59",
    "title": "Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-logical,\n    title = \"Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models\",\n    author = \"Wang, Borui  and\n      Huang, Qiuyuan  and\n      Deb, Budhaditya  and\n      Halfaker, Aaron  and\n      Shao, Liqun  and\n      McDuff, Daniel  and\n      Awadallah, Ahmed Hassan  and\n      Radev, Dragomir  and\n      Gao, Jianfeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.111\",\n    doi = \"10.18653/v1/2023.findings-acl.111\",\n    pages = \"1762--1773\",\n    abstract = \"Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP models{'} performance on many important NLU and NLG tasks. Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences. To overcome such limitations, in this paper we first propose a novel approach to construct logic-aware input embeddings for transformer language models through a combination of logic detection, logic mapping and hierarchical logical projections, and then develop a corresponding new modeling paradigm that can upgrade existing transformer language models into logical transformers to boost their performance on different NLU and NLG tasks. Our empirical experiments on four important and challenging NLU and NLG tasks demonstrate that our proposed logical transformer language models can achieve superior performance over their baseline transformer models through a deeper understanding of the logical structures of texts.\",\n}\n",
    "authors": [
        "Borui Wang",
        "Qiuyuan Huang",
        "Budhaditya Deb",
        "Aaron Halfaker",
        "Liqun Shao",
        "Daniel McDuff",
        "Ahmed Hassan Awadallah",
        "Dragomir Radev",
        "Jianfeng Gao"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.111.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3e3ef280-f483-5adf-b3e0-13ce4a363d59.pdf",
    "abstract": "Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP modelsâ€™ performance on many important NLU and NLG tasks. Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences. To overcome such limitations, in this paper we first propose a novel approach to construct logic-aware input embeddings for transformer language models through a combination of logic detection, logic mapping and hierarchical logical projections, and then develop a corresponding new modeling paradigm that can upgrade existing transformer language models into logical transformers to boost their performance on different NLU and NLG tasks. Our empirical experiments on four important and challenging NLU and NLG tasks demonstrate that our proposed logical transformer language models can achieve superior performance over their baseline transformer models through a deeper understanding of the logical structures of texts.",
    "num_pages": 12
}