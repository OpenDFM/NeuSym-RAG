{
    "uuid": "3097e545-7cad-5021-9d16-57938472fc77",
    "title": "Attribute First, then Generate: Locally-attributable Grounded Text Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{slobodkin-etal-2024-attribute,\n    title = \"Attribute First, then Generate: Locally-attributable Grounded Text Generation\",\n    author = \"Slobodkin, Aviv  and\n      Hirsch, Eran  and\n      Cattan, Arie  and\n      Schuster, Tal  and\n      Dagan, Ido\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.182\",\n    doi = \"10.18653/v1/2024.acl-long.182\",\n    pages = \"3309--3344\",\n    abstract = \"Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named {``}Attribute First, then Generate{``}, breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments ({``}select first{``}) and then conditioning the generation process on them ({``}then generate{``}), we ensure these segments also act as the output{'}s fine-grained attributions ({``}select{``} becomes {``}attribute{``}). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.\",\n}\n",
    "authors": [
        "Aviv Slobodkin",
        "Eran Hirsch",
        "Arie Cattan",
        "Tal Schuster",
        "Ido Dagan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.182.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3097e545-7cad-5021-9d16-57938472fc77.pdf",
    "abstract": "Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named “Attribute First, then Generate“, breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (“select first“) and then conditioning the generation process on them (“then generate“), we ensure these segments also act as the output’s fine-grained attributions (“select“ becomes “attribute“). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.",
    "num_pages": 36
}