{
    "uuid": "b5225826-9fd9-54f7-8259-161afed6237b",
    "title": "LLaMA-Based Models for Aspect-Based Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{smid-etal-2024-llama,\n    title = \"{LL}a{MA}-Based Models for Aspect-Based Sentiment Analysis\",\n    author = \"{\\v{S}}m{\\'\\i}d, Jakub  and\n      Priban, Pavel  and\n      Kral, Pavel\",\n    editor = \"De Clercq, Orph{\\'e}e  and\n      Barriere, Valentin  and\n      Barnes, Jeremy  and\n      Klinger, Roman  and\n      Sedoc, Jo{\\~a}o  and\n      Tafreshi, Shabnam\",\n    booktitle = \"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.wassa-1.6\",\n    doi = \"10.18653/v1/2024.wassa-1.6\",\n    pages = \"63--70\",\n    abstract = \"While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca 2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.\",\n}\n",
    "authors": [
        "Jakub Šmíd",
        "Pavel Priban",
        "Pavel Kral"
    ],
    "pdf_url": "https://aclanthology.org/2024.wassa-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b5225826-9fd9-54f7-8259-161afed6237b.pdf",
    "abstract": "While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca 2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.",
    "num_pages": 8
}