{
    "uuid": "bcdd2986-5a1d-5cd5-afe9-96e9ed90842d",
    "title": "Prompt-based Zero-shot Text Classification with Conceptual Knowledge",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{wang-etal-2023-prompt,\n    title = \"Prompt-based Zero-shot Text Classification with Conceptual Knowledge\",\n    author = \"Wang, Yuqi  and\n      Wang, Wei  and\n      Chen, Qi  and\n      Huang, Kaizhu  and\n      Nguyen, Anh  and\n      De, Suparna\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.4\",\n    doi = \"10.18653/v1/2023.acl-srw.4\",\n    pages = \"30--38\",\n    abstract = \"In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings.\",\n}\n",
    "authors": [
        "Yuqi Wang",
        "Wei Wang",
        "Qi Chen",
        "Kaizhu Huang",
        "Anh Nguyen",
        "Suparna De"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.4.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/bcdd2986-5a1d-5cd5-afe9-96e9ed90842d.pdf",
    "abstract": "In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings.",
    "num_pages": 9
}