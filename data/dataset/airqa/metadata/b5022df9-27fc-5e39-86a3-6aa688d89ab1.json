{
    "uuid": "b5022df9-27fc-5e39-86a3-6aa688d89ab1",
    "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-self-supervised,\n    title = \"Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion\",\n    author = \"Li, Ruiqi  and\n      Huang, Rongjie  and\n      Wang, Yongqi  and\n      Hong, Zhiqing  and\n      Zhao, Zhou\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.585\",\n    doi = \"10.18653/v1/2024.findings-acl.585\",\n    pages = \"9819--9831\",\n    abstract = \"Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model.We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.\",\n}\n",
    "authors": [
        "Ruiqi Li",
        "Rongjie Huang",
        "Yongqi Wang",
        "Zhiqing Hong",
        "Zhou Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.585.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b5022df9-27fc-5e39-86a3-6aa688d89ab1.pdf",
    "abstract": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model.We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io.",
    "num_pages": 13
}