{
    "uuid": "989ce611-2396-519c-b587-9e8b2f0ce48b",
    "title": "Efficient and Interpretable Compressive Text Summarisation with Unsupervised Dual-Agent Reinforcement Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{tang-etal-2023-efficient,\n    title = \"Efficient and Interpretable Compressive Text Summarisation with Unsupervised Dual-Agent Reinforcement Learning\",\n    author = \"Tang, Peggy  and\n      Gao, Junbin  and\n      Zhang, Lei  and\n      Wang, Zhiyong\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.17\",\n    doi = \"10.18653/v1/2023.sustainlp-1.17\",\n    pages = \"227--238\",\n}\n",
    "authors": [
        "Peggy Tang",
        "Junbin Gao",
        "Lei Zhang",
        "Zhiyong Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/989ce611-2396-519c-b587-9e8b2f0ce48b.pdf",
    "abstract": "Recently, compressive text summarisation offers a balance between the conciseness issue of extractive summarisation and the factual hallucination issue of abstractive summarisation. However, most existing compressive summarisation methods are supervised, relying on the expensive effort of creating a new training dataset with corresponding compressive summaries. In this paper, we propose an efﬁcient and interpretable compressive summarisation method that utilises unsupervised dual-agent reinforcement learning to optimise a summary’s semantic coverage and ﬂuency by simulating human judgment on summarisation quality. Our model consists of an extractor agent and a compressor agent, and both agents have a multi-head attentional pointerbased structure. The extractor agent ﬁrst chooses salient sentences from a document, and then the compressor agent compresses these extracted sentences by selecting salient words to form a summary without using reference summaries to compute the summary reward. To our best knowledge, this is the ﬁrst work on unsupervised compressive summarisation. Experimental results on three widely used datasets (e.g., Newsroom, CNN/DM, and XSum) show that our model achieves promising performance and a signiﬁcant improvement on Newsroom in terms of the ROUGE metric, as well as interpretability of semantic coverage of summarisation results. 1",
    "num_pages": 12
}