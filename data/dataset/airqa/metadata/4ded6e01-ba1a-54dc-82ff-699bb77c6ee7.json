{
    "uuid": "4ded6e01-ba1a-54dc-82ff-699bb77c6ee7",
    "title": "PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{qi-etal-2023-pragmaticqa,\n    title = \"{P}ragmati{CQA}: A Dataset for Pragmatic Question Answering in Conversations\",\n    author = \"Qi, Peng  and\n      Du, Nina  and\n      Manning, Christopher  and\n      Huang, Jing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.385\",\n    doi = \"10.18653/v1/2023.findings-acl.385\",\n    pages = \"6175--6191\",\n    abstract = \"Pragmatic reasoning about another speaker{'}s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks {``}do you have a minute?{''}, instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics. We designed innovative crowdsourcing mechanisms for interest-based and task-driven data collection to address the common issue of incentive misalignment between crowdworkers and potential users. To compare computational models{'} capability at pragmatic reasoning, we also propose several quantitative metrics to evaluate question answering systems on PragmatiCQA. We find that state-of-the-art systems still struggle to perform human-like pragmatic reasoning, and highlight their limitations for future research.\",\n}\n",
    "authors": [
        "Peng Qi",
        "Nina Du",
        "Christopher Manning",
        "Jing Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.385.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4ded6e01-ba1a-54dc-82ff-699bb77c6ee7.pdf",
    "abstract": "Pragmatic reasoning about another speaker’s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks “do you have a minute?”, instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics. We designed innovative crowdsourcing mechanisms for interest-based and task-driven data collection to address the common issue of incentive misalignment between crowdworkers and potential users. To compare computational models’ capability at pragmatic reasoning, we also propose several quantitative metrics to evaluate question answering systems on PragmatiCQA. We find that state-of-the-art systems still struggle to perform human-like pragmatic reasoning, and highlight their limitations for future research.",
    "num_pages": 17
}