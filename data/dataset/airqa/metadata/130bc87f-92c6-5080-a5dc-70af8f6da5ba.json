{
    "uuid": "130bc87f-92c6-5080-a5dc-70af8f6da5ba",
    "title": "LLM Factoscope: Uncovering LLMs’ Factual Discernment through Measuring Inner States",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{he-etal-2024-llm,\n    title = \"{LLM} Factoscope: Uncovering {LLM}s{'} Factual Discernment through Measuring Inner States\",\n    author = \"He, Jinwen  and\n      Gong, Yujia  and\n      Lin, Zijin  and\n      Wei, Cheng{'}an  and\n      Zhao, Yue  and\n      Chen, Kai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.608\",\n    doi = \"10.18653/v1/2024.findings-acl.608\",\n    pages = \"10218--10230\",\n    abstract = \"Large Language Models (LLMs) have revolutionized various domains with extensive knowledge and creative capabilities. However, a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality. This phenomenon is particularly concerning in sensitive applications such as medical consultation and legal advice, where accuracy is paramount. Inspired by human lie detectors using physiological responses, we introduce the LLM Factoscope, a novel Siamese network-based model that leverages the inner states of LLMs for factual detection. Our investigation reveals distinguishable patterns in LLMs{'} inner states when generating factual versus non-factual content. We demonstrate its effectiveness across various architectures, achieving over 96{\\%} accuracy on our custom-collected factual detection dataset. Our work opens a new avenue for utilizing LLMs{'} inner states for factual detection and encourages further exploration into LLMs{'} inner workings for enhanced reliability and transparency.\",\n}\n",
    "authors": [
        "Jinwen He",
        "Yujia Gong",
        "Zijin Lin",
        "Cheng’an Wei",
        "Yue Zhao",
        "Kai Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.608.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/130bc87f-92c6-5080-a5dc-70af8f6da5ba.pdf",
    "abstract": "Large Language Models (LLMs) have revolutionized various domains with extensive knowledge and creative capabilities. However, a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality. This phenomenon is particularly concerning in sensitive applications such as medical consultation and legal advice, where accuracy is paramount. Inspired by human lie detectors using physiological responses, we introduce the LLM Factoscope, a novel Siamese network-based model that leverages the inner states of LLMs for factual detection. Our investigation reveals distinguishable patterns in LLMs’ inner states when generating factual versus non-factual content. We demonstrate its effectiveness across various architectures, achieving over 96% accuracy on our custom-collected factual detection dataset. Our work opens a new avenue for utilizing LLMs’ inner states for factual detection and encourages further exploration into LLMs’ inner workings for enhanced reliability and transparency.",
    "num_pages": 13
}