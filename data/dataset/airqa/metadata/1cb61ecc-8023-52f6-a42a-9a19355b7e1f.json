{
    "uuid": "1cb61ecc-8023-52f6-a42a-9a19355b7e1f",
    "title": "Demonstration Augmentation for Zero-shot In-context Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{su-etal-2024-demonstration,\n    title = \"Demonstration Augmentation for Zero-shot In-context Learning\",\n    author = \"Su, Yi  and\n      Tai, Yunpeng  and\n      Ji, Yixin  and\n      Li, Juntao  and\n      Bowen, Yan  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.846\",\n    doi = \"10.18653/v1/2024.findings-acl.846\",\n    pages = \"14232--14244\",\n    abstract = \"Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates.However, many studies have highlighted that the model{'}s performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries.Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs.In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model{'}s reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming.To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model{'}s previously predicted historical samples as demonstrations for subsequent ones.DAIL brings no additional inference cost and does not rely on the model{'}s generative capabilities.Our experiments reveal that DAIL can significantly improve the model{'}s performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.\",\n}\n",
    "authors": [
        "Yi Su",
        "Yunpeng Tai",
        "Yixin Ji",
        "Juntao Li",
        "Yan Bowen",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.846.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1cb61ecc-8023-52f6-a42a-9a19355b7e1f.pdf",
    "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates.However, many studies have highlighted that the model’s performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries.Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs.In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model’s reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming.To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model’s previously predicted historical samples as demonstrations for subsequent ones.DAIL brings no additional inference cost and does not rely on the model’s generative capabilities.Our experiments reveal that DAIL can significantly improve the model’s performance over direct zero-shot inference and can even outperform few-shot ICL without any external information.",
    "num_pages": 13
}