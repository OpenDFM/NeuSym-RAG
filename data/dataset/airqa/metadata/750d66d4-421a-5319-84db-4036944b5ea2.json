{
    "uuid": "750d66d4-421a-5319-84db-4036944b5ea2",
    "title": "Lessons on Parameter Sharing across Layers in Transformers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{takase-kiyono-2023-lessons,\n    title = \"Lessons on Parameter Sharing across Layers in Transformers\",\n    author = \"Takase, Sho  and\n      Kiyono, Shun\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.5\",\n    doi = \"10.18653/v1/2023.sustainlp-1.5\",\n    pages = \"78--90\",\n}\n",
    "authors": [
        "Sho Takase",
        "Shun Kiyono"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/750d66d4-421a-5319-84db-4036944b5ea2.pdf",
    "abstract": "We propose a novel parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares the parameters of one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to improve the efficiency. We propose three strategies: SEQUENCE, CYCLE, and CYCLE (REV) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in terms of the parameter size and computational time in the machine translation task. We also demonstrate that the proposed strategies are effective in the configuration where we use many training data such as the recent WMT competition. Moreover, we indicate that the proposed strategies are also more efficient than the previous approach (Dehghani et al., 2019) on automatic speech recognition and language modeling tasks.",
    "num_pages": 13
}