{
    "uuid": "041131b5-f598-5019-a6a0-df1cc3837460",
    "title": "Computational Expressivity of Neural Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 5: Tutorial Abstracts)",
    "bibtex": "@inproceedings{butoi-etal-2024-computational,\n    title = \"Computational Expressivity of Neural Language Models\",\n    author = \"Butoi, Alexandra  and\n      Cotterell, Ryan  and\n      Svete, Anej\",\n    editor = \"Chiruzzo, Luis  and\n      Lee, Hung-yi  and\n      Ribeiro, Leonardo\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 5: Tutorial Abstracts)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-tutorials.3\",\n    doi = \"10.18653/v1/2024.acl-tutorials.3\",\n    pages = \"5--5\",\n    abstract = \"Language models (LMs) are currently at the forefront of NLP research due to their remarkable versatility across diverse tasks. However, a large gap exists between their observed capabilities and the explanations proposed by established formal machinery. To motivate a better theoretical characterization of LMs{'} abilities and limitations, this tutorial aims to provide a comprehensive introduction to a specific framework for formal analysis of modern LMs using tools from formal language theory (FLT). We present how tools from FLT can be useful in understanding the inner workings and predicting the capabilities of modern neural LM architectures. We will cover recent results using FLT to make precise and practically relevant statements about LMs based on recurrent neural networks and transformers by relating them to formal devices such as finite-state automata, Turing machines, and analog circuits. Altogether, the results covered in this tutorial will allow us to make precise statements and explanations about the observed as well as predicted behaviors of LMs, as well as provide theoretically motivated suggestions on the aspects of the architectures that could be improved.\",\n}\n",
    "authors": [
        "Alexandra Butoi",
        "Ryan Cotterell",
        "Anej Svete"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-tutorials.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/041131b5-f598-5019-a6a0-df1cc3837460.pdf",
    "abstract": "Language models (LMs) are currently at the forefront of NLP research due to their remarkable versatility across diverse tasks. However, a large gap exists between their observed capabilities and the explanations proposed by established formal machinery. To motivate a better theoretical characterization of LMsâ€™ abilities and limitations, this tutorial aims to provide a comprehensive introduction to a specific framework for formal analysis of modern LMs using tools from formal language theory (FLT). We present how tools from FLT can be useful in understanding the inner workings and predicting the capabilities of modern neural LM architectures. We will cover recent results using FLT to make precise and practically relevant statements about LMs based on recurrent neural networks and transformers by relating them to formal devices such as finite-state automata, Turing machines, and analog circuits. Altogether, the results covered in this tutorial will allow us to make precise statements and explanations about the observed as well as predicted behaviors of LMs, as well as provide theoretically motivated suggestions on the aspects of the architectures that could be improved.",
    "num_pages": 1
}