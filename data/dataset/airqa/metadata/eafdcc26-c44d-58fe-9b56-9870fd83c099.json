{
    "uuid": "eafdcc26-c44d-58fe-9b56-9870fd83c099",
    "title": "From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)",
    "bibtex": "@inproceedings{chen-etal-2024-good,\n    title = \"From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting\",\n    author = \"Chen, Nuo  and\n      Li, Hongguang  and\n      Wang, Baoyuan  and\n      Li, Jia\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Lipkin, Ben  and\n      Neves Ribeiro, Danilo  and\n      Wong, Lionel  and\n      Ye, Xi  and\n      Zhao, Wenting\",\n    booktitle = \"Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlrse-1.7\",\n    pages = \"64--79\",\n    abstract = \"This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMR-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMR-TIP follows the {``}From Good to Great{''} concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an interleaved manner. Experimental analysis shows that IMR-TIP achieves enhanced mathematical capabilities and outperforms traditional LLMs and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning tasks. For instance, IMR-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard from 56.0{\\%} to 65.2 {\\%}.\",\n}\n",
    "authors": [
        "Nuo Chen",
        "Hongguang Li",
        "Baoyuan Wang",
        "Jia Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlrse-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/eafdcc26-c44d-58fe-9b56-9870fd83c099.pdf",
    "abstract": "This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMR-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMR-TIP follows the “From Good to Great” concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an interleaved manner. Experimental analysis shows that IMR-TIP achieves enhanced mathematical capabilities and outperforms traditional LLMs and tool-augmented LLMs in accuracy and reasoning diversity on math reasoning tasks. For instance, IMR-TIP can improve Tool-augmented ChatGPT on GSM8K-Hard from 56.0% to 65.2 %.",
    "num_pages": 16
}