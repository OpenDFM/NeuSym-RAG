{
    "uuid": "0e8142ad-91cf-5b31-8020-8b2472ecac34",
    "title": "Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{yerukola-etal-2024-pope,\n    title = \"Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Non-Literal Intent Resolution in {LLM}s\",\n    author = \"Yerukola, Akhila  and\n      Vaduguru, Saujas  and\n      Fried, Daniel  and\n      Sap, Maarten\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.26\",\n    doi = \"10.18653/v1/2024.acl-short.26\",\n    pages = \"265--275\",\n    abstract = \"Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors{---}human or AI{---}to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models{'} (LLMs{'}) intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate contextually relevant responses to non-literal language. We also find that providing oracle intentions substantially improves response appropriateness, but using chain-of-thought to make models spell out intentions before responding improves much less. These findings suggest that LLMs are not yet pragmatic interlocutors, and that explicitly modeling intention could improve LLM responses to non-literal language.\",\n}\n",
    "authors": [
        "Akhila Yerukola",
        "Saujas Vaduguru",
        "Daniel Fried",
        "Maarten Sap"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.26.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0e8142ad-91cf-5b31-8020-8b2472ecac34.pdf",
    "abstract": "Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors—human or AI—to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models’ (LLMs’) intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate contextually relevant responses to non-literal language. We also find that providing oracle intentions substantially improves response appropriateness, but using chain-of-thought to make models spell out intentions before responding improves much less. These findings suggest that LLMs are not yet pragmatic interlocutors, and that explicitly modeling intention could improve LLM responses to non-literal language.",
    "num_pages": 11
}