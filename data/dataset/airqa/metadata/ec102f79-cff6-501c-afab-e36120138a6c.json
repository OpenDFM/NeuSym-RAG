{
    "uuid": "ec102f79-cff6-501c-afab-e36120138a6c",
    "title": "Learning Neuro-Symbolic World Models with Conversational Proprioception",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{agravante-etal-2023-learning,\n    title = \"Learning Neuro-Symbolic World Models with Conversational Proprioception\",\n    author = \"Agravante, Don Joven  and\n      Kimura, Daiki  and\n      Tatsubori, Michiaki  and\n      Munawar, Asim  and\n      Gray, Alexander\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.57\",\n    doi = \"10.18653/v1/2023.acl-short.57\",\n    pages = \"648--656\",\n    abstract = \"The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we describe a method that can learn neuro-symbolic world models on the TextWorld-Commonsense set of games. We then show how this can be improved further by taking inspiration from the concept of proprioception, but for conversation. This is done by enhancing the internal logic state with a memory of previous actions while also guiding future actions by augmenting the learned model with constraints based on this memory. This greatly improves the game-solving agents performance in a TextWorld setting, where the advantage over the baseline is an 85{\\%} average steps reduction and x2.3 average score.\",\n}\n",
    "authors": [
        "Don Joven Agravante",
        "Daiki Kimura",
        "Michiaki Tatsubori",
        "Asim Munawar",
        "Alexander Gray"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.57.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ec102f79-cff6-501c-afab-e36120138a6c.pdf",
    "abstract": "The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we describe a method that can learn neuro-symbolic world models on the TextWorld-Commonsense set of games. We then show how this can be improved further by taking inspiration from the concept of proprioception, but for conversation. This is done by enhancing the internal logic state with a memory of previous actions while also guiding future actions by augmenting the learned model with constraints based on this memory. This greatly improves the game-solving agents performance in a TextWorld setting, where the advantage over the baseline is an 85% average steps reduction and x2.3 average score.",
    "num_pages": 9
}