{
    "uuid": "27f4fca1-9f3d-588f-830d-5fccc43731bf",
    "title": "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yao-etal-2023-human,\n    title = \"Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations\",\n    author = \"Yao, Bingsheng  and\n      Sen, Prithviraj  and\n      Popa, Lucian  and\n      Hendler, James  and\n      Wang, Dakuo\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.821\",\n    doi = \"10.18653/v1/2023.acl-long.821\",\n    pages = \"14698--14713\",\n    abstract = \"Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation{'}s quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models{'} performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short.\",\n}\n",
    "authors": [
        "Bingsheng Yao",
        "Prithviraj Sen",
        "Lucian Popa",
        "James Hendler",
        "Dakuo Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.821.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/27f4fca1-9f3d-588f-830d-5fccc43731bf.pdf",
    "abstract": "Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation’s quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models’ performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short.",
    "num_pages": 16
}