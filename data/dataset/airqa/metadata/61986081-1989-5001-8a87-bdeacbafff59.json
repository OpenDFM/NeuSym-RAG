{
    "uuid": "61986081-1989-5001-8a87-bdeacbafff59",
    "title": "MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{sharma-etal-2023-memex,\n    title = \"{MEMEX}: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization\",\n    author = \"Sharma, Shivam  and\n      S, Ramaneswaran  and\n      Arora, Udit  and\n      Akhtar, Md. Shad  and\n      Chakraborty, Tanmoy\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.289\",\n    doi = \"10.18653/v1/2023.acl-long.289\",\n    pages = \"5272--5290\",\n    abstract = \"Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme{'}s context dynamically. In this work, we propose a novel task, MEMEX - given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses external knowledge-enriched meme representation and a multi-level approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of 4{\\%} F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME{'}s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations.\",\n}\n",
    "authors": [
        "Shivam Sharma",
        "Ramaneswaran S",
        "Udit Arora",
        "Md. Shad Akhtar",
        "Tanmoy Chakraborty"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.289.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/61986081-1989-5001-8a87-bdeacbafff59.pdf",
    "abstract": "Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme’s context dynamically. In this work, we propose a novel task, MEMEX - given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses external knowledge-enriched meme representation and a multi-level approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of 4% F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME’s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations.",
    "num_pages": 19
}