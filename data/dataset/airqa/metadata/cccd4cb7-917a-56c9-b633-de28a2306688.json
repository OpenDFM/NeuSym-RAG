{
    "uuid": "cccd4cb7-917a-56c9-b633-de28a2306688",
    "title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{mao-etal-2023-exploring,\n    title = \"Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation\",\n    author = \"Mao, Zhuoyuan  and\n      Dabre, Raj  and\n      Liu, Qianying  and\n      Song, Haiyue  and\n      Chu, Chenhui  and\n      Kurohashi, Sadao\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.112\",\n    doi = \"10.18653/v1/2023.acl-short.112\",\n    pages = \"1300--1316\",\n    abstract = \"This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.\",\n}\n",
    "authors": [
        "Zhuoyuan Mao",
        "Raj Dabre",
        "Qianying Liu",
        "Haiyue Song",
        "Chenhui Chu",
        "Sadao Kurohashi"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.112.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/cccd4cb7-917a-56c9-b633-de28a2306688.pdf",
    "abstract": "This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.",
    "num_pages": 17
}