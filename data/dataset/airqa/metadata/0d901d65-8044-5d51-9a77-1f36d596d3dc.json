{
    "uuid": "0d901d65-8044-5d51-9a77-1f36d596d3dc",
    "title": "Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{chen-etal-2023-stochastic,\n    title = \"Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning\",\n    author = \"Chen, Weize  and\n      Han, Xu  and\n      Lin, Yankai  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Zhou, Jie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.661\",\n    doi = \"10.18653/v1/2023.findings-acl.661\",\n    pages = \"10400--10420\",\n    abstract = \"Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states. Since it is non-trivial to directly model the intermediate states and design a running cost function, we propose to use latent stochastic bridges to regularize the intermediate states and use the regularization as the running cost of PETs. As the first work to propose regularized PETs that use stochastic bridges as the regularizers (running costs) for the intermediate states, we show the effectiveness and generality of this regularization across different tasks, PLMs and PETs. In view of the great potential and capacity, we believe more sophisticated regularizers can be designed for PETs and better performance can be achieved in the future.\",\n}\n",
    "authors": [
        "Weize Chen",
        "Xu Han",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.661.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/0d901d65-8044-5d51-9a77-1f36d596d3dc.pdf",
    "abstract": "Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states. Since it is non-trivial to directly model the intermediate states and design a running cost function, we propose to use latent stochastic bridges to regularize the intermediate states and use the regularization as the running cost of PETs. As the first work to propose regularized PETs that use stochastic bridges as the regularizers (running costs) for the intermediate states, we show the effectiveness and generality of this regularization across different tasks, PLMs and PETs. In view of the great potential and capacity, we believe more sophisticated regularizers can be designed for PETs and better performance can be achieved in the future.",
    "num_pages": 21
}