{
    "uuid": "38b17d40-9771-5dd1-8076-d714ba6b8ea9",
    "title": "Tagged End-to-End Simultaneous Speech Translation Training Using Simultaneous Interpretation Data",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{ko-etal-2023-tagged,\n    title = \"Tagged End-to-End Simultaneous Speech Translation Training Using Simultaneous Interpretation Data\",\n    author = \"Ko, Yuka  and\n      Fukuda, Ryo  and\n      Nishikawa, Yuta  and\n      Kano, Yasumasa  and\n      Sudoh, Katsuhito  and\n      Nakamura, Satoshi\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.34\",\n    doi = \"10.18653/v1/2023.iwslt-1.34\",\n    pages = \"363--375\",\n    abstract = \"Simultaneous speech translation (SimulST) translates partial speech inputs incrementally. Although the monotonic correspondence between input and output is preferable for smaller latency, it is not the case for distant language pairs such as English and Japanese. A prospective approach to this problem is to mimic simultaneous interpretation (SI) using SI data to train a SimulST model. However, the size of such SI data is limited, so the SI data should be used together with ordinary bilingual data whose translations are given in offline. In this paper, we propose an effective way to train a SimulST model using mixed data of SI and offline. The proposed method trains a single model using the mixed data with style tags that tell the model to generate SI- or offline-style outputs. Experiment results show improvements of BLEURT in different latency ranges, and our analyses revealed the proposed model generates SI-style outputs more than the baseline.\",\n}\n",
    "authors": [
        "Yuka Ko",
        "Ryo Fukuda",
        "Yuta Nishikawa",
        "Yasumasa Kano",
        "Katsuhito Sudoh",
        "Satoshi Nakamura"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.34.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/38b17d40-9771-5dd1-8076-d714ba6b8ea9.pdf",
    "abstract": "Simultaneous speech translation (SimulST) translates partial speech inputs incrementally. Although the monotonic correspondence between input and output is preferable for smaller latency, it is not the case for distant language pairs such as English and Japanese. A prospective approach to this problem is to mimic simultaneous interpretation (SI) using SI data to train a SimulST model. However, the size of such SI data is limited, so the SI data should be used together with ordinary bilingual data whose translations are given in offline. In this paper, we propose an effective way to train a SimulST model using mixed data of SI and offline. The proposed method trains a single model using the mixed data with style tags that tell the model to generate SI- or offline-style outputs. Experiment results show improvements of BLEURT in different latency ranges, and our analyses revealed the proposed model generates SI-style outputs more than the baseline.",
    "num_pages": 13
}