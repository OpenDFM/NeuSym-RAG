{
    "uuid": "3e27da57-f78f-5131-a189-7c5e1bf76c33",
    "title": "CLASP: Cross-modal Alignment Using Pre-trained Unimodal Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhou-etal-2024-clasp,\n    title = \"{CLASP}: Cross-modal Alignment Using Pre-trained Unimodal Models\",\n    author = \"Zhou, Jianing  and\n      Zeng, Ziheng  and\n      Gong, Hongyu  and\n      Bhat, Suma\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.684\",\n    doi = \"10.18653/v1/2024.findings-acl.684\",\n    pages = \"11518--11531\",\n    abstract = \"Recent advancements in joint speech-text pre-training have significantly advanced the processing of natural language. However, a key limitation is their reliance on parallel speech-text data, posing challenges due to data accessibility. Addressing this, our paper introduces an innovative framework for jointly performing speech and text processing without parallel corpora during pre-training but only downstream. Utilizing pre-trained unimodal models, we extract distinct representations for speech and text, aligning them effectively in a newly defined space using a multi-level contrastive learning mechanism. A unique swap reconstruction mechanism enhances the alignment and is followed by fusion via a multi-head mechanism, seamlessly merging modality-invariant and modality-specific representations. Testing for emotion recognition (SLU task) and idiom usage detection (NLU task) demonstrates robust performance, with commendable robustness to noise in text or speech data.\",\n}\n",
    "authors": [
        "Jianing Zhou",
        "Ziheng Zeng",
        "Hongyu Gong",
        "Suma Bhat"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.684.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3e27da57-f78f-5131-a189-7c5e1bf76c33.pdf",
    "abstract": "Recent advancements in joint speech-text pre-training have significantly advanced the processing of natural language. However, a key limitation is their reliance on parallel speech-text data, posing challenges due to data accessibility. Addressing this, our paper introduces an innovative framework for jointly performing speech and text processing without parallel corpora during pre-training but only downstream. Utilizing pre-trained unimodal models, we extract distinct representations for speech and text, aligning them effectively in a newly defined space using a multi-level contrastive learning mechanism. A unique swap reconstruction mechanism enhances the alignment and is followed by fusion via a multi-head mechanism, seamlessly merging modality-invariant and modality-specific representations. Testing for emotion recognition (SLU task) and idiom usage detection (NLU task) demonstrates robust performance, with commendable robustness to noise in text or speech data.",
    "num_pages": 14
}