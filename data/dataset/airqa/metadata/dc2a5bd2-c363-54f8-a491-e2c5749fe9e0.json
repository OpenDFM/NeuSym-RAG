{
    "uuid": "dc2a5bd2-c363-54f8-a491-e2c5749fe9e0",
    "title": "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhao-etal-2023-alignment,\n    title = \"From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment\",\n    author = \"Zhao, Yu  and\n      Wu, Yike  and\n      Cai, Xiangrui  and\n      Zhang, Ying  and\n      Zhang, Haiwei  and\n      Yuan, Xiaojie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.559\",\n    doi = \"10.18653/v1/2023.findings-acl.559\",\n    pages = \"8795--8806\",\n    abstract = \"Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at \\url{https://github.com/OreOZhao/TEA}.\",\n}\n",
    "authors": [
        "Yu Zhao",
        "Yike Wu",
        "Xiangrui Cai",
        "Ying Zhang",
        "Haiwei Zhang",
        "Xiaojie Yuan"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.559.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dc2a5bd2-c363-54f8-a491-e2c5749fe9e0.pdf",
    "abstract": "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at https://github.com/OreOZhao/TEA.",
    "num_pages": 12
}