{
    "uuid": "60fa38be-973e-5bb7-86d0-a966b3f2fc24",
    "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ke-etal-2024-critiquellm,\n    title = \"{C}ritique{LLM}: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\",\n    author = \"Ke, Pei  and\n      Wen, Bosi  and\n      Feng, Andrew  and\n      Liu, Xiao  and\n      Lei, Xuanyu  and\n      Cheng, Jiale  and\n      Wang, Shengyuan  and\n      Zeng, Aohan  and\n      Dong, Yuxiao  and\n      Wang, Hongning  and\n      Tang, Jie  and\n      Huang, Minlie\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.704\",\n    doi = \"10.18653/v1/2024.acl-long.704\",\n    pages = \"13034--13054\",\n    abstract = \"Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4{'}s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.\",\n}\n",
    "authors": [
        "Pei Ke",
        "Bosi Wen",
        "Andrew Feng",
        "Xiao Liu",
        "Xuanyu Lei",
        "Jiale Cheng",
        "Shengyuan Wang",
        "Aohan Zeng",
        "Yuxiao Dong",
        "Hongning Wang",
        "Jie Tang",
        "Minlie Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.704.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/60fa38be-973e-5bb7-86d0-a966b3f2fc24.pdf",
    "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4â€™s direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT.",
    "num_pages": 21
}