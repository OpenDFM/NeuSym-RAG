{
    "uuid": "31c162b1-1259-528b-89a7-3dbaa2dc45e6",
    "title": "Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{jawahar-etal-2023-small,\n    title = \"Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints\",\n    author = \"Jawahar, Ganesh  and\n      Mukherjee, Subhabrata  and\n      Dey, Debadeepta  and\n      Abdul-mageed, Muhammad  and\n      Lakshmanan, V.s., Laks  and\n      Mendes, Caio  and\n      De Rosa, Gustavo  and\n      Shah, Shital\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.22\",\n    doi = \"10.18653/v1/2023.sustainlp-1.22\",\n    pages = \"274--289\",\n}\n",
    "authors": [
        "Ganesh Jawahar",
        "Subhabrata Mukherjee",
        "Debadeepta Dey",
        "Muhammad Abdul-mageed",
        "Laks Lakshmanan, V.s.",
        "Caio Mendes",
        "Gustavo De Rosa",
        "Shital Shah"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.22.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/31c162b1-1259-528b-89a7-3dbaa2dc45e6.pdf",
    "abstract": "Autocomplete is a task where the user inputs a piece of text, termed prompt, which is conditioned by the model to generate semantically coherent continuation. Existing works for this task have primarily focused on datasets (e.g., email, chat) with high frequency user prompt patterns (or focused prompts) where word-based language models have been quite effective. In this work, we study the more challenging setting consisting of low frequency user prompt patterns (or broad prompts, e.g., prompt about 93rd academy awards) and demonstrate the effectiveness of character-based language models. We study this problem under memoryconstrained settings (e.g., edge devices and smartphones), where character-based representation is effective in reducing the overall model size (in terms of parameters). We use WikiText103 benchmark to simulate broad prompts and demonstrate that character models rival word models in exact match accuracy for the autocomplete task, when controlled for the model size. For instance, we show that a 20M parameter character model performs similar to an 80M parameter word model in the vanilla setting. We further propose novel methods to improve character models by incorporating inductive bias in the form of compositional information and representation transfer from large word models. Datasets and code used in this work are available at https://github.com/ UBC-NLP/char_autocomplete.",
    "num_pages": 16
}