{
    "uuid": "5773327e-57c3-557c-bb0c-c27369b417e4",
    "title": "Do Clinicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{yao-etal-2024-clinicians,\n    title = \"Do Clinicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation\",\n    author = \"Yao, Zonghai  and\n      Jaafar, Ahmed  and\n      Wang, Beining  and\n      Yang, Zhichao  and\n      Yu, Hong\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.15\",\n    doi = \"10.18653/v1/2024.bionlp-1.15\",\n    pages = \"182--201\",\n    abstract = \"This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4-APO{'}s superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.\",\n}\n",
    "authors": [
        "Zonghai Yao",
        "Ahmed Jaafar",
        "Beining Wang",
        "Zhichao Yang",
        "Hong Yu"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5773327e-57c3-557c-bb0c-c27369b417e4.pdf",
    "abstract": "This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4-APOâ€™s superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.",
    "num_pages": 20
}