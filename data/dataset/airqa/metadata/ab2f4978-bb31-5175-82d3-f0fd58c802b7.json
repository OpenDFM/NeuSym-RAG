{
    "uuid": "ab2f4978-bb31-5175-82d3-f0fd58c802b7",
    "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{sun-etal-2024-f,\n    title = \"{F}-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods\",\n    author = \"Sun, Yu  and\n      Keyuchen, Keyuchen  and\n      Wang, Shujie  and\n      Li, Peiji  and\n      Guo, Qipeng  and\n      Yan, Hang  and\n      Qiu, Xipeng  and\n      Huang, Xuanjing  and\n      Lin, Dahua\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.507\",\n    doi = \"10.18653/v1/2024.acl-long.507\",\n    pages = \"9348--9369\",\n    abstract = \"Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs{'} fundamental abilities.\",\n}\n",
    "authors": [
        "Yu Sun",
        "Keyuchen Keyuchen",
        "Shujie Wang",
        "Peiji Li",
        "Qipeng Guo",
        "Hang Yan",
        "Xipeng Qiu",
        "Xuanjing Huang",
        "Dahua Lin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.507.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ab2f4978-bb31-5175-82d3-f0fd58c802b7.pdf",
    "abstract": "Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMsâ€™ fundamental abilities.",
    "num_pages": 22
}