{
    "uuid": "2bf8853f-56eb-541f-aa88-7c563d037fd2",
    "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{jain-etal-2023-multi,\n    title = \"Multi-Dimensional Evaluation of Text Summarization with In-Context Learning\",\n    author = \"Jain, Sameer  and\n      Keshava, Vaishakh  and\n      Mysore Sathyendra, Swarnashree  and\n      Fernandes, Patrick  and\n      Liu, Pengfei  and\n      Neubig, Graham  and\n      Zhou, Chunting\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.537\",\n    doi = \"10.18653/v1/2023.findings-acl.537\",\n    pages = \"8487--8495\",\n    abstract = \"Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning-based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.\",\n}\n",
    "authors": [
        "Sameer Jain",
        "Vaishakh Keshava",
        "Swarnashree Mysore Sathyendra",
        "Patrick Fernandes",
        "Pengfei Liu",
        "Graham Neubig",
        "Chunting Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.537.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2bf8853f-56eb-541f-aa88-7c563d037fd2.pdf",
    "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning-based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
    "num_pages": 9
}