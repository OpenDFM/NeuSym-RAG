{
    "uuid": "2acd1063-883b-5f60-99e7-9fc3caf2bf3f",
    "title": "PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{xue-etal-2024-pacit,\n    title = \"{PACIT}: Unlocking the Power of Examples for Better In-Context Instruction Tuning\",\n    author = \"Xue, Tianci  and\n      Wang, Ziqi  and\n      Li, Yixia  and\n      Chen, Yun  and\n      Chen, Guanhua\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.36\",\n    doi = \"10.18653/v1/2024.findings-acl.36\",\n    pages = \"654--665\",\n    abstract = \"Instruction tuning enhances the instruction following ability of large language models by finetuning with supervised instruction data. Previous work proposes in-context instruction tuning (ICIT) where specific positive or negative examples are incorporated into the prompt for better performance. In this work, we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading. The model is expected to first verify the correctness of the provided example according to the task description, which is then set as the condition for generating a better response to the task instance. Our extensive experiments prove the effectiveness of PACIT, outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16 and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably enhance the performance of instruction tuning even when all positive and negative examples are generated with a self-instruct method.\",\n}\n",
    "authors": [
        "Tianci Xue",
        "Ziqi Wang",
        "Yixia Li",
        "Yun Chen",
        "Guanhua Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.36.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2acd1063-883b-5f60-99e7-9fc3caf2bf3f.pdf",
    "abstract": "Instruction tuning enhances the instruction following ability of large language models by finetuning with supervised instruction data. Previous work proposes in-context instruction tuning (ICIT) where specific positive or negative examples are incorporated into the prompt for better performance. In this work, we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading. The model is expected to first verify the correctness of the provided example according to the task description, which is then set as the condition for generating a better response to the task instance. Our extensive experiments prove the effectiveness of PACIT, outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16 and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably enhance the performance of instruction tuning even when all positive and negative examples are generated with a self-instruct method.",
    "num_pages": 12
}