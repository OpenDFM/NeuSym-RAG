{
    "uuid": "9407f34a-e3b2-569e-8d8c-95491add4a3c",
    "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{yancey-etal-2023-rating,\n    title = \"Rating Short {L}2 Essays on the {CEFR} Scale with {GPT}-4\",\n    author = \"Yancey, Kevin P.  and\n      Laflair, Geoffrey  and\n      Verardi, Anthony  and\n      Burstein, Jill\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.49\",\n    doi = \"10.18653/v1/2023.bea-1.49\",\n    pages = \"576--584\",\n    abstract = \"Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker{'}s first language (L1).\",\n}\n",
    "authors": [
        "Kevin P. Yancey",
        "Geoffrey Laflair",
        "Anthony Verardi",
        "Jill Burstein"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.49.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9407f34a-e3b2-569e-8d8c-95491add4a3c.pdf",
    "abstract": "Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-takerâ€™s first language (L1).",
    "num_pages": 9
}