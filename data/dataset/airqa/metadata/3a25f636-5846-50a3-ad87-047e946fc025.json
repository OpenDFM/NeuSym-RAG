{
    "uuid": "3a25f636-5846-50a3-ad87-047e946fc025",
    "title": "ACROSS: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{li-etal-2023-across,\n    title = \"{ACROSS}: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization\",\n    author = \"Li, Peiyao  and\n      Zhang, Zhengkun  and\n      Wang, Jun  and\n      Li, Liang  and\n      Jatowt, Adam  and\n      Yang, Zhenglu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.154\",\n    doi = \"10.18653/v1/2023.findings-acl.154\",\n    pages = \"2458--2472\",\n    abstract = \"This research addresses the challenges of Cross-Lingual Summarization (CLS) in low-resource scenarios and over imbalanced multilingual data. Existing CLS studies mostly resort to pipeline frameworks or multi-task methods in bilingual settings. However, they ignore the data imbalance in multilingual scenarios and do not utilize the high-resource monolingual summarization data. In this paper, we propose the Aligned CROSs-lingual Summarization (ACROSS) model to tackle these issues. Our framework aligns low-resource cross-lingual data with high-resource monolingual data via contrastive and consistency loss, which help enrich low-resource information for high-quality summaries. In addition, we introduce a data augmentation method that can select informative monolingual sentences, which facilitates a deep exploration of high-resource information and introduce new information for low-resource languages. Experiments on the CrossSum dataset show that ACROSS outperforms baseline models and obtains consistently dominant performance on 45 language pairs.\",\n}\n",
    "authors": [
        "Peiyao Li",
        "Zhengkun Zhang",
        "Jun Wang",
        "Liang Li",
        "Adam Jatowt",
        "Zhenglu Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.154.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3a25f636-5846-50a3-ad87-047e946fc025.pdf",
    "abstract": "This research addresses the challenges of Cross-Lingual Summarization (CLS) in low-resource scenarios and over imbalanced multilingual data. Existing CLS studies mostly resort to pipeline frameworks or multi-task methods in bilingual settings. However, they ignore the data imbalance in multilingual scenarios and do not utilize the high-resource monolingual summarization data. In this paper, we propose the Aligned CROSs-lingual Summarization (ACROSS) model to tackle these issues. Our framework aligns low-resource cross-lingual data with high-resource monolingual data via contrastive and consistency loss, which help enrich low-resource information for high-quality summaries. In addition, we introduce a data augmentation method that can select informative monolingual sentences, which facilitates a deep exploration of high-resource information and introduce new information for low-resource languages. Experiments on the CrossSum dataset show that ACROSS outperforms baseline models and obtains consistently dominant performance on 45 language pairs.",
    "num_pages": 15
}