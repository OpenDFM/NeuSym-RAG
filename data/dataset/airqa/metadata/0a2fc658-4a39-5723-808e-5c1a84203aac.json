{
    "uuid": "0a2fc658-4a39-5723-808e-5c1a84203aac",
    "title": "Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-leveraging,\n    title = \"Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization\",\n    author = \"Zhang, Yanghai  and\n      Liu, Ye  and\n      Wu, Shiwei  and\n      Zhang, Kai  and\n      Liu, Xukai  and\n      Liu, Qi  and\n      Chen, Enhong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.587\",\n    doi = \"10.18653/v1/2024.findings-acl.587\",\n    pages = \"9851--9862\",\n    abstract = \"The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem.\",\n}\n",
    "authors": [
        "Yanghai Zhang",
        "Ye Liu",
        "Shiwei Wu",
        "Kai Zhang",
        "Xukai Liu",
        "Qi Liu",
        "Enhong Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.587.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0a2fc658-4a39-5723-808e-5c1a84203aac.pdf",
    "abstract": "The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem.",
    "num_pages": 12
}