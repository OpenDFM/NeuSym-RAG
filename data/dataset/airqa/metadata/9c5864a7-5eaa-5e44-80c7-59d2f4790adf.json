{
    "uuid": "9c5864a7-5eaa-5e44-80c7-59d2f4790adf",
    "title": "Including Facial Expressions in Contextual Embeddings for Sign Language Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{viegas-etal-2023-including,\n    title = \"Including Facial Expressions in Contextual Embeddings for Sign Language Generation\",\n    author = \"Viegas, Carla  and\n      Inan, Mert  and\n      Quandt, Lorna  and\n      Alikhani, Malihe\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.1\",\n    doi = \"10.18653/v1/2023.starsem-1.1\",\n    pages = \"1--10\",\n    abstract = \"State-of-the-art sign language generation frameworks lack expressivity and naturalness which is the result of only focusing manual signs, neglecting the affective, grammatical and semantic functions of facial expressions. The purpose of this work is to augment semantic representation of sign language through grounding facial expressions. We study the effect of modeling the relationship between text, gloss, and facial expressions on the performance of the sign generation systems. In particular, we propose a Dual Encoder Transformer able to generate manual signs as well as facial expressions by capturing the similarities and differences found in text and sign gloss annotation. We take into consideration the role of facial muscle activity to express intensities of manual signs by being the first to employ facial action units in sign language generation. We perform a series of experiments showing that our proposed model improves the quality of automatically generated sign language.\",\n}\n",
    "authors": [
        "Carla Viegas",
        "Mert Inan",
        "Lorna Quandt",
        "Malihe Alikhani"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9c5864a7-5eaa-5e44-80c7-59d2f4790adf.pdf",
    "abstract": "State-of-the-art sign language generation frameworks lack expressivity and naturalness which is the result of only focusing manual signs, neglecting the affective, grammatical and semantic functions of facial expressions. The purpose of this work is to augment semantic representation of sign language through grounding facial expressions. We study the effect of modeling the relationship between text, gloss, and facial expressions on the performance of the sign generation systems. In particular, we propose a Dual Encoder Transformer able to generate manual signs as well as facial expressions by capturing the similarities and differences found in text and sign gloss annotation. We take into consideration the role of facial muscle activity to express intensities of manual signs by being the first to employ facial action units in sign language generation. We perform a series of experiments showing that our proposed model improves the quality of automatically generated sign language.",
    "num_pages": 10
}