{
    "uuid": "68304534-505a-53db-a90f-d6584d373a6d",
    "title": "Ertim at SemEval-2023 Task 2: Fine-tuning of Transformer Language Models and External Knowledge Leveraging for NER in Farsi, English, French and Chinese",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{deturck-etal-2023-ertim,\n    title = \"Ertim at {S}em{E}val-2023 Task 2: Fine-tuning of Transformer Language Models and External Knowledge Leveraging for {NER} in {F}arsi, {E}nglish, {F}rench and {C}hinese\",\n    author = \"Deturck, Kevin  and\n      Magistry, Pierre  and\n      Diot-Parvaz Ahmad, B{\\'e}n{\\'e}dicte  and\n      Wang, Ilaine  and\n      Nouvel, Damien  and\n      Lafayette, Hugo\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.306\",\n    doi = \"10.18653/v1/2023.semeval-1.306\",\n    pages = \"2211--2215\",\n    abstract = \"Transformer language models are now a solid baseline for Named Entity Recognition and can be significantly improved by leveraging complementary resources, either by integrating external knowledge or by annotating additional data. In a preliminary step, this work presents experiments on fine-tuning transformer models. Then, a set of experiments has been conducted with a Wikipedia-based reclassification system. Additionally, we conducted a small annotation campaign on the Farsi language to evaluate the impact of additional data. These two methods with complementary resources showed improvements compared to fine-tuning only.\",\n}\n",
    "authors": [
        "Kevin Deturck",
        "Pierre Magistry",
        "Bénédicte Diot-Parvaz Ahmad",
        "Ilaine Wang",
        "Damien Nouvel",
        "Hugo Lafayette"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.306.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/68304534-505a-53db-a90f-d6584d373a6d.pdf",
    "abstract": "Transformer language models are now a solid baseline for Named Entity Recognition and can be significantly improved by leveraging complementary resources, either by integrating external knowledge or by annotating additional data. In a preliminary step, this work presents experiments on fine-tuning transformer models. Then, a set of experiments has been conducted with a Wikipedia-based reclassification system. Additionally, we conducted a small annotation campaign on the Farsi language to evaluate the impact of additional data. These two methods with complementary resources showed improvements compared to fine-tuning only.",
    "num_pages": 5
}