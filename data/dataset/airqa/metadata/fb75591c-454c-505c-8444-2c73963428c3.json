{
    "uuid": "fb75591c-454c-505c-8444-2c73963428c3",
    "title": "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-knowcoder,\n    title = \"{K}now{C}oder: Coding Structured Knowledge into {LLM}s for Universal Information Extraction\",\n    author = \"Li, Zixuan  and\n      Zeng, Yutao  and\n      Zuo, Yuxin  and\n      Ren, Weicheng  and\n      Liu, Wenxuan  and\n      Su, Miao  and\n      Guo, Yucan  and\n      Liu, Yantao  and\n      Lixiang, Lixiang  and\n      Hu, Zhilei  and\n      Bai, Long  and\n      Li, Wei  and\n      Liu, Yidan  and\n      Yang, Pan  and\n      Jin, Xiaolong  and\n      Guo, Jiafeng  and\n      Cheng, Xueqi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.475\",\n    doi = \"10.18653/v1/2024.acl-long.475\",\n    pages = \"8758--8779\",\n    abstract = \"\",\n}\n",
    "authors": [
        "Zixuan Li",
        "Yutao Zeng",
        "Yuxin Zuo",
        "Weicheng Ren",
        "Wenxuan Liu",
        "Miao Su",
        "Yucan Guo",
        "Yantao Liu",
        "Lixiang Lixiang",
        "Zhilei Hu",
        "Long Bai",
        "Wei Li",
        "Yidan Liu",
        "Pan Yang",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.475.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/fb75591c-454c-505c-8444-2c73963428c3.pdf",
    "abstract": "In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over 30,000 types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a twophase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by 49.8% F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to 12.5% and 21.9%, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to 7.5% under the supervised setting.",
    "num_pages": 22
}