{
    "uuid": "9cdef282-a83a-5fab-8c2a-333a28a7a6bd",
    "title": "Compositional Data Augmentation for Abstractive Conversation Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ouyang-etal-2023-compositional,\n    title = \"Compositional Data Augmentation for Abstractive Conversation Summarization\",\n    author = \"Ouyang, Siru  and\n      Chen, Jiaao  and\n      Han, Jiawei  and\n      Yang, Diyi\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.82\",\n    doi = \"10.18653/v1/2023.acl-long.82\",\n    pages = \"1471--1488\",\n    abstract = \"Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and action triples as basic units. Then we organize these semantically meaningful conversation snippets compositionally to create new training instances. Additionally, we explore noise-tolerant settings in both self-training and joint-training paradigms to make the most of these augmented samples. Our experiments on benchmark datasets, SAMSum and DialogSum, show that Compo substantially outperforms prior baseline methods by achieving a nearly 10{\\%} increase of ROUGE scores with limited data. Code is available at \\url{https://github.com/ozyyshr/Compo}.\",\n}\n",
    "authors": [
        "Siru Ouyang",
        "Jiaao Chen",
        "Jiawei Han",
        "Diyi Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.82.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9cdef282-a83a-5fab-8c2a-333a28a7a6bd.pdf",
    "abstract": "Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and action triples as basic units. Then we organize these semantically meaningful conversation snippets compositionally to create new training instances. Additionally, we explore noise-tolerant settings in both self-training and joint-training paradigms to make the most of these augmented samples. Our experiments on benchmark datasets, SAMSum and DialogSum, show that Compo substantially outperforms prior baseline methods by achieving a nearly 10% increase of ROUGE scores with limited data. Code is available at https://github.com/ozyyshr/Compo.",
    "num_pages": 18
}