{
    "uuid": "71b02003-9f63-50e7-8b92-7521c6e7e57a",
    "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-small,\n    title = \"Small Language Models Need Strong Verifiers to Self-Correct Reasoning\",\n    author = \"Zhang, Yunxiang  and\n      Khalifa, Muhammad  and\n      Logeswaran, Lajanugen  and\n      Kim, Jaekyeom  and\n      Lee, Moontae  and\n      Lee, Honglak  and\n      Wang, Lu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.924\",\n    doi = \"10.18653/v1/2024.findings-acl.924\",\n    pages = \"15637--15653\",\n    abstract = \"Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small ($\\leq 13$B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.\",\n}\n",
    "authors": [
        "Yunxiang Zhang",
        "Muhammad Khalifa",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Moontae Lee",
        "Honglak Lee",
        "Lu Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.924.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/71b02003-9f63-50e7-8b92-7521c6e7e57a.pdf",
    "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (â‰¤ 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.",
    "num_pages": 17
}