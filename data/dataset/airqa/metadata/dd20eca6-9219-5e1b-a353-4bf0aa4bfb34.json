{
    "uuid": "dd20eca6-9219-5e1b-a353-4bf0aa4bfb34",
    "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{vashishtha-etal-2023-evaluating,\n    title = \"On Evaluating and Mitigating Gender Biases in Multilingual Settings\",\n    author = \"Vashishtha, Aniket  and\n      Ahuja, Kabir  and\n      Sitaram, Sunayana\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.21\",\n    doi = \"10.18653/v1/2023.findings-acl.21\",\n    pages = \"307--318\",\n    abstract = \"While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.\",\n}\n",
    "authors": [
        "Aniket Vashishtha",
        "Kabir Ahuja",
        "Sunayana Sitaram"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.21.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dd20eca6-9219-5e1b-a353-4bf0aa4bfb34.pdf",
    "abstract": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.",
    "num_pages": 12
}