{
    "uuid": "edd881cf-b224-596e-8eb1-fc0100f30435",
    "title": "Evaluating Chinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Data Contamination (CONDA)",
    "bibtex": "@inproceedings{liu-etal-2024-evaluating,\n    title = \"Evaluating {C}hinese Large Language Models on Discipline Knowledge Acquisition via Memorization and Robustness Assessment\",\n    author = \"Liu, Chuang  and\n      Jin, Renren  and\n      Steedman, Mark  and\n      Xiong, Deyi\",\n    editor = \"Sainz, Oscar  and\n      Garc{\\'\\i}a Ferrero, Iker  and\n      Agirre, Eneko  and\n      Ander Campos, Jon  and\n      Jacovi, Alon  and\n      Elazar, Yanai  and\n      Goldberg, Yoav\",\n    booktitle = \"Proceedings of the 1st Workshop on Data Contamination (CONDA)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.conda-1.1\",\n    doi = \"10.18653/v1/2024.conda-1.1\",\n    pages = \"1--12\",\n    abstract = \"Chinese LLMs demonstrate impressive performance on NLP tasks, particularly on discipline knowledge benchmarks, with some results approaching those of GPT-4. Previous research has viewed these advancements as potential outcomes of data contamination or leakage, prompting efforts to create new detection methods and address evaluation issues in LLM benchmarks. However, there has been a lack of comprehensive assessment of the evolution of Chinese LLMs. To address this gap, this paper offers a thorough investigation of Chinese LLMs on discipline knowledge evaluation, delving into the advancements of various LLMs, including a group of related models and others. Specifically, we have conducted six assessments ranging from knowledge memorization to comprehension for robustness, encompassing tasks like predicting incomplete questions and options, identifying behaviors by the contaminational fine-tuning, and answering rephrased questions. Experimental findings indicate a positive correlation between the release time of LLMs and their memorization capabilities, but they struggle with variations in original question-options pairs. Additionally, our findings suggest that question descriptions have a more significant impact on LLMs{'} performance.\",\n}\n",
    "authors": [
        "Chuang Liu",
        "Renren Jin",
        "Mark Steedman",
        "Deyi Xiong"
    ],
    "pdf_url": "https://aclanthology.org/2024.conda-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/edd881cf-b224-596e-8eb1-fc0100f30435.pdf",
    "abstract": "Chinese LLMs demonstrate impressive performance on NLP tasks, particularly on discipline knowledge benchmarks, with some results approaching those of GPT-4. Previous research has viewed these advancements as potential outcomes of data contamination or leakage, prompting efforts to create new detection methods and address evaluation issues in LLM benchmarks. However, there has been a lack of comprehensive assessment of the evolution of Chinese LLMs. To address this gap, this paper offers a thorough investigation of Chinese LLMs on discipline knowledge evaluation, delving into the advancements of various LLMs, including a group of related models and others. Specifically, we have conducted six assessments ranging from knowledge memorization to comprehension for robustness, encompassing tasks like predicting incomplete questions and options, identifying behaviors by the contaminational fine-tuning, and answering rephrased questions. Experimental findings indicate a positive correlation between the release time of LLMs and their memorization capabilities, but they struggle with variations in original question-options pairs. Additionally, our findings suggest that question descriptions have a more significant impact on LLMsâ€™ performance.",
    "num_pages": 12
}