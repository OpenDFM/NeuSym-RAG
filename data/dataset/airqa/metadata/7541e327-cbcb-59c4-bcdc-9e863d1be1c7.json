{
    "uuid": "7541e327-cbcb-59c4-bcdc-9e863d1be1c7",
    "title": "Reducing Privacy Risks in Online Self-Disclosures with Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{dou-etal-2024-reducing,\n    title = \"Reducing Privacy Risks in Online Self-Disclosures with Language Models\",\n    author = \"Dou, Yao  and\n      Krsek, Isadora  and\n      Naous, Tarek  and\n      Kabra, Anubha  and\n      Das, Sauvik  and\n      Ritter, Alan  and\n      Xu, Wei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.741\",\n    doi = \"10.18653/v1/2024.acl-long.741\",\n    pages = \"13732--13754\",\n    abstract = \"Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65{\\%} partial span F$_1$. We further conduct an HCI user study, with 82{\\%} of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is rephrasing disclosures into less specific terms while preserving their utility, e.g., {``}Im 16F{''} to {``}I{'}m a teenage girl{''}. We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation. To help users in deciding which disclosures to abstract, we present a task of rating their importance for context understanding. Our fine-tuned model achieves 80{\\%} accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we will only release our corpus and models to researcher who agree to the ethical guidelines outlined in Ethics Statement.\",\n}\n",
    "authors": [
        "Yao Dou",
        "Isadora Krsek",
        "Tarek Naous",
        "Anubha Kabra",
        "Sauvik Das",
        "Alan Ritter",
        "Wei Xu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.741.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7541e327-cbcb-59c4-bcdc-9e863d1be1c7.pdf",
    "abstract": "Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F1. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is rephrasing disclosures into less specific terms while preserving their utility, e.g., “Im 16F” to “I’m a teenage girl”. We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation. To help users in deciding which disclosures to abstract, we present a task of rating their importance for context understanding. Our fine-tuned model achieves 80% accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we will only release our corpus and models to researcher who agree to the ethical guidelines outlined in Ethics Statement.",
    "num_pages": 23
}