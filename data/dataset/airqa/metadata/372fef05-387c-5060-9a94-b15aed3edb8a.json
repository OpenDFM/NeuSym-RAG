{
    "uuid": "372fef05-387c-5060-9a94-b15aed3edb8a",
    "title": "DUB: Discrete Unit Back-translation for Speech Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhang-etal-2023-dub,\n    title = \"{DUB}: Discrete Unit Back-translation for Speech Translation\",\n    author = \"Zhang, Dong  and\n      Ye, Rong  and\n      Ko, Tom  and\n      Wang, Mingxuan  and\n      Zhou, Yaqian\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.447\",\n    doi = \"10.18653/v1/2023.findings-acl.447\",\n    pages = \"7147--7164\",\n    abstract = \"How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at \\url{https://anonymous.4open.science/r/DUB/}.\",\n}\n",
    "authors": [
        "Dong Zhang",
        "Rong Ye",
        "Tom Ko",
        "Mingxuan Wang",
        "Yaqian Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.447.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/372fef05-387c-5060-9a94-b15aed3edb8a.pdf",
    "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/.",
    "num_pages": 18
}