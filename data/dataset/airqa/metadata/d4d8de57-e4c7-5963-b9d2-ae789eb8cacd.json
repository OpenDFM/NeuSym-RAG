{
    "uuid": "d4d8de57-e4c7-5963-b9d2-ae789eb8cacd",
    "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wu-etal-2024-scimmir,\n    title = \"{S}ci{MMIR}: Benchmarking Scientific Multi-modal Information Retrieval\",\n    author = \"Wu, Siwei  and\n      Li, Yizhi  and\n      Zhu, Kang  and\n      Zhang, Ge  and\n      Liang, Yiming  and\n      Ma, Kaijing  and\n      Xiao, Chenghao  and\n      Zhang, Haoran  and\n      Yang, Bohao  and\n      Chen, Wenhu  and\n      Huang, Wenhao  and\n      Al Moubayed, Noura  and\n      Fu, Jie  and\n      Lin, Chenghua\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.746\",\n    doi = \"10.18653/v1/2024.findings-acl.746\",\n    pages = \"12560--12574\",\n    abstract = \"Multi-modal information retrieval (MMIR) is a rapidly evolving field where significant progress has been made through advanced representation learning and cross-modality alignment research, particularly in image-text pairing.However, current benchmarks for evaluating MMIR performance on image-text pairings overlook the scientific domain, which has a notable gap with the generic data since the caption of scientific charts and tables usually describes the analysis of experimental results or scientific principles in contrast to human activity or scenery depicted in generic images.To bridge this gap, we develop a \\textbf{sci}entific domain-specific \\textbf{MMIR} benchmark (\\textbf{SciMMIR}) by leveraging open-access research paper corpora to extract data relevant to the scientific domain. This benchmark comprises \\textbf{530K} meticulously curated image-text pairs, extracted from figures and tables with detailed captions from scientific documents.We further annotate the image-text pairs with a two-level subset-subcategory hierarchy to facilitate a more comprehensive evaluation of the baselines. We conduct zero-shot and fine-tuned evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP, BLIP, and BLIP-2.Our findings offer critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the effects of different visual and textual encoders.\",\n}\n",
    "authors": [
        "Siwei Wu",
        "Yizhi Li",
        "Kang Zhu",
        "Ge Zhang",
        "Yiming Liang",
        "Kaijing Ma",
        "Chenghao Xiao",
        "Haoran Zhang",
        "Bohao Yang",
        "Wenhu Chen",
        "Wenhao Huang",
        "Noura Al Moubayed",
        "Jie Fu",
        "Chenghua Lin"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.746.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d4d8de57-e4c7-5963-b9d2-ae789eb8cacd.pdf",
    "abstract": "Multi-modal information retrieval (MMIR) is a rapidly evolving field where significant progress has been made through advanced representation learning and cross-modality alignment research, particularly in image-text pairing.However, current benchmarks for evaluating MMIR performance on image-text pairings overlook the scientific domain, which has a notable gap with the generic data since the caption of scientific charts and tables usually describes the analysis of experimental results or scientific principles in contrast to human activity or scenery depicted in generic images.To bridge this gap, we develop a scientific domain-specific MMIR benchmark (SciMMIR) by leveraging open-access research paper corpora to extract data relevant to the scientific domain. This benchmark comprises 530K meticulously curated image-text pairs, extracted from figures and tables with detailed captions from scientific documents.We further annotate the image-text pairs with a two-level subset-subcategory hierarchy to facilitate a more comprehensive evaluation of the baselines. We conduct zero-shot and fine-tuned evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP, BLIP, and BLIP-2.Our findings offer critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the effects of different visual and textual encoders.",
    "num_pages": 15
}