{
    "uuid": "80cefe3d-1ca1-5ee6-89af-a784b389f8df",
    "title": "Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{hellman-etal-2023-scalable,\n    title = \"Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems\",\n    author = \"Hellman, Scott  and\n      Andrade, Alejandro  and\n      Habermehl, Kyle\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.12\",\n    doi = \"10.18653/v1/2023.bea-1.12\",\n    pages = \"137--147\",\n    abstract = \"Open-ended constructed response math word problems ({``}math plus text{''}, or MPT) are a powerful tool in the assessment of students{'} abilities to engage in mathematical reasoning and creative thinking. Such problems ask the student to compute a value or construct an expression and then explain, potentially in prose, what steps they took and why they took them. MPT items can be scored against highly structured rubrics, and we develop a novel technique for the automated scoring of MPT items that leverages these rubrics to provide explainable scoring. We show that our approach can be trained automatically and performs well on a large dataset of 34,417 responses across 14 MPT items.\",\n}\n",
    "authors": [
        "Scott Hellman",
        "Alejandro Andrade",
        "Kyle Habermehl"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/80cefe3d-1ca1-5ee6-89af-a784b389f8df.pdf",
    "abstract": "Open-ended constructed response math word problems (“math plus text”, or MPT) are a powerful tool in the assessment of students’ abilities to engage in mathematical reasoning and creative thinking. Such problems ask the student to compute a value or construct an expression and then explain, potentially in prose, what steps they took and why they took them. MPT items can be scored against highly structured rubrics, and we develop a novel technique for the automated scoring of MPT items that leverages these rubrics to provide explainable scoring. We show that our approach can be trained automatically and performs well on a large dataset of 34,417 responses across 14 MPT items.",
    "num_pages": 11
}