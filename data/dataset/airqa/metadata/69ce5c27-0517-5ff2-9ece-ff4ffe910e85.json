{
    "uuid": "69ce5c27-0517-5ff2-9ece-ff4ffe910e85",
    "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wu-etal-2024-llama,\n    title = \"{LL}a{MA} Pro: Progressive {LL}a{MA} with Block Expansion\",\n    author = \"Wu, Chengyue  and\n      Gan, Yukang  and\n      Ge, Yixiao  and\n      Lu, Zeyu  and\n      Wang, Jiahao  and\n      Feng, Ye  and\n      Shan, Ying  and\n      Luo, Ping\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.352\",\n    doi = \"10.18653/v1/2024.acl-long.352\",\n    pages = \"6518--6537\",\n    abstract = \"Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model{'}s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.\",\n}\n",
    "authors": [
        "Chengyue Wu",
        "Yukang Gan",
        "Yixiao Ge",
        "Zeyu Lu",
        "Jiahao Wang",
        "Ye Feng",
        "Ying Shan",
        "Ping Luo"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.352.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/69ce5c27-0517-5ff2-9ece-ff4ffe910e85.pdf",
    "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the modelâ€™s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
    "num_pages": 20
}