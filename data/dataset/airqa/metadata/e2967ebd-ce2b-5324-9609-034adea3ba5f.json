{
    "uuid": "e2967ebd-ce2b-5324-9609-034adea3ba5f",
    "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{jawahar-etal-2024-mixture,\n    title = \"Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts\",\n    author = \"Jawahar, Ganesh  and\n      Yang, Haichuan  and\n      Xiong, Yunyang  and\n      Liu, Zechun  and\n      Wang, Dilin  and\n      Sun, Fei  and\n      Li, Meng  and\n      Pappu, Aasish  and\n      Oguz, Barlas  and\n      Abdul-Mageed, Muhammad  and\n      Lakshmanan, Laks  and\n      Krishnamoorthi, Raghuraman  and\n      Chandra, Vikas\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.621\",\n    doi = \"10.18653/v1/2024.findings-acl.621\",\n    pages = \"10424--10443\",\n    abstract = \"Weight-sharing supernets are crucial for performance estimation in cutting-edge neural architecture search (NAS) frameworks. Despite their ability to generate diverse subnetworks without retraining, the quality of these subnetworks is not guaranteed due to weight sharing. In NLP tasks like machine translation and pre-trained language modeling, there is a significant performance gap between supernet and training from scratch for the same model architecture, necessitating retraining post optimal architecture identification.This study introduces a solution called mixture-of-supernets, a generalized supernet formulation leveraging mixture-of-experts (MoE) to enhance supernet model expressiveness with minimal training overhead. Unlike conventional supernets, this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks. This customization of weights for specific architectures, learned through gradient descent, minimizes retraining time, significantly enhancing training efficiency in NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS for fast machine translation models, exhibiting a superior latency-BLEU tradeoff compared to HAT, the SoTA NAS framework for machine translation. Furthermore, it excels in NAS for building memory-efficient task-agnostic BERT models, surpassing NAS-BERT and AutoDistil across various model sizes. The code can be found at: https://github.com/UBC-NLP/MoS.\",\n}\n",
    "authors": [
        "Ganesh Jawahar",
        "Haichuan Yang",
        "Yunyang Xiong",
        "Zechun Liu",
        "Dilin Wang",
        "Fei Sun",
        "Meng Li",
        "Aasish Pappu",
        "Barlas Oguz",
        "Muhammad Abdul-Mageed",
        "Laks Lakshmanan",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.621.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e2967ebd-ce2b-5324-9609-034adea3ba5f.pdf",
    "abstract": "Weight-sharing supernets are crucial for performance estimation in cutting-edge neural architecture search (NAS) frameworks. Despite their ability to generate diverse subnetworks without retraining, the quality of these subnetworks is not guaranteed due to weight sharing. In NLP tasks like machine translation and pre-trained language modeling, there is a significant performance gap between supernet and training from scratch for the same model architecture, necessitating retraining post optimal architecture identification.This study introduces a solution called mixture-of-supernets, a generalized supernet formulation leveraging mixture-of-experts (MoE) to enhance supernet model expressiveness with minimal training overhead. Unlike conventional supernets, this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks. This customization of weights for specific architectures, learned through gradient descent, minimizes retraining time, significantly enhancing training efficiency in NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS for fast machine translation models, exhibiting a superior latency-BLEU tradeoff compared to HAT, the SoTA NAS framework for machine translation. Furthermore, it excels in NAS for building memory-efficient task-agnostic BERT models, surpassing NAS-BERT and AutoDistil across various model sizes. The code can be found at: https://github.com/UBC-NLP/MoS.",
    "num_pages": 20
}