{
    "uuid": "99251a3c-b90f-5d75-b083-7151b67437cb",
    "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jacovi-etal-2024-chain,\n    title = \"A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains\",\n    author = \"Jacovi, Alon  and\n      Bitton, Yonatan  and\n      Bohnet, Bernd  and\n      Herzig, Jonathan  and\n      Honovich, Or  and\n      Tseng, Michael  and\n      Collins, Michael  and\n      Aharoni, Roee  and\n      Geva, Mor\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.254\",\n    doi = \"10.18653/v1/2024.acl-long.254\",\n    pages = \"4615--4634\",\n    abstract = \"Prompting language models to provide step-by-step answers (e.g., {``}Chain-of-Thought{''}) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model{'}s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .\",\n}\n",
    "authors": [
        "Alon Jacovi",
        "Yonatan Bitton",
        "Bernd Bohnet",
        "Jonathan Herzig",
        "Or Honovich",
        "Michael Tseng",
        "Michael Collins",
        "Roee Aharoni",
        "Mor Geva"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.254.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/99251a3c-b90f-5d75-b083-7151b67437cb.pdf",
    "abstract": "Prompting language models to provide step-by-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .",
    "num_pages": 20
}