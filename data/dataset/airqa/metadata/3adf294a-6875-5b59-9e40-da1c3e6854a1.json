{
    "uuid": "3adf294a-6875-5b59-9e40-da1c3e6854a1",
    "title": "ContextBLIP: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{lin-etal-2024-contextblip,\n    title = \"{C}ontext{BLIP}: Doubly Contextual Alignment for Contrastive Image Retrieval from Linguistically Complex Descriptions\",\n    author = \"Lin, Honglin  and\n      Li, Siyu  and\n      Nan, Guoshun  and\n      Tang, Chaoyue  and\n      Wang, Xueting  and\n      Xu, Jingxin  and\n      Yankai, Rong  and\n      Zhouzhili, Zhouzhili  and\n      Gao, Yutong  and\n      Cui, Qimei  and\n      Tao, Xiaofeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.961\",\n    doi = \"10.18653/v1/2024.findings-acl.961\",\n    pages = \"16240--16258\",\n    abstract = \"Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text. Despite the success of VLMs, they still significantly lag behind human performance in IRCD. The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions. This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss. The adapter learns to capture fine-grained visual cues. The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues. We term such a way as intra-contextual alignment. 2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images. We term this step as inter-contextual alignment. Consequently, the nuanced cues concealed in each modality can be effectively aligned. Experiments on two benchmarks show the superiority of our method. We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters.\",\n}\n",
    "authors": [
        "Honglin Lin",
        "Siyu Li",
        "Guoshun Nan",
        "Chaoyue Tang",
        "Xueting Wang",
        "Jingxin Xu",
        "Rong Yankai",
        "Zhouzhili Zhouzhili",
        "Yutong Gao",
        "Qimei Cui",
        "Xiaofeng Tao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.961.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3adf294a-6875-5b59-9e40-da1c3e6854a1.pdf",
    "abstract": "Image retrieval from contextual descriptions (IRCD) aims to identify an image within a set of minimally contrastive candidates based on linguistically complex text. Despite the success of VLMs, they still significantly lag behind human performance in IRCD. The main challenges lie in aligning key contextual cues in two modalities, where these subtle cues are concealed in tiny areas of multiple contrastive images and within the complex linguistics of textual descriptions. This motivates us to propose ContextBLIP, a simple yet effective method that relies on a doubly contextual alignment scheme for challenging IRCD. Specifically, 1) our model comprises a multi-scale adapter, a matching loss, and a text-guided masking loss. The adapter learns to capture fine-grained visual cues. The two losses enable iterative supervision for the adapter, gradually highlighting the focal patches of a single image to the key textual cues. We term such a way as intra-contextual alignment. 2) Then, ContextBLIP further employs an inter-context encoder to learn dependencies among candidates, facilitating alignment between the text to multiple images. We term this step as inter-contextual alignment. Consequently, the nuanced cues concealed in each modality can be effectively aligned. Experiments on two benchmarks show the superiority of our method. We observe that ContextBLIP can yield comparable results with GPT-4V, despite involving about 7,500 times fewer parameters.",
    "num_pages": 19
}