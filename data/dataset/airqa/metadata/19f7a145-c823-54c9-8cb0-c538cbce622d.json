{
    "uuid": "19f7a145-c823-54c9-8cb0-c538cbce622d",
    "title": "Explicit Syntactic Guidance for Neural Text Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2023-explicit,\n    title = \"Explicit Syntactic Guidance for Neural Text Generation\",\n    author = \"Li, Yafu  and\n      Cui, Leyang  and\n      Yan, Jianhao  and\n      Yin, Yongjing  and\n      Bi, Wei  and\n      Shi, Shuming  and\n      Zhang, Yue\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.788\",\n    doi = \"10.18653/v1/2023.acl-long.788\",\n    pages = \"14095--14112\",\n    abstract = \"Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.\",\n}\n",
    "authors": [
        "Yafu Li",
        "Leyang Cui",
        "Jianhao Yan",
        "Yongjing Yin",
        "Wei Bi",
        "Shuming Shi",
        "Yue Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.788.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/19f7a145-c823-54c9-8cb0-c538cbce622d.pdf",
    "abstract": "Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity.",
    "num_pages": 18
}