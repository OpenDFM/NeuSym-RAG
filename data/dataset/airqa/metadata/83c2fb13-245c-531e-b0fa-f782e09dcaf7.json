{
    "uuid": "83c2fb13-245c-531e-b0fa-f782e09dcaf7",
    "title": "Compositional Generalization with Grounded Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wold-etal-2024-compositional,\n    title = \"Compositional Generalization with Grounded Language Models\",\n    author = \"Wold, Sondre  and\n      Simon, {\\'E}tienne  and\n      Charpentier, Lucas  and\n      Kostylev, Egor  and\n      Velldal, Erik  and\n      {\\O}vrelid, Lilja\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.205\",\n    doi = \"10.18653/v1/2024.findings-acl.205\",\n    pages = \"3447--3460\",\n    abstract = \"Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training. By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs. We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights. We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components. While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations.\",\n}\n",
    "authors": [
        "Sondre Wold",
        "Étienne Simon",
        "Lucas Charpentier",
        "Egor Kostylev",
        "Erik Velldal",
        "Lilja Øvrelid"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.205.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/83c2fb13-245c-531e-b0fa-f782e09dcaf7.pdf",
    "abstract": "Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training. By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs. We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights. We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components. While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations.",
    "num_pages": 14
}