{
    "uuid": "97159aac-2f78-5680-8cf8-d7bb84dbd947",
    "title": "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jin-etal-2024-persuading,\n    title = \"Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model\",\n    author = \"Jin, Chuhao  and\n      Ren, Kening  and\n      Kong, Lingzhen  and\n      Wang, Xiting  and\n      Song, Ruihua  and\n      Chen, Huan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.92\",\n    doi = \"10.18653/v1/2024.acl-long.92\",\n    pages = \"1678--1706\",\n    abstract = \"Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user{'}s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.\",\n}\n",
    "authors": [
        "Chuhao Jin",
        "Kening Ren",
        "Lingzhen Kong",
        "Xiting Wang",
        "Ruihua Song",
        "Huan Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.92.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/97159aac-2f78-5680-8cf8-d7bb84dbd947.pdf",
    "abstract": "Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of userâ€™s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.",
    "num_pages": 29
}