{
    "uuid": "bee2edb5-07f7-50c0-81b5-14bf94402fc3",
    "title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xu-etal-2023-sescore2,\n    title = \"{SESCORE}2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes\",\n    author = \"Xu, Wenda  and\n      Qian, Xian  and\n      Wang, Mingxuan  and\n      Li, Lei  and\n      Wang, William Yang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.283\",\n    doi = \"10.18653/v1/2023.acl-long.283\",\n    pages = \"5166--5183\",\n    abstract = \"Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. We evaluate SEScore2 and previous methods on four text generation tasks across three languages. SEScore2 outperforms all prior unsupervised metrics on four text generation evaluation benchmarks, with an average Kendall improvement of 0.158. Surprisingly, SEScore2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks.\",\n}\n",
    "authors": [
        "Wenda Xu",
        "Xian Qian",
        "Mingxuan Wang",
        "Lei Li",
        "William Yang Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.283.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/bee2edb5-07f7-50c0-81b5-14bf94402fc3.pdf",
    "abstract": "Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. We evaluate SEScore2 and previous methods on four text generation tasks across three languages. SEScore2 outperforms all prior unsupervised metrics on four text generation evaluation benchmarks, with an average Kendall improvement of 0.158. Surprisingly, SEScore2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks.",
    "num_pages": 18
}