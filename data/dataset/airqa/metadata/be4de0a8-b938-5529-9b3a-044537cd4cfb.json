{
    "uuid": "be4de0a8-b938-5529-9b3a-044537cd4cfb",
    "title": "Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{an-etal-2024-large,\n    title = \"Do Large Language Models Discriminate in Hiring Decisions on the Basis of Race, Ethnicity, and Gender?\",\n    author = \"An, Haozhe  and\n      Acquaye, Christabel  and\n      Wang, Colin  and\n      Li, Zongxia  and\n      Rudinger, Rachel\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.37\",\n    doi = \"10.18653/v1/2024.acl-short.37\",\n    pages = \"386--397\",\n    abstract = \"We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant{'}s first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs{'} race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.\",\n}\n",
    "authors": [
        "Haozhe An",
        "Christabel Acquaye",
        "Colin Wang",
        "Zongxia Li",
        "Rachel Rudinger"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.37.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/be4de0a8-b938-5529-9b3a-044537cd4cfb.pdf",
    "abstract": "We examine whether large language models (LLMs) exhibit race- and gender-based name discrimination in hiring decisions, similar to classic findings in the social sciences (Bertrand and Mullainathan, 2004). We design a series of templatic prompts to LLMs to write an email to a named job applicant informing them of a hiring decision. By manipulating the applicant’s first name, we measure the effect of perceived race, ethnicity, and gender on the probability that the LLM generates an acceptance or rejection email. We find that the hiring decisions of LLMs in many settings are more likely to favor White applicants over Hispanic applicants. In aggregate, the groups with the highest and lowest acceptance rates respectively are masculine White names and masculine Hispanic names. However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs’ race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.",
    "num_pages": 12
}