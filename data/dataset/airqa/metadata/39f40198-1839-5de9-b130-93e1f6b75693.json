{
    "uuid": "39f40198-1839-5de9-b130-93e1f6b75693",
    "title": "HiPool: Modeling Long Documents Using Graph Neural Networks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{li-etal-2023-hipool,\n    title = \"{H}i{P}ool: Modeling Long Documents Using Graph Neural Networks\",\n    author = \"Li, Irene  and\n      Feng, Aosong  and\n      Radev, Dragomir  and\n      Ying, Rex\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.16\",\n    doi = \"10.18653/v1/2023.acl-short.16\",\n    pages = \"161--171\",\n    abstract = \"Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens{'} length. Evaluation shows our model surpasses competitive baselines by 2.6{\\%} in F1 score, and 4.8{\\%} on the longest sequence dataset. Our method is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences.\",\n}\n",
    "authors": [
        "Irene Li",
        "Aosong Feng",
        "Dragomir Radev",
        "Rex Ying"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/39f40198-1839-5de9-b130-93e1f6b75693.pdf",
    "abstract": "Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokensâ€™ length. Evaluation shows our model surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset. Our method is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences.",
    "num_pages": 11
}