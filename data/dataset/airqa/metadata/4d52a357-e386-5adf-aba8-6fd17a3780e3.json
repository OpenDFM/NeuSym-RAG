{
    "uuid": "4d52a357-e386-5adf-aba8-6fd17a3780e3",
    "title": "UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{mou-etal-2023-uppam,\n    title = \"{UPPAM}: A Unified Pre-training Architecture for Political Actor Modeling based on Language\",\n    author = \"Mou, Xinyi  and\n      Wei, Zhongyu  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.670\",\n    doi = \"10.18653/v1/2023.acl-long.670\",\n    pages = \"11996--12012\",\n    abstract = \"Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structure-aware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method.\",\n}\n",
    "authors": [
        "Xinyi Mou",
        "Zhongyu Wei",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.670.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4d52a357-e386-5adf-aba8-6fd17a3780e3.pdf",
    "abstract": "Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structure-aware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method.",
    "num_pages": 17
}