{
    "uuid": "cc60cda1-bb4a-5b15-a935-89a3ac5d8e06",
    "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cho-etal-2023-discrete,\n    title = \"Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker\",\n    author = \"Cho, Sukmin  and\n      Jeong, Soyeong  and\n      Seo, Jeong yeon  and\n      Park, Jong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.61\",\n    doi = \"10.18653/v1/2023.findings-acl.61\",\n    pages = \"960--971\",\n    abstract = \"Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.\",\n}\n",
    "authors": [
        "Sukmin Cho",
        "Soyeong Jeong",
        "Jeong yeon Seo",
        "Jong Park"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.61.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/cc60cda1-bb4a-5b15-a935-89a3ac5d8e06.pdf",
    "abstract": "Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.",
    "num_pages": 12
}