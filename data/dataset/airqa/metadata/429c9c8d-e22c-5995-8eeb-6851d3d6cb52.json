{
    "uuid": "429c9c8d-e22c-5995-8eeb-6851d3d6cb52",
    "title": "Learning Human Action Representations from Temporal Context in Lifestyle Vlogs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
    "bibtex": "@inproceedings{ignat-etal-2024-learning,\n    title = \"Learning Human Action Representations from Temporal Context in Lifestyle Vlogs\",\n    author = \"Ignat, Oana  and\n      Castro, Santiago  and\n      Li, Weiji  and\n      Mihalcea, Rada\",\n    editor = \"Ustalov, Dmitry  and\n      Gao, Yanjun  and\n      Panchenko, Alexander  and\n      Tutubalina, Elena  and\n      Nikishina, Irina  and\n      Ramesh, Arti  and\n      Sakhovskiy, Andrey  and\n      Usbeck, Ricardo  and\n      Penn, Gerald  and\n      Valentino, Marco\",\n    booktitle = \"Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.textgraphs-1.1\",\n    pages = \"1--18\",\n    abstract = \"We address the task of human action representation and show how the approach to generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time.We create and make publicly available the Co-Act (Action Co-occurrence) dataset, consisting of a large graph of {\\textasciitilde}12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring.We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains.\",\n}\n",
    "authors": [
        "Oana Ignat",
        "Santiago Castro",
        "Weiji Li",
        "Rada Mihalcea"
    ],
    "pdf_url": "https://aclanthology.org/2024.textgraphs-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/429c9c8d-e22c-5995-8eeb-6851d3d6cb52.pdf",
    "abstract": "We address the task of human action representation and show how the approach to generating word representations based on co-occurrence can be adapted to generate human action representations by analyzing their co-occurrence in videos. To this end, we formalize the new task of human action co-occurrence identification in online videos, i.e., determine whether two human actions are likely to co-occur in the same interval of time.We create and make publicly available the Co-Act (Action Co-occurrence) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring.We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains.",
    "num_pages": 18
}