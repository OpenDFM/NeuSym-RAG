{
    "uuid": "f45f6a32-c7f7-5ea2-b1ee-07b1d4d7cb2b",
    "title": "Universal Information Extraction with Meta-Pretrained Self-Retrieval",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{cong-etal-2023-universal,\n    title = \"Universal Information Extraction with Meta-Pretrained Self-Retrieval\",\n    author = \"Cong, Xin  and\n      Yu, Bowen  and\n      Fang, Mengcheng  and\n      Liu, Tingwen  and\n      Yu, Haiyang  and\n      Hu, Zhongkai  and\n      Huang, Fei  and\n      Li, Yongbin  and\n      Wang, Bin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.251\",\n    doi = \"10.18653/v1/2023.findings-acl.251\",\n    pages = \"4084--4100\",\n    abstract = \"Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge bases may help models to overcome this problem but it is impossible to construct a knowledge base suitable for various IE tasks. Inspired by the fact that large amount of knowledge are stored in the pretrained language models (PLM) and can be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve task-specific knowledge from PLMs to enhance universal IE. As different IE tasks need different knowledge, we further propose a Meta-Pretraining Algorithm which allows MetaRetriever to quicktly achieve maximum task-specific retrieval performance when fine-tuning on downstream IE tasks. Experimental results show that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets under fully-supervised, low-resource and few-shot scenarios.\",\n}\n",
    "authors": [
        "Xin Cong",
        "Bowen Yu",
        "Mengcheng Fang",
        "Tingwen Liu",
        "Haiyang Yu",
        "Zhongkai Hu",
        "Fei Huang",
        "Yongbin Li",
        "Bin Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.251.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f45f6a32-c7f7-5ea2-b1ee-07b1d4d7cb2b.pdf",
    "abstract": "Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge bases may help models to overcome this problem but it is impossible to construct a knowledge base suitable for various IE tasks. Inspired by the fact that large amount of knowledge are stored in the pretrained language models (PLM) and can be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve task-specific knowledge from PLMs to enhance universal IE. As different IE tasks need different knowledge, we further propose a Meta-Pretraining Algorithm which allows MetaRetriever to quicktly achieve maximum task-specific retrieval performance when fine-tuning on downstream IE tasks. Experimental results show that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets under fully-supervised, low-resource and few-shot scenarios.",
    "num_pages": 17
}