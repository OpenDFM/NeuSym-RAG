{
    "uuid": "273bcea3-4b8a-55ec-b2a8-c9c2247a7686",
    "title": "Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{su-etal-2024-living,\n    title = \"Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?\",\n    author = \"Su, Zhaochen  and\n      Li, Juntao  and\n      Zhang, Jun  and\n      Zhu, Tong  and\n      Qu, Xiaoye  and\n      Zhou, Pan  and\n      Bowen, Yan  and\n      Cheng, Yu  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.703\",\n    doi = \"10.18653/v1/2024.acl-long.703\",\n    pages = \"13014--13033\",\n    abstract = \"Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs{'} co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.\",\n}\n",
    "authors": [
        "Zhaochen Su",
        "Juntao Li",
        "Jun Zhang",
        "Tong Zhu",
        "Xiaoye Qu",
        "Pan Zhou",
        "Yan Bowen",
        "Yu Cheng",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.703.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/273bcea3-4b8a-55ec-b2a8-c9c2247a7686.pdf",
    "abstract": "Temporal reasoning is fundamental for large language models (LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce CoTempQA, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios (Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on CoTempQA tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMsâ€™ co-temporal reasoning from a mathematical perspective. We hope that our CoTempQA datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs.",
    "num_pages": 20
}