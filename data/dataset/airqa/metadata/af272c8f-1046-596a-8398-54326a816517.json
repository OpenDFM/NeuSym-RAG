{
    "uuid": "af272c8f-1046-596a-8398-54326a816517",
    "title": "Focused Prefix Tuning for Controllable Text Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{ma-etal-2023-focused,\n    title = \"Focused Prefix Tuning for Controllable Text Generation\",\n    author = \"Ma, Congda  and\n      Zhao, Tianyu  and\n      Shing, Makoto  and\n      Sawada, Kei  and\n      Okumura, Manabu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.96\",\n    doi = \"10.18653/v1/2023.acl-short.96\",\n    pages = \"1116--1127\",\n    abstract = \"In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.\",\n}\n",
    "authors": [
        "Congda Ma",
        "Tianyu Zhao",
        "Makoto Shing",
        "Kei Sawada",
        "Manabu Okumura"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.96.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/af272c8f-1046-596a-8398-54326a816517.pdf",
    "abstract": "In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.",
    "num_pages": 12
}