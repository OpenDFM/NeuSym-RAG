{
    "uuid": "99458b2a-2512-52c7-b460-97e6b913ebbc",
    "title": "CHARP: Conversation History AwaReness Probing for Knowledge-grounded Dialogue Systems",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ghaddar-etal-2024-charp,\n    title = \"{CHARP}: Conversation History {A}wa{R}eness Probing for Knowledge-grounded Dialogue Systems\",\n    author = \"Ghaddar, Abbas  and\n      Alfonso-Hermelo, David  and\n      Langlais, Philippe  and\n      Rezagholizadeh, Mehdi  and\n      Chen, Boxing  and\n      Parthasarathi, Prasanna\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.90\",\n    doi = \"10.18653/v1/2024.findings-acl.90\",\n    pages = \"1534--1551\",\n    abstract = \"In this work, we dive deep into one of the popular knowledge-grounded dialogue benchmarks that focus on faithfulness, FaithDial. We show that a significant portion of the FaithDial data contains annotation artifacts, which may bias models towards completely ignoring the conversation history. We therefore introduce CHARP, a testbed, designed for evaluating supposedly non-hallucinatory models trained on the FaithDial dataset. Our extensive analysis reveals that models primarily exhibit poor performance on CHARP due to their inability to effectively attend to and reason over the conversation history. Furthermore, the evaluation methods of FaithDial fail to capture these shortcomings, neglecting the conversational history. Our findings indicate that there is substantial room for contribution in both dataset creation and hallucination evaluation for knowledge-grounded dialogue, and that CHARP can serve as a tool for monitoring the progress in this particular research area. Data, models, and source code will be publicly available upon acceptance.\",\n}\n",
    "authors": [
        "Abbas Ghaddar",
        "David Alfonso-Hermelo",
        "Philippe Langlais",
        "Mehdi Rezagholizadeh",
        "Boxing Chen",
        "Prasanna Parthasarathi"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.90.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/99458b2a-2512-52c7-b460-97e6b913ebbc.pdf",
    "abstract": "In this work, we dive deep into one of the popular knowledge-grounded dialogue benchmarks that focus on faithfulness, FaithDial. We show that a significant portion of the FaithDial data contains annotation artifacts, which may bias models towards completely ignoring the conversation history. We therefore introduce CHARP, a testbed, designed for evaluating supposedly non-hallucinatory models trained on the FaithDial dataset. Our extensive analysis reveals that models primarily exhibit poor performance on CHARP due to their inability to effectively attend to and reason over the conversation history. Furthermore, the evaluation methods of FaithDial fail to capture these shortcomings, neglecting the conversational history. Our findings indicate that there is substantial room for contribution in both dataset creation and hallucination evaluation for knowledge-grounded dialogue, and that CHARP can serve as a tool for monitoring the progress in this particular research area. Data, models, and source code will be publicly available upon acceptance.",
    "num_pages": 18
}