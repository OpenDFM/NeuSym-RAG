{
    "uuid": "31efd461-8152-589a-8c1f-9f2848e8982a",
    "title": "Can Sequence-to-Sequence Transformers Naturally Understand Sequential Instructions?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{zhou-etal-2023-sequence,\n    title = \"Can Sequence-to-Sequence Transformers Naturally Understand Sequential Instructions?\",\n    author = \"Zhou, Xiang  and\n      Gupta, Aditya  and\n      Upadhyay, Shyam  and\n      Bansal, Mohit  and\n      Faruqui, Manaal\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.45\",\n    doi = \"10.18653/v1/2023.starsem-1.45\",\n    pages = \"527--534\",\n    abstract = \"While many real-life tasks require reasoning over multi-step sequential instructions, collecting fine-grained annotations for each intermediate step can be prohibitively expensive. In this work, we study how general pretrained sequence-to-sequence transformers perform under varying types of annotation for sequential instruction understanding. We conduct experiments using T5 (Raffel et al., 2020) on a commonly-used multi-step instruction understanding dataset SCONE (Long et al., 2016) that includes three sub-tasks. First, we show that with only gold supervision for the final step of a multi-step instruction sequence, depending on the sequential properties of different tasks, transformers may exhibit extremely bad performance on intermediate steps, in stark contrast with their performance on the final step. Next, we explore two directions to relieve this problem. We show that with the same limited annotation budget, using supervision uniformly distributed across different steps (instead of only final-step supervision), we can greatly improve the performance on intermediate steps with a drop in final-step performance. Further, we explore a contrastive learning approach to provide training signals on intermediate steps with zero intermediate gold supervision. This, however, achieves mixed results. It significantly improves the model{'}s bad intermediate-step performance on one subtask, but also shows decreased performance on another subtask.\",\n}\n",
    "authors": [
        "Xiang Zhou",
        "Aditya Gupta",
        "Shyam Upadhyay",
        "Mohit Bansal",
        "Manaal Faruqui"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.45.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/31efd461-8152-589a-8c1f-9f2848e8982a.pdf",
    "abstract": "While many real-life tasks require reasoning over multi-step sequential instructions, collecting fine-grained annotations for each intermediate step can be prohibitively expensive. In this work, we study how general pretrained sequence-to-sequence transformers perform under varying types of annotation for sequential instruction understanding. We conduct experiments using T5 (Raffel et al., 2020) on a commonly-used multi-step instruction understanding dataset SCONE (Long et al., 2016) that includes three sub-tasks. First, we show that with only gold supervision for the final step of a multi-step instruction sequence, depending on the sequential properties of different tasks, transformers may exhibit extremely bad performance on intermediate steps, in stark contrast with their performance on the final step. Next, we explore two directions to relieve this problem. We show that with the same limited annotation budget, using supervision uniformly distributed across different steps (instead of only final-step supervision), we can greatly improve the performance on intermediate steps with a drop in final-step performance. Further, we explore a contrastive learning approach to provide training signals on intermediate steps with zero intermediate gold supervision. This, however, achieves mixed results. It significantly improves the modelâ€™s bad intermediate-step performance on one subtask, but also shows decreased performance on another subtask.",
    "num_pages": 8
}