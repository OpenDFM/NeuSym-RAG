{
    "uuid": "974d8db2-f3d0-5299-8045-0f3e633c44ba",
    "title": "Training for Grammatical Error Correction Without Human-Annotated L2 Learners’ Corpora",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{oda-2023-training,\n    title = \"Training for Grammatical Error Correction Without Human-Annotated {L}2 Learners{'} Corpora\",\n    author = \"Oda, Mikio\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.38\",\n    doi = \"10.18653/v1/2023.bea-1.38\",\n    pages = \"455--465\",\n    abstract = \"Grammatical error correction (GEC) is a challenging task for non-native second language (L2) learners and learning machines. Data-driven GEC learning requires as much human-annotated genuine training data as possible. However, it is difficult to produce larger-scale human-annotated data, and synthetically generated large-scale parallel training data is valuable for GEC systems. In this paper, we propose a method for rebuilding a corpus of synthetic parallel data using target sentences predicted by a GEC model to improve performance. Experimental results show that our proposed pre-training outperforms that on the original synthetic datasets. Moreover, it is also shown that our proposed training without human-annotated L2 learners{'} corpora is as practical as conventional full pipeline training with both synthetic datasets and L2 learners{'} corpora in terms of accuracy.\",\n}\n",
    "authors": [
        "Mikio Oda"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.38.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/974d8db2-f3d0-5299-8045-0f3e633c44ba.pdf",
    "abstract": "Grammatical error correction (GEC) is a challenging task for non-native second language (L2) learners and learning machines. Data-driven GEC learning requires as much human-annotated genuine training data as possible. However, it is difficult to produce larger-scale human-annotated data, and synthetically generated large-scale parallel training data is valuable for GEC systems. In this paper, we propose a method for rebuilding a corpus of synthetic parallel data using target sentences predicted by a GEC model to improve performance. Experimental results show that our proposed pre-training outperforms that on the original synthetic datasets. Moreover, it is also shown that our proposed training without human-annotated L2 learners’ corpora is as practical as conventional full pipeline training with both synthetic datasets and L2 learners’ corpora in terms of accuracy.",
    "num_pages": 11
}