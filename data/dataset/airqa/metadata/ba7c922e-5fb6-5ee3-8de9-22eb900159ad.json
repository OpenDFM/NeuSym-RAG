{
    "uuid": "ba7c922e-5fb6-5ee3-8de9-22eb900159ad",
    "title": "Prompting language models improves performance in imbalanced setting",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{mohta-2023-prompting,\n    title = \"Prompting language models improves performance in imbalanced setting\",\n    author = \"Mohta, Jay\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.14\",\n    doi = \"10.18653/v1/2023.sustainlp-1.14\",\n    pages = \"201--211\",\n}\n",
    "authors": [
        "Jay Mohta"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.14.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ba7c922e-5fb6-5ee3-8de9-22eb900159ad.pdf",
    "abstract": "Prompting is a widely adopted technique for fine-tuning large language models. Recent research by Scao and Rush (2021) has demonstrated its effectiveness in improving few-shot learning performance compared to vanilla finetuning and also showed that prompting and vanilla fine tuning achieves similar performance in high data regime (∼> 2000 samples). This paper investigates the impact of imbalanced data distribution on prompting. Through rigorous experimentation on diverse datasets and models, our findings reveals that even in scenarios with high data regimes, prompting consistently outperforms vanilla fine-tuning by exhibiting average performance improvement of 2 −5%.",
    "num_pages": 11
}