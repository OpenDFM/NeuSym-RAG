{
    "uuid": "4a062969-187c-5ffb-bf0a-ae7e3a43f17a",
    "title": "On Prefix-tuning for Lightweight Out-of-distribution Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ouyang-etal-2023-prefix,\n    title = \"On Prefix-tuning for Lightweight Out-of-distribution Detection\",\n    author = \"Ouyang, Yawen  and\n      Cao, Yongchang  and\n      Gao, Yuan  and\n      Wu, Zhen  and\n      Zhang, Jianbing  and\n      Dai, Xinyu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.85\",\n    doi = \"10.18653/v1/2023.acl-long.85\",\n    pages = \"1533--1545\",\n    abstract = \"Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified. Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types.\",\n}\n",
    "authors": [
        "Yawen Ouyang",
        "Yongchang Cao",
        "Yuan Gao",
        "Zhen Wu",
        "Jianbing Zhang",
        "Xinyu Dai"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.85.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4a062969-187c-5ffb-bf0a-ae7e3a43f17a.pdf",
    "abstract": "Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified. Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types.",
    "num_pages": 13
}