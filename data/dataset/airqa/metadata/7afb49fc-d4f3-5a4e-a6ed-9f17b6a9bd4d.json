{
    "uuid": "7afb49fc-d4f3-5a4e-a6ed-9f17b6a9bd4d",
    "title": "Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
    "bibtex": "@inproceedings{akoju-etal-2023-synthetic,\n    title = \"Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference\",\n    author = \"Akoju, Sushma Anand  and\n      Vacareanu, Robert  and\n      Blanco, Eduardo  and\n      Riaz, Haris  and\n      Surdeanu, Mihai\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Neves Ribeiro, Danilo  and\n      Wei, Jason\",\n    booktitle = \"Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)\",\n    month = jun,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.nlrse-1.12\",\n    doi = \"10.18653/v1/2023.nlrse-1.12\",\n    pages = \"157--168\",\n    abstract = \"We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.\",\n}\n",
    "authors": [
        "Sushma Anand Akoju",
        "Robert Vacareanu",
        "Eduardo Blanco",
        "Haris Riaz",
        "Mihai Surdeanu"
    ],
    "pdf_url": "https://aclanthology.org/2023.nlrse-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7afb49fc-d4f3-5a4e-a6ed-9f17b6a9bd4d.pdf",
    "abstract": "We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.",
    "num_pages": 12
}