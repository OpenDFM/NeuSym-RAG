{
    "uuid": "03869d6c-f46d-5401-8e37-a55b6f8906dc",
    "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
    "bibtex": "@inproceedings{zhao-etal-2024-measuring,\n    title = \"Measuring the Inconsistency of Large Language Models in Preferential Ranking\",\n    author = \"Zhao, Xiutian  and\n      Wang, Ke  and\n      Peng, Wei\",\n    editor = \"Li, Sha  and\n      Li, Manling  and\n      Zhang, Michael JQ  and\n      Choi, Eunsol  and\n      Geva, Mor  and\n      Hase, Peter  and\n      Ji, Heng\",\n    booktitle = \"Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.knowllm-1.14\",\n    doi = \"10.18653/v1/2024.knowllm-1.14\",\n    pages = \"171--176\",\n    abstract = \"Despite large language models{'} (LLMs{'}) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.\",\n}\n",
    "authors": [
        "Xiutian Zhao",
        "Ke Wang",
        "Wei Peng"
    ],
    "pdf_url": "https://aclanthology.org/2024.knowllm-1.14.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/03869d6c-f46d-5401-8e37-a55b6f8906dc.pdf",
    "abstract": "Despite large language models’ (LLMs’) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
    "num_pages": 6
}