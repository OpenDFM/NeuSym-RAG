{
    "uuid": "1ca20220-c923-5de2-b3ff-e7c44cdb0681",
    "title": "Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{han-etal-2024-deep,\n    title = \"Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning\",\n    author = \"Han, Janghoon  and\n      Lee, Changho  and\n      Shin, Joongbo  and\n      Choi, Stanley Jungkyu  and\n      Lee, Honglak  and\n      Bae, Kyunghoon\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.912\",\n    doi = \"10.18653/v1/2024.findings-acl.912\",\n    pages = \"15436--15452\",\n    abstract = \"Instruction tuning has emerged as a powerful technique, significantly boosting zero-shot performance on unseen tasks. While recent work has explored cross-lingual generalization by applying instruction tuning to multilingual models, previous studies have primarily focused on English, with a limited exploration of non-English tasks. For in-depth exploration of cross-lingual generalization in instruction tuning, we perform instruction tuning individually for two distinct language meta-datasets. Subsequently, we assess the performance on unseen tasks in the language different from the one used for training. To facilitate this investigation, we introduce a novel non-English meta-dataset named {``}KORANI{''} (Korean Natural Instruction), comprising 51 Korean benchmarks. Moreover, we design cross-lingual templates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. Our experiments reveal consistent improvements through cross-lingual generalization in both English and Korean, outperforming baseline by average scores of 20.7{\\%} and 13.6{\\%}, respectively. Remarkably, these enhancements are comparable to those achieved by mono-lingual instruction tuning and even surpass them in some tasks. The result underscores the significance of relevant data acquisition across languages over linguistic congruence with unseen tasks during instruction tuning.\",\n}\n",
    "authors": [
        "Janghoon Han",
        "Changho Lee",
        "Joongbo Shin",
        "Stanley Jungkyu Choi",
        "Honglak Lee",
        "Kyunghoon Bae"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.912.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1ca20220-c923-5de2-b3ff-e7c44cdb0681.pdf",
    "abstract": "Instruction tuning has emerged as a powerful technique, significantly boosting zero-shot performance on unseen tasks. While recent work has explored cross-lingual generalization by applying instruction tuning to multilingual models, previous studies have primarily focused on English, with a limited exploration of non-English tasks. For in-depth exploration of cross-lingual generalization in instruction tuning, we perform instruction tuning individually for two distinct language meta-datasets. Subsequently, we assess the performance on unseen tasks in the language different from the one used for training. To facilitate this investigation, we introduce a novel non-English meta-dataset named “KORANI” (Korean Natural Instruction), comprising 51 Korean benchmarks. Moreover, we design cross-lingual templates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. Our experiments reveal consistent improvements through cross-lingual generalization in both English and Korean, outperforming baseline by average scores of 20.7% and 13.6%, respectively. Remarkably, these enhancements are comparable to those achieved by mono-lingual instruction tuning and even surpass them in some tasks. The result underscores the significance of relevant data acquisition across languages over linguistic congruence with unseen tasks during instruction tuning.",
    "num_pages": 17
}