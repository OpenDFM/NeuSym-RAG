{
    "uuid": "26b7ee9d-3cab-5057-97f3-16c1a9c8ead3",
    "title": "READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{si-etal-2023-readin,\n    title = \"{READIN}: A {C}hinese Multi-Task Benchmark with Realistic and Diverse Input Noises\",\n    author = \"Si, Chenglei  and\n      Zhang, Zhengyan  and\n      Chen, Yingfa  and\n      Wang, Xiaozhi  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.460\",\n    doi = \"10.18653/v1/2023.acl-long.460\",\n    pages = \"8272--8285\",\n    abstract = \"For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from \\url{https://github.com/thunlp/READIN}.\",\n}\n",
    "authors": [
        "Chenglei Si",
        "Zhengyan Zhang",
        "Yingfa Chen",
        "Xiaozhi Wang",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.460.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/26b7ee9d-3cab-5057-97f3-16c1a9c8ead3.pdf",
    "abstract": "For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from https://github.com/thunlp/READIN.",
    "num_pages": 14
}