{
    "uuid": "c574415b-3356-5539-b724-bbb90ada8e78",
    "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{huang-etal-2024-clomo,\n    title = \"{CLOMO}: Counterfactual Logical Modification with Large Language Models\",\n    author = \"Huang, Yinya  and\n      Hong, Ruixin  and\n      Zhang, Hongming  and\n      Shao, Wei  and\n      Yang, Zhicheng  and\n      Yu, Dong  and\n      Zhang, Changshui  and\n      Liang, Xiaodan  and\n      Song, Linqi\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.593\",\n    doi = \"10.18653/v1/2024.acl-long.593\",\n    pages = \"11012--11034\",\n    abstract = \"In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model{'}s counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at https://github.com/Eleanor-H/CLOMO.\",\n}\n",
    "authors": [
        "Yinya Huang",
        "Ruixin Hong",
        "Hongming Zhang",
        "Wei Shao",
        "Zhicheng Yang",
        "Dong Yu",
        "Changshui Zhang",
        "Xiaodan Liang",
        "Linqi Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.593.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c574415b-3356-5539-b724-bbb90ada8e78.pdf",
    "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation modelâ€™s counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at https://github.com/Eleanor-H/CLOMO.",
    "num_pages": 23
}