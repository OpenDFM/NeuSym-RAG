{
    "uuid": "046e9dbd-e771-5c3c-a3eb-3b910a912530",
    "title": "LT4SG@SMM4H’24: Tweets Classification for Digital Epidemiology of Childhood Health Outcomes Using Pre-Trained Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks",
    "bibtex": "@inproceedings{athukoralage-etal-2024-lt4sg,\n    title = \"{LT}4{SG}@{SMM}4{H}{'}24: Tweets Classification for Digital Epidemiology of Childhood Health Outcomes Using Pre-Trained Language Models\",\n    author = \"Athukoralage, Dasun  and\n      Atapattu, Thushari  and\n      Thilakaratne, Menasha  and\n      Falkner, Katrina\",\n    editor = \"Xu, Dongfang  and\n      Gonzalez-Hernandez, Graciela\",\n    booktitle = \"Proceedings of The 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.smm4h-1.9\",\n    pages = \"38--41\",\n    abstract = \"This paper presents our approaches for the SMM4H{'}24 Shared Task 5 on the binary classification of English tweets reporting children{'}s medical disorders. Our first approach involves fine-tuning a single RoBERTa-large model, while the second approach entails ensembling the results of three fine-tuned BERTweet-large models. We demonstrate that although both approaches exhibit identical performance on validation data, the BERTweet-large ensemble excels on test data. Our best-performing system achieves an F1-score of 0.938 on test data, outperforming the benchmark classifier by 1.18{\\%}.\",\n}\n",
    "authors": [
        "Dasun Athukoralage",
        "Thushari Atapattu",
        "Menasha Thilakaratne",
        "Katrina Falkner"
    ],
    "pdf_url": "https://aclanthology.org/2024.smm4h-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/046e9dbd-e771-5c3c-a3eb-3b910a912530.pdf",
    "abstract": "This paper presents our approaches for the SMM4H’24 Shared Task 5 on the binary classification of English tweets reporting children’s medical disorders. Our first approach involves fine-tuning a single RoBERTa-large model, while the second approach entails ensembling the results of three fine-tuned BERTweet-large models. We demonstrate that although both approaches exhibit identical performance on validation data, the BERTweet-large ensemble excels on test data. Our best-performing system achieves an F1-score of 0.938 on test data, outperforming the benchmark classifier by 1.18%.",
    "num_pages": 4
}