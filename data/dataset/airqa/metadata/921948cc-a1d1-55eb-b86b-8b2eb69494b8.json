{
    "uuid": "921948cc-a1d1-55eb-b86b-8b2eb69494b8",
    "title": "An Empirical Analysis of Leveraging Knowledge for Low-Resource Task-Oriented Semantic Parsing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{kulkarni-etal-2023-empirical,\n    title = \"An Empirical Analysis of Leveraging Knowledge for Low-Resource Task-Oriented Semantic Parsing\",\n    author = \"Kulkarni, Mayank  and\n      Zhong, Aoxiao  and\n      Guenon des mesnards, Nicolas  and\n      Movaghati, Sahar  and\n      Sridhar, Mukund  and\n      Xie, He  and\n      Lu, Jianhua\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.123\",\n    doi = \"10.18653/v1/2023.findings-acl.123\",\n    pages = \"1955--1969\",\n    abstract = \"Task-oriented semantic parsing has drawn a lot of interest from the NLP community, and especially the voice assistant industry as it enables representing the meaning of user requests with arbitrarily nested semantics, including multiple intents and compound entities. SOTA models are large seq2seq transformers and require hundreds of thousands of annotated examples to be trained. However annotating such data to bootstrap new domains or languages is expensive and error-prone, especially for requests made of nested semantics. In addition large models easily break the tight latency constraints imposed in a user-facing production environment. As part of this work we explore leveraging external knowledge to improve model accuracy in low-resource and low-compute settings. We demonstrate that using knowledge-enhanced encoders inside seq2seq models does not result in performance gains by itself, but jointly learning to uncover entities in addition to the parse generation is a simple yet effective way of improving performance across the board. We show this is especially true in the low-compute scarce-data setting and for entity-rich domains, with relative gains up to 74.48{\\%} on the TOPv2 dataset.\",\n}\n",
    "authors": [
        "Mayank Kulkarni",
        "Aoxiao Zhong",
        "Nicolas Guenon des mesnards",
        "Sahar Movaghati",
        "Mukund Sridhar",
        "He Xie",
        "Jianhua Lu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.123.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/921948cc-a1d1-55eb-b86b-8b2eb69494b8.pdf",
    "abstract": "Task-oriented semantic parsing has drawn a lot of interest from the NLP community, and especially the voice assistant industry as it enables representing the meaning of user requests with arbitrarily nested semantics, including multiple intents and compound entities. SOTA models are large seq2seq transformers and require hundreds of thousands of annotated examples to be trained. However annotating such data to bootstrap new domains or languages is expensive and error-prone, especially for requests made of nested semantics. In addition large models easily break the tight latency constraints imposed in a user-facing production environment. As part of this work we explore leveraging external knowledge to improve model accuracy in low-resource and low-compute settings. We demonstrate that using knowledge-enhanced encoders inside seq2seq models does not result in performance gains by itself, but jointly learning to uncover entities in addition to the parse generation is a simple yet effective way of improving performance across the board. We show this is especially true in the low-compute scarce-data setting and for entity-rich domains, with relative gains up to 74.48% on the TOPv2 dataset.",
    "num_pages": 15
}