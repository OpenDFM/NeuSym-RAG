{
    "uuid": "fe24a64b-db14-558f-9685-ba7e6d3f00e9",
    "title": "QUESPA Submission for the IWSLT 2024 Dialectal and Low-resource Speech Translation Task",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    "bibtex": "@inproceedings{e-ortega-etal-2024-quespa,\n    title = \"{QUESPA} Submission for the {IWSLT} 2024 Dialectal and Low-resource Speech Translation Task\",\n    author = \"E. Ortega, John  and\n      Joel Zevallos, Rodolfo  and\n      Said Ahmad, Ibrahim  and\n      Chen, William\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.iwslt-1.17\",\n    doi = \"10.18653/v1/2024.iwslt-1.17\",\n    pages = \"125--133\",\n    abstract = \"This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE{--}SPA) track featured in the Evaluation Campaign of IWSLT 2024: dialectal and low-resource speech translation. Two main submission types were supported in the campaign: constrained and unconstrained. This is our second year submitting our ST systems to the IWSLT shared task and we feel that we have achieved novel performance, surpassing last year{'}s submissions. Again, we were able to submit six total systems of which our best (primary) constrained system consisted of an ST model based on the Fairseq S2T framework where the audio representations were created using log mel-scale filter banks as features and the translations were performed using a transformer. The system was similar to last year{'}s submission with slight configuration changes, allowing us to achieve slightly higher performance (2 BLEU). Contrastingly, we were able to achieve much better performance than last year on the unconstrained task using a larger pre-trained language (PLM) model for ST (without cascading) and the inclusion of parallel QUE{--}SPA data found on the internet. The fine-tuning of Microsoft{'}s SpeechT5 model in a ST setting along with the addition of new data and a data augmentation technique allowed us to achieve 19.7 BLEU. Additionally, we present the other four submissions (2 constrained and 2 unconstrained) which are part of additional efforts of hyper-parameter and configuration tuning on existent models and the inclusion of Whisper for speech recognition\",\n}\n",
    "authors": [
        "John E. Ortega",
        "Rodolfo Joel Zevallos",
        "Ibrahim Said Ahmad",
        "William Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.iwslt-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/fe24a64b-db14-558f-9685-ba7e6d3f00e9.pdf",
    "abstract": "This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE–SPA) track featured in the Evaluation Campaign of IWSLT 2024: dialectal and low-resource speech translation. Two main submission types were supported in the campaign: constrained and unconstrained. This is our second year submitting our ST systems to the IWSLT shared task and we feel that we have achieved novel performance, surpassing last year’s submissions. Again, we were able to submit six total systems of which our best (primary) constrained system consisted of an ST model based on the Fairseq S2T framework where the audio representations were created using log mel-scale filter banks as features and the translations were performed using a transformer. The system was similar to last year’s submission with slight configuration changes, allowing us to achieve slightly higher performance (2 BLEU). Contrastingly, we were able to achieve much better performance than last year on the unconstrained task using a larger pre-trained language (PLM) model for ST (without cascading) and the inclusion of parallel QUE–SPA data found on the internet. The fine-tuning of Microsoft’s SpeechT5 model in a ST setting along with the addition of new data and a data augmentation technique allowed us to achieve 19.7 BLEU. Additionally, we present the other four submissions (2 constrained and 2 unconstrained) which are part of additional efforts of hyper-parameter and configuration tuning on existent models and the inclusion of Whisper for speech recognition",
    "num_pages": 9
}