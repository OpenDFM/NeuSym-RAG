{
    "uuid": "f2a4964c-d9b6-5dbf-840d-cdebda82365c",
    "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jin-etal-2023-darkbert,\n    title = \"{D}ark{BERT}: A Language Model for the Dark Side of the {I}nternet\",\n    author = \"Jin, Youngjin  and\n      Jang, Eugene  and\n      Cui, Jian  and\n      Chung, Jin-Woo  and\n      Lee, Yongjae  and\n      Shin, Seungwon\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.415\",\n    doi = \"10.18653/v1/2023.acl-long.415\",\n    pages = \"7515--7533\",\n    abstract = \"Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.\",\n}\n",
    "authors": [
        "Youngjin Jin",
        "Eugene Jang",
        "Jian Cui",
        "Jin-Woo Chung",
        "Yongjae Lee",
        "Seungwon Shin"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.415.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f2a4964c-d9b6-5dbf-840d-cdebda82365c.pdf",
    "abstract": "Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.",
    "num_pages": 19
}