{
    "uuid": "c5bbd13b-85ec-54e1-827e-0f9d6eb6caa0",
    "title": "OLMo: Accelerating the Science of Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{groeneveld-etal-2024-olmo,\n    title = \"{OLM}o: Accelerating the Science of Language Models\",\n    author = \"Groeneveld, Dirk  and\n      Beltagy, Iz  and\n      Walsh, Evan  and\n      Bhagia, Akshita  and\n      Kinney, Rodney  and\n      Tafjord, Oyvind  and\n      Jha, Ananya  and\n      Ivison, Hamish  and\n      Magnusson, Ian  and\n      Wang, Yizhong  and\n      Arora, Shane  and\n      Atkinson, David  and\n      Authur, Russell  and\n      Chandu, Khyathi  and\n      Cohan, Arman  and\n      Dumas, Jennifer  and\n      Elazar, Yanai  and\n      Gu, Yuling  and\n      Hessel, Jack  and\n      Khot, Tushar  and\n      Merrill, William  and\n      Morrison, Jacob  and\n      Muennighoff, Niklas  and\n      Naik, Aakanksha  and\n      Nam, Crystal  and\n      Peters, Matthew  and\n      Pyatkin, Valentina  and\n      Ravichander, Abhilasha  and\n      Schwenk, Dustin  and\n      Shah, Saurabh  and\n      Smith, William  and\n      Strubell, Emma  and\n      Subramani, Nishant  and\n      Wortsman, Mitchell  and\n      Dasigi, Pradeep  and\n      Lambert, Nathan  and\n      Richardson, Kyle  and\n      Zettlemoyer, Luke  and\n      Dodge, Jesse  and\n      Lo, Kyle  and\n      Soldaini, Luca  and\n      Smith, Noah  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.841\",\n    doi = \"10.18653/v1/2024.acl-long.841\",\n    pages = \"15789--15809\",\n    abstract = \"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.\",\n}\n",
    "authors": [
        "Dirk Groeneveld",
        "Iz Beltagy",
        "Evan Walsh",
        "Akshita Bhagia",
        "Rodney Kinney",
        "Oyvind Tafjord",
        "Ananya Jha",
        "Hamish Ivison",
        "Ian Magnusson",
        "Yizhong Wang",
        "Shane Arora",
        "David Atkinson",
        "Russell Authur",
        "Khyathi Chandu",
        "Arman Cohan",
        "Jennifer Dumas",
        "Yanai Elazar",
        "Yuling Gu",
        "Jack Hessel",
        "Tushar Khot",
        "William Merrill",
        "Jacob Morrison",
        "Niklas Muennighoff",
        "Aakanksha Naik",
        "Crystal Nam",
        "Matthew Peters",
        "Valentina Pyatkin",
        "Abhilasha Ravichander",
        "Dustin Schwenk",
        "Saurabh Shah",
        "William Smith",
        "Emma Strubell",
        "Nishant Subramani",
        "Mitchell Wortsman",
        "Pradeep Dasigi",
        "Nathan Lambert",
        "Kyle Richardson",
        "Luke Zettlemoyer",
        "Jesse Dodge",
        "Kyle Lo",
        "Luca Soldaini",
        "Noah Smith",
        "Hannaneh Hajishirzi"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.841.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c5bbd13b-85ec-54e1-827e-0f9d6eb6caa0.pdf",
    "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
    "num_pages": 21
}