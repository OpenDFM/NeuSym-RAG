{
    "uuid": "bc4b8f4c-f5ea-5ff6-8409-222ad0645c36",
    "title": "Transforming Visual Scene Graphs to Image Captions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yang-etal-2023-transforming,\n    title = \"Transforming Visual Scene Graphs to Image Captions\",\n    author = \"Yang, Xu  and\n      Peng, Jiawei  and\n      Wang, Zihua  and\n      Xu, Haiyang  and\n      Ye, Qinghao  and\n      Li, Chenliang  and\n      Huang, Songfang  and\n      Huang, Fei  and\n      Li, Zhangzikang  and\n      Zhang, Yu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.694\",\n    doi = \"10.18653/v1/2023.acl-long.694\",\n    pages = \"12427--12440\",\n    abstract = \"We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a simple and homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TFSGC. The code is in: \\url{https://anonymous.4open.science/r/ACL23_TFSGC}.\",\n}\n",
    "authors": [
        "Xu Yang",
        "Jiawei Peng",
        "Zihua Wang",
        "Haiyang Xu",
        "Qinghao Ye",
        "Chenliang Li",
        "Songfang Huang",
        "Fei Huang",
        "Zhangzikang Li",
        "Yu Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.694.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/bc4b8f4c-f5ea-5ff6-8409-222ad0645c36.pdf",
    "abstract": "We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a simple and homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TFSGC. The code is in: https://anonymous.4open.science/r/ACL23_TFSGC.",
    "num_pages": 14
}