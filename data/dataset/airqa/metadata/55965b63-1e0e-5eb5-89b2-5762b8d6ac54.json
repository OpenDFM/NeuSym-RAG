{
    "uuid": "55965b63-1e0e-5eb5-89b2-5762b8d6ac54",
    "title": "FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{wang-etal-2023-fashionklip,\n    title = \"{F}ashion{KLIP}: Enhancing {E}-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph\",\n    author = \"Wang, Xiaodan  and\n      Wang, Chengyu  and\n      Li, Lei  and\n      Li, Zhixu  and\n      Chen, Ben  and\n      Jin, Linbo  and\n      Huang, Jun  and\n      Xiao, Yanghua  and\n      Gao, Ming\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.16\",\n    doi = \"10.18653/v1/2023.acl-industry.16\",\n    pages = \"149--158\",\n    abstract = \"Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP.\",\n}\n",
    "authors": [
        "Xiaodan Wang",
        "Chengyu Wang",
        "Lei Li",
        "Zhixu Li",
        "Ben Chen",
        "Linbo Jin",
        "Jun Huang",
        "Yanghua Xiao",
        "Ming Gao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/55965b63-1e0e-5eb5-89b2-5762b8d6ac54.pdf",
    "abstract": "Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP.",
    "num_pages": 10
}