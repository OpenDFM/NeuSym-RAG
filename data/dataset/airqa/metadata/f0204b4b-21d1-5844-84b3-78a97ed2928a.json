{
    "uuid": "f0204b4b-21d1-5844-84b3-78a97ed2928a",
    "title": "nancy-hicks-gribble at SemEval-2023 Task 5: Classifying and generating clickbait spoilers with RoBERTa",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{keller-etal-2023-nancy,\n    title = \"nancy-hicks-gribble at {S}em{E}val-2023 Task 5: Classifying and generating clickbait spoilers with {R}o{BERT}a\",\n    author = {Keller, J{\\\"u}ri  and\n      Rehbach, Nicolas  and\n      Zafar, Ibrahim},\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.238\",\n    doi = \"10.18653/v1/2023.semeval-1.238\",\n    pages = \"1712--1717\",\n    abstract = \"Clickbait spoiling and spoiler type classification in the setting of the SemEval2023 shared task five was used to explore transformer based text classification in comparison to conventional, shallow learned classifying models. Additionally, an initial model for spoiler creation was explored. The task was to classify or create spoilers for clickbait social media posts. The classification task was addressed by comparing different classifiers trained on hand crafted features to pre-trained and fine-tuned RoBERTa transformer models. The spoiler generation task was formulated as a question answering task, using the clickbait posts as questions and the articles as foundation to retrieve the answer from. The results show that even of the shelve transformer models outperform shallow learned models in the classification task. The spoiler generation task is more complex and needs an advanced system.\",\n}\n",
    "authors": [
        "JÃ¼ri Keller",
        "Nicolas Rehbach",
        "Ibrahim Zafar"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.238.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f0204b4b-21d1-5844-84b3-78a97ed2928a.pdf",
    "abstract": "Clickbait spoiling and spoiler type classification in the setting of the SemEval2023 shared task five was used to explore transformer based text classification in comparison to conventional, shallow learned classifying models. Additionally, an initial model for spoiler creation was explored. The task was to classify or create spoilers for clickbait social media posts. The classification task was addressed by comparing different classifiers trained on hand crafted features to pre-trained and fine-tuned RoBERTa transformer models. The spoiler generation task was formulated as a question answering task, using the clickbait posts as questions and the articles as foundation to retrieve the answer from. The results show that even of the shelve transformer models outperform shallow learned models in the classification task. The spoiler generation task is more complex and needs an advanced system.",
    "num_pages": 6
}