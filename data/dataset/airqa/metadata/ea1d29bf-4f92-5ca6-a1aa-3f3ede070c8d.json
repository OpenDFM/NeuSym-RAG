{
    "uuid": "ea1d29bf-4f92-5ca6-a1aa-3f3ede070c8d",
    "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yang-etal-2023-ganlm,\n    title = \"{G}an{LM}: Encoder-Decoder Pre-training with an Auxiliary Discriminator\",\n    author = \"Yang, Jian  and\n      Ma, Shuming  and\n      Dong, Li  and\n      Huang, Shaohan  and\n      Huang, Haoyang  and\n      Yin, Yuwei  and\n      Zhang, Dongdong  and\n      Yang, Liqun  and\n      Wei, Furu  and\n      Li, Zhoujun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.522\",\n    doi = \"10.18653/v1/2023.acl-long.522\",\n    pages = \"9394--9412\",\n    abstract = \"Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GanLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance.\",\n}\n",
    "authors": [
        "Jian Yang",
        "Shuming Ma",
        "Li Dong",
        "Shaohan Huang",
        "Haoyang Huang",
        "Yuwei Yin",
        "Dongdong Zhang",
        "Liqun Yang",
        "Furu Wei",
        "Zhoujun Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.522.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ea1d29bf-4f92-5ca6-a1aa-3f3ede070c8d.pdf",
    "abstract": "Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GanLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance.",
    "num_pages": 19
}