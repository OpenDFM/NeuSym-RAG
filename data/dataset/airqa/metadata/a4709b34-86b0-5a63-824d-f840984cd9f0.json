{
    "uuid": "a4709b34-86b0-5a63-824d-f840984cd9f0",
    "title": "Large Language Models respond to Influence like Humans",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the First Workshop on Social Influence in Conversations (SICon 2023)",
    "bibtex": "@inproceedings{griffin-etal-2023-large,\n    title = \"Large Language Models respond to Influence like Humans\",\n    author = \"Griffin, Lewis  and\n      Kleinberg, Bennett  and\n      Mozes, Maximilian  and\n      Mai, Kimberly  and\n      Vau, Maria Do Mar  and\n      Caldwell, Matthew  and\n      Mavor-Parker, Augustine\",\n    editor = \"Chawla, Kushal  and\n      Shi, Weiyan\",\n    booktitle = \"Proceedings of the First Workshop on Social Influence in Conversations (SICon 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sicon-1.3\",\n    doi = \"10.18653/v1/2023.sicon-1.3\",\n    pages = \"15--24\",\n    abstract = \"Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement boosts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same pattern of effects in both populations; although with greater per statement variability for the LLM. The second study concerns a specific mode of influence {--} populist framing of news to increase its persuasion and political mobilization. Newly collected data from simulated subjects was compared to previously published data from a 15 country experiment on 7286 human participants. Several effects from the human study were replicated by the simulated study, including ones that surprised the authors of the human study by contradicting their theoretical expectations; but some significant relationships found in human data were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.\",\n}\n",
    "authors": [
        "Lewis Griffin",
        "Bennett Kleinberg",
        "Maximilian Mozes",
        "Kimberly Mai",
        "Maria Do Mar Vau",
        "Matthew Caldwell",
        "Augustine Mavor-Parker"
    ],
    "pdf_url": "https://aclanthology.org/2023.sicon-1.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a4709b34-86b0-5a63-824d-f840984cd9f0.pdf",
    "abstract": "Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement boosts a later truthfulness test rating. Analysis of newly collected data from human and LLM-simulated subjects (1000 of each) showed the same pattern of effects in both populations; although with greater per statement variability for the LLM. The second study concerns a specific mode of influence â€“ populist framing of news to increase its persuasion and political mobilization. Newly collected data from simulated subjects was compared to previously published data from a 15 country experiment on 7286 human participants. Several effects from the human study were replicated by the simulated study, including ones that surprised the authors of the human study by contradicting their theoretical expectations; but some significant relationships found in human data were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.",
    "num_pages": 10
}