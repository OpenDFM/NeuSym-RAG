{
    "uuid": "f2078199-551b-54e7-97d3-c28db4d4a805",
    "title": "Fine-tuning Sentence-RoBERTa to Construct Word Embeddings for Low-resource Languages from Bilingual Dictionaries",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    "bibtex": "@inproceedings{bear-cook-2023-fine,\n    title = \"Fine-tuning Sentence-{R}o{BERT}a to Construct Word Embeddings for Low-resource Languages from Bilingual Dictionaries\",\n    author = \"Bear, Diego  and\n      Cook, Paul\",\n    editor = \"Mager, Manuel  and\n      Ebrahimi, Abteen  and\n      Oncevay, Arturo  and\n      Rice, Enora  and\n      Rijhwani, Shruti  and\n      Palmer, Alexis  and\n      Kann, Katharina\",\n    booktitle = \"Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.americasnlp-1.7\",\n    doi = \"10.18653/v1/2023.americasnlp-1.7\",\n    pages = \"47--57\",\n    abstract = \"Conventional approaches to learning word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are limited to relatively few languages with sufficiently large training corpora. To address this limitation, we propose an alternative approach to deriving word embeddings for Wolastoqey and Mi{'}kmaq that leverages definitions from a bilingual dictionary. More specifically, following Bear and Cook (2022), we experiment with encoding English definitions of Wolastoqey and Mi{'}kmaq words into vector representations using English sequence representation models. For this, we consider using and finetuning sentence-RoBERTa models (Reimers and Gurevych, 2019). We evaluate our word embeddings using a similar methodology to that of Bear and Cook using evaluations based on word classification, clustering and reverse dictionary search. We additionally construct word embeddings for higher-resource languages English, German and Spanishusing our methods and evaluate our embeddings on existing word-similarity datasets. Our findings indicate that our word embedding methods can be used to produce meaningful vector representations for low-resource languages such as Wolastoqey and Mi{'}kmaq and for higher-resource languages.\",\n}\n",
    "authors": [
        "Diego Bear",
        "Paul Cook"
    ],
    "pdf_url": "https://aclanthology.org/2023.americasnlp-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f2078199-551b-54e7-97d3-c28db4d4a805.pdf",
    "abstract": "Conventional approaches to learning word embeddings (Mikolov et al., 2013; Pennington et al., 2014) are limited to relatively few languages with sufficiently large training corpora. To address this limitation, we propose an alternative approach to deriving word embeddings for Wolastoqey and Mi’kmaq that leverages definitions from a bilingual dictionary. More specifically, following Bear and Cook (2022), we experiment with encoding English definitions of Wolastoqey and Mi’kmaq words into vector representations using English sequence representation models. For this, we consider using and finetuning sentence-RoBERTa models (Reimers and Gurevych, 2019). We evaluate our word embeddings using a similar methodology to that of Bear and Cook using evaluations based on word classification, clustering and reverse dictionary search. We additionally construct word embeddings for higher-resource languages English, German and Spanishusing our methods and evaluate our embeddings on existing word-similarity datasets. Our findings indicate that our word embedding methods can be used to produce meaningful vector representations for low-resource languages such as Wolastoqey and Mi’kmaq and for higher-resource languages.",
    "num_pages": 11
}