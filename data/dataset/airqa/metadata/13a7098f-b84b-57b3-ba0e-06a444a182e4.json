{
    "uuid": "13a7098f-b84b-57b3-ba0e-06a444a182e4",
    "title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2024-era,\n    title = \"{ERA}-{C}o{T}: Improving Chain-of-Thought through Entity Relationship Analysis\",\n    author = \"Liu, Yanming  and\n      Peng, Xinyue  and\n      Du, Tianyu  and\n      Yin, Jianwei  and\n      Liu, Weihao  and\n      Zhang, Xuhong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.476\",\n    doi = \"10.18653/v1/2024.acl-long.476\",\n    pages = \"8780--8794\",\n    abstract = \"Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1{\\%} on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM{'}s understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.\",\n}\n",
    "authors": [
        "Yanming Liu",
        "Xinyue Peng",
        "Tianyu Du",
        "Jianwei Yin",
        "Weihao Liu",
        "Xuhong Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.476.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/13a7098f-b84b-57b3-ba0e-06a444a182e4.pdf",
    "abstract": "Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLMâ€™s understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.",
    "num_pages": 15
}