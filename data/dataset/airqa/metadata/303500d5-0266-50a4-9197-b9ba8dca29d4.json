{
    "uuid": "303500d5-0266-50a4-9197-b9ba8dca29d4",
    "title": "The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{peng-etal-2023-devil,\n    title = \"The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation\",\n    author = \"Peng, Hao  and\n      Wang, Xiaozhi  and\n      Yao, Feng  and\n      Zeng, Kaisheng  and\n      Hou, Lei  and\n      Li, Juanzi  and\n      Liu, Zhiyuan  and\n      Shen, Weixing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.586\",\n    doi = \"10.18653/v1/2023.findings-acl.586\",\n    pages = \"9206--9227\",\n    abstract = \"Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OmniEvent, which can be obtained from \\url{https://github.com/THU-KEG/OmniEvent}.\",\n}\n",
    "authors": [
        "Hao Peng",
        "Xiaozhi Wang",
        "Feng Yao",
        "Kaisheng Zeng",
        "Lei Hou",
        "Juanzi Li",
        "Zhiyuan Liu",
        "Weixing Shen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.586.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/303500d5-0266-50a4-9197-b9ba8dca29d4.pdf",
    "abstract": "Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OmniEvent, which can be obtained from https://github.com/THU-KEG/OmniEvent.",
    "num_pages": 22
}