{
    "uuid": "dd0089c4-ab7d-58b2-87a1-58cca0a93049",
    "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yuan-etal-2023-distilling,\n    title = \"Distilling Script Knowledge from Large Language Models for Constrained Language Planning\",\n    author = \"Yuan, Siyu  and\n      Chen, Jiangjie  and\n      Fu, Ziquan  and\n      Ge, Xuyang  and\n      Shah, Soham  and\n      Jankowski, Charles  and\n      Xiao, Yanghua  and\n      Yang, Deqing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.236\",\n    doi = \"10.18653/v1/2023.acl-long.236\",\n    pages = \"4303--4325\",\n    abstract = \"In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., {``}make a cake{''}), but leaves more specific goals with multi-facet constraints understudied (e.g., {``}make a cake for diabetics{''}). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.\",\n}\n",
    "authors": [
        "Siyu Yuan",
        "Jiangjie Chen",
        "Ziquan Fu",
        "Xuyang Ge",
        "Soham Shah",
        "Charles Jankowski",
        "Yanghua Xiao",
        "Deqing Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.236.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dd0089c4-ab7d-58b2-87a1-58cca0a93049.pdf",
    "abstract": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.",
    "num_pages": 23
}