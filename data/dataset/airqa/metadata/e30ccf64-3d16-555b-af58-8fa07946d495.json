{
    "uuid": "e30ccf64-3d16-555b-af58-8fa07946d495",
    "title": "Evaluating Factual Consistency of Texts with Semantic Role Labeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{fan-etal-2023-evaluating,\n    title = \"Evaluating Factual Consistency of Texts with Semantic Role Labeling\",\n    author = \"Fan, Jing  and\n      Aumiller, Dennis  and\n      Gertz, Michael\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.9\",\n    doi = \"10.18653/v1/2023.starsem-1.9\",\n    pages = \"89--100\",\n    abstract = \"Automated evaluation of text generation systems has recently seen increasing attention, particularly checking whether generated text stays truthful to input sources. Existing methods frequently rely on an evaluation using task-specific language models, which in turn allows for little interpretability of generated scores. We introduce SRLScore, a reference-free evaluation metric designed with text summarization in mind. Our approach generates fact tuples constructed from Semantic Role Labels, applied to both input and summary texts.A final factuality score is computed by an adjustable scoring mechanism, which allows for easy adaption of the method across domains. Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning. We experiment with an optional co-reference resolution step, but find that the performance boost is mostly outweighed by the additional compute required. Our metric is available online at: \\url{https://github.com/heyjing/SRLScore}\",\n}\n",
    "authors": [
        "Jing Fan",
        "Dennis Aumiller",
        "Michael Gertz"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e30ccf64-3d16-555b-af58-8fa07946d495.pdf",
    "abstract": "Automated evaluation of text generation systems has recently seen increasing attention, particularly checking whether generated text stays truthful to input sources. Existing methods frequently rely on an evaluation using task-specific language models, which in turn allows for little interpretability of generated scores. We introduce SRLScore, a reference-free evaluation metric designed with text summarization in mind. Our approach generates fact tuples constructed from Semantic Role Labels, applied to both input and summary texts.A final factuality score is computed by an adjustable scoring mechanism, which allows for easy adaption of the method across domains. Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning. We experiment with an optional co-reference resolution step, but find that the performance boost is mostly outweighed by the additional compute required. Our metric is available online at: https://github.com/heyjing/SRLScore",
    "num_pages": 12
}