{
    "uuid": "1427e907-2d92-5ff9-8c3a-dd7f695a97d6",
    "title": "Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{huang-etal-2024-make,\n    title = \"Make-A-Voice: Revisiting Voice Large Language Models as Scalable Multilingual and Multitask Learners\",\n    author = \"Huang, Rongjie  and\n      Zhang, Chunlei  and\n      Wang, Yongqi  and\n      Yang, Dongchao  and\n      Tian, Jinchuan  and\n      Ye, Zhenhui  and\n      Liu, Luping  and\n      Wang, Zehan  and\n      Jiang, Ziyue  and\n      Chang, Xuankai  and\n      Shi, Jiatong  and\n      Weng, Chao  and\n      Zhao, Zhou  and\n      Yu, Dong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.589\",\n    doi = \"10.18653/v1/2024.acl-long.589\",\n    pages = \"10929--10942\",\n    abstract = \"Large language models (LLMs) have successfully served as a general-purpose interface across multiple tasks and languages, while the adaptation of voice LLMs is mostly designed for specific purposes (either single-task or monolingual), where the advantages of LLMs especially for low-resource language processing and zero-shot task generalization are less exploited in the audio community. To bridge the gap, we introduce Make-A-Voice as a multi-modal voice LLM and conduct a comprehensive study on its capability to deal with multiple tasks/languages. When trained on {\\textasciitilde}200K hours of 6-language data for 4 voice generation applications, Make-A-Voice emerges notable advantages: 1) as scalable learners to improve performance with end-to-end local and global multiscale transformers; and 2) as multitask learners by adjusting prompts to share common knowledge across modalities (speech/singing) and present in-context learning abilities by generalizing to unseen tasks not explicitly train on; 3) as multilingual learners to alleviate data scarcity of low-resource languages by including rich-resource language training data. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models in monolingual/cross-lingual voice generation. Audio samples are available at https://M-Voice.github.io\",\n}\n",
    "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Jinchuan Tian",
        "Zhenhui Ye",
        "Luping Liu",
        "Zehan Wang",
        "Ziyue Jiang",
        "Xuankai Chang",
        "Jiatong Shi",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.589.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1427e907-2d92-5ff9-8c3a-dd7f695a97d6.pdf",
    "abstract": "Large language models (LLMs) have successfully served as a general-purpose interface across multiple tasks and languages, while the adaptation of voice LLMs is mostly designed for specific purposes (either single-task or monolingual), where the advantages of LLMs especially for low-resource language processing and zero-shot task generalization are less exploited in the audio community. To bridge the gap, we introduce Make-A-Voice as a multi-modal voice LLM and conduct a comprehensive study on its capability to deal with multiple tasks/languages. When trained on ~200K hours of 6-language data for 4 voice generation applications, Make-A-Voice emerges notable advantages: 1) as scalable learners to improve performance with end-to-end local and global multiscale transformers; and 2) as multitask learners by adjusting prompts to share common knowledge across modalities (speech/singing) and present in-context learning abilities by generalizing to unseen tasks not explicitly train on; 3) as multilingual learners to alleviate data scarcity of low-resource languages by including rich-resource language training data. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models in monolingual/cross-lingual voice generation. Audio samples are available at https://M-Voice.github.io",
    "num_pages": 14
}