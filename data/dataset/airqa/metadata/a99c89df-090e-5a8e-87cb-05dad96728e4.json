{
    "uuid": "a99c89df-090e-5a8e-87cb-05dad96728e4",
    "title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{jia-etal-2023-kafa,\n    title = \"{KAFA}: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models\",\n    author = \"Jia, Zhiwei  and\n      Narayana, Pradyumna  and\n      Akula, Arjun  and\n      Pruthi, Garima  and\n      Su, Hao  and\n      Basu, Sugato  and\n      Jampani, Varun\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.74\",\n    doi = \"10.18653/v1/2023.acl-industry.74\",\n    pages = \"772--785\",\n    abstract = \"Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.\",\n}\n",
    "authors": [
        "Zhiwei Jia",
        "Pradyumna Narayana",
        "Arjun Akula",
        "Garima Pruthi",
        "Hao Su",
        "Sugato Basu",
        "Varun Jampani"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.74.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a99c89df-090e-5a8e-87cb-05dad96728e4.pdf",
    "abstract": "Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.",
    "num_pages": 14
}