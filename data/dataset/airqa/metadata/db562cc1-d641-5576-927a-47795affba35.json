{
    "uuid": "db562cc1-d641-5576-927a-47795affba35",
    "title": "Compromesso! Italian Many-Shot Jailbreaks undermine the safety of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{pernisi-etal-2024-compromesso,\n    title = \"Compromesso! {I}talian Many-Shot Jailbreaks undermine the safety of Large Language Models\",\n    author = \"Pernisi, Fabio  and\n      Hovy, Dirk  and\n      Rï¿½ttger, Paul\",\n    editor = \"Fu, Xiyan  and\n      Fleisig, Eve\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-srw.29\",\n    doi = \"10.18653/v1/2024.acl-srw.29\",\n    pages = \"245--251\",\n    abstract = \"As diverse linguistic communities and users adopt Large Language Models (LLMs), assessing their safety across languages becomes critical. Despite ongoing efforts to align these models with safe and ethical guidelines, they can still be induced into unsafe behavior with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. What research has been conducted on these vulnerabilities was predominantly on English, limiting the understanding of LLM behavior in other languages. We address this gap by investigating Many-Shot Jailbreaking (MSJ) in Italian, underscoring the importance of understanding LLM behavior in different languages. We base our analysis on a newly created Italian dataset to identify unique safety vulnerabilities in 4 families of open-source LLMs.We find that the models exhibit unsafe behaviors even with minimal exposure to harmful prompts, and{--}more alarmingly{--}this tendency rapidly escalates with more demonstrations.\",\n}\n",
    "authors": [
        "Fabio Pernisi",
        "Dirk Hovy",
        "Paul R�ttger"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-srw.29.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/db562cc1-d641-5576-927a-47795affba35.pdf",
    "abstract": "As diverse linguistic communities and users adopt Large Language Models (LLMs), assessing their safety across languages becomes critical. Despite ongoing efforts to align these models with safe and ethical guidelines, they can still be induced into unsafe behavior with jailbreaking, a technique in which models are prompted to act outside their operational guidelines. What research has been conducted on these vulnerabilities was predominantly on English, limiting the understanding of LLM behavior in other languages. We address this gap by investigating Many-Shot Jailbreaking (MSJ) in Italian, underscoring the importance of understanding LLM behavior in different languages. We base our analysis on a newly created Italian dataset to identify unique safety vulnerabilities in 4 families of open-source LLMs.We find that the models exhibit unsafe behaviors even with minimal exposure to harmful prompts, and–more alarmingly–this tendency rapidly escalates with more demonstrations.",
    "num_pages": 7
}