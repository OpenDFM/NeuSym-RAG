{
    "uuid": "309a8f36-23b1-5141-88a3-e053f9374f22",
    "title": "Evaluating LLMs’ Mathematical Reasoning in Financial Document Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{srivastava-etal-2024-evaluating,\n    title = \"Evaluating {LLM}s{'} Mathematical Reasoning in Financial Document Question Answering\",\n    author = \"Srivastava, Pragya  and\n      Malik, Manuj  and\n      Gupta, Vivek  and\n      Ganu, Tanuja  and\n      Roth, Dan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.231\",\n    doi = \"10.18653/v1/2024.findings-acl.231\",\n    pages = \"3853--3878\",\n    abstract = \"Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs{'} mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs{'} capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique EEDP tailored to semi-structured documents, matching or outperforming baselines performance while providing a nuanced understanding of LLMs abilities.\",\n}\n",
    "authors": [
        "Pragya Srivastava",
        "Manuj Malik",
        "Vivek Gupta",
        "Tanuja Ganu",
        "Dan Roth"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.231.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/309a8f36-23b1-5141-88a3-e053f9374f22.pdf",
    "abstract": "Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with a hybrid of structured tables and unstructured text remain uncertain. This study explores LLMs’ mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs’ capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique EEDP tailored to semi-structured documents, matching or outperforming baselines performance while providing a nuanced understanding of LLMs abilities.",
    "num_pages": 26
}