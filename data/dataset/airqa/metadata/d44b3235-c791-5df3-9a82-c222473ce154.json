{
    "uuid": "d44b3235-c791-5df3-9a82-c222473ce154",
    "title": "The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{zhu-etal-2024-model,\n    title = \"The Model Arena for Cross-lingual Sentiment Analysis: A Comparative Study in the Era of Large Language Models\",\n    author = \"Zhu, Xiliang  and\n      Gardiner, Shayna  and\n      Rold{\\'a}n, Tere  and\n      Rossouw, David\",\n    editor = \"De Clercq, Orph{\\'e}e  and\n      Barriere, Valentin  and\n      Barnes, Jeremy  and\n      Klinger, Roman  and\n      Sedoc, Jo{\\~a}o  and\n      Tafreshi, Shabnam\",\n    booktitle = \"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.wassa-1.12\",\n    doi = \"10.18653/v1/2024.wassa-1.12\",\n    pages = \"141--152\",\n    abstract = \"Sentiment analysis serves as a pivotal component in Natural Language Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R and mT5 have contributed to the increasing interest in cross-lingual sentiment analysis. The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied. This work undertakes an empirical analysis to compare the cross-lingual transfer capability of public Small Multilingual Language Models (SMLM) like XLM-R, against English-centric LLMs such as Llama-3, in the context of sentiment analysis across English, Spanish, French and Chinese. Our findings reveal that among public models, SMLMs exhibit superior zero-shot cross-lingual performance relative to LLMs. However, in few-shot cross-lingual settings, public LLMs demonstrate an enhanced adaptive potential. In addition, we observe that proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but are outpaced by public models in few-shot scenarios.\",\n}\n",
    "authors": [
        "Xiliang Zhu",
        "Shayna Gardiner",
        "Tere Rold√°n",
        "David Rossouw"
    ],
    "pdf_url": "https://aclanthology.org/2024.wassa-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d44b3235-c791-5df3-9a82-c222473ce154.pdf",
    "abstract": "Sentiment analysis serves as a pivotal component in Natural Language Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R and mT5 have contributed to the increasing interest in cross-lingual sentiment analysis. The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied. This work undertakes an empirical analysis to compare the cross-lingual transfer capability of public Small Multilingual Language Models (SMLM) like XLM-R, against English-centric LLMs such as Llama-3, in the context of sentiment analysis across English, Spanish, French and Chinese. Our findings reveal that among public models, SMLMs exhibit superior zero-shot cross-lingual performance relative to LLMs. However, in few-shot cross-lingual settings, public LLMs demonstrate an enhanced adaptive potential. In addition, we observe that proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but are outpaced by public models in few-shot scenarios.",
    "num_pages": 12
}