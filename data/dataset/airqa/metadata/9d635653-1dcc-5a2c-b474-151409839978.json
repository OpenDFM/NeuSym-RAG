{
    "uuid": "9d635653-1dcc-5a2c-b474-151409839978",
    "title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2023-kga,\n    title = \"{KGA}: A General Machine Unlearning Framework Based on Knowledge Gap Alignment\",\n    author = \"Wang, Lingzhi  and\n      Chen, Tong  and\n      Yuan, Wei  and\n      Zeng, Xingshan  and\n      Wong, Kam-Fai  and\n      Yin, Hongzhi\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.740\",\n    doi = \"10.18653/v1/2023.acl-long.740\",\n    pages = \"13264--13276\",\n    abstract = \"Recent legislation of the {``}right to be forgotten{''} has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks.\",\n}\n",
    "authors": [
        "Lingzhi Wang",
        "Tong Chen",
        "Wei Yuan",
        "Xingshan Zeng",
        "Kam-Fai Wong",
        "Hongzhi Yin"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.740.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9d635653-1dcc-5a2c-b474-151409839978.pdf",
    "abstract": "Recent legislation of the “right to be forgotten” has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks.",
    "num_pages": 13
}