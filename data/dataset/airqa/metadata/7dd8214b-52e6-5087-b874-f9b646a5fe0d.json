{
    "uuid": "7dd8214b-52e6-5087-b874-f9b646a5fe0d",
    "title": "Topic-Guided Self-Introduction Generation for Social Media Users",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{xu-etal-2023-topic,\n    title = \"Topic-Guided Self-Introduction Generation for Social Media Users\",\n    author = \"Xu, Chunpu  and\n      Li, Jing  and\n      Li, Piji  and\n      Yang, Min\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.722\",\n    doi = \"10.18653/v1/2023.findings-acl.722\",\n    pages = \"11387--11402\",\n    abstract = \"Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user{'}s personal interests. While most prior work profiling users with tags (e.g., ages), we investigate sentence-level self-introductions to provide a more natural and engaging way for users to know each other. Here we exploit a user{'}s tweeting history to generate their self-introduction. The task is non-trivial because the history content may be lengthy, noisy, and exhibit various personal interests. To address this challenge, we propose a novel unified topic-guided encoder-decoder (UTGED) framework; it models latent topics to reflect salient user interest, whose topic mixture then guides encoding a user{'}s history and topic words control decoding their self-introduction. For experiments, we collect a large-scale Twitter dataset, and extensive results show the superiority of our UTGED to the advanced encoder-decoder models without topic modeling.\",\n}\n",
    "authors": [
        "Chunpu Xu",
        "Jing Li",
        "Piji Li",
        "Min Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.722.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7dd8214b-52e6-5087-b874-f9b646a5fe0d.pdf",
    "abstract": "Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user’s personal interests. While most prior work profiling users with tags (e.g., ages), we investigate sentence-level self-introductions to provide a more natural and engaging way for users to know each other. Here we exploit a user’s tweeting history to generate their self-introduction. The task is non-trivial because the history content may be lengthy, noisy, and exhibit various personal interests. To address this challenge, we propose a novel unified topic-guided encoder-decoder (UTGED) framework; it models latent topics to reflect salient user interest, whose topic mixture then guides encoding a user’s history and topic words control decoding their self-introduction. For experiments, we collect a large-scale Twitter dataset, and extensive results show the superiority of our UTGED to the advanced encoder-decoder models without topic modeling.",
    "num_pages": 16
}