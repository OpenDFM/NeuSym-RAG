{
    "uuid": "300d526e-cb15-5b75-9b98-ca558a50bbb6",
    "title": "Labels are not necessary: Assessing peer-review helpfulness using domain adaptation based on self-training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{liu-etal-2023-labels,\n    title = \"Labels are not necessary: Assessing peer-review helpfulness using domain adaptation based on self-training\",\n    author = \"Liu, Chengyuan  and\n      Doshi, Divyang  and\n      Bhargava, Muskaan  and\n      Shang, Ruixuan  and\n      Cui, Jialin  and\n      Xu, Dongkuan  and\n      Gehringer, Edward\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.15\",\n    doi = \"10.18653/v1/2023.bea-1.15\",\n    pages = \"173--183\",\n    abstract = \"A peer-assessment system allows students to provide feedback on each other{'}s work. An effective peer assessment system urgently requires helpful reviews to facilitate students to make improvements and progress. Automated evaluation of review helpfulness, with the help of deep learning models and natural language processing techniques, gains much interest in the field of peer assessment. However, collecting labeled data with the {``}helpfulness{''} tag to build these prediction models remains challenging. A straightforward solution would be using a supervised learning algorithm to train a prediction model on a similar domain and apply it to our peer review domain for inference. But naively doing so can degrade the model performance in the presence of the distributional gap between domains. Such a distributional gap can be effectively addressed by Domain Adaptation (DA). Self-training has recently been shown as a powerful branch of DA to address the distributional gap. The first goal of this study is to evaluate the performance of self-training-based DA in predicting the helpfulness of peer reviews as well as the ability to overcome the distributional gap. Our second goal is to propose an advanced self-training framework to overcome the weakness of the existing self-training by tailoring knowledge distillation and noise injection, to further improve the model performance and better address the distributional gap.\",\n}\n",
    "authors": [
        "Chengyuan Liu",
        "Divyang Doshi",
        "Muskaan Bhargava",
        "Ruixuan Shang",
        "Jialin Cui",
        "Dongkuan Xu",
        "Edward Gehringer"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.15.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/300d526e-cb15-5b75-9b98-ca558a50bbb6.pdf",
    "abstract": "A peer-assessment system allows students to provide feedback on each other’s work. An effective peer assessment system urgently requires helpful reviews to facilitate students to make improvements and progress. Automated evaluation of review helpfulness, with the help of deep learning models and natural language processing techniques, gains much interest in the field of peer assessment. However, collecting labeled data with the “helpfulness” tag to build these prediction models remains challenging. A straightforward solution would be using a supervised learning algorithm to train a prediction model on a similar domain and apply it to our peer review domain for inference. But naively doing so can degrade the model performance in the presence of the distributional gap between domains. Such a distributional gap can be effectively addressed by Domain Adaptation (DA). Self-training has recently been shown as a powerful branch of DA to address the distributional gap. The first goal of this study is to evaluate the performance of self-training-based DA in predicting the helpfulness of peer reviews as well as the ability to overcome the distributional gap. Our second goal is to propose an advanced self-training framework to overcome the weakness of the existing self-training by tailoring knowledge distillation and noise injection, to further improve the model performance and better address the distributional gap.",
    "num_pages": 11
}