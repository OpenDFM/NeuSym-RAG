{
    "uuid": "f5b259c8-79b5-5e60-a008-28c80841ecb7",
    "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{guo-etal-2024-stabletoolbench,\n    title = \"{S}table{T}ool{B}ench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\",\n    author = \"Guo, Zhicheng  and\n      Cheng, Sijie  and\n      Wang, Hao  and\n      Liang, Shihao  and\n      Qin, Yujia  and\n      Li, Peng  and\n      Liu, Zhiyuan  and\n      Sun, Maosong  and\n      Liu, Yang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.664\",\n    doi = \"10.18653/v1/2024.findings-acl.664\",\n    pages = \"11143--11156\",\n    abstract = \"Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.\",\n}\n",
    "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Hao Wang",
        "Shihao Liang",
        "Yujia Qin",
        "Peng Li",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Yang Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.664.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f5b259c8-79b5-5e60-a008-28c80841ecb7.pdf",
    "abstract": "Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate the stability of StableToolBench, and further discuss the effectiveness of API simulators, the caching system, and the evaluator system.",
    "num_pages": 14
}