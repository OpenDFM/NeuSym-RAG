{
    "uuid": "d14af8e9-59e2-52f1-a6a4-70519cfa2418",
    "title": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{xu-etal-2023-unleash,\n    title = \"How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?\",\n    author = \"Xu, Xin  and\n      Zhu, Yuqi  and\n      Wang, Xiaohan  and\n      Zhang, Ningyu\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.13\",\n    doi = \"10.18653/v1/2023.sustainlp-1.13\",\n    pages = \"190--200\",\n}\n",
    "authors": [
        "Xin Xu",
        "Yuqi Zhu",
        "Xiaohan Wang",
        "Ningyu Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d14af8e9-59e2-52f1-a6a4-70519cfa2418.pdf",
    "abstract": "Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction1.",
    "num_pages": 11
}