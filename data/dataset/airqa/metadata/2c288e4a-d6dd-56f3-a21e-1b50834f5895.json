{
    "uuid": "2c288e4a-d6dd-56f3-a21e-1b50834f5895",
    "title": "HW-TSC’s submission to the IWSLT 2024 Subtitling track",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    "bibtex": "@inproceedings{xie-etal-2024-hw,\n    title = \"{HW}-{TSC}{'}s submission to the {IWSLT} 2024 Subtitling track\",\n    author = \"Xie, Yuhao  and\n      Luo, Yuanchang  and\n      Li, Zongyao  and\n      Wu, Zhanglin  and\n      Chen, Xiaoyu  and\n      Rao, Zhiqiang  and\n      Li, Shaojun  and\n      Shang, Hengchao  and\n      Guo, Jiaxin  and\n      Wei, Daimeng  and\n      Yang, Hao\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.iwslt-1.34\",\n    doi = \"10.18653/v1/2024.iwslt-1.34\",\n    pages = \"286--290\",\n    abstract = \"This paper introduces HW-TSC{'}s submission to the IWSLT 2024 Subtitling track. For the automatic subtitling track, we use an unconstrained cascaded strategy, with the main steps being: ASR with word-level timestamps, sentence segmentation based on punctuation restoration, further alignment using CTC or using machine translation with length penalty. For the subtitle compression track, we employ a subtitle compression strategy that integrates machine translation models and extensive rewriting models. We acquire the subtitle text requiring revision through the CPS index, then utilize a translation model to obtain the English version of this text. Following this, we extract the compressed-length subtitle text through controlled decoding. If this method fails to compress the text successfully, we resort to the Llama2 few-shot model for further compression.\",\n}\n",
    "authors": [
        "Yuhao Xie",
        "Yuanchang Luo",
        "Zongyao Li",
        "Zhanglin Wu",
        "Xiaoyu Chen",
        "Zhiqiang Rao",
        "Shaojun Li",
        "Hengchao Shang",
        "Jiaxin Guo",
        "Daimeng Wei",
        "Hao Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.iwslt-1.34.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2c288e4a-d6dd-56f3-a21e-1b50834f5895.pdf",
    "abstract": "This paper introduces HW-TSC’s submission to the IWSLT 2024 Subtitling track. For the automatic subtitling track, we use an unconstrained cascaded strategy, with the main steps being: ASR with word-level timestamps, sentence segmentation based on punctuation restoration, further alignment using CTC or using machine translation with length penalty. For the subtitle compression track, we employ a subtitle compression strategy that integrates machine translation models and extensive rewriting models. We acquire the subtitle text requiring revision through the CPS index, then utilize a translation model to obtain the English version of this text. Following this, we extract the compressed-length subtitle text through controlled decoding. If this method fails to compress the text successfully, we resort to the Llama2 few-shot model for further compression.",
    "num_pages": 5
}