{
    "uuid": "16edf1d6-12ff-5f2e-84b0-6f6246cf6fc5",
    "title": "InstructEd: Soft-Instruction Tuning for Model Editing with Hops",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{han-etal-2024-instructed,\n    title = \"{I}nstruct{E}d: Soft-Instruction Tuning for Model Editing with Hops\",\n    author = \"Han, XiaoQi  and\n      Li, Ru  and\n      Li, Xiaoli  and\n      Liang, Jiye  and\n      Zhang, Zifang  and\n      Pan, Jeff\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.888\",\n    doi = \"10.18653/v1/2024.findings-acl.888\",\n    pages = \"14953--14968\",\n    abstract = \"The task of model editing becomes popular for correcting inaccurate or outdated parametric knowledge in Large Language Models (LLMs). However, there are major limitations of state of the art (SOTA) model editing methods, including the excessive memorization issue caused by the direct editing methods, as well as the error propagation and knowledge conflict issues from the memory enhancement methods, resulting in hindering models{'} *portability*, e.g., the ability to transfer the new knowledge to related one-hop or multi-hop content. To address these issues, we propose the InstructEd method, the idea of which is to insert soft instructions into the attention module so as to facilitate interactions between instructions and questions and to understand and utilize new facts. Our main findings are: (i) InstructEd has achieved SOTA performance on three datasets for one-hop/multi-hop evaluation with LLaMAs and GPT2, achieving 10{\\%} (5{\\%}) improvement in one-hop (multi-hop) model editing.(ii) Different from earlier methods on editing parameters in FFN, we show that editing attention can also help. (iii) Model editing is highly related to retrieval augmented methods, which can help improve the locality of model editing while slightly decrease the editing performance with hops.\",\n}\n",
    "authors": [
        "XiaoQi Han",
        "Ru Li",
        "Xiaoli Li",
        "Jiye Liang",
        "Zifang Zhang",
        "Jeff Pan"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.888.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/16edf1d6-12ff-5f2e-84b0-6f6246cf6fc5.pdf",
    "abstract": "The task of model editing becomes popular for correcting inaccurate or outdated parametric knowledge in Large Language Models (LLMs). However, there are major limitations of state of the art (SOTA) model editing methods, including the excessive memorization issue caused by the direct editing methods, as well as the error propagation and knowledge conflict issues from the memory enhancement methods, resulting in hindering modelsâ€™ *portability*, e.g., the ability to transfer the new knowledge to related one-hop or multi-hop content. To address these issues, we propose the InstructEd method, the idea of which is to insert soft instructions into the attention module so as to facilitate interactions between instructions and questions and to understand and utilize new facts. Our main findings are: (i) InstructEd has achieved SOTA performance on three datasets for one-hop/multi-hop evaluation with LLaMAs and GPT2, achieving 10% (5%) improvement in one-hop (multi-hop) model editing.(ii) Different from earlier methods on editing parameters in FFN, we show that editing attention can also help. (iii) Model editing is highly related to retrieval augmented methods, which can help improve the locality of model editing while slightly decrease the editing performance with hops.",
    "num_pages": 16
}