{
    "uuid": "7b510502-5804-5008-8c04-b8a71c5948f9",
    "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{singh-etal-2024-aya,\n    title = \"Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning\",\n    author = {Singh, Shivalika  and\n      Vargus, Freddie  and\n      D{'}souza, Daniel  and\n      Karlsson, B{\\\"o}rje  and\n      Mahendiran, Abinaya  and\n      Ko, Wei-Yin  and\n      Shandilya, Herumb  and\n      Patel, Jay  and\n      Mataciunas, Deividas  and\n      O{'}Mahony, Laura  and\n      Zhang, Mike  and\n      Hettiarachchi, Ramith  and\n      Wilson, Joseph  and\n      Machado, Marina  and\n      Moura, Luisa  and\n      Krzemi{\\'n}ski, Dominik  and\n      Fadaei, Hakimeh  and\n      Ergun, Irem  and\n      Okoh, Ifeoma  and\n      Alaagib, Aisha  and\n      Mudannayake, Oshan  and\n      Alyafeai, Zaid  and\n      Chien, Vu  and\n      Ruder, Sebastian  and\n      Guthikonda, Surya  and\n      Alghamdi, Emad  and\n      Gehrmann, Sebastian  and\n      Muennighoff, Niklas  and\n      Bartolo, Max  and\n      Kreutzer, Julia  and\n      {\\\"U}st{\\\"u}n, Ahmet  and\n      Fadaee, Marzieh  and\n      Hooker, Sara},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.620\",\n    doi = \"10.18653/v1/2024.acl-long.620\",\n    pages = \"11521--11567\",\n    abstract = \"Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and open-source the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.\",\n}\n",
    "authors": [
        "Shivalika Singh",
        "Freddie Vargus",
        "Daniel D’souza",
        "Börje Karlsson",
        "Abinaya Mahendiran",
        "Wei-Yin Ko",
        "Herumb Shandilya",
        "Jay Patel",
        "Deividas Mataciunas",
        "Laura O’Mahony",
        "Mike Zhang",
        "Ramith Hettiarachchi",
        "Joseph Wilson",
        "Marina Machado",
        "Luisa Moura",
        "Dominik Krzemiński",
        "Hakimeh Fadaei",
        "Irem Ergun",
        "Ifeoma Okoh",
        "Aisha Alaagib",
        "Oshan Mudannayake",
        "Zaid Alyafeai",
        "Vu Chien",
        "Sebastian Ruder",
        "Surya Guthikonda",
        "Emad Alghamdi",
        "Sebastian Gehrmann",
        "Niklas Muennighoff",
        "Max Bartolo",
        "Julia Kreutzer",
        "Ahmet Üstün",
        "Marzieh Fadaee",
        "Sara Hooker"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.620.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7b510502-5804-5008-8c04-b8a71c5948f9.pdf",
    "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the fine-tuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and augmenting existing datasets across 114 languages. In total, we contribute three key resources: we develop and open-source the Aya Dataset, the Aya Collection, and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case study in participatory research, involving collaborators from 119 countries. We see this as an important framework for future research collaborations that aim to bridge gaps in resources.",
    "num_pages": 47
}