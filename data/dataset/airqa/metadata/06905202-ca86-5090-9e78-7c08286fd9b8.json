{
    "uuid": "06905202-ca86-5090-9e78-7c08286fd9b8",
    "title": "Data Sampling and (In)stability in Machine Translation Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lo-knowles-2023-data,\n    title = \"Data Sampling and (In)stability in Machine Translation Evaluation\",\n    author = \"Lo, Chi-kiu  and\n      Knowles, Rebecca\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.826\",\n    doi = \"10.18653/v1/2023.findings-acl.826\",\n    pages = \"13064--13074\",\n    abstract = \"We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling procedure as separate from questions about human annotator consistency. We provide evidence that the latest data sampling approach used at WMT skews the annotated data toward shorter documents, not necessarily representative of the full test set. Lastly, we examine a new data sampling method that uses the available labour budget to sample data in a more representative manner, with the goals of improving representation of various document lengths in the sample and producing more stable rankings of system translation quality.\",\n}\n",
    "authors": [
        "Chi-kiu Lo",
        "Rebecca Knowles"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.826.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/06905202-ca86-5090-9e78-7c08286fd9b8.pdf",
    "abstract": "We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling procedure as separate from questions about human annotator consistency. We provide evidence that the latest data sampling approach used at WMT skews the annotated data toward shorter documents, not necessarily representative of the full test set. Lastly, we examine a new data sampling method that uses the available labour budget to sample data in a more representative manner, with the goals of improving representation of various document lengths in the sample and producing more stable rankings of system translation quality.",
    "num_pages": 11
}