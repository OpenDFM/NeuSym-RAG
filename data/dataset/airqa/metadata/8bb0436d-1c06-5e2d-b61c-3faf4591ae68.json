{
    "uuid": "8bb0436d-1c06-5e2d-b61c-3faf4591ae68",
    "title": "Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{sharma-etal-2023-multi,\n    title = \"Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning\",\n    author = \"Sharma, Brihat  and\n      Gao, Yanjun  and\n      Miller, Timothy  and\n      Churpek, Matthew  and\n      Afshar, Majid  and\n      Dligach, Dmitriy\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.10\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.10\",\n    pages = \"78--85\",\n    abstract = \"Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH. We demonstrate that a multi-task, clinically-trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.\",\n}\n",
    "authors": [
        "Brihat Sharma",
        "Yanjun Gao",
        "Timothy Miller",
        "Matthew Churpek",
        "Majid Afshar",
        "Dmitriy Dligach"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8bb0436d-1c06-5e2d-b61c-3faf4591ae68.pdf",
    "abstract": "Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH. We demonstrate that a multi-task, clinically-trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.",
    "num_pages": 8
}