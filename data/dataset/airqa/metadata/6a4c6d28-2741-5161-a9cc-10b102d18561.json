{
    "uuid": "6a4c6d28-2741-5161-a9cc-10b102d18561",
    "title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ghosh-etal-2024-abex,\n    title = \"{ABEX}: Data Augmentation for Low-Resource {NLU} via Expanding Abstract Descriptions\",\n    author = \"Ghosh, Sreyan  and\n      Tyagi, Utkarsh  and\n      Kumar, Sonal  and\n      Evuru, Chandra Kiran  and\n      S, Ramaneswaran  and\n      Sakshi, S  and\n      Manocha, Dinesh\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.43\",\n    doi = \"10.18653/v1/2024.acl-long.43\",\n    pages = \"726--748\",\n    abstract = \"We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document {--} we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04{\\%} - 38.8{\\%}. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.\",\n}\n",
    "authors": [
        "Sreyan Ghosh",
        "Utkarsh Tyagi",
        "Sonal Kumar",
        "Chandra Kiran Evuru",
        "Ramaneswaran S",
        "S Sakshi",
        "Dinesh Manocha"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.43.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6a4c6d28-2741-5161-a9cc-10b102d18561.pdf",
    "abstract": "We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document â€“ we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.",
    "num_pages": 23
}