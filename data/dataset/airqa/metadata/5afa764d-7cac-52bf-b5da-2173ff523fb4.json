{
    "uuid": "5afa764d-7cac-52bf-b5da-2173ff523fb4",
    "title": "MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{zhang-etal-2023-monet,\n    title = \"{M}o{NET}: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking\",\n    author = \"Zhang, Haoning  and\n      Bao, Junwei  and\n      Sun, Haipeng  and\n      Wu, Youzheng  and\n      Li, Wenye  and\n      Cui, Shuguang  and\n      He, Xiaodong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.33\",\n    doi = \"10.18653/v1/2023.findings-acl.33\",\n    pages = \"520--534\",\n    abstract = \"Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizes all history information, the dialogue state in the previous turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the previous turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the model{'}s ability to update and correct slot values. Furthermore, a contrastive contextmatching framework is designed to narrow the representation distance between a state and itscorresponding noised variant, which reduces the impact of noised state and makes the model better understand the dialogue history. Experimental results on MultiWOZ datasets show that MoNET outperforms previous DST methods. Ablations and analysis verify the effectiveness of MoNET in alleviating state momentum issues and improving the anti-noise ability.\",\n}\n",
    "authors": [
        "Haoning Zhang",
        "Junwei Bao",
        "Haipeng Sun",
        "Youzheng Wu",
        "Wenye Li",
        "Shuguang Cui",
        "Xiaodong He"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.33.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5afa764d-7cac-52bf-b5da-2173ff523fb4.pdf",
    "abstract": "Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizes all history information, the dialogue state in the previous turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the previous turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the modelâ€™s ability to update and correct slot values. Furthermore, a contrastive contextmatching framework is designed to narrow the representation distance between a state and itscorresponding noised variant, which reduces the impact of noised state and makes the model better understand the dialogue history. Experimental results on MultiWOZ datasets show that MoNET outperforms previous DST methods. Ablations and analysis verify the effectiveness of MoNET in alleviating state momentum issues and improving the anti-noise ability.",
    "num_pages": 15
}