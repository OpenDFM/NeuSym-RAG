{
    "uuid": "c9575445-b621-58f6-8fc4-9d5275f219e6",
    "title": "LEGENT: Open Platform for Embodied Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{cheng-etal-2024-legent,\n    title = \"{LEGENT}: Open Platform for Embodied Agents\",\n    author = \"Cheng, Zhili  and\n      Wang, Zhitong  and\n      Hu, Jinyi  and\n      Hu, Shengding  and\n      Liu, An  and\n      Tu, Yuge  and\n      Li, Pengkai  and\n      Shi, Lei  and\n      Liu, Zhiyuan  and\n      Sun, Maosong\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.32\",\n    doi = \"10.18653/v1/2024.acl-demos.32\",\n    pages = \"335--345\",\n    abstract = \"Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in 3D environments. Existing integrations often feature limited open-sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich 3D environment with interactive, communicable, and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities. The demo video is available at the following link https://video.legent.ai.\",\n}\n",
    "authors": [
        "Zhili Cheng",
        "Zhitong Wang",
        "Jinyi Hu",
        "Shengding Hu",
        "An Liu",
        "Yuge Tu",
        "Pengkai Li",
        "Lei Shi",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c9575445-b621-58f6-8fc4-9d5275f219e6.pdf",
    "abstract": "Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in 3D environments. Existing integrations often feature limited open-sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich 3D environment with interactive, communicable, and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities. The demo video is available at the following link https://video.legent.ai.",
    "num_pages": 11
}