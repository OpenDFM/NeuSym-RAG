{
    "uuid": "eaffb2f5-2f00-50d3-a232-6b28711db4dd",
    "title": "Vision-Language Models under Cultural and Inclusive Considerations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 1st Human-Centered Large Language Modeling Workshop",
    "bibtex": "@inproceedings{karamolegkou-etal-2024-vision,\n    title = \"Vision-Language Models under Cultural and Inclusive Considerations\",\n    author = \"Karamolegkou, Antonia  and\n      Rust, Phillip  and\n      Cui, Ruixiang  and\n      Cao, Yong  and\n      S{\\o}gaard, Anders  and\n      Hershcovich, Daniel\",\n    editor = \"Soni, Nikita  and\n      Flek, Lucie  and\n      Sharma, Ashish  and\n      Yang, Diyi  and\n      Hooker, Sara  and\n      Schwartz, H. Andrew\",\n    booktitle = \"Proceedings of the 1st Human-Centered Large Language Modeling Workshop\",\n    month = aug,\n    year = \"2024\",\n    address = \"TBD\",\n    publisher = \"ACL\",\n    url = \"https://aclanthology.org/2024.hucllm-1.5\",\n    doi = \"10.18653/v1/2024.hucllm-1.5\",\n    pages = \"53--66\",\n    abstract = \"Large Vision Language Models can be used to assist visually impaired individuals by describing images they capture in their daily lives. Current evaluation datasets may not reflect the diverse cultural user backgrounds nor the situational context of this use case. To address this problem, we create a survey to determine caption preferences and propose a culture-centric evaluation benchmark by filtering VizWiz, an existing dataset with images taken by people who are blind. We then evaluate different models and prompts, investigating their reliability as visual assistants. While the evaluation results for state-of-the-art models seem promising, we identified some weak spots such as hallucinations and problems with conventional evaluation metrics. Our survey, data, code, and model outputs will be publicly available.\",\n}\n",
    "authors": [
        "Antonia Karamolegkou",
        "Phillip Rust",
        "Ruixiang Cui",
        "Yong Cao",
        "Anders SÃ¸gaard",
        "Daniel Hershcovich"
    ],
    "pdf_url": "https://aclanthology.org/2024.hucllm-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/eaffb2f5-2f00-50d3-a232-6b28711db4dd.pdf",
    "abstract": "Large Vision Language Models can be used to assist visually impaired individuals by describing images they capture in their daily lives. Current evaluation datasets may not reflect the diverse cultural user backgrounds nor the situational context of this use case. To address this problem, we create a survey to determine caption preferences and propose a culture-centric evaluation benchmark by filtering VizWiz, an existing dataset with images taken by people who are blind. We then evaluate different models and prompts, investigating their reliability as visual assistants. While the evaluation results for state-of-the-art models seem promising, we identified some weak spots such as hallucinations and problems with conventional evaluation metrics. Our survey, data, code, and model outputs will be publicly available.",
    "num_pages": 14
}