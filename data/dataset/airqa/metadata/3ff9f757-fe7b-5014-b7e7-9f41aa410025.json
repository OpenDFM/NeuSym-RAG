{
    "uuid": "3ff9f757-fe7b-5014-b7e7-9f41aa410025",
    "title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{huang-etal-2024-lost,\n    title = \"Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation\",\n    author = \"Huang, Xu  and\n      Zhang, Zhirui  and\n      Geng, Xiang  and\n      Du, Yichao  and\n      Chen, Jiajun  and\n      Huang, Shujian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.211\",\n    doi = \"10.18653/v1/2024.findings-acl.211\",\n    pages = \"3546--3562\",\n    abstract = \"This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task.We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information.We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs{'} inability to fully leverage the cross-lingual capability when evaluating translations.Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results.These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks.\",\n}\n",
    "authors": [
        "Xu Huang",
        "Zhirui Zhang",
        "Xiang Geng",
        "Yichao Du",
        "Jiajun Chen",
        "Shujian Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.211.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3ff9f757-fe7b-5014-b7e7-9f41aa410025.pdf",
    "abstract": "This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task.We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information.We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMsâ€™ inability to fully leverage the cross-lingual capability when evaluating translations.Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results.These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks.",
    "num_pages": 17
}