{
    "uuid": "3864e505-e5ba-5a58-be82-53d6eb3c97c2",
    "title": "Exploring Continual Learning for Code Generation Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{yadav-etal-2023-exploring,\n    title = \"Exploring Continual Learning for Code Generation Models\",\n    author = \"Yadav, Prateek  and\n      Sun, Qing  and\n      Ding, Hantian  and\n      Li, Xiaopeng  and\n      Zhang, Dejiao  and\n      Tan, Ming  and\n      Bhatia, Parminder  and\n      Ma, Xiaofei  and\n      Nallapati, Ramesh  and\n      Ramanathan, Murali Krishna  and\n      Bansal, Mohit  and\n      Xiang, Bing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.68\",\n    doi = \"10.18653/v1/2023.acl-short.68\",\n    pages = \"782--792\",\n    abstract = \"Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54{\\%} improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models.\",\n}\n",
    "authors": [
        "Prateek Yadav",
        "Qing Sun",
        "Hantian Ding",
        "Xiaopeng Li",
        "Dejiao Zhang",
        "Ming Tan",
        "Parminder Bhatia",
        "Xiaofei Ma",
        "Ramesh Nallapati",
        "Murali Krishna Ramanathan",
        "Mohit Bansal",
        "Bing Xiang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.68.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3864e505-e5ba-5a58-be82-53d6eb3c97c2.pdf",
    "abstract": "Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models.",
    "num_pages": 11
}