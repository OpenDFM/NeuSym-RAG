{
    "uuid": "53f7900a-0bf9-5488-a60e-ec4045633dd7",
    "title": "What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{watson-etal-2023-social,\n    title = \"What social attitudes about gender does {BERT} encode? Leveraging insights from psycholinguistics\",\n    author = \"Watson, Julia  and\n      Beekhuizen, Barend  and\n      Stevenson, Suzanne\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.375\",\n    doi = \"10.18653/v1/2023.acl-long.375\",\n    pages = \"6790--6809\",\n    abstract = \"Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people{'}s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT{'}s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others.\",\n}\n",
    "authors": [
        "Julia Watson",
        "Barend Beekhuizen",
        "Suzanne Stevenson"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.375.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/53f7900a-0bf9-5488-a60e-ec4045633dd7.pdf",
    "abstract": "Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people’s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT’s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others.",
    "num_pages": 20
}