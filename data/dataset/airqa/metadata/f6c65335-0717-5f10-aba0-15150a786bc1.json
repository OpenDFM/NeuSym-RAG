{
    "uuid": "f6c65335-0717-5f10-aba0-15150a786bc1",
    "title": "Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{soni-etal-2024-comparing,\n    title = \"Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?\",\n    author = \"Soni, Nikita  and\n      Balasubramanian, Niranjan  and\n      Schwartz, H. Andrew  and\n      Hovy, Dirk\",\n    editor = \"De Clercq, Orph{\\'e}e  and\n      Barriere, Valentin  and\n      Barnes, Jeremy  and\n      Klinger, Roman  and\n      Sedoc, Jo{\\~a}o  and\n      Tafreshi, Shabnam\",\n    booktitle = \"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.wassa-1.26\",\n    doi = \"10.18653/v1/2024.wassa-1.26\",\n    pages = \"316--328\",\n    abstract = \"Pre-trained language models consider the context of neighboring words and documents but lack any author context of the human generating the text. However, language depends on the author{'}s states, traits, social, situational, and environmental attributes, collectively referred to as human context (Soni et al., 2024). Human-centered natural language processing requires incorporating human context into language models. Currently, two methods exist: pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2) individual traits. Group attributes are simple but coarse {---} not all 45-year-olds write the same way {---} while individual traits allow for more personalized representations, but require more complex modeling and data. It is unclear which approach benefits what tasks. We compare pre-training models with human context via 1) group attributes, 2) individual users, and 3) a combined approach on five user- and document-level tasks. Our results show that there is no best approach, but that human-centered language modeling holds avenues for different methods.\",\n}\n",
    "authors": [
        "Nikita Soni",
        "Niranjan Balasubramanian",
        "H. Andrew Schwartz",
        "Dirk Hovy"
    ],
    "pdf_url": "https://aclanthology.org/2024.wassa-1.26.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f6c65335-0717-5f10-aba0-15150a786bc1.pdf",
    "abstract": "Pre-trained language models consider the context of neighboring words and documents but lack any author context of the human generating the text. However, language depends on the author’s states, traits, social, situational, and environmental attributes, collectively referred to as human context (Soni et al., 2024). Human-centered natural language processing requires incorporating human context into language models. Currently, two methods exist: pre-training with 1) group-wise attributes (e.g., over-45-year-olds) or 2) individual traits. Group attributes are simple but coarse — not all 45-year-olds write the same way — while individual traits allow for more personalized representations, but require more complex modeling and data. It is unclear which approach benefits what tasks. We compare pre-training models with human context via 1) group attributes, 2) individual users, and 3) a combined approach on five user- and document-level tasks. Our results show that there is no best approach, but that human-centered language modeling holds avenues for different methods.",
    "num_pages": 13
}