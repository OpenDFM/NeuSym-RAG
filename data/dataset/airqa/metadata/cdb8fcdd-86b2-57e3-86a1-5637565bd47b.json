{
    "uuid": "cdb8fcdd-86b2-57e3-86a1-5637565bd47b",
    "title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{kwan-etal-2024-m4le,\n    title = \"{M}4{LE}: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models\",\n    author = \"Kwan, Wai-Chung  and\n      Zeng, Xingshan  and\n      Wang, Yufei  and\n      Sun, Yusen  and\n      Li, Liangyou  and\n      Jiang, Yuxin  and\n      Shang, Lifeng  and\n      Liu, Qun  and\n      Wong, Kam-Fai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.832\",\n    doi = \"10.18653/v1/2024.acl-long.832\",\n    pages = \"15568--15592\",\n    abstract = \"Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M$^4$LE, a $\\textbf{M}$ulti-ability, $\\textbf{M}$ulti-range, $\\textbf{M}$ulti-task, $\\textbf{M}$ulti-domain benchmark for $\\textbf{L}$ong-context $\\textbf{E}$valuation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs{'} long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.\",\n}\n",
    "authors": [
        "Wai-Chung Kwan",
        "Xingshan Zeng",
        "Yufei Wang",
        "Yusen Sun",
        "Liangyou Li",
        "Yuxin Jiang",
        "Lifeng Shang",
        "Qun Liu",
        "Kam-Fai Wong"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.832.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/cdb8fcdd-86b2-57e3-86a1-5637565bd47b.pdf",
    "abstract": "Managing long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMsâ€™ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length. Our evaluation of 11 prominent LLMs reveals that 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.",
    "num_pages": 25
}