{
    "uuid": "7b760d50-2cf4-53d2-93b2-a9d3a80854cb",
    "title": "Ask LLMs Directly, “What shapes your bias?”: Measuring Social Bias in Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{shin-etal-2024-ask,\n    title = \"Ask {LLM}s Directly, {``}What shapes your bias?{''}: Measuring Social Bias in Large Language Models\",\n    author = \"Shin, Jisu  and\n      Song, Hoyun  and\n      Lee, Huije  and\n      Jeong, Soyeong  and\n      Park, Jong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.954\",\n    doi = \"10.18653/v1/2024.findings-acl.954\",\n    pages = \"16122--16143\",\n    abstract = \"Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.\",\n}\n",
    "authors": [
        "Jisu Shin",
        "Hoyun Song",
        "Huije Lee",
        "Soyeong Jeong",
        "Jong Park"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.954.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7b760d50-2cf4-53d2-93b2-a9d3a80854cb.pdf",
    "abstract": "Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.",
    "num_pages": 22
}