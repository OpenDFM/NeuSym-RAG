{
    "uuid": "bb21d74d-a54a-5573-adf2-838c972543e9",
    "title": "Minimalist Entity Disambiguation for Mid-Resource Languages",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{kruit-2023-minimalist,\n    title = \"Minimalist Entity Disambiguation for Mid-Resource Languages\",\n    author = \"Kruit, Benno\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.24\",\n    doi = \"10.18653/v1/2023.sustainlp-1.24\",\n    pages = \"299--306\",\n}\n",
    "authors": [
        "Benno Kruit"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.24.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/bb21d74d-a54a-5573-adf2-838c972543e9.pdf",
    "abstract": "For many languages and applications, even though enough data is available for training Named Entity Disambiguation (NED) systems, few off-the-shelf models are available for use in practice. This is due to both the large size of state-of-the-art models, and to the computational requirements for recreating them from scratch. However, we observe that in practice, acceptable models can be trained and run with far fewer resources. In this work, we introduce MiniNED, a framework for creating small NED models from medium-sized datasets. The resulting models can be tuned for applicationspecific objectives and trade-offs, depending on practitionersâ€™ requirements concerning model size, frequency bias, and out-of-domain generalization. We evaluate the framework in nine languages, and achieve reasonable performance using models that are a fraction of the size of recent work.",
    "num_pages": 8
}