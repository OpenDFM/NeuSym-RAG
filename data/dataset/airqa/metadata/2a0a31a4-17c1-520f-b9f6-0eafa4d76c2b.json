{
    "uuid": "2a0a31a4-17c1-520f-b9f6-0eafa4d76c2b",
    "title": "MELA: Multilingual Evaluation of Linguistic Acceptability",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-mela,\n    title = \"{MELA}: Multilingual Evaluation of Linguistic Acceptability\",\n    author = \"Zhang, Ziyin  and\n      Liu, Yikang  and\n      Huang, Weifang  and\n      Mao, Junyu  and\n      Wang, Rui  and\n      Hu, Hai\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.146\",\n    doi = \"10.18653/v1/2024.acl-long.146\",\n    pages = \"2658--2674\",\n    abstract = \"In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability{---}MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language{---}Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.\",\n}\n",
    "authors": [
        "Ziyin Zhang",
        "Yikang Liu",
        "Weifang Huang",
        "Junyu Mao",
        "Rui Wang",
        "Hai Hu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.146.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2a0a31a4-17c1-520f-b9f6-0eafa4d76c2b.pdf",
    "abstract": "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability—MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language—Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.",
    "num_pages": 17
}