{
    "uuid": "b7797be5-2f32-53e9-bfc2-11e5be80e300",
    "title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{hijazi-etal-2024-using,\n    title = \"Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation\",\n    author = \"Hijazi, Hashem  and\n      Molla, Diego  and\n      Nguyen, Vincent  and\n      Karimi, Sarvnaz\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.18\",\n    doi = \"10.18653/v1/2024.bionlp-1.18\",\n    pages = \"236--242\",\n    abstract = \"Biomedical question-answering systems remain popular for biomedical experts interacting with the literature to answer their medical questions. However, these systems are difficult to evaluate in the absence of costly human experts. Therefore, automatic evaluation metrics are often used in this space. Traditional automatic metrics such as ROUGE or BLEU, which rely on token overlap, have shown a low correlation with humans. We present a study that uses large language models (LLMs) to automatically evaluate systems from an international challenge on biomedical semantic indexing and question answering, called BioASQ. We measure the agreement of LLM-produced scores against human judgements. We show that LLMs correlate similarly to lexical methods when using basic prompting techniques. However, by aggregating evaluators with LLMs or by fine-tuning, we find that our methods outperform the baselines by a large margin, achieving a Spearman correlation of 0.501 and 0.511, respectively.\",\n}\n",
    "authors": [
        "Hashem Hijazi",
        "Diego Molla",
        "Vincent Nguyen",
        "Sarvnaz Karimi"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b7797be5-2f32-53e9-bfc2-11e5be80e300.pdf",
    "abstract": "Biomedical question-answering systems remain popular for biomedical experts interacting with the literature to answer their medical questions. However, these systems are difficult to evaluate in the absence of costly human experts. Therefore, automatic evaluation metrics are often used in this space. Traditional automatic metrics such as ROUGE or BLEU, which rely on token overlap, have shown a low correlation with humans. We present a study that uses large language models (LLMs) to automatically evaluate systems from an international challenge on biomedical semantic indexing and question answering, called BioASQ. We measure the agreement of LLM-produced scores against human judgements. We show that LLMs correlate similarly to lexical methods when using basic prompting techniques. However, by aggregating evaluators with LLMs or by fine-tuning, we find that our methods outperform the baselines by a large margin, achieving a Spearman correlation of 0.501 and 0.511, respectively.",
    "num_pages": 7
}