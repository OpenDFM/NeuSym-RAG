{
    "uuid": "77e939a7-5c49-528a-9261-2268646f391f",
    "title": "Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-paying,\n    title = \"Paying More Attention to Source Context: Mitigating Unfaithful Translations from Large Language Model\",\n    author = \"Zhang, Hongbin  and\n      Chen, Kehai  and\n      Bai, Xuefeng  and\n      Xiang, Yang  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.821\",\n    doi = \"10.18653/v1/2024.findings-acl.821\",\n    pages = \"13816--13836\",\n    abstract = \"Large language models (LLMs) have showcased their remarkable capabilities to handle various downstream tasks, including multilingual machine translation ability. Despite their impressive performance, decoder-only LLMs lack an explicit alignment between source and target contexts, leading to translation that may not faithfully represent the original content. To address this, we propose three learning strategies to encourage LLMs to pay more attention to the source context during translation: 1) adjusting attention weights on the source context by adaptive attention re-weighting; 2) suppressing the irrelevant target prefix using contrastive decoding; 3) avoiding excessive reliance on the target prefix through target-constrained tuning. To verify the effectiveness of our model, we curate a new dataset specifically focusing on unfaithful translations generated by LLMs. Experimental results on both human-collected and general test sets verify the effectiveness of our model across multiple language pairs. Further human evaluation demonstrates the efficacy of our method in reducing hallucinatory translation and improving the fidelity of translations.\",\n}\n",
    "authors": [
        "Hongbin Zhang",
        "Kehai Chen",
        "Xuefeng Bai",
        "Yang Xiang",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.821.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/77e939a7-5c49-528a-9261-2268646f391f.pdf",
    "abstract": "Large language models (LLMs) have showcased their remarkable capabilities to handle various downstream tasks, including multilingual machine translation ability. Despite their impressive performance, decoder-only LLMs lack an explicit alignment between source and target contexts, leading to translation that may not faithfully represent the original content. To address this, we propose three learning strategies to encourage LLMs to pay more attention to the source context during translation: 1) adjusting attention weights on the source context by adaptive attention re-weighting; 2) suppressing the irrelevant target prefix using contrastive decoding; 3) avoiding excessive reliance on the target prefix through target-constrained tuning. To verify the effectiveness of our model, we curate a new dataset specifically focusing on unfaithful translations generated by LLMs. Experimental results on both human-collected and general test sets verify the effectiveness of our model across multiple language pairs. Further human evaluation demonstrates the efficacy of our method in reducing hallucinatory translation and improving the fidelity of translations.",
    "num_pages": 21
}