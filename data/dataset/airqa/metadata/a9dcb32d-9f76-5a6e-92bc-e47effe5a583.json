{
    "uuid": "a9dcb32d-9f76-5a6e-92bc-e47effe5a583",
    "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bandarkar-etal-2024-belebele,\n    title = \"The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants\",\n    author = \"Bandarkar, Lucas  and\n      Liang, Davis  and\n      Muller, Benjamin  and\n      Artetxe, Mikel  and\n      Shukla, Satya Narayan  and\n      Husa, Donald  and\n      Goyal, Naman  and\n      Krishnan, Abhinandan  and\n      Zettlemoyer, Luke  and\n      Khabsa, Madian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.44\",\n    doi = \"10.18653/v1/2024.acl-long.44\",\n    pages = \"749--775\",\n    abstract = \"We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.\",\n}\n",
    "authors": [
        "Lucas Bandarkar",
        "Davis Liang",
        "Benjamin Muller",
        "Mikel Artetxe",
        "Satya Narayan Shukla",
        "Donald Husa",
        "Naman Goyal",
        "Abhinandan Krishnan",
        "Luke Zettlemoyer",
        "Madian Khabsa"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.44.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a9dcb32d-9f76-5a6e-92bc-e47effe5a583.pdf",
    "abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.",
    "num_pages": 27
}