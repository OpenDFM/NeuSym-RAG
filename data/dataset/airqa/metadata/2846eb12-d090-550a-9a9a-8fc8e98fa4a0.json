{
    "uuid": "2846eb12-d090-550a-9a9a-8fc8e98fa4a0",
    "title": "Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{sekizawa-yanaka-2023-analyzing,\n    title = \"Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on {J}apanese Honorific Conversion\",\n    author = \"Sekizawa, Ryo  and\n      Yanaka, Hitomi\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.5\",\n    doi = \"10.18653/v1/2023.starsem-1.5\",\n    pages = \"40--47\",\n    abstract = \"Using Japanese honorifics is challenging because it requires not only knowledge of the grammatical rules but also contextual information, such as social relationships. It remains unclear whether pre-trained large language models (LLMs) can flexibly handle Japanese honorifics like humans. To analyze this, we introduce an honorific conversion task that considers social relationships among people mentioned in a conversation. We construct a Japanese honorifics dataset from problem templates of various sentence structures to investigate the syntactic generalization capacity of GPT-3, one of the leading LLMs, on this task under two settings: fine-tuning and prompt learning. Our results showed that the fine-tuned GPT-3 performed better in a context-aware honorific conversion task than the prompt-based one. The fine-tuned model demonstrated overall syntactic generalizability towards compound honorific sentences, except when tested with the data involving direct speech.\",\n}\n",
    "authors": [
        "Ryo Sekizawa",
        "Hitomi Yanaka"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2846eb12-d090-550a-9a9a-8fc8e98fa4a0.pdf",
    "abstract": "Using Japanese honorifics is challenging because it requires not only knowledge of the grammatical rules but also contextual information, such as social relationships. It remains unclear whether pre-trained large language models (LLMs) can flexibly handle Japanese honorifics like humans. To analyze this, we introduce an honorific conversion task that considers social relationships among people mentioned in a conversation. We construct a Japanese honorifics dataset from problem templates of various sentence structures to investigate the syntactic generalization capacity of GPT-3, one of the leading LLMs, on this task under two settings: fine-tuning and prompt learning. Our results showed that the fine-tuned GPT-3 performed better in a context-aware honorific conversion task than the prompt-based one. The fine-tuned model demonstrated overall syntactic generalizability towards compound honorific sentences, except when tested with the data involving direct speech.",
    "num_pages": 8
}