{
    "uuid": "a099e95b-dc5c-57f1-a617-f8d35c58c92d",
    "title": "The NYA’s Offline Speech Translation System for IWSLT 2024",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    "bibtex": "@inproceedings{zhang-etal-2024-nyas,\n    title = \"The {NYA}{'}s Offline Speech Translation System for {IWSLT} 2024\",\n    author = \"Zhang, Yingxin  and\n      Ma, Guodong  and\n      Du, Binbin\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.iwslt-1.6\",\n    doi = \"10.18653/v1/2024.iwslt-1.6\",\n    pages = \"39--45\",\n    abstract = \"This paper reports the NYA{'}s submissions to IWSLT 2024 Offline Speech Translation (ST) task on the sub-tasks including English to Chinese, Japanese, and German. In detail, we participate in the unconstrained training track using the cascaded ST structure. For the automatic speech recognition (ASR) model, we use the Whisper large-v3 model. For the neural machine translation (NMT) model, the wider and deeper Transformer is adapted as the backbone model. Furthermore, we use data augmentation technologies to augment training data and data filtering strategies to improve the quality of training data. In addition, we explore many MT technologies such as Back Translation, Forward Translation, R-Drop, and Domain Adaptation.\",\n}\n",
    "authors": [
        "Yingxin Zhang",
        "Guodong Ma",
        "Binbin Du"
    ],
    "pdf_url": "https://aclanthology.org/2024.iwslt-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a099e95b-dc5c-57f1-a617-f8d35c58c92d.pdf",
    "abstract": "This paper reports the NYA’s submissions to IWSLT 2024 Offline Speech Translation (ST) task on the sub-tasks including English to Chinese, Japanese, and German. In detail, we participate in the unconstrained training track using the cascaded ST structure. For the automatic speech recognition (ASR) model, we use the Whisper large-v3 model. For the neural machine translation (NMT) model, the wider and deeper Transformer is adapted as the backbone model. Furthermore, we use data augmentation technologies to augment training data and data filtering strategies to improve the quality of training data. In addition, we explore many MT technologies such as Back Translation, Forward Translation, R-Drop, and Domain Adaptation.",
    "num_pages": 7
}