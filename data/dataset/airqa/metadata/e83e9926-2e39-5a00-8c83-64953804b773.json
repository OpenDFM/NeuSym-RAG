{
    "uuid": "e83e9926-2e39-5a00-8c83-64953804b773",
    "title": "Local Temperature Beam Search: Avoid Neural Text DeGeneration via Enhanced Calibration",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lee-etal-2023-local,\n    title = \"Local Temperature Beam Search: Avoid Neural Text {D}e{G}eneration via Enhanced Calibration\",\n    author = \"Lee, Dongkyu  and\n      Kim, Gyeonghun  and\n      Han, Janghoon  and\n      Hong, Taesuk  and\n      Kim, Yi-Reun  and\n      Choi, Stanley Jungkyu  and\n      Zhang, Nevin L.\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.628\",\n    doi = \"10.18653/v1/2023.findings-acl.628\",\n    pages = \"9903--9915\",\n    abstract = \"Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the {``}self-loop{''}. However, the remedy comes at the cost of sacrificing output quality due to the randomness involved. In this work, we introduce a deterministic decoding scheme, local temperature beam search. This inference algorithm is an embarrassingly simple variant of beam search, yet it reduces repetition, whose level is superior to that of a sampling-based decoding algorithm, while maintaining the level of coherence as in beam search. Our idea is rooted in the concept of model calibration; we view a repetition as a casualty from overconfidence in a model. Therefore, our work mitigates the miscalibration present in the course of inference with a post-calibration approach applied in beam-specific manner. Our inference scheme is validated on text completion tasks, in which the repetition problem is seen most clearly, and is exhaustively compared with existing inference schemes.\",\n}\n",
    "authors": [
        "Dongkyu Lee",
        "Gyeonghun Kim",
        "Janghoon Han",
        "Taesuk Hong",
        "Yi-Reun Kim",
        "Stanley Jungkyu Choi",
        "Nevin L. Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.628.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e83e9926-2e39-5a00-8c83-64953804b773.pdf",
    "abstract": "Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the “self-loop”. However, the remedy comes at the cost of sacrificing output quality due to the randomness involved. In this work, we introduce a deterministic decoding scheme, local temperature beam search. This inference algorithm is an embarrassingly simple variant of beam search, yet it reduces repetition, whose level is superior to that of a sampling-based decoding algorithm, while maintaining the level of coherence as in beam search. Our idea is rooted in the concept of model calibration; we view a repetition as a casualty from overconfidence in a model. Therefore, our work mitigates the miscalibration present in the course of inference with a post-calibration approach applied in beam-specific manner. Our inference scheme is validated on text completion tasks, in which the repetition problem is seen most clearly, and is exhaustively compared with existing inference schemes.",
    "num_pages": 13
}