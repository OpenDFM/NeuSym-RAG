{
    "uuid": "dc86a9ff-e29b-5e80-a8b3-da52b1ce6fa9",
    "title": "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bai-etal-2024-advancing,\n    title = \"Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation\",\n    author = \"Bai, Jiaxin  and\n      Wang, Yicheng  and\n      Zheng, Tianshi  and\n      Guo, Yue  and\n      Liu, Xin  and\n      Song, Yangqiu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.72\",\n    doi = \"10.18653/v1/2024.acl-long.72\",\n    pages = \"1312--1329\",\n    abstract = \"Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG. Experiments show that, with RLF-KG{'}s assistance, the generated hypotheses provide better explanations, and achieve state-of-the-art results on three widely used KGs.\",\n}\n",
    "authors": [
        "Jiaxin Bai",
        "Yicheng Wang",
        "Tianshi Zheng",
        "Yue Guo",
        "Xin Liu",
        "Yangqiu Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.72.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/dc86a9ff-e29b-5e80-a8b3-da52b1ce6fa9.pdf",
    "abstract": "Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG. Experiments show that, with RLF-KGâ€™s assistance, the generated hypotheses provide better explanations, and achieve state-of-the-art results on three widely used KGs.",
    "num_pages": 18
}