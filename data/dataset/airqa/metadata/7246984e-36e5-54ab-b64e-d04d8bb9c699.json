{
    "uuid": "7246984e-36e5-54ab-b64e-d04d8bb9c699",
    "title": "Can ChatGPT Understand Causal Language in Science Claims?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{kim-etal-2023-chatgpt,\n    title = \"Can {C}hat{GPT} Understand Causal Language in Science Claims?\",\n    author = \"Kim, Yuheun  and\n      Guo, Lu  and\n      Yu, Bei  and\n      Li, Yingya\",\n    editor = \"Barnes, Jeremy  and\n      De Clercq, Orph{\\'e}e  and\n      Klinger, Roman\",\n    booktitle = \"Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.wassa-1.33\",\n    doi = \"10.18653/v1/2023.wassa-1.33\",\n    pages = \"379--389\",\n    abstract = \"This study evaluated ChatGPT{'}s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.\",\n}\n",
    "authors": [
        "Yuheun Kim",
        "Lu Guo",
        "Bei Yu",
        "Yingya Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.wassa-1.33.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7246984e-36e5-54ab-b64e-d04d8bb9c699.pdf",
    "abstract": "This study evaluated ChatGPTâ€™s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.",
    "num_pages": 11
}