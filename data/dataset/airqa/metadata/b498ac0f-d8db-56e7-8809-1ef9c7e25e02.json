{
    "uuid": "b498ac0f-d8db-56e7-8809-1ef9c7e25e02",
    "title": "YNU-HPCC at WASSA-2023 Shared Task 1: Large-scale Language Model with LoRA Fine-Tuning for Empathy Detection and Emotion Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{wang-etal-2023-ynu,\n    title = \"{YNU}-{HPCC} at {WASSA}-2023 Shared Task 1: Large-scale Language Model with {L}o{RA} Fine-Tuning for Empathy Detection and Emotion Classification\",\n    author = \"Wang, Yukun  and\n      Wang, Jin  and\n      Zhang, Xuejie\",\n    editor = \"Barnes, Jeremy  and\n      De Clercq, Orph{\\'e}e  and\n      Klinger, Roman\",\n    booktitle = \"Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.wassa-1.45\",\n    doi = \"10.18653/v1/2023.wassa-1.45\",\n    pages = \"526--530\",\n    abstract = \"This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared Task 1: Empathy Detection and Emotion Classification. This task needs to predict the empathy, emotion, and personality of the empathic reactions. This system is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa) model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning in PEFT is used to reduce the training parameters of large language models. Moreover, back translation is introduced to augment the training dataset. This system achieved relatively good results on the competition{'}s official leaderboard. The code of this system is available here.\",\n}\n",
    "authors": [
        "Yukun Wang",
        "Jin Wang",
        "Xuejie Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2023.wassa-1.45.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b498ac0f-d8db-56e7-8809-1ef9c7e25e02.pdf",
    "abstract": "This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared Task 1: Empathy Detection and Emotion Classification. This task needs to predict the empathy, emotion, and personality of the empathic reactions. This system is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa) model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning in PEFT is used to reduce the training parameters of large language models. Moreover, back translation is introduced to augment the training dataset. This system achieved relatively good results on the competitionâ€™s official leaderboard. The code of this system is available here.",
    "num_pages": 5
}