{
    "uuid": "2c2853e1-c0b5-5d7d-8709-9da0b5c8202b",
    "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{monajatipoor-etal-2023-metavl,\n    title = \"{M}eta{VL}: Transferring In-Context Learning Ability From Language Models to Vision-Language Models\",\n    author = \"Monajatipoor, Masoud  and\n      Li, Liunian Harold  and\n      Rouhsedaghat, Mozhdeh  and\n      Yang, Lin  and\n      Chang, Kai-Wei\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.43\",\n    doi = \"10.18653/v1/2023.acl-short.43\",\n    pages = \"495--508\",\n    abstract = \"Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having {\\textasciitilde}20 times fewer parameters.\",\n}\n",
    "authors": [
        "Masoud Monajatipoor",
        "Liunian Harold Li",
        "Mozhdeh Rouhsedaghat",
        "Lin Yang",
        "Kai-Wei Chang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.43.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2c2853e1-c0b5-5d7d-8709-9da0b5c8202b.pdf",
    "abstract": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters.",
    "num_pages": 14
}