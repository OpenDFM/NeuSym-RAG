{
    "uuid": "e809b167-e94d-57ea-bba8-b1c3e27203e3",
    "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{huang-etal-2024-mitigating,\n    title = \"Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal\",\n    author = \"Huang, Jianheng  and\n      Cui, Leyang  and\n      Wang, Ante  and\n      Yang, Chengyi  and\n      Liao, Xinting  and\n      Song, Linfeng  and\n      Yao, Junfeng  and\n      Su, Jinsong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.77\",\n    doi = \"10.18653/v1/2024.acl-long.77\",\n    pages = \"1416--1428\",\n    abstract = \"Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model{'}s ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.\",\n}\n",
    "authors": [
        "Jianheng Huang",
        "Leyang Cui",
        "Ante Wang",
        "Chengyi Yang",
        "Xinting Liao",
        "Linfeng Song",
        "Junfeng Yao",
        "Jinsong Su"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.77.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e809b167-e94d-57ea-bba8-b1c3e27203e3.pdf",
    "abstract": "Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the modelâ€™s ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.",
    "num_pages": 13
}