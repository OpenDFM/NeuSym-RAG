{
    "uuid": "f361a52a-23d3-5425-9a34-a26bab750cd7",
    "title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhao-etal-2023-abductive,\n    title = \"Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations\",\n    author = \"Zhao, Wenting  and\n      Chiu, Justin  and\n      Cardie, Claire  and\n      Rush, Alexander\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.831\",\n    doi = \"10.18653/v1/2023.acl-long.831\",\n    pages = \"14883--14896\",\n    abstract = \"Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods.\",\n}\n",
    "authors": [
        "Wenting Zhao",
        "Justin Chiu",
        "Claire Cardie",
        "Alexander Rush"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.831.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f361a52a-23d3-5425-9a34-a26bab750cd7.pdf",
    "abstract": "Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods.",
    "num_pages": 14
}