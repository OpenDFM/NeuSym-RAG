{
    "uuid": "8fce0cf9-e22e-568b-94cc-d0ba03f8d50c",
    "title": "I2R’s End-to-End Speech Translation System for IWSLT 2023 Offline Shared Task",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{huzaifah-etal-2023-i2rs,\n    title = \"{I}2{R}{'}s End-to-End Speech Translation System for {IWSLT} 2023 Offline Shared Task\",\n    author = \"Huzaifah, Muhammad  and\n      Min Tan, Kye  and\n      Duan, Richeng\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.16\",\n    doi = \"10.18653/v1/2023.iwslt-1.16\",\n    pages = \"202--210\",\n    abstract = \"This paper describes I2R{'}s submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year{'}s edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.\",\n}\n",
    "authors": [
        "Muhammad Huzaifah",
        "Kye Min Tan",
        "Richeng Duan"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8fce0cf9-e22e-568b-94cc-d0ba03f8d50c.pdf",
    "abstract": "This paper describes I2R’s submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year’s edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.",
    "num_pages": 9
}