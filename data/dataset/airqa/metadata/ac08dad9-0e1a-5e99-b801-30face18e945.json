{
    "uuid": "ac08dad9-0e1a-5e99-b801-30face18e945",
    "title": "SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)",
    "bibtex": "@inproceedings{wang-etal-2024-slideavsr,\n    title = \"{S}lide{AVSR}: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition\",\n    author = \"Wang, Hao  and\n      Kurita, Shuhei  and\n      Shimizu, Shuichiro  and\n      Kawahara, Daisuke\",\n    editor = \"Gu, Jing  and\n      Fu, Tsu-Jui (Ray)  and\n      Hudson, Drew  and\n      Celikyilmaz, Asli  and\n      Wang, William\",\n    booktitle = \"Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.alvr-1.11\",\n    doi = \"10.18653/v1/2024.alvr-1.11\",\n    pages = \"129--137\",\n    abstract = \"Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR.\",\n}\n",
    "authors": [
        "Hao Wang",
        "Shuhei Kurita",
        "Shuichiro Shimizu",
        "Daisuke Kawahara"
    ],
    "pdf_url": "https://aclanthology.org/2024.alvr-1.11.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ac08dad9-0e1a-5e99-b801-30face18e945.pdf",
    "abstract": "Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR.",
    "num_pages": 9
}