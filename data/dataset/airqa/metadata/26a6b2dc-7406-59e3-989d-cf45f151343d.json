{
    "uuid": "26a6b2dc-7406-59e3-989d-cf45f151343d",
    "title": "Can Language Models Serve as Text-Based World Simulators?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-language,\n    title = \"Can Language Models Serve as Text-Based World Simulators?\",\n    author = \"Wang, Ruoyao  and\n      Todd, Graham  and\n      Xiao, Ziang  and\n      Yuan, Xingdi  and\n      C{\\^o}t{\\'e}, Marc-Alexandre  and\n      Clark, Peter  and\n      Jansen, Peter\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.1\",\n    doi = \"10.18653/v1/2024.acl-short.1\",\n    pages = \"1--17\",\n    abstract = \"Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM{'}s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.\",\n}\n",
    "authors": [
        "Ruoyao Wang",
        "Graham Todd",
        "Ziang Xiao",
        "Xingdi Yuan",
        "Marc-Alexandre Côté",
        "Peter Clark",
        "Peter Jansen"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/26a6b2dc-7406-59e3-989d-cf45f151343d.pdf",
    "abstract": "Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM’s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.",
    "num_pages": 17
}