{
    "uuid": "a68c5042-a6b4-5a72-941c-1db7a7136440",
    "title": "Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    "bibtex": "@inproceedings{shen-etal-2023-sen2pro,\n    title = \"{S}en2{P}ro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model\",\n    author = \"Shen, Lingfeng  and\n      Jiang, Haiyun  and\n      Liu, Lemao  and\n      Shi, Shuming\",\n    editor = \"Can, Burcu  and\n      Mozes, Maximilian  and\n      Cahyawijaya, Samuel  and\n      Saphra, Naomi  and\n      Kassner, Nora  and\n      Ravfogel, Shauli  and\n      Ravichander, Abhilasha  and\n      Zhao, Chen  and\n      Augenstein, Isabelle  and\n      Rogers, Anna  and\n      Cho, Kyunghyun  and\n      Grefenstette, Edward  and\n      Voita, Lena\",\n    booktitle = \"Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.repl4nlp-1.26\",\n    doi = \"10.18653/v1/2023.repl4nlp-1.26\",\n    pages = \"315--333\",\n}\n",
    "authors": [
        "Lingfeng Shen",
        "Haiyun Jiang",
        "Lemao Liu",
        "Shuming Shi"
    ],
    "pdf_url": "https://aclanthology.org/2023.repl4nlp-1.26.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a68c5042-a6b4-5a72-941c-1db7a7136440.pdf",
    "abstract": "Sentence embedding is one of the most fundamental tasks in Natural Language Processing and plays an important role in various tasks. The recent breakthrough in sentence embedding is achieved by pre-trained language models (PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point estimate does not naturally express uncertainty in a taskagnostic way. This paper thereby proposes an efficient framework on probabilistic sentence embedding (Sen2Pro) from PLMs, and it represents a sentence as a probability density distribution in an embedding space to reflect both model uncertainty and data uncertainty (i.e., many-to-one nature) in the sentence representation. The proposed framework performs in a plug-and-play way without retraining PLMs anymore, and it is easy to implement and generally applied on top of any PLM. The superiority of Sen2Pro over Sen2Vec has been theoretically verified and practically illustrated on different NLP tasks.",
    "num_pages": 19
}