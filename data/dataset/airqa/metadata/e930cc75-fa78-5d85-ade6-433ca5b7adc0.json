{
    "uuid": "e930cc75-fa78-5d85-ade6-433ca5b7adc0",
    "title": "Abstractive Text Summarization Using the BRIO Training Paradigm",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lam-etal-2023-abstractive,\n    title = \"Abstractive Text Summarization Using the {BRIO} Training Paradigm\",\n    author = \"Lam, Khang  and\n      Doan, Thieu  and\n      Pham, Khang  and\n      Kalita, Jugal\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.7\",\n    doi = \"10.18653/v1/2023.findings-acl.7\",\n    pages = \"92--99\",\n    abstract = \"Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model{'}s dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.\",\n}\n",
    "authors": [
        "Khang Lam",
        "Thieu Doan",
        "Khang Pham",
        "Jugal Kalita"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e930cc75-fa78-5d85-ade6-433ca5b7adc0.pdf",
    "abstract": "Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the modelâ€™s dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese.",
    "num_pages": 8
}