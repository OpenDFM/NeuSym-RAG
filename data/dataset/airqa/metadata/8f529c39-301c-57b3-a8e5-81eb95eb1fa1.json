{
    "uuid": "8f529c39-301c-57b3-a8e5-81eb95eb1fa1",
    "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{cheng-etal-2023-task,\n    title = \"Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering\",\n    author = \"Cheng, Hao  and\n      Fang, Hao  and\n      Liu, Xiaodong  and\n      Gao, Jianfeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.159\",\n    doi = \"10.18653/v1/2023.acl-short.159\",\n    pages = \"1864--1875\",\n    abstract = \"Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60{\\%} of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at \\url{https://github.com/microsoft/taser}.\",\n}\n",
    "authors": [
        "Hao Cheng",
        "Hao Fang",
        "Xiaodong Liu",
        "Jianfeng Gao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.159.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8f529c39-301c-57b3-a8e5-81eb95eb1fa1.pdf",
    "abstract": "Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser.",
    "num_pages": 12
}