{
    "uuid": "aad25ce3-2d26-5a48-9653-a41e0a749e55",
    "title": "Incorporating Graph Information in Transformer-based AMR Parsing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{vasylenko-etal-2023-incorporating,\n    title = \"Incorporating Graph Information in Transformer-based {AMR} Parsing\",\n    author = \"Vasylenko, Pavlo  and\n      Huguet Cabot, Pere Llu{\\'\\i}s  and\n      Mart{\\'\\i}nez Lorenzo, Abelardo Carlos  and\n      Navigli, Roberto\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.125\",\n    doi = \"10.18653/v1/2023.findings-acl.125\",\n    pages = \"1995--2011\",\n    abstract = \"Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [\\url{http://www.github.com/sapienzanlp/LeakDistill}](\\url{http://www.github.com/sapienzanlp/LeakDistill}).\",\n}\n",
    "authors": [
        "Pavlo Vasylenko",
        "Pere Lluís Huguet Cabot",
        "Abelardo Carlos Martínez Lorenzo",
        "Roberto Navigli"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.125.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/aad25ce3-2d26-5a48-9653-a41e0a749e55.pdf",
    "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [http://www.github.com/sapienzanlp/LeakDistill](http://www.github.com/sapienzanlp/LeakDistill).",
    "num_pages": 17
}