{
    "uuid": "8efd0cd9-d94c-54f9-b072-b104606d2be8",
    "title": "BASS: Batched Attention-optimized Speculative Sampling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{qian-etal-2024-bass,\n    title = \"{BASS}: Batched Attention-optimized Speculative Sampling\",\n    author = \"Qian, Haifeng  and\n      Gonugondla, Sujan Kumar  and\n      Ha, Sungsoo  and\n      Shang, Mingyue  and\n      Gouda, Sanjay Krishna  and\n      Nallapati, Ramesh  and\n      Sengupta, Sudipta  and\n      Ma, Xiaofei  and\n      Deoras, Anoop\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.489\",\n    doi = \"10.18653/v1/2024.findings-acl.489\",\n    pages = \"8214--8224\",\n    abstract = \"Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15$\\times$ speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43{\\%} and Pass@All of 61{\\%}, far exceeding what{'}s feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8{\\%}, more than 3$\\times$ the highest of that of regular decoding and around 10$\\times$ of single-sequence speculative decoding.\",\n}\n",
    "authors": [
        "Haifeng Qian",
        "Sujan Kumar Gonugondla",
        "Sungsoo Ha",
        "Mingyue Shang",
        "Sanjay Krishna Gouda",
        "Ramesh Nallapati",
        "Sudipta Sengupta",
        "Xiaofei Ma",
        "Anoop Deoras"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.489.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8efd0cd9-d94c-54f9-b072-b104606d2be8.pdf",
    "abstract": "Speculative decoding has emerged as a powerful method to improve latency and throughput in hosting large language models. However, most existing implementations focus on generating a single sequence. Real-world generative AI applications often require multiple responses and how to perform speculative decoding in a batched setting while preserving its latency benefits poses non-trivial challenges. This paper describes a system of batched speculative decoding that sets a new state of the art in multi-sequence generation latency and that demonstrates superior GPU utilization as well as quality of generations within a time budget. For example, for a 7.8B-size model on a single A100 GPU and with a batch size of 8, each sequence is generated at an average speed of 5.8ms per token, the overall throughput being 1.1K tokens per second. These results represent state-of-the-art latency and a 2.15× speed-up over optimized regular decoding. Within a time budget that regular decoding does not finish, our system is able to generate sequences with HumanEval Pass@First of 43% and Pass@All of 61%, far exceeding what’s feasible with single-sequence speculative decoding. Our peak GPU utilization during decoding reaches as high as 15.8%, more than 3× the highest of that of regular decoding and around 10× of single-sequence speculative decoding.",
    "num_pages": 11
}