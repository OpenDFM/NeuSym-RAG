{
    "uuid": "afb9cccc-1e81-570b-b152-2dcab3e37978",
    "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{fadeeva-etal-2024-fact,\n    title = \"Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification\",\n    author = \"Fadeeva, Ekaterina  and\n      Rubashevskii, Aleksandr  and\n      Shelmanov, Artem  and\n      Petrakov, Sergey  and\n      Li, Haonan  and\n      Mubarak, Hamdy  and\n      Tsymbalov, Evgenii  and\n      Kuzmin, Gleb  and\n      Panchenko, Alexander  and\n      Baldwin, Timothy  and\n      Nakov, Preslav  and\n      Panov, Maxim\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.558\",\n    doi = \"10.18653/v1/2024.findings-acl.558\",\n    pages = \"9367--9385\",\n    abstract = \"Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven different LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.\",\n}\n",
    "authors": [
        "Ekaterina Fadeeva",
        "Aleksandr Rubashevskii",
        "Artem Shelmanov",
        "Sergey Petrakov",
        "Haonan Li",
        "Hamdy Mubarak",
        "Evgenii Tsymbalov",
        "Gleb Kuzmin",
        "Alexander Panchenko",
        "Timothy Baldwin",
        "Preslav Nakov",
        "Maxim Panov"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.558.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/afb9cccc-1e81-570b-b152-2dcab3e37978.pdf",
    "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven different LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
    "num_pages": 19
}