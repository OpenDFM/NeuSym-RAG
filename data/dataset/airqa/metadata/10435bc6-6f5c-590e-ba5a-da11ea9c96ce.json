{
    "uuid": "10435bc6-6f5c-590e-ba5a-da11ea9c96ce",
    "title": "Diane Simmons at SemEval-2023 Task 5: Is it possible to make good clickbait spoilers using a Zero-Shot approach? Check it out!",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{krog-agirrezabal-2023-diane,\n    title = \"Diane Simmons at {S}em{E}val-2023 Task 5: Is it possible to make good clickbait spoilers using a Zero-Shot approach? Check it out!\",\n    author = \"Krog, Niels  and\n      Agirrezabal, Manex\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.66\",\n    doi = \"10.18653/v1/2023.semeval-1.66\",\n    pages = \"477--481\",\n    abstract = \"In this paper, we present a possible solution to the SemEval23 shared task of generating spoilers for clickbait headlines. Using a Zero-Shot approach with two different Transformer architectures, BLOOM and RoBERTa, we generate three different types of spoilers: phrase, passage and multi. We found, RoBERTa pretrained for Question-Answering to perform better than BLOOM for causal language modelling, however both architectures proved promising for future attempts at such tasks.\",\n}\n",
    "authors": [
        "Niels Krog",
        "Manex Agirrezabal"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.66.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/10435bc6-6f5c-590e-ba5a-da11ea9c96ce.pdf",
    "abstract": "In this paper, we present a possible solution to the SemEval23 shared task of generating spoilers for clickbait headlines. Using a Zero-Shot approach with two different Transformer architectures, BLOOM and RoBERTa, we generate three different types of spoilers: phrase, passage and multi. We found, RoBERTa pretrained for Question-Answering to perform better than BLOOM for causal language modelling, however both architectures proved promising for future attempts at such tasks.",
    "num_pages": 5
}