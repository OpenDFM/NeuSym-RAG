{
    "uuid": "b4352dbf-7216-549f-a2ea-3a09e41d95c7",
    "title": "SERENGETI: Massively Multilingual Language Models for Africa",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{adebara-etal-2023-serengeti,\n    title = \"{SERENGETI}: Massively Multilingual Language Models for {A}frica\",\n    author = \"Adebara, Ife  and\n      Elmadany, AbdelRahim  and\n      Abdul-Mageed, Muhammad  and\n      Alcoba Inciarte, Alcides\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.97\",\n    doi = \"10.18653/v1/2023.findings-acl.97\",\n    pages = \"1498--1537\",\n    abstract = \"Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only {\\textasciitilde}31 out of {\\textasciitilde}2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F{\\_}1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research. Anonymous link\",\n}\n",
    "authors": [
        "Ife Adebara",
        "AbdelRahim Elmadany",
        "Muhammad Abdul-Mageed",
        "Alcides Alcoba Inciarte"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.97.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b4352dbf-7216-549f-a2ea-3a09e41d95c7.pdf",
    "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research. Anonymous link",
    "num_pages": 40
}