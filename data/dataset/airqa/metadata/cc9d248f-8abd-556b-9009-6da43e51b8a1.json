{
    "uuid": "cc9d248f-8abd-556b-9009-6da43e51b8a1",
    "title": "TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cheng-etal-2024-transface,\n    title = \"{T}rans{F}ace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation\",\n    author = \"Cheng, Xize  and\n      Huang, Rongjie  and\n      Li, Linjun  and\n      Wang, Zehan  and\n      Jin, Tao  and\n      Yin, Aoxiong  and\n      Feiyang, Chen  and\n      Duan, Xinyu  and\n      Huai, Baoxing  and\n      Zhao, Zhou\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.593\",\n    doi = \"10.18653/v1/2024.findings-acl.593\",\n    pages = \"9973--9986\",\n    abstract = \"Direct speech-to-speech translation achieves high-quality results through the introduction of discrete units obtained from self-supervised learning. However, talking head translation, converting audio-visual speech (i.e., talking head video) from one language into another, still confronts several challenges compared to audio speech: (1) Existing methods invariably rely on cascading, synthesizing via both audio and text, resulting in delays and cascading errors. (2) Talking head translation has a limited set of reference frames. If the generated translation exceeds the length of the original speech, the video sequence needs to be supplemented by repeating frames, leading to jarring video transitions. In this work, we propose a model for talking head translation, \\textbf{TransFace}, which can directly translate audio-visual speech into audio-visual speech in other languages. It consists of a speech-to-unit translation model to convert audio speech into discrete units and a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize synchronized audio-visual speech from discrete units in parallel. Furthermore, we introduce a Bounded Duration Predictor, ensuring isometric talking head translation and preventing duplicate reference frames. Experiments demonstrate that Unit2Lip significantly improves synchronization and boosts inference speed by a factor of 4.35 on LRS2. Additionally, TransFace achieves impressive BLEU scores of 61.93 and 47.55 for Es-En and Fr-En on LRS3-T and 100{\\%} isochronous translations. The samples are available at https://transface-demo.github.io .\",\n}\n",
    "authors": [
        "Xize Cheng",
        "Rongjie Huang",
        "Linjun Li",
        "Zehan Wang",
        "Tao Jin",
        "Aoxiong Yin",
        "Chen Feiyang",
        "Xinyu Duan",
        "Baoxing Huai",
        "Zhou Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.593.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/cc9d248f-8abd-556b-9009-6da43e51b8a1.pdf",
    "abstract": "Direct speech-to-speech translation achieves high-quality results through the introduction of discrete units obtained from self-supervised learning. However, talking head translation, converting audio-visual speech (i.e., talking head video) from one language into another, still confronts several challenges compared to audio speech: (1) Existing methods invariably rely on cascading, synthesizing via both audio and text, resulting in delays and cascading errors. (2) Talking head translation has a limited set of reference frames. If the generated translation exceeds the length of the original speech, the video sequence needs to be supplemented by repeating frames, leading to jarring video transitions. In this work, we propose a model for talking head translation, TransFace, which can directly translate audio-visual speech into audio-visual speech in other languages. It consists of a speech-to-unit translation model to convert audio speech into discrete units and a unit-based audio-visual speech synthesizer, Unit2Lip, to re-synthesize synchronized audio-visual speech from discrete units in parallel. Furthermore, we introduce a Bounded Duration Predictor, ensuring isometric talking head translation and preventing duplicate reference frames. Experiments demonstrate that Unit2Lip significantly improves synchronization and boosts inference speed by a factor of 4.35 on LRS2. Additionally, TransFace achieves impressive BLEU scores of 61.93 and 47.55 for Es-En and Fr-En on LRS3-T and 100% isochronous translations. The samples are available at https://transface-demo.github.io .",
    "num_pages": 14
}