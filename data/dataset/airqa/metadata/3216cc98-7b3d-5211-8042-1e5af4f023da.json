{
    "uuid": "3216cc98-7b3d-5211-8042-1e5af4f023da",
    "title": "PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-privlm,\n    title = \"{P}riv{LM}-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models\",\n    author = \"Li, Haoran  and\n      Guo, Dadi  and\n      Li, Donghao  and\n      Fan, Wei  and\n      Hu, Qi  and\n      Liu, Xin  and\n      Chan, Chunkit  and\n      Yao, Duanyi  and\n      Yao, Yuan  and\n      Song, Yangqiu\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.4\",\n    doi = \"10.18653/v1/2024.acl-long.4\",\n    pages = \"54--73\",\n    abstract = \"The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.\",\n}\n",
    "authors": [
        "Haoran Li",
        "Dadi Guo",
        "Donghao Li",
        "Wei Fan",
        "Qi Hu",
        "Xin Liu",
        "Chunkit Chan",
        "Duanyi Yao",
        "Yuan Yao",
        "Yangqiu Song"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.4.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3216cc98-7b3d-5211-8042-1e5af4f023da.pdf",
    "abstract": "The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.",
    "num_pages": 20
}