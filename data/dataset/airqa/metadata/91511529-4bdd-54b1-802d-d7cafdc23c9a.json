{
    "uuid": "91511529-4bdd-54b1-802d-d7cafdc23c9a",
    "title": "Query Refinement Prompts for Closed-Book Long-Form QA",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{amplayo-etal-2023-query,\n    title = \"Query Refinement Prompts for Closed-Book Long-Form {QA}\",\n    author = \"Amplayo, Reinald Kim  and\n      Webster, Kellie  and\n      Collins, Michael  and\n      Das, Dipanjan  and\n      Narayan, Shashi\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.444\",\n    doi = \"10.18653/v1/2023.acl-long.444\",\n    pages = \"7997--8012\",\n    abstract = \"Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once {--} to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.\",\n}\n",
    "authors": [
        "Reinald Kim Amplayo",
        "Kellie Webster",
        "Michael Collins",
        "Dipanjan Das",
        "Shashi Narayan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.444.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/91511529-4bdd-54b1-802d-d7cafdc23c9a.pdf",
    "abstract": "Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once â€“ to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.",
    "num_pages": 16
}