{
    "uuid": "65c896ba-2979-5fa4-a61d-d2600ccee588",
    "title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{krumdick-etal-2024-bizbench,\n    title = \"{B}iz{B}ench: A Quantitative Reasoning Benchmark for Business and Finance\",\n    author = \"Krumdick, Michael  and\n      Koncel-Kedziorski, Rik  and\n      Lai, Viet Dac  and\n      Reddy, Varshini  and\n      Lovering, Charles  and\n      Tanner, Chris\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.452\",\n    doi = \"10.18653/v1/2024.acl-long.452\",\n    pages = \"8309--8332\",\n    abstract = \"Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models{'} ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model{'}s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs{'} limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.\",\n}\n",
    "authors": [
        "Michael Krumdick",
        "Rik Koncel-Kedziorski",
        "Viet Dac Lai",
        "Varshini Reddy",
        "Charles Lovering",
        "Chris Tanner"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.452.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/65c896ba-2979-5fa4-a61d-d2600ccee588.pdf",
    "abstract": "Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.",
    "num_pages": 24
}