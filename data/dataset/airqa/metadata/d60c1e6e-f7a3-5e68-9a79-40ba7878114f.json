{
    "uuid": "d60c1e6e-f7a3-5e68-9a79-40ba7878114f",
    "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{schoch-etal-2023-data,\n    title = \"Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values\",\n    author = \"Schoch, Stephanie  and\n      Mishra, Ritwick  and\n      Ji, Yangfeng\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.37\",\n    doi = \"10.18653/v1/2023.acl-srw.37\",\n    pages = \"266--275\",\n    abstract = \"Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the entire training set, and 2) a value transfer method that leverages value information extracted from a simple classifier trained using representations from the target language model. Our experiments applying TS-DShapley to select data for fine-tuning BERT-based language models on benchmark natural language understanding (NLU) datasets show that TS-DShapley outperforms existing data selection methods. Further, TS-DShapley can filter fine-tuning data to increase language model performance compared to training with the full fine-tuning dataset.\",\n}\n",
    "authors": [
        "Stephanie Schoch",
        "Ritwick Mishra",
        "Yangfeng Ji"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.37.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d60c1e6e-f7a3-5e68-9a79-40ba7878114f.pdf",
    "abstract": "Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the entire training set, and 2) a value transfer method that leverages value information extracted from a simple classifier trained using representations from the target language model. Our experiments applying TS-DShapley to select data for fine-tuning BERT-based language models on benchmark natural language understanding (NLU) datasets show that TS-DShapley outperforms existing data selection methods. Further, TS-DShapley can filter fine-tuning data to increase language model performance compared to training with the full fine-tuning dataset.",
    "num_pages": 10
}