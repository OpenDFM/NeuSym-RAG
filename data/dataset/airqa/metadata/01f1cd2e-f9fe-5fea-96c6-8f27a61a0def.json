{
    "uuid": "01f1cd2e-f9fe-5fea-96c6-8f27a61a0def",
    "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{foley-etal-2023-matching,\n    title = \"Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models\",\n    author = \"Foley, Myles  and\n      Rawat, Ambrish  and\n      Lee, Taesung  and\n      Hou, Yufang  and\n      Picco, Gabriele  and\n      Zizzo, Giulio\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.410\",\n    doi = \"10.18653/v1/2023.acl-long.410\",\n    pages = \"7423--7442\",\n    abstract = \"The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.\",\n}\n",
    "authors": [
        "Myles Foley",
        "Ambrish Rawat",
        "Taesung Lee",
        "Yufang Hou",
        "Gabriele Picco",
        "Giulio Zizzo"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.410.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/01f1cd2e-f9fe-5fea-96c6-8f27a61a0def.pdf",
    "abstract": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.",
    "num_pages": 20
}