{
    "uuid": "2124f36f-be42-5489-a801-611e613bf9d6",
    "title": "LJPCheck: Functional Tests for Legal Judgment Prediction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-ljpcheck,\n    title = \"{LJPC}heck: Functional Tests for Legal Judgment Prediction\",\n    author = \"Zhang, Yuan  and\n      Huang, Wanhong  and\n      Feng, Yi  and\n      Li, Chuanyi  and\n      Fei, Zhiwei  and\n      Ge, Jidong  and\n      Luo, Bin  and\n      Ng, Vincent\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.350\",\n    doi = \"10.18653/v1/2024.findings-acl.350\",\n    pages = \"5878--5894\",\n    abstract = \"Legal Judgment Prediction (LJP) refers to the task of automatically predicting judgment results (e.g., charges, law articles and term of penalty) given the fact description of cases. While SOTA models have achieved high accuracy and F1 scores on public datasets, existing datasets fail to evaluate specific aspects of these models (e.g., legal fairness, which significantly impact their applications in real scenarios). Inspired by functional testing in software engineering, we introduce LJPCHECK, a suite of functional tests for LJP models, to comprehend LJP models{'} behaviors and offer diagnostic insights. We illustrate the utility of LJPCHECK on five SOTA LJP models. Extensive experiments reveal vulnerabilities in these models, prompting an in-depth discussion into the underlying reasons of their shortcomings.\",\n}\n",
    "authors": [
        "Yuan Zhang",
        "Wanhong Huang",
        "Yi Feng",
        "Chuanyi Li",
        "Zhiwei Fei",
        "Jidong Ge",
        "Bin Luo",
        "Vincent Ng"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.350.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2124f36f-be42-5489-a801-611e613bf9d6.pdf",
    "abstract": "Legal Judgment Prediction (LJP) refers to the task of automatically predicting judgment results (e.g., charges, law articles and term of penalty) given the fact description of cases. While SOTA models have achieved high accuracy and F1 scores on public datasets, existing datasets fail to evaluate specific aspects of these models (e.g., legal fairness, which significantly impact their applications in real scenarios). Inspired by functional testing in software engineering, we introduce LJPCHECK, a suite of functional tests for LJP models, to comprehend LJP modelsâ€™ behaviors and offer diagnostic insights. We illustrate the utility of LJPCHECK on five SOTA LJP models. Extensive experiments reveal vulnerabilities in these models, prompting an in-depth discussion into the underlying reasons of their shortcomings.",
    "num_pages": 17
}