{
    "uuid": "61955b4c-530c-5872-80fb-b339a7d7af37",
    "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-reasoning,\n    title = \"Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models\",\n    author = \"Wang, Xiaolong  and\n      Wang, Yile  and\n      Zhang, Yuanchi  and\n      Luo, Fuwen  and\n      Li, Peng  and\n      Sun, Maosong  and\n      Liu, Yang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.844\",\n    doi = \"10.18653/v1/2024.acl-long.844\",\n    pages = \"15880--15893\",\n    abstract = \"Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.\",\n}\n",
    "authors": [
        "Xiaolong Wang",
        "Yile Wang",
        "Yuanchi Zhang",
        "Fuwen Luo",
        "Peng Li",
        "Maosong Sun",
        "Yang Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.844.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/61955b4c-530c-5872-80fb-b339a7d7af37.pdf",
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.",
    "num_pages": 14
}