{
    "uuid": "addfd556-78a7-57df-805e-ac873de4212f",
    "title": "Cross-Modal Projection in Multimodal LLMs Doesnâ€™t Really Project Visual Attributes to Textual Space",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{verma-etal-2024-cross,\n    title = \"Cross-Modal Projection in Multimodal {LLM}s Doesn{'}t Really Project Visual Attributes to Textual Space\",\n    author = \"Verma, Gaurav  and\n      Choi, Minje  and\n      Sharma, Kartik  and\n      Watson-Daniels, Jamelle  and\n      Oh, Sejoon  and\n      Kumar, Srijan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.60\",\n    doi = \"10.18653/v1/2024.acl-short.60\",\n    pages = \"657--664\",\n    abstract = \"Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned. Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures.\",\n}\n",
    "authors": [
        "Gaurav Verma",
        "Minje Choi",
        "Kartik Sharma",
        "Jamelle Watson-Daniels",
        "Sejoon Oh",
        "Srijan Kumar"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.60.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/addfd556-78a7-57df-805e-ac873de4212f.pdf",
    "abstract": "Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the LLM, even when only the projection is fine-tuned. Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures.",
    "num_pages": 8
}