{
    "uuid": "261c29a1-3102-5388-99b6-b5a845b5e644",
    "title": "Linear Discriminative Learning: a competitive non-neural baseline for morphological inflection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    "bibtex": "@inproceedings{jeong-etal-2023-linear,\n    title = \"Linear Discriminative Learning: a competitive non-neural baseline for morphological inflection\",\n    author = \"Jeong, Cheonkam  and\n      Schmitz, Dominic  and\n      Kakolu Ramarao, Akhilesh  and\n      Stein, Anna  and\n      Tang, Kevin\",\n    editor = {Nicolai, Garrett  and\n      Chodroff, Eleanor  and\n      Mailhot, Frederic  and\n      {\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}},\n    booktitle = \"Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sigmorphon-1.16\",\n    doi = \"10.18653/v1/2023.sigmorphon-1.16\",\n    pages = \"138--150\",\n    abstract = \"This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus and experimental data showed the best performance with the overall accuracy of around 83{\\%}. We found that the best model must be trained on both corpus data and the experimental data of one particular participant. Our examination of speaker-variability and speaker-specific information did not explain why a particular participant combined well with the corpus data. We recommend Linear Discriminative Learning models as a future non-neural baseline system, owning to its training speed, accuracy, model interpretability and cognitive plausibility. In order to improve the model performance, we suggest using bigger data and/or performing data augmentation and incorporating speaker- and item-specifics considerably.\",\n}\n",
    "authors": [
        "Cheonkam Jeong",
        "Dominic Schmitz",
        "Akhilesh Kakolu Ramarao",
        "Anna Stein",
        "Kevin Tang"
    ],
    "pdf_url": "https://aclanthology.org/2023.sigmorphon-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/261c29a1-3102-5388-99b6-b5a845b5e644.pdf",
    "abstract": "This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus and experimental data showed the best performance with the overall accuracy of around 83%. We found that the best model must be trained on both corpus data and the experimental data of one particular participant. Our examination of speaker-variability and speaker-specific information did not explain why a particular participant combined well with the corpus data. We recommend Linear Discriminative Learning models as a future non-neural baseline system, owning to its training speed, accuracy, model interpretability and cognitive plausibility. In order to improve the model performance, we suggest using bigger data and/or performing data augmentation and incorporating speaker- and item-specifics considerably.",
    "num_pages": 13
}