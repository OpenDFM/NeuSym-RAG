{
    "uuid": "374f188f-f9e9-5281-b278-7ac47d3dbc1e",
    "title": "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{feng-etal-2024-bot,\n    title = \"What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection\",\n    author = \"Feng, Shangbin  and\n      Wan, Herun  and\n      Wang, Ningnan  and\n      Tan, Zhaoxuan  and\n      Luo, Minnan  and\n      Tsvetkov, Yulia\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.196\",\n    doi = \"10.18653/v1/2024.acl-long.196\",\n    pages = \"3580--3601\",\n    abstract = \"Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1{\\%} on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6{\\%} and harm the calibration and reliability of bot detection systems.\",\n}\n",
    "authors": [
        "Shangbin Feng",
        "Herun Wan",
        "Ningnan Wang",
        "Zhaoxuan Tan",
        "Minnan Luo",
        "Yulia Tsvetkov"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.196.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/374f188f-f9e9-5281-b278-7ac47d3dbc1e.pdf",
    "abstract": "Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.",
    "num_pages": 22
}