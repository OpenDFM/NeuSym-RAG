{
    "uuid": "a056a529-327d-5f66-984a-f2f36bfd6b29",
    "title": "Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the “What” and Focus on the “How”!",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2023-ideology,\n    title = \"Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the {``}What{''} and Focus on the {``}How{''}!\",\n    author = \"Chen, Chen  and\n      Walker, Dylan  and\n      Saligrama, Venkatesh\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.530\",\n    doi = \"10.18653/v1/2023.acl-long.530\",\n    pages = \"9529--9549\",\n    abstract = \"We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant selection bias. We propose a novel statistical model that decomposes the document embeddings into a linear superposition of two vectors; a latent neutral \\textit{context} vector independent of ideology, and a latent \\textit{position} vector aligned with ideology. We train an end-to-end model that has intermediate contextual and positional vectors as outputs. At deployment time, our model predicts labels for input documents by exclusively leveraging the predicted positional vectors. On two benchmark datasets we show that our model is capable of outputting predictions even when trained with as little as 5{\\%} biased data, and is significantly more accurate than the state-of-the-art. Through crowd-sourcing we validate the neutrality of contextual vectors, and show that context filtering results in ideological concentration, allowing for prediction on out-of-distribution examples.\",\n}\n",
    "authors": [
        "Chen Chen",
        "Dylan Walker",
        "Venkatesh Saligrama"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.530.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a056a529-327d-5f66-984a-f2f36bfd6b29.pdf",
    "abstract": "We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant selection bias. We propose a novel statistical model that decomposes the document embeddings into a linear superposition of two vectors; a latent neutral context vector independent of ideology, and a latent position vector aligned with ideology. We train an end-to-end model that has intermediate contextual and positional vectors as outputs. At deployment time, our model predicts labels for input documents by exclusively leveraging the predicted positional vectors. On two benchmark datasets we show that our model is capable of outputting predictions even when trained with as little as 5% biased data, and is significantly more accurate than the state-of-the-art. Through crowd-sourcing we validate the neutrality of contextual vectors, and show that context filtering results in ideological concentration, allowing for prediction on out-of-distribution examples.",
    "num_pages": 21
}