{
    "uuid": "a739cac2-436e-5e1a-8502-ad0945270152",
    "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{j-etal-2024-romansetu,\n    title = \"{R}oman{S}etu: Efficiently unlocking multilingual capabilities of Large Language Models via {R}omanization\",\n    author = \"J, Jaavid  and\n      Dabre, Raj  and\n      M, Aswanth  and\n      Gala, Jay  and\n      Jayakumar, Thanmay  and\n      Puduppully, Ratish  and\n      Kunchukuttan, Anoop\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.833\",\n    doi = \"10.18653/v1/2024.acl-long.833\",\n    pages = \"15593--15615\",\n    abstract = \"This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Roman scripts. We propose an approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Our approach involve the continual pretraining of a English LLM like Llama 2 on romanized text of non-English, non-Roman script languages, followed by instruction tuning on romanized data. The results indicate that romanized text not only reduces token fertility by 2x-4x but also matches if not outperforms native script representation across various NLU, NLG and MT tasks. Moreover, the embeddings computed on romanized text exhibit closer alignment with their English translations than those from the native script. Our approach presents a promising direction for leveraging the power of English LLMs in languages traditionally underrepresented in NLP research.\",\n}\n",
    "authors": [
        "Jaavid J",
        "Raj Dabre",
        "Aswanth M",
        "Jay Gala",
        "Thanmay Jayakumar",
        "Ratish Puduppully",
        "Anoop Kunchukuttan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.833.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a739cac2-436e-5e1a-8502-ad0945270152.pdf",
    "abstract": "This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Roman scripts. We propose an approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Our approach involve the continual pretraining of a English LLM like Llama 2 on romanized text of non-English, non-Roman script languages, followed by instruction tuning on romanized data. The results indicate that romanized text not only reduces token fertility by 2x-4x but also matches if not outperforms native script representation across various NLU, NLG and MT tasks. Moreover, the embeddings computed on romanized text exhibit closer alignment with their English translations than those from the native script. Our approach presents a promising direction for leveraging the power of English LLMs in languages traditionally underrepresented in NLP research.",
    "num_pages": 23
}