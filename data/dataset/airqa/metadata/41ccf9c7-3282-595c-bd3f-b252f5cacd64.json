{
    "uuid": "41ccf9c7-3282-595c-bd3f-b252f5cacd64",
    "title": "IXA at SemEval-2023 Task 2: Baseline Xlm-Roberta-base Approach",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{andres-santamaria-2023-ixa,\n    title = \"{IXA} at {S}em{E}val-2023 Task 2: Baseline Xlm-Roberta-base Approach\",\n    author = \"Andres Santamaria, Edgar\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.50\",\n    doi = \"10.18653/v1/2023.semeval-1.50\",\n    pages = \"379--381\",\n    abstract = \"IXA proposes a Sequence labeling fine-tune approach, which consists of a lightweight few-shot baseline (10e), the system takes advantage of transfer learning from pre-trained Named Entity Recognition and cross-lingual knowledge from the LM checkpoint. This technique obtains a drastic reduction in the effective training costs that works as a perfect baseline, future improvements in the baseline approach could fit: 1) Domain adequation, 2) Data augmentation, and 3) Intermediate task learning.\",\n}\n",
    "authors": [
        "Edgar Andres Santamaria"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.50.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/41ccf9c7-3282-595c-bd3f-b252f5cacd64.pdf",
    "abstract": "IXA proposes a Sequence labeling fine-tune approach, which consists of a lightweight few-shot baseline (10e), the system takes advantage of transfer learning from pre-trained Named Entity Recognition and cross-lingual knowledge from the LM checkpoint. This technique obtains a drastic reduction in the effective training costs that works as a perfect baseline, future improvements in the baseline approach could fit: 1) Domain adequation, 2) Data augmentation, and 3) Intermediate task learning.",
    "num_pages": 3
}