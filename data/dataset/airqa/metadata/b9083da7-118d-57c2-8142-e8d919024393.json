{
    "uuid": "b9083da7-118d-57c2-8142-e8d919024393",
    "title": "Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-hybrid,\n    title = \"Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation\",\n    author = \"Wang, Qiang  and\n      Hu, Xinhui  and\n      Chen, Ming\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.367\",\n    doi = \"10.18653/v1/2023.findings-acl.367\",\n    pages = \"5931--5945\",\n    abstract = \"This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT. Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction for every $k$ tokens, $k>1$) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Experiments on five translation tasks show that HRT achieves comparable translation quality with AT while having at least 1.5x faster inference regardless of batch size and device. Additionally, HRT successfully inherits the sound characteristics of AT in the deep-encoder-shallow-decoder architecture, allowing for further speedup without BLEU loss.\",\n}\n",
    "authors": [
        "Qiang Wang",
        "Xinhui Hu",
        "Ming Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.367.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b9083da7-118d-57c2-8142-e8d919024393.pdf",
    "abstract": "This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT. Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction for every k tokens, k>1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Experiments on five translation tasks show that HRT achieves comparable translation quality with AT while having at least 1.5x faster inference regardless of batch size and device. Additionally, HRT successfully inherits the sound characteristics of AT in the deep-encoder-shallow-decoder architecture, allowing for further speedup without BLEU loss.",
    "num_pages": 15
}