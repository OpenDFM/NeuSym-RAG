{
    "uuid": "136a5c15-f401-5c09-907c-caccf5859c4f",
    "title": "Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{ha-etal-2023-two,\n    title = \"Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning\",\n    author = \"Ha, Hyeonmin  and\n      Jung, Soyoung  and\n      Park, Jinsol  and\n      Seo, Minjoon  and\n      Hwang, Seung-won  and\n      Chun, Byung-Gon\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.206\",\n    doi = \"10.18653/v1/2023.findings-acl.206\",\n    pages = \"3335--3350\",\n    abstract = \"Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is gradient-based search, which iteratively updates a (randomly) initialized prompt towards the optimal one with the guide of gradients. We propose a novel regularization method, CoRe, for gradient-based prompt tuning techniques, which guides a prompt to produce a task context properly. CoRe realizes two regularization effects {---} context attuning and context filtering {---} that improve prediction performance in a zero-shot in-context learning setting where a model makes inferences only with the prompt tuned by CoRe, without any demonstration examples for in-context learning. Context attuning guides the context generated by the input and the tuned prompt toward embedding the appropriate context for the task. In our theoretical analysis, regularizing the context extends to improving zero-shot in-context learning performance. Context filtering steers the prompt to select only the task-related context so that context attuning solely focuses on creating and sending the right task context. We evaluate CoRe on natural language understanding datasets and two large language models, GPT2-XL and GPT-J.Our training scheme shows performance improvements up to 11.9{\\%} on GPT2-XL, and up to 6.3{\\%} on GPT-J in zero-shot settings.\",\n}\n",
    "authors": [
        "Hyeonmin Ha",
        "Soyoung Jung",
        "Jinsol Park",
        "Minjoon Seo",
        "Seung-won Hwang",
        "Byung-Gon Chun"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.206.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/136a5c15-f401-5c09-907c-caccf5859c4f.pdf",
    "abstract": "Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is gradient-based search, which iteratively updates a (randomly) initialized prompt towards the optimal one with the guide of gradients. We propose a novel regularization method, CoRe, for gradient-based prompt tuning techniques, which guides a prompt to produce a task context properly. CoRe realizes two regularization effects — context attuning and context filtering — that improve prediction performance in a zero-shot in-context learning setting where a model makes inferences only with the prompt tuned by CoRe, without any demonstration examples for in-context learning. Context attuning guides the context generated by the input and the tuned prompt toward embedding the appropriate context for the task. In our theoretical analysis, regularizing the context extends to improving zero-shot in-context learning performance. Context filtering steers the prompt to select only the task-related context so that context attuning solely focuses on creating and sending the right task context. We evaluate CoRe on natural language understanding datasets and two large language models, GPT2-XL and GPT-J.Our training scheme shows performance improvements up to 11.9% on GPT2-XL, and up to 6.3% on GPT-J in zero-shot settings.",
    "num_pages": 16
}