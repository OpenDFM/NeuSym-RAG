{
    "uuid": "3ae6e82d-b62f-5219-a8e6-29e7e24f6916",
    "title": "What does Kiki look like? Cross-modal associations between speech sounds and visual shapes in vision-and-language models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    "bibtex": "@inproceedings{verhoef-etal-2024-kiki,\n    title = \"What does Kiki look like? Cross-modal associations between speech sounds and visual shapes in vision-and-language models\",\n    author = \"Verhoef, Tessa  and\n      Shahrasbi, Kiana  and\n      Kouwenhoven, Tom\",\n    editor = \"Kuribayashi, Tatsuki  and\n      Rambelli, Giulia  and\n      Takmaz, Ece  and\n      Wicke, Philipp  and\n      Oseki, Yohei\",\n    booktitle = \"Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.cmcl-1.17\",\n    doi = \"10.18653/v1/2024.cmcl-1.17\",\n    pages = \"199--213\",\n    abstract = \"Humans have clear cross-modal preferences when matching certain novel words to visual shapes. Evidence suggests that these preferences play a prominent role in our linguistic processing, language learning, and the origins of signal-meaning mappings. With the rise of multimodal models in AI, such as vision-and-language (VLM) models, it becomes increasingly important to uncover the kinds of visio-linguistic associations these models encode and whether they align with human representations. Informed by experiments with humans, we probe and compare four VLMs for a well-known human cross-modal preference, the bouba-kiki effect. We do not find conclusive evidence for this effect but suggest that results may depend on features of the models, such as architecture design, model size, and training details. Our findings inform discussions on the origins of the bouba-kiki effect in human cognition and future developments of VLMs that align well with human cross-modal associations.\",\n}\n",
    "authors": [
        "Tessa Verhoef",
        "Kiana Shahrasbi",
        "Tom Kouwenhoven"
    ],
    "pdf_url": "https://aclanthology.org/2024.cmcl-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3ae6e82d-b62f-5219-a8e6-29e7e24f6916.pdf",
    "abstract": "Humans have clear cross-modal preferences when matching certain novel words to visual shapes. Evidence suggests that these preferences play a prominent role in our linguistic processing, language learning, and the origins of signal-meaning mappings. With the rise of multimodal models in AI, such as vision-and-language (VLM) models, it becomes increasingly important to uncover the kinds of visio-linguistic associations these models encode and whether they align with human representations. Informed by experiments with humans, we probe and compare four VLMs for a well-known human cross-modal preference, the bouba-kiki effect. We do not find conclusive evidence for this effect but suggest that results may depend on features of the models, such as architecture design, model size, and training details. Our findings inform discussions on the origins of the bouba-kiki effect in human cognition and future developments of VLMs that align well with human cross-modal associations.",
    "num_pages": 15
}