{
    "uuid": "9898f9b2-a95c-538a-965a-68dbc34b1b60",
    "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)",
    "bibtex": "@inproceedings{du-etal-2024-perltqa,\n    title = \"{P}er{LTQA}: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering\",\n    author = \"Du, Yiming  and\n      Wang, Hongru  and\n      Zhao, Zhengyi  and\n      Liang, Bin  and\n      Wang, Baojun  and\n      Zhong, Wanjun  and\n      Wang, Zezhong  and\n      Wong, Kam-Fai\",\n    editor = \"Wong, Kam-Fai  and\n      Zhang, Min  and\n      Xu, Ruifeng  and\n      Li, Jing  and\n      Wei, Zhongyu  and\n      Gui, Lin  and\n      Liang, Bin  and\n      Zhao, Runcong\",\n    booktitle = \"Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sighan-1.18\",\n    pages = \"152--164\",\n    abstract = \"In conversational AI, effectively employing long-term memory improves personalized and consistent response generation. Existing work only concentrated on a single type of long-term memory, such as preferences, dialogue history, or social relationships, overlooking their interaction in real-world contexts. To this end, inspired by the concept of semantic memory and episodic memory from cognitive psychology, we create a new and more comprehensive Chinese dataset, coined as PerLTQA, in which world knowledge, profiles, social relationships, events, and dialogues are considered to leverage the interaction between different types of long-term memory for question answering (QA) in conversation. Further, based on PerLTQA, we propose a novel framework for memory integration in QA, consisting of three subtasks: \\textbf{Memory Classification}, \\textbf{Memory Retrieval}, and \\textbf{Memory Fusion}, which provides a comprehensive paradigm for memory modeling, enabling consistent and personalized memory utilization. This essentially allows the exploitation of more accurate memory information for better responses in QA. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate the importance of personal long-term memory in the QA task\",\n}\n",
    "authors": [
        "Yiming Du",
        "Hongru Wang",
        "Zhengyi Zhao",
        "Bin Liang",
        "Baojun Wang",
        "Wanjun Zhong",
        "Zezhong Wang",
        "Kam-Fai Wong"
    ],
    "pdf_url": "https://aclanthology.org/2024.sighan-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9898f9b2-a95c-538a-965a-68dbc34b1b60.pdf",
    "abstract": "In conversational AI, effectively employing long-term memory improves personalized and consistent response generation. Existing work only concentrated on a single type of long-term memory, such as preferences, dialogue history, or social relationships, overlooking their interaction in real-world contexts. To this end, inspired by the concept of semantic memory and episodic memory from cognitive psychology, we create a new and more comprehensive Chinese dataset, coined as PerLTQA, in which world knowledge, profiles, social relationships, events, and dialogues are considered to leverage the interaction between different types of long-term memory for question answering (QA) in conversation. Further, based on PerLTQA, we propose a novel framework for memory integration in QA, consisting of three subtasks: Memory Classification, Memory Retrieval, and Memory Fusion, which provides a comprehensive paradigm for memory modeling, enabling consistent and personalized memory utilization. This essentially allows the exploitation of more accurate memory information for better responses in QA. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate the importance of personal long-term memory in the QA task",
    "num_pages": 13
}