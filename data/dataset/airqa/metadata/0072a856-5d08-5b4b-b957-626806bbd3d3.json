{
    "uuid": "0072a856-5d08-5b4b-b957-626806bbd3d3",
    "title": "Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{shor-etal-2023-clinical,\n    title = \"Clinical {BERTS}core: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings\",\n    author = \"Shor, Joel  and\n      Bi, Ruyue Agnes  and\n      Venugopalan, Subhashini  and\n      Ibara, Steven  and\n      Goldenberg, Roman  and\n      Rivlin, Ehud\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.1\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.1\",\n    pages = \"1--7\",\n    abstract = \"Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We collect a benchmark of 18 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP) and make it publicly available for the community to further develop clinically-aware ASR metrics. To our knowledge, this is the first public dataset of its kind. We demonstrate that our metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins.\",\n}\n",
    "authors": [
        "Joel Shor",
        "Ruyue Agnes Bi",
        "Subhashini Venugopalan",
        "Steven Ibara",
        "Roman Goldenberg",
        "Ehud Rivlin"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/0072a856-5d08-5b4b-b957-626806bbd3d3.pdf",
    "abstract": "Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We collect a benchmark of 18 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP) and make it publicly available for the community to further develop clinically-aware ASR metrics. To our knowledge, this is the first public dataset of its kind. We demonstrate that our metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins.",
    "num_pages": 7
}