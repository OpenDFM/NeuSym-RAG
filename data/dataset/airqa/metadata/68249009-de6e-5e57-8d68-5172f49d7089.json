{
    "uuid": "68249009-de6e-5e57-8d68-5172f49d7089",
    "title": "Toward Interactive Dictation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2023-toward,\n    title = \"Toward Interactive Dictation\",\n    author = \"Li, Belinda Z.  and\n      Eisner, Jason  and\n      Pauls, Adam  and\n      Thomson, Sam\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.854\",\n    doi = \"10.18653/v1/2023.acl-long.854\",\n    pages = \"15319--15338\",\n    abstract = \"Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30{\\%} end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55{\\%} end-state accuracy with 7 seconds of latency.\",\n}\n",
    "authors": [
        "Belinda Z. Li",
        "Jason Eisner",
        "Adam Pauls",
        "Sam Thomson"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.854.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/68249009-de6e-5e57-8d68-5172f49d7089.pdf",
    "abstract": "Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency.",
    "num_pages": 20
}