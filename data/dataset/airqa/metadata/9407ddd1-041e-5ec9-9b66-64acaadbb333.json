{
    "uuid": "9407ddd1-041e-5ec9-9b66-64acaadbb333",
    "title": "Indirectly Supervised Natural Language Processing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)",
    "bibtex": "@inproceedings{yin-etal-2023-indirectly,\n    title = \"Indirectly Supervised Natural Language Processing\",\n    author = \"Yin, Wenpeng  and\n      Chen, Muhao  and\n      Zhou, Ben  and\n      Ning, Qiang  and\n      Chang, Kai-Wei  and\n      Roth, Dan\",\n    editor = \"Chen, Yun-Nung (Vivian)  and\n      Margot, Margot  and\n      Reddy, Siva\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-tutorials.5\",\n    doi = \"10.18653/v1/2023.acl-tutorials.5\",\n    pages = \"32--40\",\n    abstract = \"This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a {``}related{''} task Tâ²? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T â² that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations{---}all having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.\",\n}\n",
    "authors": [
        "Wenpeng Yin",
        "Muhao Chen",
        "Ben Zhou",
        "Qiang Ning",
        "Kai-Wei Chang",
        "Dan Roth"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-tutorials.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9407ddd1-041e-5ec9-9b66-64acaadbb333.pdf",
    "abstract": "This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a “related” task T′? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T ′ that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations—all having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.",
    "num_pages": 9
}