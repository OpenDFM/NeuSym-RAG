{
    "uuid": "aaaafd36-3cc6-5eb8-a640-ce38dbbca8ab",
    "title": "Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lee-etal-2024-improving-conversational,\n    title = \"Improving Conversational Abilities of Quantized Large Language Models via Direct Preference Alignment\",\n    author = \"Lee, Janghwan  and\n      Park, Seongmin  and\n      Hong, Sukjin  and\n      Kim, Minsoo  and\n      Chang, Du-Seong  and\n      Choi, Jungwook\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.612\",\n    doi = \"10.18653/v1/2024.acl-long.612\",\n    pages = \"11346--11364\",\n    abstract = \"The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.\",\n}\n",
    "authors": [
        "Janghwan Lee",
        "Seongmin Park",
        "Sukjin Hong",
        "Minsoo Kim",
        "Du-Seong Chang",
        "Jungwook Choi"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.612.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/aaaafd36-3cc6-5eb8-a640-ce38dbbca8ab.pdf",
    "abstract": "The rapid advancement of large language models (LLMs) has facilitated their transformation into conversational chatbots that can grasp contextual nuances and generate pertinent sentences, closely mirroring human values through advanced techniques such as instruction tuning and reinforcement learning from human feedback (RLHF). However, the computational efficiency required for LLMs, achieved through techniques like post-training quantization (PTQ), presents challenges such as token-flipping that can impair chatbot performance. In response, we propose a novel preference alignment approach, quantization-aware direct preference optimization (QDPO), that aligns quantized LLMs with their full-precision counterparts, improving conversational abilities. Evaluated on two instruction-tuned LLMs in various languages, QDPO demonstrated superior performance in improving conversational abilities compared to established PTQ and knowledge-distillation fine-tuning techniques, marking a significant step forward in the development of efficient and effective conversational LLMs.",
    "num_pages": 19
}