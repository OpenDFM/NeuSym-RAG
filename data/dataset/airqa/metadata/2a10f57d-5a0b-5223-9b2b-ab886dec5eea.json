{
    "uuid": "2a10f57d-5a0b-5223-9b2b-ab886dec5eea",
    "title": "Large Scale Sequence-to-Sequence Models for Clinical Note Generation from Patient-Doctor Conversations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{singh-etal-2023-large,\n    title = \"Large Scale Sequence-to-Sequence Models for Clinical Note Generation from Patient-Doctor Conversations\",\n    author = \"Singh, Gagandeep  and\n      Pan, Yue  and\n      Andres-Ferrer, Jesus  and\n      Del-Agua, Miguel  and\n      Diehl, Frank  and\n      Pinto, Joel  and\n      Vozila, Paul\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.18\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.18\",\n    pages = \"138--143\",\n    abstract = \"We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhancements to this baseline model which include using subword and multiword tokenization scheme, prefixing the targets with a chain-of-clinical-facts, and training with contrastive loss that is defined over various candidate summaries. We also use flash attention during training and query chunked attention during inference to be able to process long input and output sequences and to improve computational efficiency. Experiments are conducted on a dataset containing about 900K encounters from around 1800 healthcare providers covering 27 specialties. The results are broken down into primary care and non-primary care specialties. Consistent accuracy improvements are observed across both of these categories.\",\n}\n",
    "authors": [
        "Gagandeep Singh",
        "Yue Pan",
        "Jesus Andres-Ferrer",
        "Miguel Del-Agua",
        "Frank Diehl",
        "Joel Pinto",
        "Paul Vozila"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2a10f57d-5a0b-5223-9b2b-ab886dec5eea.pdf",
    "abstract": "We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhancements to this baseline model which include using subword and multiword tokenization scheme, prefixing the targets with a chain-of-clinical-facts, and training with contrastive loss that is defined over various candidate summaries. We also use flash attention during training and query chunked attention during inference to be able to process long input and output sequences and to improve computational efficiency. Experiments are conducted on a dataset containing about 900K encounters from around 1800 healthcare providers covering 27 specialties. The results are broken down into primary care and non-primary care specialties. Consistent accuracy improvements are observed across both of these categories.",
    "num_pages": 6
}