{
    "uuid": "e86745d3-fce5-5451-8163-79be34c11937",
    "title": "RIGA at SemEval-2023 Task 2: NER Enhanced with GPT-3",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{mukans-barzdins-2023-riga,\n    title = \"{RIGA} at {S}em{E}val-2023 Task 2: {NER} Enhanced with {GPT}-3\",\n    author = \"Mukans, Eduards  and\n      Barzdins, Guntis\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.45\",\n    doi = \"10.18653/v1/2023.semeval-1.45\",\n    pages = \"331--339\",\n    abstract = \"The following is a description of the RIGA team{'}s submissions for the English track of the SemEval-2023 Task 2: Multilingual Complex Named Entity Recognition (MultiCoNER) II. Our approach achieves 17{\\%} boost in results by utilizing pre-existing Large-scale Language Models (LLMs), such as GPT-3, to gather additional contexts. We then fine-tune a pre-trained neural network utilizing these contexts. The final step of our approach involves meticulous model and compute resource scaling, which results in improved performance. Our results placed us 12th out of 34 teams in terms of overall ranking and 7th in terms of the noisy subset ranking. The code for our method is available on GitHub (\\url{https://github.com/emukans/multiconer2-riga}).\",\n}\n",
    "authors": [
        "Eduards Mukans",
        "Guntis Barzdins"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.45.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e86745d3-fce5-5451-8163-79be34c11937.pdf",
    "abstract": "The following is a description of the RIGA teamâ€™s submissions for the English track of the SemEval-2023 Task 2: Multilingual Complex Named Entity Recognition (MultiCoNER) II. Our approach achieves 17% boost in results by utilizing pre-existing Large-scale Language Models (LLMs), such as GPT-3, to gather additional contexts. We then fine-tune a pre-trained neural network utilizing these contexts. The final step of our approach involves meticulous model and compute resource scaling, which results in improved performance. Our results placed us 12th out of 34 teams in terms of overall ranking and 7th in terms of the noisy subset ranking. The code for our method is available on GitHub (https://github.com/emukans/multiconer2-riga).",
    "num_pages": 9
}