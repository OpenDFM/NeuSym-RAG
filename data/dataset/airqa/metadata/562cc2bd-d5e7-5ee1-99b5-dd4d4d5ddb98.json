{
    "uuid": "562cc2bd-d5e7-5ee1-99b5-dd4d4d5ddb98",
    "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zheng-etal-2024-opencodeinterpreter,\n    title = \"{O}pen{C}ode{I}nterpreter: Integrating Code Generation with Execution and Refinement\",\n    author = \"Zheng, Tianyu  and\n      Zhang, Ge  and\n      Shen, Tianhao  and\n      Liu, Xueling  and\n      Lin, Bill Yuchen  and\n      Fu, Jie  and\n      Chen, Wenhu  and\n      Yue, Xiang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.762\",\n    doi = \"10.18653/v1/2024.findings-acl.762\",\n    pages = \"12834--12859\",\n    abstract = \"The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4{'}s 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreterbrings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.\",\n}\n",
    "authors": [
        "Tianyu Zheng",
        "Ge Zhang",
        "Tianhao Shen",
        "Xueling Liu",
        "Bill Yuchen Lin",
        "Jie Fu",
        "Wenhu Chen",
        "Xiang Yue"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.762.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/562cc2bd-d5e7-5ee1-99b5-dd4d4d5ddb98.pdf",
    "abstract": "The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4â€™s 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreterbrings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.",
    "num_pages": 26
}