{
    "uuid": "127611cd-45d3-5639-bd8d-aa2a6d1fe447",
    "title": "SocialBench: Sociality Evaluation of Role-Playing Conversational Agents",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chen-etal-2024-socialbench,\n    title = \"{S}ocial{B}ench: Sociality Evaluation of Role-Playing Conversational Agents\",\n    author = \"Chen, Hongzhan  and\n      Chen, Hehong  and\n      Yan, Ming  and\n      Xu, Wenshen  and\n      Xing, Gao  and\n      Shen, Weizhou  and\n      Quan, Xiaojun  and\n      Li, Chenliang  and\n      Zhang, Ji  and\n      Huang, Fei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.125\",\n    doi = \"10.18653/v1/2024.findings-acl.125\",\n    pages = \"2108--2126\",\n    abstract = \"Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge and style of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing agents at both individual and group levels of social interactions. SocialBench is constructed from various sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Experimental results on SocialBench confirm its significance as a testbed for assessing the social interaction of role-playing agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.\",\n}\n",
    "authors": [
        "Hongzhan Chen",
        "Hehong Chen",
        "Ming Yan",
        "Wenshen Xu",
        "Gao Xing",
        "Weizhou Shen",
        "Xiaojun Quan",
        "Chenliang Li",
        "Ji Zhang",
        "Fei Huang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.125.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/127611cd-45d3-5639-bd8d-aa2a6d1fe447.pdf",
    "abstract": "Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge and style of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing agents at both individual and group levels of social interactions. SocialBench is constructed from various sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Experimental results on SocialBench confirm its significance as a testbed for assessing the social interaction of role-playing agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.",
    "num_pages": 19
}