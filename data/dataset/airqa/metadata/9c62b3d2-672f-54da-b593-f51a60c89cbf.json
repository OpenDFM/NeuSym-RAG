{
    "uuid": "9c62b3d2-672f-54da-b593-f51a60c89cbf",
    "title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{park-etal-2024-open,\n    title = \"Open {K}o-{LLM} Leaderboard: Evaluating Large Language Models in {K}orean with {K}o-H5 Benchmark\",\n    author = \"Park, Chanjun  and\n      Kim, Hyeonwoo  and\n      Kim, Dahyun  and\n      Cho, SeongHwan  and\n      Kim, Sanghoon  and\n      Lee, Sukyung  and\n      Kim, Yungi  and\n      Lee, Hwalsuk\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.177\",\n    doi = \"10.18653/v1/2024.acl-long.177\",\n    pages = \"3220--3234\",\n    abstract = \"This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.\",\n}\n",
    "authors": [
        "Chanjun Park",
        "Hyeonwoo Kim",
        "Dahyun Kim",
        "SeongHwan Cho",
        "Sanghoon Kim",
        "Sukyung Lee",
        "Yungi Kim",
        "Hwalsuk Lee"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.177.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9c62b3d2-672f-54da-b593-f51a60c89cbf.pdf",
    "abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",
    "num_pages": 15
}