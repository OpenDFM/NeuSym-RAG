{
    "uuid": "c2314234-ca4b-5a6f-b620-45c7386e2e9a",
    "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{lu-etal-2024-sofa,\n    title = \"{S}o{FA}: Shielded On-the-fly Alignment via Priority Rule Following\",\n    author = \"Lu, Xinyu  and\n      Yu, Bowen  and\n      Lu, Yaojie  and\n      Lin, Hongyu  and\n      Yu, Haiyang  and\n      Sun, Le  and\n      Han, Xianpei  and\n      Li, Yongbin\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.424\",\n    doi = \"10.18653/v1/2024.findings-acl.424\",\n    pages = \"7108--7136\",\n    abstract = \"The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.\",\n}\n",
    "authors": [
        "Xinyu Lu",
        "Bowen Yu",
        "Yaojie Lu",
        "Hongyu Lin",
        "Haiyang Yu",
        "Le Sun",
        "Xianpei Han",
        "Yongbin Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.424.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c2314234-ca4b-5a6f-b620-45c7386e2e9a.pdf",
    "abstract": "The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.",
    "num_pages": 29
}