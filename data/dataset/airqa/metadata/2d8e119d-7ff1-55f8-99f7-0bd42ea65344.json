{
    "uuid": "2d8e119d-7ff1-55f8-99f7-0bd42ea65344",
    "title": "Realistic Evaluation of Toxicity in Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{luong-etal-2024-realistic,\n    title = \"Realistic Evaluation of Toxicity in Large Language Models\",\n    author = \"Luong, Tinh  and\n      Le, Thanh-Thien  and\n      Ngo, Linh  and\n      Nguyen, Thien\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.61\",\n    doi = \"10.18653/v1/2024.findings-acl.61\",\n    pages = \"1038--1047\",\n    abstract = \"Large language models (LLMs) have become integral to our professional workflows and daily lives. Nevertheless, these machine companions of ours have a critical flaw: the huge amount of data which endows them with vast and diverse knowledge, also exposes them to the inevitable toxicity and bias. While most LLMs incorporate defense mechanisms to prevent the generation of harmful content, these safeguards can be easily bypassed with minimal prompt engineering. In this paper, we introduce the new Thoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models. Through extensive evaluations, we demonstrate the pivotal role of TET in providing a rigorous benchmark for evaluation of toxicity awareness in several popular LLMs: it highlights the toxicity in the LLMs that might remain hidden when using normal prompts, thus revealing subtler issues in their behavior.\",\n}\n",
    "authors": [
        "Tinh Luong",
        "Thanh-Thien Le",
        "Linh Ngo",
        "Thien Nguyen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.61.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2d8e119d-7ff1-55f8-99f7-0bd42ea65344.pdf",
    "abstract": "Large language models (LLMs) have become integral to our professional workflows and daily lives. Nevertheless, these machine companions of ours have a critical flaw: the huge amount of data which endows them with vast and diverse knowledge, also exposes them to the inevitable toxicity and bias. While most LLMs incorporate defense mechanisms to prevent the generation of harmful content, these safeguards can be easily bypassed with minimal prompt engineering. In this paper, we introduce the new Thoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models. Through extensive evaluations, we demonstrate the pivotal role of TET in providing a rigorous benchmark for evaluation of toxicity awareness in several popular LLMs: it highlights the toxicity in the LLMs that might remain hidden when using normal prompts, thus revealing subtler issues in their behavior.",
    "num_pages": 10
}