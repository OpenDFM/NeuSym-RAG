{
    "uuid": "ac09db78-9fc8-5486-aa07-dbd99c8ae311",
    "title": "All Languages Matter: On the Multilingual Safety of LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{wang-etal-2024-languages,\n    title = \"All Languages Matter: On the Multilingual Safety of {LLM}s\",\n    author = \"Wang, Wenxuan  and\n      Tu, Zhaopeng  and\n      Chen, Chang  and\n      Yuan, Youliang  and\n      Huang, Jen-tse  and\n      Jiao, Wenxiang  and\n      Lyu, Michael\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.349\",\n    doi = \"10.18653/v1/2024.findings-acl.349\",\n    pages = \"5865--5877\",\n    abstract = \"Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose a simple and effective prompting method to improve the multilingual safety of ChatGPT by enhancing cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses by 42{\\%} for non-English queries. We will release all the data and results to facilitate future research on LLMs{'} safety.\",\n}\n",
    "authors": [
        "Wenxuan Wang",
        "Zhaopeng Tu",
        "Chang Chen",
        "Youliang Yuan",
        "Jen-tse Huang",
        "Wenxiang Jiao",
        "Michael Lyu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.349.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ac09db78-9fc8-5486-aa07-dbd99c8ae311.pdf",
    "abstract": "Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose a simple and effective prompting method to improve the multilingual safety of ChatGPT by enhancing cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses by 42% for non-English queries. We will release all the data and results to facilitate future research on LLMsâ€™ safety.",
    "num_pages": 13
}