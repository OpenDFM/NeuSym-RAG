{
    "uuid": "b83928c7-efbc-5d2a-983c-adfdcb38a2fe",
    "title": "In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{shankar-etal-2024-context,\n    title = \"In-context Mixing ({ICM}): Code-mixed Prompts for Multilingual {LLM}s\",\n    author = \"Shankar, Bhavani  and\n      Jyothi, Preethi  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.228\",\n    doi = \"10.18653/v1/2024.acl-long.228\",\n    pages = \"4162--4176\",\n    abstract = \"We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context learning (ICL) with multilingual large language models (MLLMs). With ICM, we modify the few-shot examples within ICL prompts to be intra-sententially code-mixed by randomly swapping content words in the target languages with their English translations. We observe that ICM prompts yield superior performance in NLP tasks such as disfluency correction, grammar error correction and text simplification that demand a close correspondence between the input and output sequences. Significant improvements are observed mainly for low-resource languages that are under-represented during the pretraining and finetuning of MLLMs. We present an extensive set of experiments to analyze when ICM is effective and what design choices contribute towards its effectiveness. ICM works consistently and significantly better than other prompting techniques across models of varying capacity such as mT0-XXL, BloomZ and GPT-4.\",\n}\n",
    "authors": [
        "Bhavani Shankar",
        "Preethi Jyothi",
        "Pushpak Bhattacharyya"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.228.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/b83928c7-efbc-5d2a-983c-adfdcb38a2fe.pdf",
    "abstract": "We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context learning (ICL) with multilingual large language models (MLLMs). With ICM, we modify the few-shot examples within ICL prompts to be intra-sententially code-mixed by randomly swapping content words in the target languages with their English translations. We observe that ICM prompts yield superior performance in NLP tasks such as disfluency correction, grammar error correction and text simplification that demand a close correspondence between the input and output sequences. Significant improvements are observed mainly for low-resource languages that are under-represented during the pretraining and finetuning of MLLMs. We present an extensive set of experiments to analyze when ICM is effective and what design choices contribute towards its effectiveness. ICM works consistently and significantly better than other prompting techniques across models of varying capacity such as mT0-XXL, BloomZ and GPT-4.",
    "num_pages": 15
}