{
    "uuid": "2b6124bf-7e25-56cc-8ab6-3873017cdd7b",
    "title": "Expanding Scope: Adapting English Adversarial Attacks to Chinese",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)",
    "bibtex": "@inproceedings{liu-etal-2023-expanding,\n    title = \"Expanding Scope: Adapting {E}nglish Adversarial Attacks to {C}hinese\",\n    author = \"Liu, Hanyu  and\n      Cai, Chengyuan  and\n      Qi, Yanjun\",\n    editor = \"Ovalle, Anaelia  and\n      Chang, Kai-Wei  and\n      Mehrabi, Ninareh  and\n      Pruksachatkun, Yada  and\n      Galystan, Aram  and\n      Dhamala, Jwala  and\n      Verma, Apurv  and\n      Cao, Trista  and\n      Kumar, Anoop  and\n      Gupta, Rahul\",\n    booktitle = \"Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.trustnlp-1.24\",\n    doi = \"10.18653/v1/2023.trustnlp-1.24\",\n    pages = \"276--286\",\n    abstract = \"Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and sentiment consistency by focusing on the Chinese language{'}s morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.\",\n}\n",
    "authors": [
        "Hanyu Liu",
        "Chengyuan Cai",
        "Yanjun Qi"
    ],
    "pdf_url": "https://aclanthology.org/2023.trustnlp-1.24.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2b6124bf-7e25-56cc-8ab6-3873017cdd7b.pdf",
    "abstract": "Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and sentiment consistency by focusing on the Chinese languageâ€™s morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.",
    "num_pages": 11
}