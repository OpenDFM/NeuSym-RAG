{
    "uuid": "153496d8-c0b8-5f53-b97c-6c984817f142",
    "title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ai-etal-2024-advancement,\n    title = \"Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models\",\n    author = \"Ai, Qihang  and\n      Li, Jiafan  and\n      Dai, Jincheng  and\n      Zhou, Jianwu  and\n      Liu, Lemao  and\n      Jiang, Haiyun  and\n      Shi, Shuming\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.404\",\n    doi = \"10.18653/v1/2024.acl-long.404\",\n    pages = \"7485--7501\",\n    abstract = \"Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5{\\%}-15{\\%} compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.\",\n}\n",
    "authors": [
        "Qihang Ai",
        "Jiafan Li",
        "Jincheng Dai",
        "Jianwu Zhou",
        "Lemao Liu",
        "Haiyun Jiang",
        "Shuming Shi"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.404.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/153496d8-c0b8-5f53-b97c-6c984817f142.pdf",
    "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.",
    "num_pages": 17
}