{
    "uuid": "da1b382d-b320-5324-8ff2-06ae62d97ac1",
    "title": "Self-Supervised Sentence Polishing by Adding Engaging Modifiers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{zhang-etal-2023-self-supervised,\n    title = \"Self-Supervised Sentence Polishing by Adding Engaging Modifiers\",\n    author = \"Zhang, Zhexin  and\n      Guan, Jian  and\n      Cui, Xin  and\n      Ran, Yu  and\n      Liu, Bo  and\n      Huang, Minlie\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.48\",\n    doi = \"10.18653/v1/2023.acl-demo.48\",\n    pages = \"499--507\",\n    abstract = \"Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at \\url{http://coai.cs.tsinghua.edu.cn/static/polishSent/}. A demo video is available at \\url{https://youtu.be/Y6gFHOgSv8Y}.\",\n}\n",
    "authors": [
        "Zhexin Zhang",
        "Jian Guan",
        "Xin Cui",
        "Yu Ran",
        "Bo Liu",
        "Minlie Huang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.48.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/da1b382d-b320-5324-8ff2-06ae62d97ac1.pdf",
    "abstract": "Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at http://coai.cs.tsinghua.edu.cn/static/polishSent/. A demo video is available at https://youtu.be/Y6gFHOgSv8Y.",
    "num_pages": 9
}