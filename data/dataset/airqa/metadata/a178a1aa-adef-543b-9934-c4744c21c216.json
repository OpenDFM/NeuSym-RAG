{
    "uuid": "a178a1aa-adef-543b-9934-c4744c21c216",
    "title": "Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{ahuja-etal-2023-scalable,\n    title = \"Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems\",\n    author = \"Ahuja, Sarthak  and\n      Kachuee, Mohammad  and\n      Sheikholeslami, Fatemeh  and\n      Liu, Weiqing  and\n      Do, Jaeyoung\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.35\",\n    doi = \"10.18653/v1/2023.acl-industry.35\",\n    pages = \"361--367\",\n    abstract = \"Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.\",\n}\n",
    "authors": [
        "Sarthak Ahuja",
        "Mohammad Kachuee",
        "Fatemeh Sheikholeslami",
        "Weiqing Liu",
        "Jaeyoung Do"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.35.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a178a1aa-adef-543b-9934-c4744c21c216.pdf",
    "abstract": "Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.",
    "num_pages": 7
}