{
    "uuid": "ba17a836-72af-58f2-8420-1da9bc4b3e78",
    "title": "HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource TweetData for Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{salahudeen-etal-2023-hausanlp,\n    title = \"{H}ausa{NLP} at {S}em{E}val-2023 Task 12: Leveraging {A}frican Low Resource {T}weet{D}ata for Sentiment Analysis\",\n    author = \"Salahudeen, Saheed Abdullahi  and\n      Lawan, Falalu Ibrahim  and\n      Wali, Ahmad  and\n      Imam, Amina Abubakar  and\n      Shuaibu, Aliyu Rabiu  and\n      Yusuf, Aliyu  and\n      Rabiu, Nur Bala  and\n      Bello, Musa  and\n      Adamu, Shamsuddeen Umaru  and\n      Aliyu, Saminu Mohammad\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.6\",\n    doi = \"10.18653/v1/2023.semeval-1.6\",\n    pages = \"50--57\",\n    abstract = \"We present the findings of SemEval-2023 Task 12, a shared task on sentiment analysis for low-resource African languages using Twitter dataset. The task featured three subtasks; subtask A is monolingual sentiment classification with 12 tracks which are all monolingual languages, subtask B is multilingual sentiment classification using the tracks in subtask A and subtask C is a zero-shot sentiment classification. We present the results and findings of subtask A, subtask B and subtask C. We also release the code on github. Our goal is to leverage low-resource tweet data using pre-trained Afro-xlmr-large, AfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment (Arabic-camelbert), Multilingual-BERT (mBERT) and BERT models for sentiment analysis of 14 African languages. The datasets for these subtasks consists of a gold standard multi-class labeled Twitter datasets from these languages. Our results demonstrate that Afro-xlmr-large model performed better compared to the other models in most of the languages datasets. Similarly, Nigerian languages: Hausa, Igbo, and Yoruba achieved better performance compared to other languages and this can be attributed to the higher volume of data present in the languages.\",\n}\n",
    "authors": [
        "Saheed Abdullahi Salahudeen",
        "Falalu Ibrahim Lawan",
        "Ahmad Wali",
        "Amina Abubakar Imam",
        "Aliyu Rabiu Shuaibu",
        "Aliyu Yusuf",
        "Nur Bala Rabiu",
        "Musa Bello",
        "Shamsuddeen Umaru Adamu",
        "Saminu Mohammad Aliyu"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ba17a836-72af-58f2-8420-1da9bc4b3e78.pdf",
    "abstract": "We present the findings of SemEval-2023 Task 12, a shared task on sentiment analysis for low-resource African languages using Twitter dataset. The task featured three subtasks; subtask A is monolingual sentiment classification with 12 tracks which are all monolingual languages, subtask B is multilingual sentiment classification using the tracks in subtask A and subtask C is a zero-shot sentiment classification. We present the results and findings of subtask A, subtask B and subtask C. We also release the code on github. Our goal is to leverage low-resource tweet data using pre-trained Afro-xlmr-large, AfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment (Arabic-camelbert), Multilingual-BERT (mBERT) and BERT models for sentiment analysis of 14 African languages. The datasets for these subtasks consists of a gold standard multi-class labeled Twitter datasets from these languages. Our results demonstrate that Afro-xlmr-large model performed better compared to the other models in most of the languages datasets. Similarly, Nigerian languages: Hausa, Igbo, and Yoruba achieved better performance compared to other languages and this can be attributed to the higher volume of data present in the languages.",
    "num_pages": 8
}