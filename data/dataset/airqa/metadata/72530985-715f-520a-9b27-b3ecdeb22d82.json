{
    "uuid": "72530985-715f-520a-9b27-b3ecdeb22d82",
    "title": "MGR: Multi-generator Based Rationalization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-mgr,\n    title = \"{MGR}: Multi-generator Based Rationalization\",\n    author = \"Liu, Wei  and\n      Wang, Haozhao  and\n      Wang, Jun  and\n      Li, Ruixuan  and\n      Li, Xinyang  and\n      Zhang, YuanKai  and\n      Qiu, Yang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.715\",\n    doi = \"10.18653/v1/2023.acl-long.715\",\n    pages = \"12771--12787\",\n    abstract = \"Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies have been proposed to address the two challenges, they are usually designed separately and do not take both of them into account. In this paper, we propose a simple yet effective method named MGR to simultaneously solve the two problems. The key idea of MGR is to employ multiple generators such that the occurrence stability of real pieces is improved and more meaningful pieces are delivered to the predictor. Empirically, we show that MGR improves the F1 score by up to 20.9{\\%} as compared to state-of-the-art methods.\",\n}\n",
    "authors": [
        "Wei Liu",
        "Haozhao Wang",
        "Jun Wang",
        "Ruixuan Li",
        "Xinyang Li",
        "YuanKai Zhang",
        "Yang Qiu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.715.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/72530985-715f-520a-9b27-b3ecdeb22d82.pdf",
    "abstract": "Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies have been proposed to address the two challenges, they are usually designed separately and do not take both of them into account. In this paper, we propose a simple yet effective method named MGR to simultaneously solve the two problems. The key idea of MGR is to employ multiple generators such that the occurrence stability of real pieces is improved and more meaningful pieces are delivered to the predictor. Empirically, we show that MGR improves the F1 score by up to 20.9% as compared to state-of-the-art methods.",
    "num_pages": 17
}