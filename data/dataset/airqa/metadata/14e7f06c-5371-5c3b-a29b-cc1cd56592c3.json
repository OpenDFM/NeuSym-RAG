{
    "uuid": "14e7f06c-5371-5c3b-a29b-cc1cd56592c3",
    "title": "Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{chan-etal-2023-spurious,\n    title = \"Which Spurious Correlations Impact Reasoning in {NLI} Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals\",\n    author = \"Chan, Robin  and\n      Amini, Afra  and\n      El-Assady, Mennatallah\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.44\",\n    doi = \"10.18653/v1/2023.acl-demo.44\",\n    pages = \"463--470\",\n    abstract = \"We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models{'} robustness by creating adversarial test suites.\",\n}\n",
    "authors": [
        "Robin Chan",
        "Afra Amini",
        "Mennatallah El-Assady"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.44.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/14e7f06c-5371-5c3b-a29b-cc1cd56592c3.pdf",
    "abstract": "We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI modelsâ€™ robustness by creating adversarial test suites.",
    "num_pages": 8
}