{
    "uuid": "ad7484cc-16a8-5f9a-880c-fa876b103193",
    "title": "Dissecting Biases in Relation Extraction: A Cross-Dataset Analysis on People’s Gender and Origin",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    "bibtex": "@inproceedings{stranisci-etal-2024-dissecting,\n    title = \"Dissecting Biases in Relation Extraction: A Cross-Dataset Analysis on People{'}s Gender and Origin\",\n    author = \"Stranisci, Marco  and\n      Huguet Cabot, Pere-Llu{\\'\\i}s  and\n      Bassignana, Elisa  and\n      Navigli, Roberto\",\n    editor = \"Fale{\\'n}ska, Agnieszka  and\n      Basta, Christine  and\n      Costa-juss{\\`a}, Marta  and\n      Goldfarb-Tarrant, Seraphina  and\n      Nozza, Debora\",\n    booktitle = \"Proceedings of the 5th Workshop on Gender Bias in Natural Language Processing (GeBNLP)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.gebnlp-1.12\",\n    doi = \"10.18653/v1/2024.gebnlp-1.12\",\n    pages = \"190--202\",\n    abstract = \"Relation Extraction (RE) is at the core of many Natural Language Understanding tasks, including knowledge-base population and Question Answering. However, any Natural Language Processing system is exposed to biases, and the analysis of these has not received much attention in RE. We propose a new method for inspecting bias in the RE pipeline, which is completely transparent in terms of interpretability. Specifically, in this work we analyze biases related to gender and place of birth. Our methodology includes (i) obtaining semantic triplets (subject, object, semantic relation) involving {`}person{'} entities from RE resources, (ii) collecting meta-information ({`}gender{'} and {`}place of birth{'}) using Entity Linking technologies, and then (iii) analyze the distribution of triplets across different groups (e.g., men versus women). We investigate bias at two levels: In the training data of three commonly used RE datasets (SREDFM, CrossRE, NYT), and in the predictions of a state-of-the-art RE approach (ReLiK). To enable cross-dataset analysis, we introduce a taxonomy of relation types mapping the label sets of different RE datasets to a unified label space. Our findings reveal that bias is a compounded issue affecting underrepresented groups within data and predictions for RE.\",\n}\n",
    "authors": [
        "Marco Stranisci",
        "Pere-Lluís Huguet Cabot",
        "Elisa Bassignana",
        "Roberto Navigli"
    ],
    "pdf_url": "https://aclanthology.org/2024.gebnlp-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ad7484cc-16a8-5f9a-880c-fa876b103193.pdf",
    "abstract": "Relation Extraction (RE) is at the core of many Natural Language Understanding tasks, including knowledge-base population and Question Answering. However, any Natural Language Processing system is exposed to biases, and the analysis of these has not received much attention in RE. We propose a new method for inspecting bias in the RE pipeline, which is completely transparent in terms of interpretability. Specifically, in this work we analyze biases related to gender and place of birth. Our methodology includes (i) obtaining semantic triplets (subject, object, semantic relation) involving ‘person’ entities from RE resources, (ii) collecting meta-information (‘gender’ and ‘place of birth’) using Entity Linking technologies, and then (iii) analyze the distribution of triplets across different groups (e.g., men versus women). We investigate bias at two levels: In the training data of three commonly used RE datasets (SREDFM, CrossRE, NYT), and in the predictions of a state-of-the-art RE approach (ReLiK). To enable cross-dataset analysis, we introduce a taxonomy of relation types mapping the label sets of different RE datasets to a unified label space. Our findings reveal that bias is a compounded issue affecting underrepresented groups within data and predictions for RE.",
    "num_pages": 13
}