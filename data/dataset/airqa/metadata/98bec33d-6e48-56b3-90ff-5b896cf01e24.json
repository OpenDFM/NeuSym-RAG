{
    "uuid": "98bec33d-6e48-56b3-90ff-5b896cf01e24",
    "title": "Training Trajectories of Language Models Across Scales",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{xia-etal-2023-training,\n    title = \"Training Trajectories of Language Models Across Scales\",\n    author = \"Xia, Mengzhou  and\n      Artetxe, Mikel  and\n      Zhou, Chunting  and\n      Lin, Xi Victoria  and\n      Pasunuru, Ramakanth  and\n      Chen, Danqi  and\n      Zettlemoyer, Luke  and\n      Stoyanov, Veselin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.767\",\n    doi = \"10.18653/v1/2023.acl-long.767\",\n    pages = \"13711--13738\",\n    abstract = \"Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022){---}from 125M to 175B parameters{---}on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.\",\n}\n",
    "authors": [
        "Mengzhou Xia",
        "Mikel Artetxe",
        "Chunting Zhou",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Danqi Chen",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.767.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/98bec33d-6e48-56b3-90ff-5b896cf01e24.pdf",
    "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125M to 175B parameters—on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
    "num_pages": 28
}