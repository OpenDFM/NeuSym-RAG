{
    "uuid": "daf20bd3-0312-5d4d-b057-93f4e21377f4",
    "title": "Text-To-KG Alignment: Comparing Current Methods on Classification Tasks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)",
    "bibtex": "@inproceedings{wold-etal-2023-text,\n    title = \"Text-To-{KG} Alignment: Comparing Current Methods on Classification Tasks\",\n    author = \"Wold, Sondre  and\n      {\\O}vrelid, Lilja  and\n      Velldal, Erik\",\n    editor = \"Hruschka, Estevam  and\n      Mitchell, Tom  and\n      Rahman, Sajjadur  and\n      Mladeni{\\'c}, Dunja  and\n      Grobelnik, Marko\",\n    booktitle = \"Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, ON, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.matching-1.1\",\n    doi = \"10.18653/v1/2023.matching-1.1\",\n    pages = \"1--13\",\n    abstract = \"In contrast to large text corpora, knowledge graphs (KG) provide dense and structured representations of factual information. This makes them attractive for systems that supplement or ground the knowledge found in pre-trained language models with an external knowledge source. This has especially been the case for classification tasks, where recent work has focused on creating pipeline models that retrieve information from KGs like ConceptNet as additional context. Many of these models consist of multiple components, and although they differ in the number and nature of these parts, they all have in common that for some given text query, they attempt to identify and retrieve a relevant subgraph from the KG. Due to the noise and idiosyncrasies often found in KGs, it is not known how current methods compare to a scenario where the aligned subgraph is completely relevant to the query. In this work, we try to bridge this knowledge gap by reviewing current approaches to text-to-KG alignment and evaluating them on two datasets where manually created graphs are available, providing insights into the effectiveness of current methods. We release our code for reproducibility.\",\n}\n",
    "authors": [
        "Sondre Wold",
        "Lilja Ã˜vrelid",
        "Erik Velldal"
    ],
    "pdf_url": "https://aclanthology.org/2023.matching-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/daf20bd3-0312-5d4d-b057-93f4e21377f4.pdf",
    "abstract": "In contrast to large text corpora, knowledge graphs (KG) provide dense and structured representations of factual information. This makes them attractive for systems that supplement or ground the knowledge found in pre-trained language models with an external knowledge source. This has especially been the case for classification tasks, where recent work has focused on creating pipeline models that retrieve information from KGs like ConceptNet as additional context. Many of these models consist of multiple components, and although they differ in the number and nature of these parts, they all have in common that for some given text query, they attempt to identify and retrieve a relevant subgraph from the KG. Due to the noise and idiosyncrasies often found in KGs, it is not known how current methods compare to a scenario where the aligned subgraph is completely relevant to the query. In this work, we try to bridge this knowledge gap by reviewing current approaches to text-to-KG alignment and evaluating them on two datasets where manually created graphs are available, providing insights into the effectiveness of current methods. We release our code for reproducibility.",
    "num_pages": 13
}