{
    "uuid": "ffd6869e-7b4c-5e7c-a123-a7ea0ad8e308",
    "title": "Who needs context? Classical techniques for Alzheimer’s disease detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{taghibeyglou-rudzicz-2023-needs,\n    title = \"Who needs context? Classical techniques for {A}lzheimer{'}s disease detection\",\n    author = \"Taghibeyglou, Behrad  and\n      Rudzicz, Frank\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.13\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.13\",\n    pages = \"102--107\",\n    abstract = \"Natural language processing (NLP) has shown great potential for Alzheimer{'}s disease (AD) detection, particularly due to the adverse effect of AD on spontaneous speech. The current body of literature has directed attention toward context-based models, especially Bidirectional Encoder Representations from Transformers (BERTs), owing to their exceptional abilities to integrate contextual information in a wide range of NLP tasks. This comes at the cost of added model opacity and computational requirements. Taking this into consideration, we propose a Word2Vec-based model for AD detection in 108 age- and sex-matched participants who were asked to describe the Cookie Theft picture. We also investigate the effectiveness of our model by fine-tuning BERT-based sequence classification models, as well as incorporating linguistic features. Our results demonstrate that our lightweight and easy-to-implement model outperforms some of the state-of-the-art models available in the literature, as well as BERT models.\",\n}\n",
    "authors": [
        "Behrad Taghibeyglou",
        "Frank Rudzicz"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ffd6869e-7b4c-5e7c-a123-a7ea0ad8e308.pdf",
    "abstract": "Natural language processing (NLP) has shown great potential for Alzheimer’s disease (AD) detection, particularly due to the adverse effect of AD on spontaneous speech. The current body of literature has directed attention toward context-based models, especially Bidirectional Encoder Representations from Transformers (BERTs), owing to their exceptional abilities to integrate contextual information in a wide range of NLP tasks. This comes at the cost of added model opacity and computational requirements. Taking this into consideration, we propose a Word2Vec-based model for AD detection in 108 age- and sex-matched participants who were asked to describe the Cookie Theft picture. We also investigate the effectiveness of our model by fine-tuning BERT-based sequence classification models, as well as incorporating linguistic features. Our results demonstrate that our lightweight and easy-to-implement model outperforms some of the state-of-the-art models available in the literature, as well as BERT models.",
    "num_pages": 6
}