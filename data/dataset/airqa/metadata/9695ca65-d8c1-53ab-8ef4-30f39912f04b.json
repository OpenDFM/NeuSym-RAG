{
    "uuid": "9695ca65-d8c1-53ab-8ef4-30f39912f04b",
    "title": "Exploring Variation of Results from Different Experimental Conditions",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{popovic-etal-2023-exploring,\n    title = \"Exploring Variation of Results from Different Experimental Conditions\",\n    author = \"Popovi{\\'c}, Maja  and\n      Arvan, Mohammad  and\n      Parde, Natalie  and\n      Belz, Anya\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.172\",\n    doi = \"10.18653/v1/2023.findings-acl.172\",\n    pages = \"2746--2757\",\n    abstract = \"It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP groups to run the training and testing pipeline for three neural text simplification models under varying experimental conditions, including different random seeds, run-time environments, and dependency versions, yielding a large number of results for each of the three models using the same data and train/dev/test set splits. From one perspective, these results can be interpreted as shedding light on the reproducibility of evaluation results for the three NTS models, and we present an in-depth analysis of the variation observed for different combinations of experimental conditions. From another perspective, the results raise the question of whether the averaged score should be considered the {`}true{'} result for each model.\",\n}\n",
    "authors": [
        "Maja Popović",
        "Mohammad Arvan",
        "Natalie Parde",
        "Anya Belz"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.172.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9695ca65-d8c1-53ab-8ef4-30f39912f04b.pdf",
    "abstract": "It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP groups to run the training and testing pipeline for three neural text simplification models under varying experimental conditions, including different random seeds, run-time environments, and dependency versions, yielding a large number of results for each of the three models using the same data and train/dev/test set splits. From one perspective, these results can be interpreted as shedding light on the reproducibility of evaluation results for the three NTS models, and we present an in-depth analysis of the variation observed for different combinations of experimental conditions. From another perspective, the results raise the question of whether the averaged score should be considered the ‘true’ result for each model.",
    "num_pages": 12
}