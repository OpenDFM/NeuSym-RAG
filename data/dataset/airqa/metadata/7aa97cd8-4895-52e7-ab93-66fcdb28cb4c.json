{
    "uuid": "7aa97cd8-4895-52e7-ab93-66fcdb28cb4c",
    "title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{tang-etal-2023-pre,\n    title = \"Are Pre-trained Language Models Useful for Model Ensemble in {C}hinese Grammatical Error Correction?\",\n    author = \"Tang, Chenming  and\n      Wu, Xiuyu  and\n      Wu, Yunfang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.77\",\n    doi = \"10.18653/v1/2023.acl-short.77\",\n    pages = \"893--901\",\n    abstract = \"Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models. However, the performance does not improve but even gets worse after the PLM-based ensemble. This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC. The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention. Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data. Our source code is available at \\url{https://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble}.\",\n}\n",
    "authors": [
        "Chenming Tang",
        "Xiuyu Wu",
        "Yunfang Wu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.77.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7aa97cd8-4895-52e7-ab93-66fcdb28cb4c.pdf",
    "abstract": "Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models. However, the performance does not improve but even gets worse after the PLM-based ensemble. This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC. The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention. Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data. Our source code is available at https://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble.",
    "num_pages": 9
}