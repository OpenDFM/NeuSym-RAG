{
    "uuid": "eac33531-fd4a-525a-bace-0e99965a6533",
    "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{sun-etal-2024-itd,\n    title = \"{I}t{D}: Large Language Models Can Teach Themselves Induction through Deduction\",\n    author = \"Sun, Wangtao  and\n      Xu, Haotian  and\n      Yu, Xuanqing  and\n      Chen, Pei  and\n      He, Shizhu  and\n      Zhao, Jun  and\n      Liu, Kang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.150\",\n    doi = \"10.18653/v1/2024.acl-long.150\",\n    pages = \"2719--2731\",\n    abstract = \"Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt {``}post processes{''} paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search {\\&} refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36{\\%} and 10{\\%} compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.\",\n}\n",
    "authors": [
        "Wangtao Sun",
        "Haotian Xu",
        "Xuanqing Yu",
        "Pei Chen",
        "Shizhu He",
        "Jun Zhao",
        "Kang Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.150.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/eac33531-fd4a-525a-bace-0e99965a6533.pdf",
    "abstract": "Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt “post processes” paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.",
    "num_pages": 13
}