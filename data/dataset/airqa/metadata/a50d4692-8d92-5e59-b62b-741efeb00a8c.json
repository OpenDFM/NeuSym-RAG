{
    "uuid": "a50d4692-8d92-5e59-b62b-741efeb00a8c",
    "title": "CRAFT: Extracting and Tuning Cultural Instructions from the Wild",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP",
    "bibtex": "@inproceedings{wang-etal-2024-craft,\n    title = \"{CRAFT}: Extracting and Tuning Cultural Instructions from the Wild\",\n    author = \"Wang, Bin  and\n      Lin, Geyu  and\n      Liu, Zhengyuan  and\n      Wei, Chengwei  and\n      Chen, Nancy\",\n    editor = \"Prabhakaran, Vinodkumar  and\n      Dev, Sunipa  and\n      Benotti, Luciana  and\n      Hershcovich, Daniel  and\n      Cabello, Laura  and\n      Cao, Yong  and\n      Adebara, Ife  and\n      Zhou, Li\",\n    booktitle = \"Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.c3nlp-1.4\",\n    doi = \"10.18653/v1/2024.c3nlp-1.4\",\n    pages = \"42--47\",\n    abstract = \"Large language models (LLMs) have rapidly evolved as the foundation of various natural language processing (NLP) applications. Despite their wide use cases, their understanding of culturally-related concepts and reasoning remains limited. Meantime, there is a significant need to enhance these models{'} cultural reasoning capabilities, especially concerning underrepresented regions. This paper introduces a novel pipeline for extracting high-quality, culturally-related instruction tuning datasets from vast unstructured corpora. We utilize a self-instruction generation pipeline to identify cultural concepts and trigger instruction. By integrating with a general-purpose instruction tuning dataset, our model demonstrates enhanced capabilities in recognizing and understanding regional cultural nuances, thereby enhancing its reasoning capabilities. We conduct experiments across three regions: Singapore, the Philippines, and the United States, achieving performance improvement of up to 6{\\%}. Our research opens new avenues for extracting cultural instruction tuning sets directly from unstructured data, setting a precedent for future innovations in the field.\",\n}\n",
    "authors": [
        "Bin Wang",
        "Geyu Lin",
        "Zhengyuan Liu",
        "Chengwei Wei",
        "Nancy Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.c3nlp-1.4.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a50d4692-8d92-5e59-b62b-741efeb00a8c.pdf",
    "abstract": "Large language models (LLMs) have rapidly evolved as the foundation of various natural language processing (NLP) applications. Despite their wide use cases, their understanding of culturally-related concepts and reasoning remains limited. Meantime, there is a significant need to enhance these modelsâ€™ cultural reasoning capabilities, especially concerning underrepresented regions. This paper introduces a novel pipeline for extracting high-quality, culturally-related instruction tuning datasets from vast unstructured corpora. We utilize a self-instruction generation pipeline to identify cultural concepts and trigger instruction. By integrating with a general-purpose instruction tuning dataset, our model demonstrates enhanced capabilities in recognizing and understanding regional cultural nuances, thereby enhancing its reasoning capabilities. We conduct experiments across three regions: Singapore, the Philippines, and the United States, achieving performance improvement of up to 6%. Our research opens new avenues for extracting cultural instruction tuning sets directly from unstructured data, setting a precedent for future innovations in the field.",
    "num_pages": 6
}