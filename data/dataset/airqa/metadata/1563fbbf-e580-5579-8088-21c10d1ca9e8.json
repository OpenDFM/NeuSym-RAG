{
    "uuid": "1563fbbf-e580-5579-8088-21c10d1ca9e8",
    "title": "Can In-context Learners Learn a Reasoning Concept from Demonstrations?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
    "bibtex": "@inproceedings{tefnik-kadlcik-2023-context,\n    title = \"Can In-context Learners Learn a Reasoning Concept from Demonstrations?\",\n    author = \"{\\v{S}}tef{\\'a}nik, Michal  and\n      Kadl{\\v{c}}{\\'\\i}k, Marek\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Neves Ribeiro, Danilo  and\n      Wei, Jason\",\n    booktitle = \"Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)\",\n    month = jun,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.nlrse-1.8\",\n    doi = \"10.18653/v1/2023.nlrse-1.8\",\n    pages = \"107--115\",\n    abstract = \"Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models{'} ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution. To disentangle models{'} in-context learning ability independent of models{'} memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations. We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.\",\n}\n",
    "authors": [
        "Michal Štefánik",
        "Marek Kadlčík"
    ],
    "pdf_url": "https://aclanthology.org/2023.nlrse-1.8.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1563fbbf-e580-5579-8088-21c10d1ca9e8.pdf",
    "abstract": "Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models’ ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution. To disentangle models’ in-context learning ability independent of models’ memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations. We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.",
    "num_pages": 9
}