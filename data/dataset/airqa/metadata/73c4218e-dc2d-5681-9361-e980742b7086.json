{
    "uuid": "73c4218e-dc2d-5681-9361-e980742b7086",
    "title": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{sarti-etal-2023-inseq,\n    title = \"Inseq: An Interpretability Toolkit for Sequence Generation Models\",\n    author = \"Sarti, Gabriele  and\n      Feldhus, Nils  and\n      Sickert, Ludwig  and\n      van der Wal, Oskar\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.40\",\n    doi = \"10.18653/v1/2023.acl-demo.40\",\n    pages = \"421--435\",\n    abstract = \"Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models{'} internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.\",\n}\n",
    "authors": [
        "Gabriele Sarti",
        "Nils Feldhus",
        "Ludwig Sickert",
        "Oskar van der Wal"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.40.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/73c4218e-dc2d-5681-9361-e980742b7086.pdf",
    "abstract": "Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of modelsâ€™ internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.",
    "num_pages": 15
}