{
    "uuid": "d0cbf94a-ae8f-5ecc-a119-b355e03e7352",
    "title": "When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-phrases,\n    title = \"When Phrases Meet Probabilities: Enabling Open Relation Extraction with Cooperating Large Language Models\",\n    author = \"Wang, Jiaxin  and\n      Zhang, Lingling  and\n      Lee, Wee Sun  and\n      Zhong, Yujie  and\n      Kang, Liwei  and\n      Liu, Jun\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.709\",\n    doi = \"10.18653/v1/2024.acl-long.709\",\n    pages = \"13130--13147\",\n    abstract = \"Current clustering-based open relation extraction (OpenRE) methods usually apply clustering algorithms on top of pre-trained language models. However, this practice has three drawbacks. First, embeddings from language models are high-dimensional and anisotropic, so using simple metrics to calculate distances between these embeddings may not accurately reflect the relational similarity. Second, there exists a gap between the pre-trained language models and downstream clustering for their different objective forms. Third, clustering with embeddings deviates from the primary aim of relation extraction, as it does not directly obtain relations. In this work, we propose a new idea for OpenRE in the era of LLMs, that is, extracting relational phrases and directly exploiting the knowledge in LLMs to assess the semantic similarity between phrases without relying on any additional metrics. Based on this idea, we developed a framework, oreLLM, that makes two LLMs work collaboratively to achieve clustering and address the above issues. Experimental results on different datasets show that oreLLM outperforms current baselines by $1.4\\%\\sim 3.13\\%$ in terms of clustering accuracy.\",\n}\n",
    "authors": [
        "Jiaxin Wang",
        "Lingling Zhang",
        "Wee Sun Lee",
        "Yujie Zhong",
        "Liwei Kang",
        "Jun Liu"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.709.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d0cbf94a-ae8f-5ecc-a119-b355e03e7352.pdf",
    "abstract": "Current clustering-based open relation extraction (OpenRE) methods usually apply clustering algorithms on top of pre-trained language models. However, this practice has three drawbacks. First, embeddings from language models are high-dimensional and anisotropic, so using simple metrics to calculate distances between these embeddings may not accurately reflect the relational similarity. Second, there exists a gap between the pre-trained language models and downstream clustering for their different objective forms. Third, clustering with embeddings deviates from the primary aim of relation extraction, as it does not directly obtain relations. In this work, we propose a new idea for OpenRE in the era of LLMs, that is, extracting relational phrases and directly exploiting the knowledge in LLMs to assess the semantic similarity between phrases without relying on any additional metrics. Based on this idea, we developed a framework, oreLLM, that makes two LLMs work collaboratively to achieve clustering and address the above issues. Experimental results on different datasets show that oreLLM outperforms current baselines by 1.4%âˆ¼ 3.13% in terms of clustering accuracy.",
    "num_pages": 18
}