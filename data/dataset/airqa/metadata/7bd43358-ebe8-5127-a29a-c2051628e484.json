{
    "uuid": "7bd43358-ebe8-5127-a29a-c2051628e484",
    "title": "Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yang-etal-2023-self,\n    title = \"Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition\",\n    author = \"Yang, Haozhe  and\n      Gao, Xianqiang  and\n      Wu, Jianlong  and\n      Gan, Tian  and\n      Ding, Ning  and\n      Jiang, Feijun  and\n      Nie, Liqiang\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.390\",\n    doi = \"10.18653/v1/2023.findings-acl.390\",\n    pages = \"6267--6281\",\n    abstract = \"The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, i.e., long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we propose the Self-adaptive Context and Modal-interaction Modeling (SCMM) framework. We first design the context representation module, which consists of three submodules to model multiple contextual representations. Thereafter, we propose the modal-interaction module, including three interaction submodules to make full use of each modality. Finally, we come up with a self-adaptive path selection module to select an appropriate path in each module and integrate the features to obtain the final representation. Extensive experiments under four settings on three multimodal datasets, including IEMOCAP, MELD, and MOSEI, demonstrate that our proposed method outperforms the state-of-the-art approaches.\",\n}\n",
    "authors": [
        "Haozhe Yang",
        "Xianqiang Gao",
        "Jianlong Wu",
        "Tian Gan",
        "Ning Ding",
        "Feijun Jiang",
        "Liqiang Nie"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.390.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7bd43358-ebe8-5127-a29a-c2051628e484.pdf",
    "abstract": "The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, i.e., long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we propose the Self-adaptive Context and Modal-interaction Modeling (SCMM) framework. We first design the context representation module, which consists of three submodules to model multiple contextual representations. Thereafter, we propose the modal-interaction module, including three interaction submodules to make full use of each modality. Finally, we come up with a self-adaptive path selection module to select an appropriate path in each module and integrate the features to obtain the final representation. Extensive experiments under four settings on three multimodal datasets, including IEMOCAP, MELD, and MOSEI, demonstrate that our proposed method outperforms the state-of-the-art approaches.",
    "num_pages": 15
}