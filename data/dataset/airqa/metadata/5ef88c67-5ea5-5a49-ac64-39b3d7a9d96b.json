{
    "uuid": "5ef88c67-5ea5-5a49-ac64-39b3d7a9d96b",
    "title": "Adversarial Textual Robustness on Visual Dialog",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yu-rieser-2023-adversarial,\n    title = \"Adversarial Textual Robustness on Visual Dialog\",\n    author = \"Yu, Lu  and\n      Rieser, Verena\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.212\",\n    doi = \"10.18653/v1/2023.findings-acl.212\",\n    pages = \"3422--3438\",\n    abstract = \"Adversarial robustness evaluates the worst-case performance scenario of a machine learning model to ensure its safety and reliability. For example, cases where the user input contains a minimal change, e.g. a synonym, which causes the previously correct model to return a wrong answer. Using this scenario, this study is the first to investigate the robustness of visually grounded dialog models towards textual attacks. We first aim to understand how multimodal input components contribute to model robustness. Our results show that models which encode dialog history are more robust by providing redundant information. This is in contrast to prior work which finds that dialog history is negligible for model performance on this task. We also evaluate how to generate adversarial test examples which successfully fool the model but remain undetected by the user/software designer. Our analysis shows that the textual, as well as the visual context are important to generate plausible attacks.\",\n}\n",
    "authors": [
        "Lu Yu",
        "Verena Rieser"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.212.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/5ef88c67-5ea5-5a49-ac64-39b3d7a9d96b.pdf",
    "abstract": "Adversarial robustness evaluates the worst-case performance scenario of a machine learning model to ensure its safety and reliability. For example, cases where the user input contains a minimal change, e.g. a synonym, which causes the previously correct model to return a wrong answer. Using this scenario, this study is the first to investigate the robustness of visually grounded dialog models towards textual attacks. We first aim to understand how multimodal input components contribute to model robustness. Our results show that models which encode dialog history are more robust by providing redundant information. This is in contrast to prior work which finds that dialog history is negligible for model performance on this task. We also evaluate how to generate adversarial test examples which successfully fool the model but remain undetected by the user/software designer. Our analysis shows that the textual, as well as the visual context are important to generate plausible attacks.",
    "num_pages": 17
}