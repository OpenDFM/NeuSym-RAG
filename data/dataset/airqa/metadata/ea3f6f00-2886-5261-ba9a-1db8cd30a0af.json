{
    "uuid": "ea3f6f00-2886-5261-ba9a-1db8cd30a0af",
    "title": "No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
    "bibtex": "@inproceedings{nighojkar-etal-2023-strong,\n    title = \"No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference\",\n    author = \"Nighojkar, Animesh  and\n      Laverghetta Jr., Antonio  and\n      Licato, John\",\n    editor = \"Prange, Jakob  and\n      Friedrich, Annemarie\",\n    booktitle = \"Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.law-1.20\",\n    doi = \"10.18653/v1/2023.law-1.20\",\n    pages = \"199--210\",\n    abstract = \"Natural Language Inference (NLI) has been a cornerstone task in evaluating language models{'} inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models{'} ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP community.\",\n}\n",
    "authors": [
        "Animesh Nighojkar",
        "Antonio Laverghetta Jr.",
        "John Licato"
    ],
    "pdf_url": "https://aclanthology.org/2023.law-1.20.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ea3f6f00-2886-5261-ba9a-1db8cd30a0af.pdf",
    "abstract": "Natural Language Inference (NLI) has been a cornerstone task in evaluating language models’ inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models’ ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP community.",
    "num_pages": 12
}