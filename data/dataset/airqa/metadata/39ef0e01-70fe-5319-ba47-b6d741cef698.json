{
    "uuid": "39ef0e01-70fe-5319-ba47-b6d741cef698",
    "title": "Multilevel Analysis of Biomedical Domain Adaptation of Llama 2: What Matters the Most? A Case Study",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
    "bibtex": "@inproceedings{sanchez-carmona-etal-2024-multilevel-analysis,\n    title = \"Multilevel Analysis of Biomedical Domain Adaptation of Llama 2: What Matters the Most? A Case Study\",\n    author = \"Sanchez Carmona, Vicente Ivan  and\n      Jiang, Shanshan  and\n      Suzuki, Takeshi  and\n      Dong, Bin\",\n    editor = \"Demner-Fushman, Dina  and\n      Ananiadou, Sophia  and\n      Miwa, Makoto  and\n      Roberts, Kirk  and\n      Tsujii, Junichi\",\n    booktitle = \"Proceedings of the 23rd Workshop on Biomedical Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.bionlp-1.36\",\n    doi = \"10.18653/v1/2024.bionlp-1.36\",\n    pages = \"449--456\",\n    abstract = \"Domain adaptation of Large Language Models (LLMs) leads to models better suited for a particular domain by capturing patterns from domain text which leads to improvements in downstream tasks. To the naked eye, these improvements are visible; however, the patterns are not so. How can we know which patterns and how much they contribute to changes in downstream scores? Through a Multilevel Analysis we discover and quantify the effect of text patterns on downstream scores of domain-adapted Llama 2 for the task of sentence similarity (BIOSSES dataset). We show that text patterns from PubMed abstracts such as clear writing and simplicity, as well as the amount of biomedical information, are the key for improving downstream scores. Also, we show how another factor not usually quantified contributes equally to downstream scores: choice of hyperparameters for both domain adaptation and fine-tuning.\",\n}\n",
    "authors": [
        "Vicente Ivan Sanchez Carmona",
        "Shanshan Jiang",
        "Takeshi Suzuki",
        "Bin Dong"
    ],
    "pdf_url": "https://aclanthology.org/2024.bionlp-1.36.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/39ef0e01-70fe-5319-ba47-b6d741cef698.pdf",
    "abstract": "Domain adaptation of Large Language Models (LLMs) leads to models better suited for a particular domain by capturing patterns from domain text which leads to improvements in downstream tasks. To the naked eye, these improvements are visible; however, the patterns are not so. How can we know which patterns and how much they contribute to changes in downstream scores? Through a Multilevel Analysis we discover and quantify the effect of text patterns on downstream scores of domain-adapted Llama 2 for the task of sentence similarity (BIOSSES dataset). We show that text patterns from PubMed abstracts such as clear writing and simplicity, as well as the amount of biomedical information, are the key for improving downstream scores. Also, we show how another factor not usually quantified contributes equally to downstream scores: choice of hyperparameters for both domain adaptation and fine-tuning.",
    "num_pages": 8
}