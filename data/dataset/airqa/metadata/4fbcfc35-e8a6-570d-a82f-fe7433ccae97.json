{
    "uuid": "4fbcfc35-e8a6-570d-a82f-fe7433ccae97",
    "title": "Ranking Large Language Models without Ground Truth",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{dhurandhar-etal-2024-ranking,\n    title = \"Ranking Large Language Models without Ground Truth\",\n    author = \"Dhurandhar, Amit  and\n      Nair, Rahul  and\n      Singh, Moninder  and\n      Daly, Elizabeth  and\n      Natesan Ramamurthy, Karthikeyan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.143\",\n    doi = \"10.18653/v1/2024.findings-acl.143\",\n    pages = \"2431--2452\",\n    abstract = \"Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover true rankings without reference data. This points to a viable low-resource mechanism for practical use.\",\n}\n",
    "authors": [
        "Amit Dhurandhar",
        "Rahul Nair",
        "Moninder Singh",
        "Elizabeth Daly",
        "Karthikeyan Natesan Ramamurthy"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.143.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4fbcfc35-e8a6-570d-a82f-fe7433ccae97.pdf",
    "abstract": "Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover true rankings without reference data. This points to a viable low-resource mechanism for practical use.",
    "num_pages": 22
}