{
    "uuid": "676157dd-b35d-5f9d-b10d-eaa659495eb9",
    "title": "BotEval: Facilitating Interactive Human Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{cho-etal-2024-boteval,\n    title = \"{B}ot{E}val: Facilitating Interactive Human Evaluation\",\n    author = \"Cho, Hyundong  and\n      Gowda, Thamme  and\n      Huang, Yuyang  and\n      Lu, Zixun  and\n      Tong, Tianli  and\n      May, Jonathan\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.11\",\n    doi = \"10.18653/v1/2024.acl-demos.11\",\n    pages = \"107--116\",\n    abstract = \"Following the rapid progress in natural language processing (NLP) models, language models are applied to increasingly more complex interactive tasks such as negotiations and conversation moderations. Having human evaluators directly interact with these NLP models is essential for adequately evaluating the performance on such interactive tasks. We develop BotEval, an easily customizable, open-source, evaluation toolkit that focuses on enabling human-bot interactions as part of the evaluation process, as opposed to human evaluators making judgements for a static input. BotEval balances flexibility for customization and user-friendliness by providing templates for common use cases that span various degrees of complexity and built-in compatibility with popular crowdsourcing platforms.We showcase the numerous useful features of BotEval through a study that evaluates the performance of various chatbots on their effectiveness for conversational moderation and discuss how BotEval differs from other annotation tools.\",\n}\n",
    "authors": [
        "Hyundong Cho",
        "Thamme Gowda",
        "Yuyang Huang",
        "Zixun Lu",
        "Tianli Tong",
        "Jonathan May"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.11.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/676157dd-b35d-5f9d-b10d-eaa659495eb9.pdf",
    "abstract": "Following the rapid progress in natural language processing (NLP) models, language models are applied to increasingly more complex interactive tasks such as negotiations and conversation moderations. Having human evaluators directly interact with these NLP models is essential for adequately evaluating the performance on such interactive tasks. We develop BotEval, an easily customizable, open-source, evaluation toolkit that focuses on enabling human-bot interactions as part of the evaluation process, as opposed to human evaluators making judgements for a static input. BotEval balances flexibility for customization and user-friendliness by providing templates for common use cases that span various degrees of complexity and built-in compatibility with popular crowdsourcing platforms.We showcase the numerous useful features of BotEval through a study that evaluates the performance of various chatbots on their effectiveness for conversational moderation and discuss how BotEval differs from other annotation tools.",
    "num_pages": 10
}