{
    "uuid": "1e93081c-a8e5-51d5-8616-2385d8f107e5",
    "title": "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{tufanov-etal-2024-lm,\n    title = \"{LM} Transparency Tool: Interactive Tool for Analyzing Transformer Language Models\",\n    author = \"Tufanov, Igor  and\n      Hambardzumyan, Karen  and\n      Ferrando, Javier  and\n      Voita, Elena\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.6\",\n    doi = \"10.18653/v1/2024.acl-demos.6\",\n    pages = \"51--60\",\n    abstract = \"We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.\",\n}\n",
    "authors": [
        "Igor Tufanov",
        "Karen Hambardzumyan",
        "Javier Ferrando",
        "Elena Voita"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1e93081c-a8e5-51d5-8616-2385d8f107e5.pdf",
    "abstract": "We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications.",
    "num_pages": 10
}