{
    "uuid": "93c74fa9-1c78-583d-a5de-e863abbc5314",
    "title": "CausalGym: Benchmarking causal interpretability methods on linguistic tasks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{arora-etal-2024-causalgym,\n    title = \"{C}ausal{G}ym: Benchmarking causal interpretability methods on linguistic tasks\",\n    author = \"Arora, Aryaman  and\n      Jurafsky, Dan  and\n      Potts, Christopher\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.785\",\n    doi = \"10.18653/v1/2024.acl-long.785\",\n    pages = \"14638--14663\",\n    abstract = \"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M{--}6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler{--}gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.\",\n}\n",
    "authors": [
        "Aryaman Arora",
        "Dan Jurafsky",
        "Christopher Potts"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.785.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/93c74fa9-1c78-583d-a5de-e863abbc5314.pdf",
    "abstract": "Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M–6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler–gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.",
    "num_pages": 26
}