{
    "uuid": "d0d474e7-efe9-5836-a473-0081b01b42e3",
    "title": "Found in the middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hsieh-etal-2024-found,\n    title = \"Found in the middle: Calibrating Positional Attention Bias Improves Long Context Utilization\",\n    author = \"Hsieh, Cheng-Yu  and\n      Chuang, Yung-Sung  and\n      Li, Chun-Liang  and\n      Wang, Zifeng  and\n      Le, Long  and\n      Kumar, Abhishek  and\n      Glass, James  and\n      Ratner, Alexander  and\n      Lee, Chen-Yu  and\n      Krishna, Ranjay  and\n      Pfister, Tomas\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.890\",\n    doi = \"10.18653/v1/2024.findings-acl.890\",\n    pages = \"14982--14995\",\n    abstract = \"Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs{'} intrinsic attention bias: LLMs exhibit an U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 10 percentage point. These findings open up future directions in understanding LLM attention bias and its potential consequences.\",\n}\n",
    "authors": [
        "Cheng-Yu Hsieh",
        "Yung-Sung Chuang",
        "Chun-Liang Li",
        "Zifeng Wang",
        "Long Le",
        "Abhishek Kumar",
        "James Glass",
        "Alexander Ratner",
        "Chen-Yu Lee",
        "Ranjay Krishna",
        "Tomas Pfister"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.890.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d0d474e7-efe9-5836-a473-0081b01b42e3.pdf",
    "abstract": "Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMsâ€™ intrinsic attention bias: LLMs exhibit an U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 10 percentage point. These findings open up future directions in understanding LLM attention bias and its potential consequences.",
    "num_pages": 14
}