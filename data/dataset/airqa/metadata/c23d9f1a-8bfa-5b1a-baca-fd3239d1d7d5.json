{
    "uuid": "c23d9f1a-8bfa-5b1a-baca-fd3239d1d7d5",
    "title": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-towards,\n    title = \"Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations\",\n    author = \"Liu, Linlin  and\n      Li, Xingxuan  and\n      Thakkar, Megh  and\n      Li, Xin  and\n      Joty, Shafiq  and\n      Si, Luo  and\n      Bing, Lidong\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.264\",\n    doi = \"10.18653/v1/2023.acl-long.264\",\n    pages = \"4799--4816\",\n    abstract = \"Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level lowresource NLP tasks.\",\n}\n",
    "authors": [
        "Linlin Liu",
        "Xingxuan Li",
        "Megh Thakkar",
        "Xin Li",
        "Shafiq Joty",
        "Luo Si",
        "Lidong Bing"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.264.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c23d9f1a-8bfa-5b1a-baca-fd3239d1d7d5.pdf",
    "abstract": "Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level lowresource NLP tasks.",
    "num_pages": 18
}