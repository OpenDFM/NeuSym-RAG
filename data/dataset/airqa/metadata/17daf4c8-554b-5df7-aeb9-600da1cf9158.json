{
    "uuid": "17daf4c8-554b-5df7-aeb9-600da1cf9158",
    "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{hu-etal-2023-fine,\n    title = \"A fine-grained comparison of pragmatic language understanding in humans and language models\",\n    author = \"Hu, Jennifer  and\n      Floyd, Sammy  and\n      Jouravlev, Olessia  and\n      Fedorenko, Evelina  and\n      Gibson, Edward\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.230\",\n    doi = \"10.18653/v1/2023.acl-long.230\",\n    pages = \"4194--4213\",\n    abstract = \"Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.\",\n}\n",
    "authors": [
        "Jennifer Hu",
        "Sammy Floyd",
        "Olessia Jouravlev",
        "Evelina Fedorenko",
        "Edward Gibson"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.230.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/17daf4c8-554b-5df7-aeb9-600da1cf9158.pdf",
    "abstract": "Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.",
    "num_pages": 20
}