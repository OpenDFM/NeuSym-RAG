{
    "uuid": "dfc8147a-1efa-55e4-b237-00137554b557",
    "title": "Resolving Ambiguities in Text-to-Image Generative Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{mehrabi-etal-2023-resolving,\n    title = \"Resolving Ambiguities in Text-to-Image Generative Models\",\n    author = \"Mehrabi, Ninareh  and\n      Goyal, Palash  and\n      Verma, Apurv  and\n      Dhamala, Jwala  and\n      Kumar, Varun  and\n      Hu, Qian  and\n      Chang, Kai-Wei  and\n      Zemel, Richard  and\n      Galstyan, Aram  and\n      Gupta, Rahul\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.804\",\n    doi = \"10.18653/v1/2023.acl-long.804\",\n    pages = \"14367--14388\",\n    abstract = \"Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.\",\n}\n",
    "authors": [
        "Ninareh Mehrabi",
        "Palash Goyal",
        "Apurv Verma",
        "Jwala Dhamala",
        "Varun Kumar",
        "Qian Hu",
        "Kai-Wei Chang",
        "Richard Zemel",
        "Aram Galstyan",
        "Rahul Gupta"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.804.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/dfc8147a-1efa-55e4-b237-00137554b557.pdf",
    "abstract": "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.",
    "num_pages": 22
}