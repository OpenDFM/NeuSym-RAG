{
    "uuid": "ab3799c1-e815-5e18-bec8-be7e428c3b0e",
    "title": "Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{huang-etal-2023-inducing,\n    title = \"Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training\",\n    author = \"Huang, Jing  and\n      Wu, Zhengxuan  and\n      Mahowald, Kyle  and\n      Potts, Christopher\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.770\",\n    doi = \"10.18653/v1/2023.findings-acl.770\",\n    pages = \"12163--12180\",\n    abstract = \"Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of Geiger et al. (2021). We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context. While character-level models still perform best on purely form-based tasks like string reversal, our method outperforms character-level models on more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games. Compared with standard subword-based models, our approach also significantly improves robustness on unseen token sequences and leads to human-interpretable internal representations of characters.\",\n}\n",
    "authors": [
        "Jing Huang",
        "Zhengxuan Wu",
        "Kyle Mahowald",
        "Christopher Potts"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.770.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ab3799c1-e815-5e18-bec8-be7e428c3b0e.pdf",
    "abstract": "Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of Geiger et al. (2021). We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context. While character-level models still perform best on purely form-based tasks like string reversal, our method outperforms character-level models on more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games. Compared with standard subword-based models, our approach also significantly improves robustness on unseen token sequences and leads to human-interpretable internal representations of characters.",
    "num_pages": 18
}