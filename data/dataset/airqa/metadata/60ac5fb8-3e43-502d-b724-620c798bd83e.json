{
    "uuid": "60ac5fb8-3e43-502d-b724-620c798bd83e",
    "title": "Meta-Task Prompting Elicits Embeddings from Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lei-etal-2024-meta,\n    title = \"Meta-Task Prompting Elicits Embeddings from Large Language Models\",\n    author = \"Lei, Yibin  and\n      Wu, Di  and\n      Zhou, Tianyi  and\n      Shen, Tao  and\n      Cao, Yu  and\n      Tao, Chongyang  and\n      Yates, Andrew\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.546\",\n    doi = \"10.18653/v1/2024.acl-long.546\",\n    pages = \"10141--10157\",\n    abstract = \"We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios.\",\n}\n",
    "authors": [
        "Yibin Lei",
        "Di Wu",
        "Tianyi Zhou",
        "Tao Shen",
        "Yu Cao",
        "Chongyang Tao",
        "Andrew Yates"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.546.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/60ac5fb8-3e43-502d-b724-620c798bd83e.pdf",
    "abstract": "We introduce a new unsupervised text embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks are versatile embeddings that yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law, offering a versatile and resource-efficient approach for embedding generation across diverse scenarios.",
    "num_pages": 17
}