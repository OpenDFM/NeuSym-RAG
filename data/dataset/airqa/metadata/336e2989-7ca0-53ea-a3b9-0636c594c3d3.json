{
    "uuid": "336e2989-7ca0-53ea-a3b9-0636c594c3d3",
    "title": "Critic-Guided Decoding for Controlled Text Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{kim-etal-2023-critic,\n    title = \"Critic-Guided Decoding for Controlled Text Generation\",\n    author = \"Kim, Minbeom  and\n      Lee, Hwanhee  and\n      Yoo, Kang Min  and\n      Park, Joonsuk  and\n      Lee, Hwaran  and\n      Jung, Kyomin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.281\",\n    doi = \"10.18653/v1/2023.findings-acl.281\",\n    pages = \"4598--4612\",\n    abstract = \"Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.\",\n}\n",
    "authors": [
        "Minbeom Kim",
        "Hwanhee Lee",
        "Kang Min Yoo",
        "Joonsuk Park",
        "Hwaran Lee",
        "Kyomin Jung"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.281.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/336e2989-7ca0-53ea-a3b9-0636c594c3d3.pdf",
    "abstract": "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.",
    "num_pages": 15
}