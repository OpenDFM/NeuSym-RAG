{
    "uuid": "807aad7a-8edb-56b4-9f6f-e389211c0bb0",
    "title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{liu-etal-2024-towards-understanding,\n    title = \"Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness\",\n    author = \"Liu, Guangliang  and\n      Afshari, Milad  and\n      Zhang, Xitong  and\n      Xue, Zhiyu  and\n      Ghosh, Avrajit  and\n      Bashyal, Bidhan  and\n      Wang, Rongrong  and\n      Johnson, Kristen\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.109\",\n    doi = \"10.18653/v1/2024.findings-acl.109\",\n    pages = \"1843--1856\",\n    abstract = \"While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs{'} bias levels from stages of pretraining and debiasing.\",\n}\n",
    "authors": [
        "Guangliang Liu",
        "Milad Afshari",
        "Xitong Zhang",
        "Zhiyu Xue",
        "Avrajit Ghosh",
        "Bidhan Bashyal",
        "Rongrong Wang",
        "Kristen Johnson"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.109.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/807aad7a-8edb-56b4-9f6f-e389211c0bb0.pdf",
    "abstract": "While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMsâ€™ bias levels from stages of pretraining and debiasing.",
    "num_pages": 14
}