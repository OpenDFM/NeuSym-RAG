{
    "uuid": "5d57705d-9abe-5472-8d07-2199542d01ef",
    "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{amayuelas-etal-2024-knowledge,\n    title = \"Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models\",\n    author = \"Amayuelas, Alfonso  and\n      Wong, Kyle  and\n      Pan, Liangming  and\n      Chen, Wenhu  and\n      Wang, William Yang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.383\",\n    doi = \"10.18653/v1/2024.findings-acl.383\",\n    pages = \"6416--6432\",\n    abstract = \"This paper investigates the capabilities of Large Language Models (LLMs) in understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models{'} improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.\",\n}\n",
    "authors": [
        "Alfonso Amayuelas",
        "Kyle Wong",
        "Liangming Pan",
        "Wenhu Chen",
        "William Yang Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.383.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5d57705d-9abe-5472-8d07-2199542d01ef.pdf",
    "abstract": "This paper investigates the capabilities of Large Language Models (LLMs) in understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the modelsâ€™ improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.",
    "num_pages": 17
}