{
    "uuid": "2c572021-43ea-51dc-9cd4-514b16524292",
    "title": "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2024-metasumperceiver,\n    title = \"{M}eta{S}um{P}erceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking\",\n    author = \"Chen, Ting-Chih  and\n      Tang, Chia-Wei  and\n      Thomas, Chris\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.474\",\n    doi = \"10.18653/v1/2024.acl-long.474\",\n    pages = \"8742--8757\",\n    abstract = \"Fact-checking real-world claims often requires reviewing multiple multimodal documents in order to assess the claim{'}s truthfulness, a highly laborious and time-consuming task. In this paper, we present a summarization model crafted to generate claim-specific summaries useful for fact-checking from multimodal multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. We introduce a dynamic perceiver-based model that is able to handle inputs from multiple modalities of arbitrary lengths. To train our model, we leverage a novel reinforcement learning-based entailment objective in order to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of our approach, we conduct experiments on both an existing benchmark as well as a new dataset of multi-document claims which we contribute. Our approach outperforms the SOTA approach by 4.6{\\%} in the claim verification task on the MOCHEG dataset and demonstrates strong performance on our new Multi-News-Fact-Checking dataset.\",\n}\n",
    "authors": [
        "Ting-Chih Chen",
        "Chia-Wei Tang",
        "Chris Thomas"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.474.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2c572021-43ea-51dc-9cd4-514b16524292.pdf",
    "abstract": "Fact-checking real-world claims often requires reviewing multiple multimodal documents in order to assess the claimâ€™s truthfulness, a highly laborious and time-consuming task. In this paper, we present a summarization model crafted to generate claim-specific summaries useful for fact-checking from multimodal multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. We introduce a dynamic perceiver-based model that is able to handle inputs from multiple modalities of arbitrary lengths. To train our model, we leverage a novel reinforcement learning-based entailment objective in order to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of our approach, we conduct experiments on both an existing benchmark as well as a new dataset of multi-document claims which we contribute. Our approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on our new Multi-News-Fact-Checking dataset.",
    "num_pages": 16
}