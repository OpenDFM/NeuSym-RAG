{
    "uuid": "2daec93b-9092-592c-9cf5-e4185a5fb682",
    "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhu-etal-2024-beyond,\n    title = \"Beyond Memorization: The Challenge of Random Memory Access in Language Models\",\n    author = \"Zhu, Tongyao  and\n      Liu, Qian  and\n      Pang, Liang  and\n      Jiang, Zhengbao  and\n      Kan, Min-Yen  and\n      Lin, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.185\",\n    doi = \"10.18653/v1/2024.acl-long.185\",\n    pages = \"3373--3388\",\n    abstract = \"Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.\",\n}\n",
    "authors": [
        "Tongyao Zhu",
        "Qian Liu",
        "Liang Pang",
        "Zhengbao Jiang",
        "Min-Yen Kan",
        "Min Lin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.185.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2daec93b-9092-592c-9cf5-e4185a5fb682.pdf",
    "abstract": "Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.",
    "num_pages": 16
}