{
    "uuid": "8803480d-3eb0-5966-ae40-fef92aeee927",
    "title": "Word Embeddings Are Steers for Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{han-etal-2024-word,\n    title = \"Word Embeddings Are Steers for Language Models\",\n    author = \"Han, Chi  and\n      Xu, Jialiang  and\n      Li, Manling  and\n      Fung, Yi  and\n      Sun, Chenkai  and\n      Jiang, Nan  and\n      Abdelzaher, Tarek  and\n      Ji, Heng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.864\",\n    doi = \"10.18653/v1/2024.acl-long.864\",\n    pages = \"16410--16430\",\n    abstract = \"Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2{\\%} of the original LMs{'} size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.\",\n}\n",
    "authors": [
        "Chi Han",
        "Jialiang Xu",
        "Manling Li",
        "Yi Fung",
        "Chenkai Sun",
        "Nan Jiang",
        "Tarek Abdelzaher",
        "Heng Ji"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.864.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8803480d-3eb0-5966-ae40-fef92aeee927.pdf",
    "abstract": "Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2% of the original LMsâ€™ size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations and can highlight text spans that most indicate the style differences. An LM-Steer is transferrable between different language models by an explicit form calculation. One can also continuously steer LMs simply by scaling the LM-Steer or compose multiple LM-Steers by adding their transformations. Our codes are publicly available at https://github.com/Glaciohound/LM-Steer.",
    "num_pages": 21
}