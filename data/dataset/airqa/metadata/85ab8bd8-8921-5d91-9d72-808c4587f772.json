{
    "uuid": "85ab8bd8-8921-5d91-9d72-808c4587f772",
    "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-textbind,\n    title = \"{T}ext{B}ind: Multi-turn Interleaved Multimodal Instruction-following in the Wild\",\n    author = \"Li, Huayang  and\n      Li, Siheng  and\n      Cai, Deng  and\n      Wang, Longyue  and\n      Liu, Lemao  and\n      Watanabe, Taro  and\n      Yang, Yujiu  and\n      Shi, Shuming\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.537\",\n    doi = \"10.18653/v1/2024.findings-acl.537\",\n    pages = \"9053--9076\",\n    abstract = \"Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering LLMs with multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. Extensive quantitative and qualitative experiments demonstrate that MIM trained on TextBind achieves remarkable generation capability in multimodal conversations compared to recent baselines.\",\n}\n",
    "authors": [
        "Huayang Li",
        "Siheng Li",
        "Deng Cai",
        "Longyue Wang",
        "Lemao Liu",
        "Taro Watanabe",
        "Yujiu Yang",
        "Shuming Shi"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.537.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/85ab8bd8-8921-5d91-9d72-808c4587f772.pdf",
    "abstract": "Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering LLMs with multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. To accommodate interleaved image-text inputs and outputs, we devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models. Extensive quantitative and qualitative experiments demonstrate that MIM trained on TextBind achieves remarkable generation capability in multimodal conversations compared to recent baselines.",
    "num_pages": 24
}