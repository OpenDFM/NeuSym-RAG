{
    "uuid": "ca116924-bf11-5529-a43f-bf68e9745c5c",
    "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{lee-etal-2023-square,\n    title = \"{SQ}u{AR}e: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration\",\n    author = \"Lee, Hwaran  and\n      Hong, Seokhee  and\n      Park, Joonsuk  and\n      Kim, Takyoung  and\n      Cha, Meeyoung  and\n      Choi, Yejin  and\n      Kim, Byoungpil  and\n      Kim, Gunhee  and\n      Lee, Eun-Ju  and\n      Lim, Yong  and\n      Oh, Alice  and\n      Park, Sangchul  and\n      Ha, Jung-Woo\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.370\",\n    doi = \"10.18653/v1/2023.acl-long.370\",\n    pages = \"6692--6712\",\n    abstract = \"The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.\",\n}\n",
    "authors": [
        "Hwaran Lee",
        "Seokhee Hong",
        "Joonsuk Park",
        "Takyoung Kim",
        "Meeyoung Cha",
        "Yejin Choi",
        "Byoungpil Kim",
        "Gunhee Kim",
        "Eun-Ju Lee",
        "Yong Lim",
        "Alice Oh",
        "Sangchul Park",
        "Jung-Woo Ha"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.370.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ca116924-bf11-5529-a43f-bf68e9745c5c.pdf",
    "abstract": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.",
    "num_pages": 21
}