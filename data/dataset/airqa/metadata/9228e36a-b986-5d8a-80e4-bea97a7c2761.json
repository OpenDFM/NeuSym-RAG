{
    "uuid": "9228e36a-b986-5d8a-80e4-bea97a7c2761",
    "title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{li-etal-2024-mugglemath,\n    title = \"{M}uggle{M}ath: Assessing the Impact of Query and Response Augmentation on Math Reasoning\",\n    author = \"Li, Chengpeng  and\n      Yuan, Zheng  and\n      Yuan, Hongyi  and\n      Dong, Guanting  and\n      Lu, Keming  and\n      Wu, Jiancan  and\n      Tan, Chuanqi  and\n      Wang, Xiang  and\n      Zhou, Chang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.551\",\n    doi = \"10.18653/v1/2024.acl-long.551\",\n    pages = \"10230--10258\",\n    abstract = \"In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks?To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH.We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH.A log-linear relationship and a segmented log-linear are presented between MuggleMath{'}s performance and the amount of augmented data on GSM8K and MATH, respectively.We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization.\",\n}\n",
    "authors": [
        "Chengpeng Li",
        "Zheng Yuan",
        "Hongyi Yuan",
        "Guanting Dong",
        "Keming Lu",
        "Jiancan Wu",
        "Chuanqi Tan",
        "Xiang Wang",
        "Chang Zhou"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.551.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9228e36a-b986-5d8a-80e4-bea97a7c2761.pdf",
    "abstract": "In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks?To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH.We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH.A log-linear relationship and a segmented log-linear are presented between MuggleMathâ€™s performance and the amount of augmented data on GSM8K and MATH, respectively.We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization.",
    "num_pages": 29
}