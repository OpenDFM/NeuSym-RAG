{
    "uuid": "53b1c25f-d95d-5cbb-b196-96604835018a",
    "title": "azaad@BND at SemEval-2023 Task 2: How to Go from a Simple Transformer Model to a Better Model to Get Better Results in Natural Language Processing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{ahmadi-etal-2023-azaad,\n    title = \"azaad@{BND} at {S}em{E}val-2023 Task 2: How to Go from a Simple Transformer Model to a Better Model to Get Better Results in Natural Language Processing\",\n    author = \"Ahmadi, Reza  and\n      Arefi, Shiva  and\n      Jafarabad, Mohammad\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.303\",\n    doi = \"10.18653/v1/2023.semeval-1.303\",\n    pages = \"2184--2187\",\n    abstract = \"In this article, which was prepared for the sameval2023 competition (task number 2), information about the implementation techniques of the transformer model and the use of the pre-trained BERT model in order to identify the named entity (NER) in the English language, has been collected and also the implementation method is explained. Finally, it led to an F1 score of about 57{\\%} for Fine-grained and 72{\\%} for Coarse-grained in the dev data. In the final test data, F1 score reached 50{\\%}.\",\n}\n",
    "authors": [
        "Reza Ahmadi",
        "Shiva Arefi",
        "Mohammad Jafarabad"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.303.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/53b1c25f-d95d-5cbb-b196-96604835018a.pdf",
    "abstract": "In this article, which was prepared for the sameval2023 competition (task number 2), information about the implementation techniques of the transformer model and the use of the pre-trained BERT model in order to identify the named entity (NER) in the English language, has been collected and also the implementation method is explained. Finally, it led to an F1 score of about 57% for Fine-grained and 72% for Coarse-grained in the dev data. In the final test data, F1 score reached 50%.",
    "num_pages": 4
}