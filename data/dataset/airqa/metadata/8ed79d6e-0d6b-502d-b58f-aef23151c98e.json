{
    "uuid": "8ed79d6e-0d6b-502d-b58f-aef23151c98e",
    "title": "ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yu-etal-2023-conki,\n    title = \"{C}on{KI}: Contrastive Knowledge Injection for Multimodal Sentiment Analysis\",\n    author = \"Yu, Yakun  and\n      Zhao, Mingjun  and\n      Qi, Shi-ang  and\n      Sun, Feiran  and\n      Wang, Baoxun  and\n      Guo, Weidong  and\n      Wang, Xiaoli  and\n      Yang, Lei  and\n      Niu, Di\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.860\",\n    doi = \"10.18653/v1/2023.findings-acl.860\",\n    pages = \"13610--13624\",\n    abstract = \"Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms all prior methods on a variety of performance metrics.\",\n}\n",
    "authors": [
        "Yakun Yu",
        "Mingjun Zhao",
        "Shi-ang Qi",
        "Feiran Sun",
        "Baoxun Wang",
        "Weidong Guo",
        "Xiaoli Wang",
        "Lei Yang",
        "Di Niu"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.860.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8ed79d6e-0d6b-502d-b58f-aef23151c98e.pdf",
    "abstract": "Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms all prior methods on a variety of performance metrics.",
    "num_pages": 15
}