{
    "uuid": "cfdaa73e-c8df-5cbd-acc1-e225cdc85e91",
    "title": "On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{gao-etal-2023-universal,\n    title = \"On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection\",\n    author = \"Gao, SongYang  and\n      Dou, Shihan  and\n      Zhang, Qi  and\n      Huang, Xuanjing  and\n      Ma, Jin  and\n      Shan, Ying\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.857\",\n    doi = \"10.18653/v1/2023.findings-acl.857\",\n    pages = \"13573--13581\",\n    abstract = \"Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.\",\n}\n",
    "authors": [
        "SongYang Gao",
        "Shihan Dou",
        "Qi Zhang",
        "Xuanjing Huang",
        "Jin Ma",
        "Ying Shan"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.857.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/cfdaa73e-c8df-5cbd-acc1-e225cdc85e91.pdf",
    "abstract": "Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference.",
    "num_pages": 9
}