{
    "uuid": "72fc0772-1bcc-5fe1-acef-478859920a8b",
    "title": "Can LMs Store and Retrieve 1-to-N Relational Knowledge?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{nagasawa-etal-2023-lms,\n    title = \"Can {LM}s Store and Retrieve 1-to-N Relational Knowledge?\",\n    author = \"Nagasawa, Haruki  and\n      Heinzerling, Benjamin  and\n      Kokuta, Kazuma  and\n      Inui, Kentaro\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.22\",\n    doi = \"10.18653/v1/2023.acl-srw.22\",\n    pages = \"130--138\",\n    abstract = \"It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational knowledge, such as {''}country and its capital,{''} with high memorization accuracy. On the other hand, world knowledge includes not only 1-to-1 but also 1-to-N relational knowledge, such as {''}parent and children.{''}However, it is not clear how accurately language models can handle 1-to-N relational knowledge. To investigate language models{'} abilities toward 1-to-N relational knowledge, we start by designing the problem settings. Specifically, we organize the character of 1-to-N relational knowledge and define two essential skills: (i) memorizing multiple objects individually and (ii) retrieving multiple stored objects without excesses or deficiencies at once. We inspect LMs{'} ability to handle 1-to-N relational knowledge on the controlled synthesized data. As a result, we report that it is possible to memorize multiple objects with high accuracy, but generalizing the retrieval ability (expressly, enumeration) is challenging.\",\n}\n",
    "authors": [
        "Haruki Nagasawa",
        "Benjamin Heinzerling",
        "Kazuma Kokuta",
        "Kentaro Inui"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.22.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/72fc0772-1bcc-5fe1-acef-478859920a8b.pdf",
    "abstract": "It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational knowledge, such as ”country and its capital,” with high memorization accuracy. On the other hand, world knowledge includes not only 1-to-1 but also 1-to-N relational knowledge, such as ”parent and children.”However, it is not clear how accurately language models can handle 1-to-N relational knowledge. To investigate language models’ abilities toward 1-to-N relational knowledge, we start by designing the problem settings. Specifically, we organize the character of 1-to-N relational knowledge and define two essential skills: (i) memorizing multiple objects individually and (ii) retrieving multiple stored objects without excesses or deficiencies at once. We inspect LMs’ ability to handle 1-to-N relational knowledge on the controlled synthesized data. As a result, we report that it is possible to memorize multiple objects with high accuracy, but generalizing the retrieval ability (expressly, enumeration) is challenging.",
    "num_pages": 9
}