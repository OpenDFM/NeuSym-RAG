{
    "uuid": "c131abf9-624d-5840-bb6c-974d628e6fa6",
    "title": "Few-shot Question Generation for Reading Comprehension",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)",
    "bibtex": "@inproceedings{poon-etal-2024-shot,\n    title = \"Few-shot Question Generation for Reading Comprehension\",\n    author = \"Poon, Yin  and\n      Lee, John Sie Yuen  and\n      Lam, Yu Yan  and\n      Suen, Wing Lam  and\n      Ong, Elsie Li Chen  and\n      Chu, Samuel Kai Wah\",\n    editor = \"Wong, Kam-Fai  and\n      Zhang, Min  and\n      Xu, Ruifeng  and\n      Li, Jing  and\n      Wei, Zhongyu  and\n      Gui, Lin  and\n      Liang, Bin  and\n      Zhao, Runcong\",\n    booktitle = \"Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.sighan-1.3\",\n    pages = \"21--27\",\n    abstract = \"According to the internationally recognized PIRLS (Progress in International Reading Literacy Study) assessment standards, reading comprehension questions should require not only information retrieval, but also higher-order processes such as inferencing, interpreting and evaluation. However, these kinds of questions are often not available in large quantities for training question generation models. This paper investigates whether pre-trained Large Language Models (LLMs) can produce higher-order questions. Human assessment on a Chinese dataset shows that few-shot LLM prompting generates more usable and higher-order questions than two competitive neural baselines.\",\n}\n",
    "authors": [
        "Yin Poon",
        "John Sie Yuen Lee",
        "Yu Yan Lam",
        "Wing Lam Suen",
        "Elsie Li Chen Ong",
        "Samuel Kai Wah Chu"
    ],
    "pdf_url": "https://aclanthology.org/2024.sighan-1.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c131abf9-624d-5840-bb6c-974d628e6fa6.pdf",
    "abstract": "According to the internationally recognized PIRLS (Progress in International Reading Literacy Study) assessment standards, reading comprehension questions should require not only information retrieval, but also higher-order processes such as inferencing, interpreting and evaluation. However, these kinds of questions are often not available in large quantities for training question generation models. This paper investigates whether pre-trained Large Language Models (LLMs) can produce higher-order questions. Human assessment on a Chinese dataset shows that few-shot LLM prompting generates more usable and higher-order questions than two competitive neural baselines.",
    "num_pages": 7
}