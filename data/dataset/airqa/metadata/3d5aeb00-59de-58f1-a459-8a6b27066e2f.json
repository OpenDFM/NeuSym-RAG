{
    "uuid": "3d5aeb00-59de-58f1-a459-8a6b27066e2f",
    "title": "Learned Transformer Position Embeddings Have a Low-Dimensional Structure",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
    "bibtex": "@inproceedings{wennberg-henter-2024-learned,\n    title = \"Learned Transformer Position Embeddings Have a Low-Dimensional Structure\",\n    author = \"Wennberg, Ulme  and\n      Henter, Gustav\",\n    editor = \"Zhao, Chen  and\n      Mosbach, Marius  and\n      Atanasova, Pepa  and\n      Goldfarb-Tarrent, Seraphina  and\n      Hase, Peter  and\n      Hosseini, Arian  and\n      Elbayad, Maha  and\n      Pezzelle, Sandro  and\n      Mozes, Maximilian\",\n    booktitle = \"Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.repl4nlp-1.17\",\n    pages = \"237--244\",\n    abstract = \"Position embeddings have long been essential for sequence-order encoding in transformer models, yet their structure is underexplored. This study uses principal component analysis (PCA) to quantitatively compare the dimensionality of absolute position and word embeddings in BERT and ALBERT. We find that, unlike word embeddings, position embeddings occupy a low-dimensional subspace, typically utilizing under 10{\\%} of the dimensions available. Additionally, the principal vectors are dominated by a few low-frequency rotational components, a structure arising independently across models.\",\n}\n",
    "authors": [
        "Ulme Wennberg",
        "Gustav Henter"
    ],
    "pdf_url": "https://aclanthology.org/2024.repl4nlp-1.17.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/3d5aeb00-59de-58f1-a459-8a6b27066e2f.pdf",
    "abstract": "Position embeddings have long been essential for sequence-order encoding in transformer models, yet their structure is underexplored. This study uses principal component analysis (PCA) to quantitatively compare the dimensionality of absolute position and word embeddings in BERT and ALBERT. We find that, unlike word embeddings, position embeddings occupy a low-dimensional subspace, typically utilizing under 10% of the dimensions available. Additionally, the principal vectors are dominated by a few low-frequency rotational components, a structure arising independently across models.",
    "num_pages": 8
}