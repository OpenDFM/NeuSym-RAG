{
    "uuid": "d77f0afc-cea4-502f-bd54-7ee80773f7e5",
    "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{he-etal-2023-fourier,\n    title = \"{F}ourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with {FFT} Operator\",\n    author = \"He, Ziwei  and\n      Yang, Meng  and\n      Feng, Minwei  and\n      Yin, Jingcheng  and\n      Wang, Xinbing  and\n      Leng, Jingwen  and\n      Lin, Zhouhan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.570\",\n    doi = \"10.18653/v1/2023.findings-acl.570\",\n    pages = \"8954--8966\",\n    abstract = \"The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer{'}s inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code will be publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}\",\n}\n",
    "authors": [
        "Ziwei He",
        "Meng Yang",
        "Minwei Feng",
        "Jingcheng Yin",
        "Xinbing Wang",
        "Jingwen Leng",
        "Zhouhan Lin"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.570.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d77f0afc-cea4-502f-bd54-7ee80773f7e5.pdf",
    "abstract": "The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformerâ€™s inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code will be publicly available at https://github.com/LUMIA-Group/FourierTransformer",
    "num_pages": 13
}