{
    "uuid": "85e0f00f-2f74-575f-acd8-de4b8a6bb856",
    "title": "MoQA: Benchmarking Multi-Type Open-Domain Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
    "bibtex": "@inproceedings{yen-etal-2023-moqa,\n    title = \"{M}o{QA}: Benchmarking Multi-Type Open-Domain Question Answering\",\n    author = \"Yen, Howard  and\n      Gao, Tianyu  and\n      Lee, Jinhyuk  and\n      Chen, Danqi\",\n    editor = \"Muresan, Smaranda  and\n      Chen, Vivian  and\n      Casey, Kennington  and\n      David, Vandyke  and\n      Nina, Dethlefs  and\n      Koji, Inoue  and\n      Erik, Ekstedt  and\n      Stefan, Ultes\",\n    booktitle = \"Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.dialdoc-1.2\",\n    doi = \"10.18653/v1/2023.dialdoc-1.2\",\n    pages = \"8--29\",\n    abstract = \"Previous research on open-domain question answering (QA) mainly focuses on questions with short answers. However, information-seeking QA often requires various formats of answers depending on the nature of the questions, e.g., why/how questions typically require a long answer. In this paper, we present MoQA, a benchmark for open-domain QA that requires building one system that can provide short, medium, long, and yes/no answers to different questions accordingly. MoQA builds upon Natural Questions with multiple types of questions and additional crowdsourcing efforts to ensure high query quality. We adapt state-of-the-art models, and reveal unique findings in multi-type open-domain QA: (1) For retriever-reader models, training one retriever on all types achieves the overall best performance, but it is challenging to train one reader model to output answers of different formats, or to train a question classifier to distinguish between types; (2) An end-to-end closed-book QA model trained on multiple types struggles with the task across the board; (3) State-of-the-art large language models such as the largest GPT-3 models (Brown et al., 2020; Ouyang et al., 2022) also lag behind open-book QA models. Our benchmark and analysis call for more effort into building versatile open-domain QA models in the future.\",\n}\n",
    "authors": [
        "Howard Yen",
        "Tianyu Gao",
        "Jinhyuk Lee",
        "Danqi Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.dialdoc-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/85e0f00f-2f74-575f-acd8-de4b8a6bb856.pdf",
    "abstract": "Previous research on open-domain question answering (QA) mainly focuses on questions with short answers. However, information-seeking QA often requires various formats of answers depending on the nature of the questions, e.g., why/how questions typically require a long answer. In this paper, we present MoQA, a benchmark for open-domain QA that requires building one system that can provide short, medium, long, and yes/no answers to different questions accordingly. MoQA builds upon Natural Questions with multiple types of questions and additional crowdsourcing efforts to ensure high query quality. We adapt state-of-the-art models, and reveal unique findings in multi-type open-domain QA: (1) For retriever-reader models, training one retriever on all types achieves the overall best performance, but it is challenging to train one reader model to output answers of different formats, or to train a question classifier to distinguish between types; (2) An end-to-end closed-book QA model trained on multiple types struggles with the task across the board; (3) State-of-the-art large language models such as the largest GPT-3 models (Brown et al., 2020; Ouyang et al., 2022) also lag behind open-book QA models. Our benchmark and analysis call for more effort into building versatile open-domain QA models in the future.",
    "num_pages": 22
}