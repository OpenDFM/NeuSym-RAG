{
    "uuid": "89c1512d-d2c7-5002-818b-8595f9982ff1",
    "title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{shi-etal-2023-rade,\n    title = \"{RADE}: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue\",\n    author = \"Shi, Zhengliang  and\n      Sun, Weiwei  and\n      Zhang, Shuo  and\n      Zhang, Zhen  and\n      Ren, Pengjie  and\n      Ren, Zhaochun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.719\",\n    doi = \"10.18653/v1/2023.acl-long.719\",\n    pages = \"12856--12875\",\n    abstract = \"Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pearson, Spearman, and Kendall correlations with human evaluation outperform state-of-the-art baselines.\",\n}\n",
    "authors": [
        "Zhengliang Shi",
        "Weiwei Sun",
        "Shuo Zhang",
        "Zhen Zhang",
        "Pengjie Ren",
        "Zhaochun Ren"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.719.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/89c1512d-d2c7-5002-818b-8595f9982ff1.pdf",
    "abstract": "Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pearson, Spearman, and Kendall correlations with human evaluation outperform state-of-the-art baselines.",
    "num_pages": 20
}