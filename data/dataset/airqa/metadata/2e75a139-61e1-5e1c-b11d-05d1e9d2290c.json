{
    "uuid": "2e75a139-61e1-5e1c-b11d-05d1e9d2290c",
    "title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
    "bibtex": "@inproceedings{youn-tagkopoulos-2023-kglm,\n    title = \"{KGLM}: Integrating Knowledge Graph Structure in Language Models for Link Prediction\",\n    author = \"Youn, Jason  and\n      Tagkopoulos, Ilias\",\n    editor = \"Palmer, Alexis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.starsem-1.20\",\n    doi = \"10.18653/v1/2023.starsem-1.20\",\n    pages = \"217--224\",\n    abstract = \"The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.\",\n}\n",
    "authors": [
        "Jason Youn",
        "Ilias Tagkopoulos"
    ],
    "pdf_url": "https://aclanthology.org/2023.starsem-1.20.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2e75a139-61e1-5e1c-b11d-05d1e9d2290c.pdf",
    "abstract": "The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.",
    "num_pages": 8
}