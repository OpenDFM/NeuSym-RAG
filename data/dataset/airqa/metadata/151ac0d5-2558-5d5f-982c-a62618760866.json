{
    "uuid": "151ac0d5-2558-5d5f-982c-a62618760866",
    "title": "MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-mike,\n    title = \"{MIKE}: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing\",\n    author = \"Li, Jiaqi  and\n      Du, Miaozeng  and\n      Zhang, Chuanyi  and\n      Chen, Yongrui  and\n      Hu, Nan  and\n      Qi, Guilin  and\n      Jiang, Haiyun  and\n      Cheng, Siyuan  and\n      Tian, Bozhong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.298\",\n    doi = \"10.18653/v1/2024.findings-acl.298\",\n    pages = \"5018--5029\",\n    abstract = \"Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations, we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring the complexity of FG knowledge editing in MLLMs. Our findings spotlight the urgent need for novel approaches in this domain, setting a clear agenda for future research and development efforts within the community.\",\n}\n",
    "authors": [
        "Jiaqi Li",
        "Miaozeng Du",
        "Chuanyi Zhang",
        "Yongrui Chen",
        "Nan Hu",
        "Guilin Qi",
        "Haiyun Jiang",
        "Siyuan Cheng",
        "Bozhong Tian"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.298.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/151ac0d5-2558-5d5f-982c-a62618760866.pdf",
    "abstract": "Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations, we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring the complexity of FG knowledge editing in MLLMs. Our findings spotlight the urgent need for novel approaches in this domain, setting a clear agenda for future research and development efforts within the community.",
    "num_pages": 12
}