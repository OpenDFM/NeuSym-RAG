{
    "uuid": "c85dc714-c4f4-5b58-9fc1-0390027d1550",
    "title": "Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)",
    "bibtex": "@inproceedings{varghese-etal-2023-exploiting,\n    title = \"Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation Classification\",\n    author = \"Varghese, Nobel  and\n      Yung, Frances  and\n      Anuranjana, Kaveri  and\n      Demberg, Vera\",\n    editor = \"Strube, Michael  and\n      Braud, Chloe  and\n      Hardmeier, Christian  and\n      Li, Junyi Jessy  and\n      Loaiciga, Sharid  and\n      Zeldes, Amir\",\n    booktitle = \"Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.codi-1.13\",\n    doi = \"10.18653/v1/2023.codi-1.13\",\n    pages = \"99--105\",\n    abstract = \"In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another on the contrary, there are several frameworks that describe the labels{'} similarities (by e.g. sorting them into a hierarchy or describing them interms of features (Sanders et al., 2021)). Recently, several methods for representing the similarities between labels have been proposed (Zhang et al., 2018; Wang et al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion Model (Guo et al., 2021) for learning a representation for discourse relation labels. We explore alternative ways of informing the model about the similarities between relations, by representing relations in terms of their names (and parent category), their typical markers, or in terms of CCR features that describe the relations. Experimental results show that exploiting label similarity improves classification results.\",\n}\n",
    "authors": [
        "Nobel Varghese",
        "Frances Yung",
        "Kaveri Anuranjana",
        "Vera Demberg"
    ],
    "pdf_url": "https://aclanthology.org/2023.codi-1.13.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c85dc714-c4f4-5b58-9fc1-0390027d1550.pdf",
    "abstract": "In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another on the contrary, there are several frameworks that describe the labelsâ€™ similarities (by e.g. sorting them into a hierarchy or describing them interms of features (Sanders et al., 2021)). Recently, several methods for representing the similarities between labels have been proposed (Zhang et al., 2018; Wang et al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion Model (Guo et al., 2021) for learning a representation for discourse relation labels. We explore alternative ways of informing the model about the similarities between relations, by representing relations in terms of their names (and parent category), their typical markers, or in terms of CCR features that describe the relations. Experimental results show that exploiting label similarity improves classification results.",
    "num_pages": 7
}