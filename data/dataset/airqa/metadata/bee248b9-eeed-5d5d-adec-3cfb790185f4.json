{
    "uuid": "bee248b9-eeed-5d5d-adec-3cfb790185f4",
    "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{fei-etal-2024-empathyear,\n    title = \"{E}mpathy{E}ar: An Open-source Avatar Multimodal Empathetic Chatbot\",\n    author = \"Fei, Hao  and\n      Zhang, Han  and\n      Wang, Bin  and\n      Liao, Lizi  and\n      Liu, Qian  and\n      Cambria, Erik\",\n    editor = \"Cao, Yixin  and\n      Feng, Yang  and\n      Xiong, Deyi\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-demos.7\",\n    doi = \"10.18653/v1/2024.acl-demos.7\",\n    pages = \"61--71\",\n    abstract = \"This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.\",\n}\n",
    "authors": [
        "Hao Fei",
        "Han Zhang",
        "Bin Wang",
        "Lizi Liao",
        "Qian Liu",
        "Erik Cambria"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-demos.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bee248b9-eeed-5d5d-adec-3cfb790185f4.pdf",
    "abstract": "This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.",
    "num_pages": 11
}