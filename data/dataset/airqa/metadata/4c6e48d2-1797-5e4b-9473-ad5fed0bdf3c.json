{
    "uuid": "4c6e48d2-1797-5e4b-9473-ad5fed0bdf3c",
    "title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fernandes-etal-2023-translation,\n    title = \"When Does Translation Require Context? A Data-driven, Multilingual Exploration\",\n    author = \"Fernandes, Patrick  and\n      Yin, Kayo  and\n      Liu, Emmy  and\n      Martins, Andr{\\'e}  and\n      Neubig, Graham\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.36\",\n    doi = \"10.18653/v1/2023.acl-long.36\",\n    pages = \"606--626\",\n    abstract = \"Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at \\url{https://github.com/neulab/contextual-mt}\",\n}\n",
    "authors": [
        "Patrick Fernandes",
        "Kayo Yin",
        "Emmy Liu",
        "Andr√© Martins",
        "Graham Neubig"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.36.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4c6e48d2-1797-5e4b-9473-ad5fed0bdf3c.pdf",
    "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt",
    "num_pages": 21
}