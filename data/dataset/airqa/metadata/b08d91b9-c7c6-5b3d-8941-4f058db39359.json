{
    "uuid": "b08d91b9-c7c6-5b3d-8941-4f058db39359",
    "title": "ROZAM at SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{rostamkhani-etal-2023-rozam,\n    title = \"{ROZAM} at {S}em{E}val 2023 Task 9: Multilingual Tweet Intimacy Analysis\",\n    author = \"Rostamkhani, Mohammadmostafa  and\n      Zamaninejad, Ghazal  and\n      Eetemadi, Sauleh\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.278\",\n    doi = \"10.18653/v1/2023.semeval-1.278\",\n    pages = \"2029--2032\",\n    abstract = \"We build a model using large multilingual pretrained language model XLM-T for regression task and fine-tune it on the MINT (Multilingual INTmacy) analysis dataset which covers 6 languages for training and 4 languages for testing zero-shot performance of the model. The dataset was annotated and the annotations are intimacy scores. We experiment with several deep learning architectures to predict intimacy score. To achieve optimal performance we modify several model settings including loss function, number and type of layers. In total, we ran 16 end-to-end experiments. Our best system achieved a Pearson Correlation score of 0.52.\",\n}\n",
    "authors": [
        "Mohammadmostafa Rostamkhani",
        "Ghazal Zamaninejad",
        "Sauleh Eetemadi"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.278.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b08d91b9-c7c6-5b3d-8941-4f058db39359.pdf",
    "abstract": "We build a model using large multilingual pretrained language model XLM-T for regression task and fine-tune it on the MINT (Multilingual INTmacy) analysis dataset which covers 6 languages for training and 4 languages for testing zero-shot performance of the model. The dataset was annotated and the annotations are intimacy scores. We experiment with several deep learning architectures to predict intimacy score. To achieve optimal performance we modify several model settings including loss function, number and type of layers. In total, we ran 16 end-to-end experiments. Our best system achieved a Pearson Correlation score of 0.52.",
    "num_pages": 4
}