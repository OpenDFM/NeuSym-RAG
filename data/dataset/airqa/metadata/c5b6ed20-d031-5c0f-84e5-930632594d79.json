{
    "uuid": "c5b6ed20-d031-5c0f-84e5-930632594d79",
    "title": "Ensemble Transfer Learning for Multilingual Coreference Resolution",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)",
    "bibtex": "@inproceedings{lai-ji-2023-ensemble,\n    title = \"Ensemble Transfer Learning for Multilingual Coreference Resolution\",\n    author = \"Lai, Tuan  and\n      Ji, Heng\",\n    editor = \"Strube, Michael  and\n      Braud, Chloe  and\n      Hardmeier, Christian  and\n      Li, Junyi Jessy  and\n      Loaiciga, Sharid  and\n      Zeldes, Amir\",\n    booktitle = \"Proceedings of the 4th Workshop on Computational Approaches to Discourse (CODI 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.codi-1.3\",\n    doi = \"10.18653/v1/2023.codi-1.3\",\n    pages = \"24--36\",\n    abstract = \"Entity coreference resolution is an important research problem with many applications, including information extraction and question answering. Coreference resolution for English has been studied extensively. However, there is relatively little work for other languages. A problem that frequently occurs when working with a non-English language is the scarcity of annotated training data. To overcome this challenge, we design a simple but effective ensemble-based framework that combines various transfer learning (TL) techniques. We first train several models using different TL methods. Then, during inference, we compute the unweighted average scores of the models{'} predictions to extract the final set of predicted clusters. Furthermore, we also propose a low-cost TL method that bootstraps coreference resolution models by utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential links naturally exist between anchor texts pointing to the same article, our method builds a sizeable distantly-supervised dataset for the target language that consists of tens of thousands of documents. We can pre-train a model on the pseudo-labeled dataset before finetuning it on the final target dataset. Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm the effectiveness of our methods. Our best ensembles consistently outperform the baseline approach of simple training by up to 7.68{\\%} in the F1 score. These ensembles also achieve new state-of-the-art results for three languages: Arabic, Dutch, and Spanish.\",\n}\n",
    "authors": [
        "Tuan Lai",
        "Heng Ji"
    ],
    "pdf_url": "https://aclanthology.org/2023.codi-1.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c5b6ed20-d031-5c0f-84e5-930632594d79.pdf",
    "abstract": "Entity coreference resolution is an important research problem with many applications, including information extraction and question answering. Coreference resolution for English has been studied extensively. However, there is relatively little work for other languages. A problem that frequently occurs when working with a non-English language is the scarcity of annotated training data. To overcome this challenge, we design a simple but effective ensemble-based framework that combines various transfer learning (TL) techniques. We first train several models using different TL methods. Then, during inference, we compute the unweighted average scores of the modelsâ€™ predictions to extract the final set of predicted clusters. Furthermore, we also propose a low-cost TL method that bootstraps coreference resolution models by utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential links naturally exist between anchor texts pointing to the same article, our method builds a sizeable distantly-supervised dataset for the target language that consists of tens of thousands of documents. We can pre-train a model on the pseudo-labeled dataset before finetuning it on the final target dataset. Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm the effectiveness of our methods. Our best ensembles consistently outperform the baseline approach of simple training by up to 7.68% in the F1 score. These ensembles also achieve new state-of-the-art results for three languages: Arabic, Dutch, and Spanish.",
    "num_pages": 13
}