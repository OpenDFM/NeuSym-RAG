{
    "uuid": "47084074-c0d8-5c0e-8c5b-23eef90b78ed",
    "title": "BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pre-training of BabyLM",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    "bibtex": "@inproceedings{shen-etal-2024-bambino,\n    title = \"{BAMBINO}-{LM}: (Bilingual-)Human-Inspired Continual Pre-training of {B}aby{LM}\",\n    author = \"Shen, Zhewen  and\n      Joshi, Aditya  and\n      Chen, Ruey-Cheng\",\n    editor = \"Kuribayashi, Tatsuki  and\n      Rambelli, Giulia  and\n      Takmaz, Ece  and\n      Wicke, Philipp  and\n      Oseki, Yohei\",\n    booktitle = \"Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.cmcl-1.1\",\n    doi = \"10.18653/v1/2024.cmcl-1.1\",\n    pages = \"1--7\",\n    abstract = \"Children from bilingual backgrounds benefit from interactions with parents and teachers to re-acquire their heritage language. In this paper, we investigate how this insight from behavioral study can be incorporated into the learning of small-scale language models. We introduce BAMBINO-LM, a continual pre-training strategy for BabyLM that uses a novel combination of alternation and PPO-based perplexity reward induced from a parent Italian model. Upon evaluation on zero-shot classification tasks for English and Italian, BAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our ablation analysis demonstrates that employing both the alternation strategy and PPO-based modeling is key to this effectiveness gain. We also show that, as a side effect, the proposed method leads to a similar degradation in L1 effectiveness as human children would have had in an equivalent learning scenario. Through its modeling and findings, BAMBINO-LM makes a focused contribution to the pre-training of small-scale language models by first developing a human-inspired strategy for pre-training and then showing that it results in behaviours similar to that of humans.\",\n}\n",
    "authors": [
        "Zhewen Shen",
        "Aditya Joshi",
        "Ruey-Cheng Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.cmcl-1.1.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/47084074-c0d8-5c0e-8c5b-23eef90b78ed.pdf",
    "abstract": "Children from bilingual backgrounds benefit from interactions with parents and teachers to re-acquire their heritage language. In this paper, we investigate how this insight from behavioral study can be incorporated into the learning of small-scale language models. We introduce BAMBINO-LM, a continual pre-training strategy for BabyLM that uses a novel combination of alternation and PPO-based perplexity reward induced from a parent Italian model. Upon evaluation on zero-shot classification tasks for English and Italian, BAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our ablation analysis demonstrates that employing both the alternation strategy and PPO-based modeling is key to this effectiveness gain. We also show that, as a side effect, the proposed method leads to a similar degradation in L1 effectiveness as human children would have had in an equivalent learning scenario. Through its modeling and findings, BAMBINO-LM makes a focused contribution to the pre-training of small-scale language models by first developing a human-inspired strategy for pre-training and then showing that it results in behaviours similar to that of humans.",
    "num_pages": 7
}