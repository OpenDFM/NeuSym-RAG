{
    "uuid": "fa6c7e72-91a2-5a07-86e1-7442a71a1ede",
    "title": "An Efficient Conversational Smart Compose System",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{zhu-etal-2023-efficient,\n    title = \"An Efficient Conversational Smart Compose System\",\n    author = \"Zhu, Yun  and\n      Chen, Xiayu  and\n      Shu, Lei  and\n      Tan, Bowen  and\n      Song, Xinying  and\n      Liu, Lijuan  and\n      Wang, Maria  and\n      Chen, Jindong  and\n      Ruan, Ning\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.43\",\n    doi = \"10.18653/v1/2023.acl-demo.43\",\n    pages = \"456--462\",\n    abstract = \"Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency. From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available at \\url{https://youtu.be/U1KXkaqr60g.We} open-sourced our phrase tokenizer in \\url{https://github.com/tensorflow/text}.\",\n}\n",
    "authors": [
        "Yun Zhu",
        "Xiayu Chen",
        "Lei Shu",
        "Bowen Tan",
        "Xinying Song",
        "Lijuan Liu",
        "Maria Wang",
        "Jindong Chen",
        "Ning Ruan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.43.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fa6c7e72-91a2-5a07-86e1-7442a71a1ede.pdf",
    "abstract": "Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency. From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available at https://youtu.be/U1KXkaqr60g.We open-sourced our phrase tokenizer in https://github.com/tensorflow/text.",
    "num_pages": 7
}