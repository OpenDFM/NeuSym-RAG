{
    "uuid": "b997a83e-f62e-5db0-8371-7abce990ace4",
    "title": "RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{liu-etal-2023-rankcse,\n    title = \"{R}ank{CSE}: Unsupervised Sentence Representations Learning via Learning to Rank\",\n    author = \"Liu, Jiduan  and\n      Liu, Jiahao  and\n      Wang, Qifan  and\n      Wang, Jingang  and\n      Wu, Wei  and\n      Xian, Yunsen  and\n      Zhao, Dongyan  and\n      Chen, Kai  and\n      Yan, Rui\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.771\",\n    doi = \"10.18653/v1/2023.acl-long.771\",\n    pages = \"13785--13802\",\n    abstract = \"Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and pushing dissimilar ones away. However, these methods fail to capture the fine-grained ranking information among the sentences, where each sentence is only treated as either positive or negative. In many real-world scenarios, one needs to distinguish and rank the sentences based on their similarities to a query sentence, e.g., very relevant, moderate relevant, less relevant, irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for unsupervised sentence representation learning, which incorporates ranking consistency and ranking distillation with contrastive learning into a unified framework. In particular, we learn semantically discriminative sentence representations by simultaneously ensuring ranking consistency between two representations with different dropout masks, and distilling listwise ranking knowledge from the teacher. An extensive set of experiments are conducted on both semantic textual similarity (STS) and transfer (TR) tasks. Experimental results demonstrate the superior performance of our approach over several state-of-the-art baselines.\",\n}\n",
    "authors": [
        "Jiduan Liu",
        "Jiahao Liu",
        "Qifan Wang",
        "Jingang Wang",
        "Wei Wu",
        "Yunsen Xian",
        "Dongyan Zhao",
        "Kai Chen",
        "Rui Yan"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.771.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b997a83e-f62e-5db0-8371-7abce990ace4.pdf",
    "abstract": "Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and pushing dissimilar ones away. However, these methods fail to capture the fine-grained ranking information among the sentences, where each sentence is only treated as either positive or negative. In many real-world scenarios, one needs to distinguish and rank the sentences based on their similarities to a query sentence, e.g., very relevant, moderate relevant, less relevant, irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for unsupervised sentence representation learning, which incorporates ranking consistency and ranking distillation with contrastive learning into a unified framework. In particular, we learn semantically discriminative sentence representations by simultaneously ensuring ranking consistency between two representations with different dropout masks, and distilling listwise ranking knowledge from the teacher. An extensive set of experiments are conducted on both semantic textual similarity (STS) and transfer (TR) tasks. Experimental results demonstrate the superior performance of our approach over several state-of-the-art baselines.",
    "num_pages": 18
}