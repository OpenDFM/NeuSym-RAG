{
    "uuid": "e394a957-18e7-55cf-984e-c77e60671439",
    "title": "PROC2PDDL: Open-Domain Planning Representations from Texts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)",
    "bibtex": "@inproceedings{zhang-etal-2024-proc2pddl,\n    title = \"{PROC}2{PDDL}: Open-Domain Planning Representations from Texts\",\n    author = \"Zhang, Tianyi  and\n      Zhang, Li  and\n      Hou, Zhaoyi  and\n      Wang, Ziyu  and\n      Gu, Yuling  and\n      Clark, Peter  and\n      Callison-Burch, Chris  and\n      Tandon, Niket\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Lipkin, Ben  and\n      Neves Ribeiro, Danilo  and\n      Wong, Lionel  and\n      Ye, Xi  and\n      Zhao, Wenting\",\n    booktitle = \"Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlrse-1.2\",\n    pages = \"13--24\",\n    abstract = \"Planning in a text-based environment continues to be a significant challenge for AI systems. Recent approaches have utilized language models to predict planning domain definitions (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL, the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate the task of predicting domain actions (parameters, preconditions, and effects). We experiment with various large language models (LLMs) and prompting mechanisms, including a novel instruction inspired by the zone of proximal development (ZPD), which reconstructs the task as incremental basic skills. Our results demonstrate that Proc2PDDL is highly challenging for end-to-end LLMs, with GPT-3.5{'}s success rate close to 0{\\%} and GPT-4o{'}s 38{\\%}. With ZPD instructions, GPT-4o{'}s success rate increases to 45{\\%}, outperforming regular chain-of-thought prompting{'}s 34{\\%}. Our analysis systematically examines both syntactic and semantic errors, providing insights into the strengths and weaknesses of language models in generating domain-specific programs.\",\n}\n",
    "authors": [
        "Tianyi Zhang",
        "Li Zhang",
        "Zhaoyi Hou",
        "Ziyu Wang",
        "Yuling Gu",
        "Peter Clark",
        "Chris Callison-Burch",
        "Niket Tandon"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlrse-1.2.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/e394a957-18e7-55cf-984e-c77e60671439.pdf",
    "abstract": "Planning in a text-based environment continues to be a significant challenge for AI systems. Recent approaches have utilized language models to predict planning domain definitions (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL, the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate the task of predicting domain actions (parameters, preconditions, and effects). We experiment with various large language models (LLMs) and prompting mechanisms, including a novel instruction inspired by the zone of proximal development (ZPD), which reconstructs the task as incremental basic skills. Our results demonstrate that Proc2PDDL is highly challenging for end-to-end LLMs, with GPT-3.5’s success rate close to 0% and GPT-4o’s 38%. With ZPD instructions, GPT-4o’s success rate increases to 45%, outperforming regular chain-of-thought prompting’s 34%. Our analysis systematically examines both syntactic and semantic errors, providing insights into the strengths and weaknesses of language models in generating domain-specific programs.",
    "num_pages": 12
}