{
    "uuid": "bf38e73f-8b5b-5e24-b184-23b25c702d79",
    "title": "SP3: Enhancing Structured Pruning via PCA Projection",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{hu-etal-2024-sp3,\n    title = \"$\\rm SP^3$: Enhancing Structured Pruning via {PCA} Projection\",\n    author = \"Hu, Yuxuan  and\n      Zhang, Jing  and\n      Zhao, Zhe  and\n      Zhao, Chen  and\n      Chen, Xiaodong  and\n      Li, Cuiping  and\n      Chen, Hong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.187\",\n    doi = \"10.18653/v1/2024.findings-acl.187\",\n    pages = \"3150--3170\",\n}\n",
    "authors": [
        "Yuxuan Hu",
        "Jing Zhang",
        "Zhe Zhao",
        "Chen Zhao",
        "Xiaodong Chen",
        "Cuiping Li",
        "Hong Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.187.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bf38e73f-8b5b-5e24-b184-23b25c702d79.pdf",
    "abstract": "Structured pruning is a widely used technique for reducing the size of pre-trained language models (PLMs), but current methods often overlook the potential of compressing the hidden dimension (d) in PLMs, a dimension critical to model size and efficiency. This paper introduces a novel structured pruning approach, Structured Pruning with PCA Projection (SP3), targeting the effective reduction of d by projecting features into a space defined by principal components before masking. Extensive experiments on benchmarks (GLUE and SQuAD) show that SP3 can reduce d by 70%, compress 94% of the BERTbase model, and maintain over 96% accuracy and outperform other methods that compress d by 6% in accuracy at the same compression ratio. SP3 has also proven effective with other models, including OPT and Llama. Our data and code are available at ours repo.",
    "num_pages": 21
}