{
    "uuid": "02ab64be-06c4-5c23-9e71-da7afd141e98",
    "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{li-etal-2023-narrowbert,\n    title = \"{N}arrow{BERT}: Accelerating Masked Language Model Pretraining and Inference\",\n    author = \"Li, Haoxin  and\n      Keung, Phillip  and\n      Cheng, Daniel  and\n      Kasai, Jungo  and\n      Smith, Noah A.\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.146\",\n    doi = \"10.18653/v1/2023.acl-short.146\",\n    pages = \"1723--1730\",\n    abstract = \"Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.\",\n}\n",
    "authors": [
        "Haoxin Li",
        "Phillip Keung",
        "Daniel Cheng",
        "Jungo Kasai",
        "Noah A. Smith"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.146.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/02ab64be-06c4-5c23-9e71-da7afd141e98.pdf",
    "abstract": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
    "num_pages": 8
}