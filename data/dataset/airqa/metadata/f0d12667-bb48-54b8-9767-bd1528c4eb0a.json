{
    "uuid": "f0d12667-bb48-54b8-9767-bd1528c4eb0a",
    "title": "KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    "bibtex": "@inproceedings{kim-etal-2023-ku,\n    title = \"{KU}-{DMIS}-{MSRA} at {R}ad{S}um23: Pre-trained Vision-Language Model for Radiology Report Summarization\",\n    author = \"Kim, Gangwoo  and\n      Kim, Hajung  and\n      Ji, Lei  and\n      Bae, Seongsu  and\n      Kim, Chanhwi  and\n      Sung, Mujeen  and\n      Kim, Hyunjae  and\n      Yan, Kun  and\n      Chang, Eric  and\n      Kang, Jaewoo\",\n    editor = \"Demner-fushman, Dina  and\n      Ananiadou, Sophia  and\n      Cohen, Kevin\",\n    booktitle = \"The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bionlp-1.59\",\n    doi = \"10.18653/v1/2023.bionlp-1.59\",\n    pages = \"567--573\",\n    abstract = \"In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.\",\n}\n",
    "authors": [
        "Gangwoo Kim",
        "Hajung Kim",
        "Lei Ji",
        "Seongsu Bae",
        "Chanhwi Kim",
        "Mujeen Sung",
        "Hyunjae Kim",
        "Kun Yan",
        "Eric Chang",
        "Jaewoo Kang"
    ],
    "pdf_url": "https://aclanthology.org/2023.bionlp-1.59.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f0d12667-bb48-54b8-9767-bd1528c4eb0a.pdf",
    "abstract": "In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema. It enables the model to effectively learn the required knowledge and skills from limited resources in the domain. Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains. With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.",
    "num_pages": 7
}