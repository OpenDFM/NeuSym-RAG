{
    "uuid": "12ac5ade-45b8-58b4-91e3-41f56837554e",
    "title": "HW-TSC at IWSLT2023: Break the Quality Ceiling of Offline Track via Pre-Training and Domain Adaptation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    "bibtex": "@inproceedings{li-etal-2023-hw,\n    title = \"{HW}-{TSC} at {IWSLT}2023: Break the Quality Ceiling of Offline Track via Pre-Training and Domain Adaptation\",\n    author = \"Li, Zongyao  and\n      Wu, Zhanglin  and\n      Rao, Zhiqiang  and\n      YuHao, Xie  and\n      JiaXin, Guo  and\n      Wei, Daimeng  and\n      Shang, Hengchao  and\n      Minghan, Wang  and\n      Chen, Xiaoyu  and\n      Yu, Zhengzhe  and\n      ShaoJun, Li  and\n      LiZhi, Lei  and\n      Yang, Hao\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.iwslt-1.14\",\n    doi = \"10.18653/v1/2023.iwslt-1.14\",\n    pages = \"187--193\",\n    abstract = \"This paper presents HW-TSC{'}s submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training, and unconstrained training) with models of cascaded architectures. We use data enhancement, pre-training models and other means to improve the ASR quality, and R-Drop, deep model, domain data selection, etc. to improve the translation quality. Compared with last year{'}s best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test set.\",\n}\n",
    "authors": [
        "Zongyao Li",
        "Zhanglin Wu",
        "Zhiqiang Rao",
        "Xie YuHao",
        "Guo JiaXin",
        "Daimeng Wei",
        "Hengchao Shang",
        "Wang Minghan",
        "Xiaoyu Chen",
        "Zhengzhe Yu",
        "Li ShaoJun",
        "Lei LiZhi",
        "Hao Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.iwslt-1.14.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/12ac5ade-45b8-58b4-91e3-41f56837554e.pdf",
    "abstract": "This paper presents HW-TSC’s submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training, and unconstrained training) with models of cascaded architectures. We use data enhancement, pre-training models and other means to improve the ASR quality, and R-Drop, deep model, domain data selection, etc. to improve the translation quality. Compared with last year’s best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test set.",
    "num_pages": 7
}