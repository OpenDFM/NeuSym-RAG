{
    "uuid": "7573fc71-e70d-57f0-b3c2-6f36db040588",
    "title": "Towards Building a Robust Toxicity Predictor",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{bespalov-etal-2023-towards,\n    title = \"Towards Building a Robust Toxicity Predictor\",\n    author = \"Bespalov, Dmitriy  and\n      Bhabesh, Sourav  and\n      Xiang, Yi  and\n      Zhou, Liutong  and\n      Qi, Yanjun\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.56\",\n    doi = \"10.18653/v1/2023.acl-industry.56\",\n    pages = \"581--598\",\n    abstract = \"Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, {\\textbackslash}texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. {\\textbackslash}texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow {\\textbackslash}texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98{\\textbackslash}{\\%} attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.\",\n}\n",
    "authors": [
        "Dmitriy Bespalov",
        "Sourav Bhabesh",
        "Yi Xiang",
        "Liutong Zhou",
        "Yanjun Qi"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.56.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/7573fc71-e70d-57f0-b3c2-6f36db040588.pdf",
    "abstract": "Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.",
    "num_pages": 18
}