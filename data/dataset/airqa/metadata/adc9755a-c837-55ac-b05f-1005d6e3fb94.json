{
    "uuid": "adc9755a-c837-55ac-b05f-1005d6e3fb94",
    "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{kabra-etal-2023-multi,\n    title = \"Multi-lingual and Multi-cultural Figurative Language Understanding\",\n    author = \"Kabra, Anubha  and\n      Liu, Emmy  and\n      Khanuja, Simran  and\n      Aji, Alham Fikri  and\n      Winata, Genta  and\n      Cahyawijaya, Samuel  and\n      Aremu, Anuoluwapo  and\n      Ogayo, Perez  and\n      Neubig, Graham\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.525\",\n    doi = \"10.18653/v1/2023.findings-acl.525\",\n    pages = \"8269--8284\",\n    abstract = \"Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, {pasted macro {`}DATASETNAME{'}}, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs{'} abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training. Data and code is released at \\url{https://anonymous.4open.science/r/Multilingual-Fig-QA-7B03/}\",\n}\n",
    "authors": [
        "Anubha Kabra",
        "Emmy Liu",
        "Simran Khanuja",
        "Alham Fikri Aji",
        "Genta Winata",
        "Samuel Cahyawijaya",
        "Anuoluwapo Aremu",
        "Perez Ogayo",
        "Graham Neubig"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.525.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/adc9755a-c837-55ac-b05f-1005d6e3fb94.pdf",
    "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, {pasted macro ‘DATASETNAME’}, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs’ abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training. Data and code is released at https://anonymous.4open.science/r/Multilingual-Fig-QA-7B03/",
    "num_pages": 16
}