{
    "uuid": "f01e821c-f5a2-531d-a42b-2d2abfb5ff5f",
    "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{fang-etal-2024-complex,\n    title = \"Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs\",\n    author = \"Fang, Tianqing  and\n      Chen, Zeming  and\n      Song, Yangqiu  and\n      Bosselut, Antoine\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.613\",\n    doi = \"10.18653/v1/2024.acl-long.613\",\n    pages = \"11365--11384\",\n    abstract = \"Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit contextunderlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense infer-ences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplexCOMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or theeffect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules andlarge language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improve ments in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations\",\n}\n",
    "authors": [
        "Tianqing Fang",
        "Zeming Chen",
        "Yangqiu Song",
        "Antoine Bosselut"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.613.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f01e821c-f5a2-531d-a42b-2d2abfb5ff5f.pdf",
    "abstract": "Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit contextunderlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense infer-ences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplexCOMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or theeffect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules andlarge language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improve ments in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations",
    "num_pages": 20
}