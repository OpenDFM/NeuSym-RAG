{
    "uuid": "a4f10216-f8c0-5dc0-8366-3a4fcd27ea05",
    "title": "TÃ¼-CL at SIGMORPHON 2023: Straight-Through Gradient Estimation for Hard Attention",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    "bibtex": "@inproceedings{girrbach-2023-tu-cl,\n    title = {T{\\\"u}-{CL} at {SIGMORPHON} 2023: Straight-Through Gradient Estimation for Hard Attention},\n    author = \"Girrbach, Leander\",\n    editor = {Nicolai, Garrett  and\n      Chodroff, Eleanor  and\n      Mailhot, Frederic  and\n      {\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}},\n    booktitle = \"Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sigmorphon-1.19\",\n    doi = \"10.18653/v1/2023.sigmorphon-1.19\",\n    pages = \"171--185\",\n    abstract = \"This paper describes our systems participating in the 2023 SIGMORPHON Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear Glossing. We propose methods to enrich predictions from neural models with discrete, i.e. interpretable, information. For morphological inflection, our models learn deterministic mappings from subsets of source lemma characters and morphological tags to individual target characters, which introduces interpretability. For interlinear glossing, our models learn a shallow morpheme segmentation in an unsupervised way jointly with predicting glossing lines. Estimated segmentation may be useful when no ground-truth segmentation is available. As both methods introduce discreteness into neural models, our technical contribution is to show that straight-through gradient estimators are effective to train hard attention models.\",\n}\n",
    "authors": [
        "Leander Girrbach"
    ],
    "pdf_url": "https://aclanthology.org/2023.sigmorphon-1.19.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a4f10216-f8c0-5dc0-8366-3a4fcd27ea05.pdf",
    "abstract": "This paper describes our systems participating in the 2023 SIGMORPHON Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear Glossing. We propose methods to enrich predictions from neural models with discrete, i.e. interpretable, information. For morphological inflection, our models learn deterministic mappings from subsets of source lemma characters and morphological tags to individual target characters, which introduces interpretability. For interlinear glossing, our models learn a shallow morpheme segmentation in an unsupervised way jointly with predicting glossing lines. Estimated segmentation may be useful when no ground-truth segmentation is available. As both methods introduce discreteness into neural models, our technical contribution is to show that straight-through gradient estimators are effective to train hard attention models.",
    "num_pages": 15
}