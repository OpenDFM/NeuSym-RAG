{
    "uuid": "09fd6255-35a6-51b2-92a9-423d6518584c",
    "title": "nlp_enjoyers at TextGraphs-17 Shared Task: Text-Graph Representations for Knowledge Graph Question Answering using all-MPNet",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
    "bibtex": "@inproceedings{kurdiukov-etal-2024-nlp,\n    title = \"nlp{\\_}enjoyers at {T}ext{G}raphs-17 Shared Task: Text-Graph Representations for Knowledge Graph Question Answering using all-{MPN}et\",\n    author = \"Kurdiukov, Nikita  and\n      Zinkovich, Viktoriia  and\n      Karpukhin, Sergey  and\n      Tikhomirov, Pavel\",\n    editor = \"Ustalov, Dmitry  and\n      Gao, Yanjun  and\n      Panchenko, Alexander  and\n      Tutubalina, Elena  and\n      Nikishina, Irina  and\n      Ramesh, Arti  and\n      Sakhovskiy, Andrey  and\n      Usbeck, Ricardo  and\n      Penn, Gerald  and\n      Valentino, Marco\",\n    booktitle = \"Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.textgraphs-1.10\",\n    pages = \"126--130\",\n    abstract = \"This paper presents a model for solving the Multiple Choice Question Answering (MCQA) problem, focusing on the impact of subgraph extraction from a Knowledge Graph on model performance. The proposed method combines textual and graph information by adding linearized subgraphs directly into the main question prompt with separate tokens, enhancing the performance of models working with each modality separately. The study also includes an examination of Large Language Model (LLM) backbones and the benefits of linearized subgraphs and sequence length, with efficient training achieved through fine-tuning with LoRA. The top benchmark, using subgraphs and MPNet, achieved an F1 score of 0.3887. The main limitation of the experiments is the reliance on pre-generated subgraphs/triplets from the graph, and the lack of exploration of in-context learning and prompting strategies with decoder-based architectures.\",\n}\n",
    "authors": [
        "Nikita Kurdiukov",
        "Viktoriia Zinkovich",
        "Sergey Karpukhin",
        "Pavel Tikhomirov"
    ],
    "pdf_url": "https://aclanthology.org/2024.textgraphs-1.10.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/09fd6255-35a6-51b2-92a9-423d6518584c.pdf",
    "abstract": "This paper presents a model for solving the Multiple Choice Question Answering (MCQA) problem, focusing on the impact of subgraph extraction from a Knowledge Graph on model performance. The proposed method combines textual and graph information by adding linearized subgraphs directly into the main question prompt with separate tokens, enhancing the performance of models working with each modality separately. The study also includes an examination of Large Language Model (LLM) backbones and the benefits of linearized subgraphs and sequence length, with efficient training achieved through fine-tuning with LoRA. The top benchmark, using subgraphs and MPNet, achieved an F1 score of 0.3887. The main limitation of the experiments is the reliance on pre-generated subgraphs/triplets from the graph, and the lack of exploration of in-context learning and prompting strategies with decoder-based architectures.",
    "num_pages": 5
}