{
    "uuid": "831182ad-606a-5c85-b321-f7d1b136fe98",
    "title": "Dialect-robust Evaluation of Generated Text",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{sun-etal-2023-dialect,\n    title = \"Dialect-robust Evaluation of Generated Text\",\n    author = \"Sun, Jiao  and\n      Sellam, Thibault  and\n      Clark, Elizabeth  and\n      Vu, Tu  and\n      Dozat, Timothy  and\n      Garrette, Dan  and\n      Siddhant, Aditya  and\n      Eisenstein, Jacob  and\n      Gehrmann, Sebastian\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.331\",\n    doi = \"10.18653/v1/2023.acl-long.331\",\n    pages = \"6010--6028\",\n    abstract = \"Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. As a step towards dialect-robust metrics for text generation, we propose NANO, which introduces regional and language information to the metric{'}s pretraining. NANO significantly improves dialect robustness while preserving the correlation between automated metrics and human ratings. It also enables a more ambitious approach to evaluation, dialect awareness, in which system outputs are scored by both semantic match to the reference and appropriateness in any specified dialect.\",\n}\n",
    "authors": [
        "Jiao Sun",
        "Thibault Sellam",
        "Elizabeth Clark",
        "Tu Vu",
        "Timothy Dozat",
        "Dan Garrette",
        "Aditya Siddhant",
        "Jacob Eisenstein",
        "Sebastian Gehrmann"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.331.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/831182ad-606a-5c85-b321-f7d1b136fe98.pdf",
    "abstract": "Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. As a step towards dialect-robust metrics for text generation, we propose NANO, which introduces regional and language information to the metricâ€™s pretraining. NANO significantly improves dialect robustness while preserving the correlation between automated metrics and human ratings. It also enables a more ambitious approach to evaluation, dialect awareness, in which system outputs are scored by both semantic match to the reference and appropriateness in any specified dialect.",
    "num_pages": 19
}