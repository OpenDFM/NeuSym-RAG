{
    "uuid": "c0cf4e6c-031e-5b8c-a10f-6a2fd243777d",
    "title": "Moûsai: Efficient Text-to-Music Diffusion Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{schneider-etal-2024-mousai,\n    title = \"Mo{\\^u}sai: Efficient Text-to-Music Diffusion Models\",\n    author = {Schneider, Flavio  and\n      Kamal, Ojasv  and\n      Jin, Zhijing  and\n      Sch{\\\"o}lkopf, Bernhard},\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.437\",\n    doi = \"10.18653/v1/2024.acl-long.437\",\n    pages = \"8050--8068\",\n    abstract = \"Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another {``}language{''} of communication {--} music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo{\\^u}sai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model{'}s competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source libraries with the hope of facilitating future work in the field. We open-source the following: Codes: https://github.com/archinetai/audio-diffusion-pytorch. Music samples for this paper: http://bit.ly/44ozWDH. Music samples for all models: https://bit.ly/audio-diffusion.\",\n}\n",
    "authors": [
        "Flavio Schneider",
        "Ojasv Kamal",
        "Zhijing Jin",
        "Bernhard Schölkopf"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.437.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c0cf4e6c-031e-5b8c-a10f-6a2fd243777d.pdf",
    "abstract": "Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another “language” of communication – music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Moûsai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model’s competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source libraries with the hope of facilitating future work in the field. We open-source the following: Codes: https://github.com/archinetai/audio-diffusion-pytorch. Music samples for this paper: http://bit.ly/44ozWDH. Music samples for all models: https://bit.ly/audio-diffusion.",
    "num_pages": 19
}