{
    "uuid": "bc194ba1-6d56-55ef-9570-392491216f84",
    "title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chu-etal-2024-beamaggr,\n    title = \"{B}eam{A}gg{R}: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering\",\n    author = \"Chu, Zheng  and\n      Chen, Jingchang  and\n      Chen, Qianglong  and\n      Wang, Haotian  and\n      Zhu, Kun  and\n      Du, Xiyuan  and\n      Yu, Weijiang  and\n      Liu, Ming  and\n      Qin, Bing\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.67\",\n    doi = \"10.18653/v1/2024.acl-long.67\",\n    pages = \"1229--1248\",\n    abstract = \"Large language models (LLMs) have demonstrated strong reasoning capabilities.Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks.Retrieval-augmented reasoning represents a promising approach.However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge.To address this, we propose Beam Aggregation Reasoning (BeamAggR), a reasoning framework for knowledge-intensive multi-hop QA.BeamAggR explores and prioritizes promising answers at each hop of question.Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning.For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates.For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory.Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5{\\%}.Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.\",\n}\n",
    "authors": [
        "Zheng Chu",
        "Jingchang Chen",
        "Qianglong Chen",
        "Haotian Wang",
        "Kun Zhu",
        "Xiyuan Du",
        "Weijiang Yu",
        "Ming Liu",
        "Bing Qin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.67.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/bc194ba1-6d56-55ef-9570-392491216f84.pdf",
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities.Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks.Retrieval-augmented reasoning represents a promising approach.However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge.To address this, we propose Beam Aggregation Reasoning (BeamAggR), a reasoning framework for knowledge-intensive multi-hop QA.BeamAggR explores and prioritizes promising answers at each hop of question.Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning.For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates.For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory.Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%.Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.",
    "num_pages": 20
}