{
    "uuid": "0748bc40-345f-5d2b-86bd-e91a04d368aa",
    "title": "Emergent Word Order Universals from Cognitively-Motivated Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{kuribayashi-etal-2024-emergent,\n    title = \"Emergent Word Order Universals from Cognitively-Motivated Language Models\",\n    author = \"Kuribayashi, Tatsuki  and\n      Ueda, Ryo  and\n      Yoshida, Ryo  and\n      Oseki, Yohei  and\n      Briscoe, Ted  and\n      Baldwin, Timothy\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.781\",\n    doi = \"10.18653/v1/2024.acl-long.781\",\n    pages = \"14522--14543\",\n    abstract = \"The world{'}s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.\",\n}\n",
    "authors": [
        "Tatsuki Kuribayashi",
        "Ryo Ueda",
        "Ryo Yoshida",
        "Yohei Oseki",
        "Ted Briscoe",
        "Timothy Baldwin"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.781.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0748bc40-345f-5d2b-86bd-e91a04d368aa.pdf",
    "abstract": "The worldâ€™s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.",
    "num_pages": 22
}