{
    "uuid": "a701b27c-1649-5b32-b066-5ddc1b4e7c07",
    "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{perez-etal-2023-discovering,\n    title = \"Discovering Language Model Behaviors with Model-Written Evaluations\",\n    author = \"Perez, Ethan  and\n      Ringer, Sam  and\n      Lukosiute, Kamile  and\n      Nguyen, Karina  and\n      Chen, Edwin  and\n      Heiner, Scott  and\n      Pettit, Craig  and\n      Olsson, Catherine  and\n      Kundu, Sandipan  and\n      Kadavath, Saurav  and\n      Jones, Andy  and\n      Chen, Anna  and\n      Mann, Benjamin  and\n      Israel, Brian  and\n      Seethor, Bryan  and\n      McKinnon, Cameron  and\n      Olah, Christopher  and\n      Yan, Da  and\n      Amodei, Daniela  and\n      Amodei, Dario  and\n      Drain, Dawn  and\n      Li, Dustin  and\n      Tran-Johnson, Eli  and\n      Khundadze, Guro  and\n      Kernion, Jackson  and\n      Landis, James  and\n      Kerr, Jamie  and\n      Mueller, Jared  and\n      Hyun, Jeeyoon  and\n      Landau, Joshua  and\n      Ndousse, Kamal  and\n      Goldberg, Landon  and\n      Lovitt, Liane  and\n      Lucas, Martin  and\n      Sellitto, Michael  and\n      Zhang, Miranda  and\n      Kingsland, Neerav  and\n      Elhage, Nelson  and\n      Joseph, Nicholas  and\n      Mercado, Noemi  and\n      DasSarma, Nova  and\n      Rausch, Oliver  and\n      Larson, Robin  and\n      McCandlish, Sam  and\n      Johnston, Scott  and\n      Kravec, Shauna  and\n      El Showk, Sheer  and\n      Lanham, Tamera  and\n      Telleen-Lawton, Timothy  and\n      Brown, Tom  and\n      Henighan, Tom  and\n      Hume, Tristan  and\n      Bai, Yuntao  and\n      Hatfield-Dodds, Zac  and\n      Clark, Jack  and\n      Bowman, Samuel R.  and\n      Askell, Amanda  and\n      Grosse, Roger  and\n      Hernandez, Danny  and\n      Ganguli, Deep  and\n      Hubinger, Evan  and\n      Schiefer, Nicholas  and\n      Kaplan, Jared\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.847\",\n    doi = \"10.18653/v1/2023.findings-acl.847\",\n    pages = \"13387--13434\",\n    abstract = \"As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100{\\%} of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user{'}s preferred answer ({``}sycophancy{''}) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.\",\n}\n",
    "authors": [
        "Ethan Perez",
        "Sam Ringer",
        "Kamile Lukosiute",
        "Karina Nguyen",
        "Edwin Chen",
        "Scott Heiner",
        "Craig Pettit",
        "Catherine Olsson",
        "Sandipan Kundu",
        "Saurav Kadavath",
        "Andy Jones",
        "Anna Chen",
        "Benjamin Mann",
        "Brian Israel",
        "Bryan Seethor",
        "Cameron McKinnon",
        "Christopher Olah",
        "Da Yan",
        "Daniela Amodei",
        "Dario Amodei",
        "Dawn Drain",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Guro Khundadze",
        "Jackson Kernion",
        "James Landis",
        "Jamie Kerr",
        "Jared Mueller",
        "Jeeyoon Hyun",
        "Joshua Landau",
        "Kamal Ndousse",
        "Landon Goldberg",
        "Liane Lovitt",
        "Martin Lucas",
        "Michael Sellitto",
        "Miranda Zhang",
        "Neerav Kingsland",
        "Nelson Elhage",
        "Nicholas Joseph",
        "Noemi Mercado",
        "Nova DasSarma",
        "Oliver Rausch",
        "Robin Larson",
        "Sam McCandlish",
        "Scott Johnston",
        "Shauna Kravec",
        "Sheer El Showk",
        "Tamera Lanham",
        "Timothy Telleen-Lawton",
        "Tom Brown",
        "Tom Henighan",
        "Tristan Hume",
        "Yuntao Bai",
        "Zac Hatfield-Dodds",
        "Jack Clark",
        "Samuel R. Bowman",
        "Amanda Askell",
        "Roger Grosse",
        "Danny Hernandez",
        "Deep Ganguli",
        "Evan Hubinger",
        "Nicholas Schiefer",
        "Jared Kaplan"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.847.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a701b27c-1649-5b32-b066-5ddc1b4e7c07.pdf",
    "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.",
    "num_pages": 48
}