{
    "uuid": "18e7b3ad-08b9-5777-bded-5dd663dc8b46",
    "title": "A Synthetic Data Generation Framework for Grounded Dialogues",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bao-etal-2023-synthetic,\n    title = \"A Synthetic Data Generation Framework for Grounded Dialogues\",\n    author = \"Bao, Jianzhu  and\n      Wang, Rui  and\n      Wang, Yasheng  and\n      Sun, Aixin  and\n      Li, Yitong  and\n      Mi, Fei  and\n      Xu, Ruifeng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.608\",\n    doi = \"10.18653/v1/2023.acl-long.608\",\n    pages = \"10866--10882\",\n    abstract = \"Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios.\",\n}\n",
    "authors": [
        "Jianzhu Bao",
        "Rui Wang",
        "Yasheng Wang",
        "Aixin Sun",
        "Yitong Li",
        "Fei Mi",
        "Ruifeng Xu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.608.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/18e7b3ad-08b9-5777-bded-5dd663dc8b46.pdf",
    "abstract": "Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios.",
    "num_pages": 17
}