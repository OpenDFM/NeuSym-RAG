{
    "uuid": "776b4e44-f693-57a4-8d0d-28ad2e42bd14",
    "title": "NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{zhang-etal-2023-nag,\n    title = \"{NAG}-{NER}: a Unified Non-Autoregressive Generation Framework for Various {NER} Tasks\",\n    author = \"Zhang, Xinpeng  and\n      Tan, Ming  and\n      Zhang, Jingfan  and\n      Zhu, Wei\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.65\",\n    doi = \"10.18653/v1/2023.acl-industry.65\",\n    pages = \"676--686\",\n    abstract = \"Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.{\\textbackslash}footnote{Code will be publicly available to the research community upon acceptance.}\",\n}\n",
    "authors": [
        "Xinpeng Zhang",
        "Ming Tan",
        "Jingfan Zhang",
        "Wei Zhu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.65.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/776b4e44-f693-57a4-8d0d-28ad2e42bd14.pdf",
    "abstract": "Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.\\footnote{Code will be publicly available to the research community upon acceptance.}",
    "num_pages": 11
}