{
    "uuid": "cac8ab78-dd61-5000-9f53-536e5cefcf23",
    "title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{jiang-etal-2024-followbench,\n    title = \"{F}ollow{B}ench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models\",\n    author = \"Jiang, Yuxin  and\n      Wang, Yufei  and\n      Zeng, Xingshan  and\n      Zhong, Wanjun  and\n      Li, Liangyou  and\n      Mi, Fei  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Wang, Wei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.257\",\n    doi = \"10.18653/v1/2024.acl-long.257\",\n    pages = \"4667--4688\",\n    abstract = \"The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs{'} outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.\",\n}\n",
    "authors": [
        "Yuxin Jiang",
        "Yufei Wang",
        "Xingshan Zeng",
        "Wanjun Zhong",
        "Liangyou Li",
        "Fei Mi",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Wei Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.257.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/cac8ab78-dd61-5000-9f53-536e5cefcf23.pdf",
    "abstract": "The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMsâ€™ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.",
    "num_pages": 22
}