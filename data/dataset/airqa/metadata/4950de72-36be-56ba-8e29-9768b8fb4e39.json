{
    "uuid": "4950de72-36be-56ba-8e29-9768b8fb4e39",
    "title": "IRIT_IRIS_A at SemEval-2023 Task 6: Legal Rhetorical Role Labeling Supported by Dynamic-Filled Contextualized Sentence Chunks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{lima-etal-2023-irit,\n    title = \"{IRIT}{\\_}{IRIS}{\\_}{A} at {S}em{E}val-2023 Task 6: Legal Rhetorical Role Labeling Supported by Dynamic-Filled Contextualized Sentence Chunks\",\n    author = \"Lima, Alexandre Gomes de  and\n      Moreno, Jose G.  and\n      H. da S. Aranha, Eduardo\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.125\",\n    doi = \"10.18653/v1/2023.semeval-1.125\",\n    pages = \"905--912\",\n    abstract = \"This work presents and evaluates an approach to efficiently leverage the context exploitation ability of pre-trained Transformer models as a way of boosting the performance of models tackling the Legal Rhetorical Role Labeling task. The core idea is to feed the model with sentence chunks that are assembled in a way that avoids the insertion of padding tokens and the truncation of sentences and, hence, obtain better sentence embeddings. The achieved results show that our proposal is efficient, despite its simplicity, since models based on it overcome strong baselines by 3.76{\\%} in the worst case and by 8.71{\\%} in the best case.\",\n}\n",
    "authors": [
        "Alexandre Gomes de Lima",
        "Jose G. Moreno",
        "Eduardo H. da S. Aranha"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.125.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4950de72-36be-56ba-8e29-9768b8fb4e39.pdf",
    "abstract": "This work presents and evaluates an approach to efficiently leverage the context exploitation ability of pre-trained Transformer models as a way of boosting the performance of models tackling the Legal Rhetorical Role Labeling task. The core idea is to feed the model with sentence chunks that are assembled in a way that avoids the insertion of padding tokens and the truncation of sentences and, hence, obtain better sentence embeddings. The achieved results show that our proposal is efficient, despite its simplicity, since models based on it overcome strong baselines by 3.76% in the worst case and by 8.71% in the best case.",
    "num_pages": 8
}