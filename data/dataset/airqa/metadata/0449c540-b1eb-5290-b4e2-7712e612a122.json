{
    "uuid": "0449c540-b1eb-5290-b4e2-7712e612a122",
    "title": "Challenging Large Language Models with New Tasks: A Study on their Adaptability and Robustness",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{li-etal-2024-challenging,\n    title = \"Challenging Large Language Models with New Tasks: A Study on their Adaptability and Robustness\",\n    author = \"Li, Chenxi  and\n      Tian, Yuanhe  and\n      Zerong, Zhaxi  and\n      Song, Yan  and\n      Xia, Fei\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.485\",\n    doi = \"10.18653/v1/2024.findings-acl.485\",\n    pages = \"8140--8162\",\n    abstract = \"Recent progress in large language models (LLMs) has marked a notable milestone in the field of artificial intelligence. The conventional evaluation of LLMs primarily relies on existing tasks and benchmarks, raising concerns about test set contamination and the genuine comprehension abilities of LLMs. To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs{'} adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies. We investigate the capacity of LLMs to adapt to new but simple tasks, especially when they diverge from the models{'} pre-existing knowledge. Our methodology emphasizes the creation of straightforward tasks, facilitating a precise error analysis to uncover the underlying causes of LLM failures. This strategic approach also aims to uncover effective strategies for enhancing LLM performance based on the detailed error analysis of system output.\",\n}\n",
    "authors": [
        "Chenxi Li",
        "Yuanhe Tian",
        "Zhaxi Zerong",
        "Yan Song",
        "Fei Xia"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.485.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0449c540-b1eb-5290-b4e2-7712e612a122.pdf",
    "abstract": "Recent progress in large language models (LLMs) has marked a notable milestone in the field of artificial intelligence. The conventional evaluation of LLMs primarily relies on existing tasks and benchmarks, raising concerns about test set contamination and the genuine comprehension abilities of LLMs. To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs’ adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies. We investigate the capacity of LLMs to adapt to new but simple tasks, especially when they diverge from the models’ pre-existing knowledge. Our methodology emphasizes the creation of straightforward tasks, facilitating a precise error analysis to uncover the underlying causes of LLM failures. This strategic approach also aims to uncover effective strategies for enhancing LLM performance based on the detailed error analysis of system output.",
    "num_pages": 23
}