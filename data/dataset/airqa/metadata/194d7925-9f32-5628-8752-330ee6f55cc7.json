{
    "uuid": "194d7925-9f32-5628-8752-330ee6f55cc7",
    "title": "DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{feng-etal-2023-dunst,\n    title = \"{D}u{NST}: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation\",\n    author = \"Feng, Yuxi  and\n      Yi, Xiaoyuan  and\n      Wang, Xiting  and\n      Lakshmanan, V.S., Laks  and\n      Xie, Xing\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.488\",\n    doi = \"10.18653/v1/2023.acl-long.488\",\n    pages = \"8760--8785\",\n    abstract = \"Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models \\textit{over-exploit} the previously learned text space and \\textit{fail to explore} a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.\",\n}\n",
    "authors": [
        "Yuxi Feng",
        "Xiaoyuan Yi",
        "Xiting Wang",
        "Laks Lakshmanan, V.S.",
        "Xing Xie"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.488.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/194d7925-9f32-5628-8752-330ee6f55cc7.pdf",
    "abstract": "Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.",
    "num_pages": 26
}