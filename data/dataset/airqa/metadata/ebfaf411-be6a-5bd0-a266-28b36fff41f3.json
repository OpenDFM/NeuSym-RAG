{
    "uuid": "ebfaf411-be6a-5bd0-a266-28b36fff41f3",
    "title": "Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)",
    "bibtex": "@inproceedings{bang-etal-2023-enabling,\n    title = \"Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values\",\n    author = \"Bang, Yejin  and\n      Yu, Tiezheng  and\n      Madotto, Andrea  and\n      Lin, Zhaojiang  and\n      Diab, Mona  and\n      Fung, Pascale\",\n    editor = \"Ovalle, Anaelia  and\n      Chang, Kai-Wei  and\n      Mehrabi, Ninareh  and\n      Pruksachatkun, Yada  and\n      Galystan, Aram  and\n      Dhamala, Jwala  and\n      Verma, Apurv  and\n      Cao, Trista  and\n      Kumar, Anoop  and\n      Gupta, Rahul\",\n    booktitle = \"Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.trustnlp-1.27\",\n    doi = \"10.18653/v1/2023.trustnlp-1.27\",\n    pages = \"311--325\",\n    abstract = \"Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56{\\%} on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity {\\&} explainability in AI.\",\n}\n",
    "authors": [
        "Yejin Bang",
        "Tiezheng Yu",
        "Andrea Madotto",
        "Zhaojiang Lin",
        "Mona Diab",
        "Pascale Fung"
    ],
    "pdf_url": "https://aclanthology.org/2023.trustnlp-1.27.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ebfaf411-be6a-5bd0-a266-28b36fff41f3.pdf",
    "abstract": "Many NLP classification tasks, such as sexism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classification that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classifiers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we fine-tune smaller classification models with the generated data for the task. Empirical results show that our VA-Models surpass multiple baselines by at least 15.56% on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classifiers with explicit human value input improves both inclusivity & explainability in AI.",
    "num_pages": 15
}