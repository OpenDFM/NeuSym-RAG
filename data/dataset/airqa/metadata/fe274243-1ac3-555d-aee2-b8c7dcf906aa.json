{
    "uuid": "fe274243-1ac3-555d-aee2-b8c7dcf906aa",
    "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{tang-etal-2023-understanding,\n    title = \"Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors\",\n    author = \"Tang, Liyan  and\n      Goyal, Tanya  and\n      Fabbri, Alex  and\n      Laban, Philippe  and\n      Xu, Jiacheng  and\n      Yavuz, Semih  and\n      Kryscinski, Wojciech  and\n      Rousseau, Justin  and\n      Durrett, Greg\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.650\",\n    doi = \"10.18653/v1/2023.acl-long.650\",\n    pages = \"11626--11644\",\n    abstract = \"The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems{'} outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights.\",\n}\n",
    "authors": [
        "Liyan Tang",
        "Tanya Goyal",
        "Alex Fabbri",
        "Philippe Laban",
        "Jiacheng Xu",
        "Semih Yavuz",
        "Wojciech Kryscinski",
        "Justin Rousseau",
        "Greg Durrett"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.650.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/fe274243-1ac3-555d-aee2-b8c7dcf906aa.pdf",
    "abstract": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systemsâ€™ outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights.",
    "num_pages": 19
}