{
    "uuid": "8196c686-8213-5efa-953b-c8219ba68e2b",
    "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2024-eval,\n    title = \"{T}-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step\",\n    author = \"Chen, Zehui  and\n      Du, Weihua  and\n      Zhang, Wenwei  and\n      Liu, Kuikun  and\n      Liu, Jiangning  and\n      Zheng, Miao  and\n      Zhuo, Jingming  and\n      Zhang, Songyang  and\n      Lin, Dahua  and\n      Chen, Kai  and\n      Zhao, Feng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.515\",\n    doi = \"10.18653/v1/2024.acl-long.515\",\n    pages = \"9510--9529\",\n    abstract = \"Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool-utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available.\",\n}\n",
    "authors": [
        "Zehui Chen",
        "Weihua Du",
        "Wenwei Zhang",
        "Kuikun Liu",
        "Jiangning Liu",
        "Miao Zheng",
        "Jingming Zhuo",
        "Songyang Zhang",
        "Dahua Lin",
        "Kai Chen",
        "Feng Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.515.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8196c686-8213-5efa-953b-c8219ba68e2b.pdf",
    "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool-utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available.",
    "num_pages": 20
}