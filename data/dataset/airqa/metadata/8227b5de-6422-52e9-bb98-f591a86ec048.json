{
    "uuid": "8227b5de-6422-52e9-bb98-f591a86ec048",
    "title": "CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zou-etal-2024-cqil,\n    title = \"{CQIL}: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers\",\n    author = \"Zou, Longwei  and\n      Wang, Qingyang  and\n      Zhao, Han  and\n      Jiangangkong, Jiangangkong  and\n      Yang, Yi  and\n      Deng, Yangdong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.394\",\n    doi = \"10.18653/v1/2024.acl-long.394\",\n    pages = \"7293--7307\",\n    abstract = \"The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3{\\%} on LLaMA-33B, while maintaining a close level of performance.\",\n}\n",
    "authors": [
        "Longwei Zou",
        "Qingyang Wang",
        "Han Zhao",
        "Jiangangkong Jiangangkong",
        "Yi Yang",
        "Yangdong Deng"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.394.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8227b5de-6422-52e9-bb98-f591a86ec048.pdf",
    "abstract": "The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on LLaMA-33B, while maintaining a close level of performance.",
    "num_pages": 15
}