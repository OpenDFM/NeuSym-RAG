{
    "uuid": "28dadbda-b363-593c-8969-d749717672c4",
    "title": "Enhancing Spanish-Quechua Machine Translation with Pre-Trained Models and Diverse Data Sources: LCT-EHU at AmericasNLP Shared Task",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    "bibtex": "@inproceedings{ahmed-etal-2023-enhancing,\n    title = \"Enhancing {S}panish-{Q}uechua Machine Translation with Pre-Trained Models and Diverse Data Sources: {LCT}-{EHU} at {A}mericas{NLP} Shared Task\",\n    author = \"Ahmed, Nouman  and\n      Flechas Manrique, Natalia  and\n      Petrovi{\\'c}, Antonije\",\n    editor = \"Mager, Manuel  and\n      Ebrahimi, Abteen  and\n      Oncevay, Arturo  and\n      Rice, Enora  and\n      Rijhwani, Shruti  and\n      Palmer, Alexis  and\n      Kann, Katharina\",\n    booktitle = \"Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.americasnlp-1.16\",\n    doi = \"10.18653/v1/2023.americasnlp-1.16\",\n    pages = \"156--162\",\n    abstract = \"We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine translation shared task. We focus on the Spanish-Quechua language pair and explore the usage of different approaches: (1) Obtain new parallel corpora from the literature and legal domains, (2) Compare a high-resource Spanish-English pre-trained MT model with a Spanish-Finnish pre-trained model (with Finnish being chosen as a target language due to its morphological similarity to Quechua), and (3) Explore additional techniques such as copied corpus and back-translation. Overall, we show that the Spanish-Finnish pre-trained model outperforms other setups, while low-quality synthetic data reduces the performance.\",\n}\n",
    "authors": [
        "Nouman Ahmed",
        "Natalia Flechas Manrique",
        "Antonije PetroviÄ‡"
    ],
    "pdf_url": "https://aclanthology.org/2023.americasnlp-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/28dadbda-b363-593c-8969-d749717672c4.pdf",
    "abstract": "We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine translation shared task. We focus on the Spanish-Quechua language pair and explore the usage of different approaches: (1) Obtain new parallel corpora from the literature and legal domains, (2) Compare a high-resource Spanish-English pre-trained MT model with a Spanish-Finnish pre-trained model (with Finnish being chosen as a target language due to its morphological similarity to Quechua), and (3) Explore additional techniques such as copied corpus and back-translation. Overall, we show that the Spanish-Finnish pre-trained model outperforms other setups, while low-quality synthetic data reduces the performance.",
    "num_pages": 7
}