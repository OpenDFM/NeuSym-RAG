{
    "uuid": "205c5fc7-6f2a-5e6d-8f9f-05f964d4967c",
    "title": "Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{yuan-etal-2023-separating,\n    title = \"Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization\",\n    author = \"Yuan, Ruifeng  and\n      Sun, Shichao  and\n      Wang, Zili  and\n      Cao, Ziqiang  and\n      Li, Wenjie\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.479\",\n    doi = \"10.18653/v1/2023.findings-acl.479\",\n    pages = \"7575--7586\",\n    abstract = \"Extractive summarization aims to select a set of salient sentences from the source document to form a summary. Context information has been considered one of the key factors for this task. Meanwhile, there also exist other pattern factors that can identify sentence importance, such as sentence position or certain n-gram tokens. However, such pattern information is only effective in specific datasets or domains and can not be generalized like the context information when there only exists limited data. In this case, current extractive summarization models may suffer from a performance drop when transferring to a new dataset. In this paper, we attempt to apply disentangled representation learning on extractive summarization, and separate the two key factors for the task, context and pattern, for a better generalization ability in the low-resource setting. To achieve this, we propose two groups of losses for encoding and disentangling sentence representations into context representations and pattern representations. In this case, we can either use only the context information in the zero-shot setting or fine-tune the pattern information in the few-shot setting. Experimental results on three summarization datasets from different domains show the effectiveness of our proposed approach.\",\n}\n",
    "authors": [
        "Ruifeng Yuan",
        "Shichao Sun",
        "Zili Wang",
        "Ziqiang Cao",
        "Wenjie Li"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.479.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/205c5fc7-6f2a-5e6d-8f9f-05f964d4967c.pdf",
    "abstract": "Extractive summarization aims to select a set of salient sentences from the source document to form a summary. Context information has been considered one of the key factors for this task. Meanwhile, there also exist other pattern factors that can identify sentence importance, such as sentence position or certain n-gram tokens. However, such pattern information is only effective in specific datasets or domains and can not be generalized like the context information when there only exists limited data. In this case, current extractive summarization models may suffer from a performance drop when transferring to a new dataset. In this paper, we attempt to apply disentangled representation learning on extractive summarization, and separate the two key factors for the task, context and pattern, for a better generalization ability in the low-resource setting. To achieve this, we propose two groups of losses for encoding and disentangling sentence representations into context representations and pattern representations. In this case, we can either use only the context information in the zero-shot setting or fine-tune the pattern information in the few-shot setting. Experimental results on three summarization datasets from different domains show the effectiveness of our proposed approach.",
    "num_pages": 12
}