{
    "uuid": "85e8f01d-9273-5e20-a37a-fb9e82cf2984",
    "title": "A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{ma-etal-2024-non,\n    title = \"A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation\",\n    author = \"Ma, Zhengrui  and\n      Fang, Qingkai  and\n      Zhang, Shaolei  and\n      Guo, Shoutao  and\n      Feng, Yang  and\n      Zhang, Min\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.85\",\n    doi = \"10.18653/v1/2024.acl-long.85\",\n    pages = \"1557--1575\",\n    abstract = \"Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation. These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener. To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2$x$), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework.We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks. The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency. Experimental results show that NAST-S2$x$ outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks. It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28{\\mbox{$\\times$}} decoding speedup in offline generation.\",\n}\n",
    "authors": [
        "Zhengrui Ma",
        "Qingkai Fang",
        "Shaolei Zhang",
        "Shoutao Guo",
        "Yang Feng",
        "Min Zhang"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.85.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/85e8f01d-9273-5e20-a37a-fb9e82cf2984.pdf",
    "abstract": "Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation. These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener. To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2x), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework.We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks. The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency. Experimental results show that NAST-S2x outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks. It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28Ã— decoding speedup in offline generation.",
    "num_pages": 19
}