{
    "uuid": "092c6b68-5a1d-519f-a737-bc4482ae3976",
    "title": "Small is the New Big: Pre-finetuned compact models are better for Asynchronous Active Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{liu-etal-2023-small,\n    title = \"Small is the New Big: Pre-finetuned compact models are better for Asynchronous Active Learning\",\n    author = \"Liu, Dantong  and\n      Pavani, Kaushik  and\n      Dasgupta, Sunny\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.7\",\n    doi = \"10.18653/v1/2023.sustainlp-1.7\",\n    pages = \"110--120\",\n}\n",
    "authors": [
        "Dantong Liu",
        "Kaushik Pavani",
        "Sunny Dasgupta"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/092c6b68-5a1d-519f-a737-bc4482ae3976.pdf",
    "abstract": "We examine the effects of model size and preﬁnetuning in an active learning setting where classiﬁers are trained from scratch on 14 binary and 3 multi-class text classiﬁcation tasks. We make an important observation that, in realistic active learning settings, where the human annotator and the active learning system operate in asynchronous mode, a compact pre-ﬁnetuned 1-layer transformer model with 4.2 million parameters is 30% more label efﬁcient when compared to the larger 24-layer 84 million parameter transformer model. Further, in line with previous studies, we note that pre-ﬁnetuning transformer models on related tasks improves label efﬁciency of downstream tasks by 12%50%. The compact pre-ﬁnetuned model does not require GPUs, making it a viable solution for large-scale real-time inference with cheaper CPU options.",
    "num_pages": 11
}