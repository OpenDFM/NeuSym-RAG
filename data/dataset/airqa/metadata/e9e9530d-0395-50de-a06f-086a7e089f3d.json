{
    "uuid": "e9e9530d-0395-50de-a06f-086a7e089f3d",
    "title": "BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2023-blaser,\n    title = \"{BLASER}: A Text-Free Speech-to-Speech Translation Evaluation Metric\",\n    author = \"Chen, Mingda  and\n      Duquenne, Paul-Ambroise  and\n      Andrews, Pierre  and\n      Kao, Justine  and\n      Mourachko, Alexandre  and\n      Schwenk, Holger  and\n      Costa-juss{\\`a}, Marta R.\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.504\",\n    doi = \"10.18653/v1/2023.acl-long.504\",\n    pages = \"9064--9079\",\n    abstract = \"End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we propose a text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the dependency on ASR systems. BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation. To evaluate our approach, we construct training and evaluation sets from more than 40k human annotations covering seven language directions. The best results of BLASER are achieved by training with supervision from human rating scores. We show that when evaluated at the sentence level, BLASER correlates significantly better with human judgment compared to ASR dependent metrics including ASR-SENTBLEU in all translation directions and ASR-COMET in five of them. Our analysis shows combining speech and text as inputs to BLASER does not increase the correlation with human scores, but best correlations are achieved when using speech, which motivates the goal of our research. Moreover, we show that using ASR for references is detrimental for text-based metrics.\",\n}\n",
    "authors": [
        "Mingda Chen",
        "Paul-Ambroise Duquenne",
        "Pierre Andrews",
        "Justine Kao",
        "Alexandre Mourachko",
        "Holger Schwenk",
        "Marta R. Costa-juss√†"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.504.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/e9e9530d-0395-50de-a06f-086a7e089f3d.pdf",
    "abstract": "End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we propose a text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the dependency on ASR systems. BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation. To evaluate our approach, we construct training and evaluation sets from more than 40k human annotations covering seven language directions. The best results of BLASER are achieved by training with supervision from human rating scores. We show that when evaluated at the sentence level, BLASER correlates significantly better with human judgment compared to ASR dependent metrics including ASR-SENTBLEU in all translation directions and ASR-COMET in five of them. Our analysis shows combining speech and text as inputs to BLASER does not increase the correlation with human scores, but best correlations are achieved when using speech, which motivates the goal of our research. Moreover, we show that using ASR for references is detrimental for text-based metrics.",
    "num_pages": 16
}