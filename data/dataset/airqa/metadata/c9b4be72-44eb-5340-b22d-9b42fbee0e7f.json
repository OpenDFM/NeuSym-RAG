{
    "uuid": "c9b4be72-44eb-5340-b22d-9b42fbee0e7f",
    "title": "Prior Knowledge-Guided Adversarial Training",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
    "bibtex": "@inproceedings{pereira-etal-2024-prior,\n    title = \"Prior Knowledge-Guided Adversarial Training\",\n    author = \"Pereira, Lis  and\n      Cheng, Fei  and\n      She, Wan Jou  and\n      Asahara, Masayuki  and\n      Kobayashi, Ichiro\",\n    editor = \"Zhao, Chen  and\n      Mosbach, Marius  and\n      Atanasova, Pepa  and\n      Goldfarb-Tarrent, Seraphina  and\n      Hase, Peter  and\n      Hosseini, Arian  and\n      Elbayad, Maha  and\n      Pezzelle, Sandro  and\n      Mozes, Maximilian\",\n    booktitle = \"Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.repl4nlp-1.5\",\n    pages = \"51--57\",\n    abstract = \"We introduce a simple yet effective Prior Knowledge-Guided ADVersarial Training (PKG-ADV) algorithm to improve adversarial training for natural language understanding. Our method simply utilizes task-specific label distribution to guide the training process. By prioritizing the use of prior knowledge of labels, we aim to generate more informative adversarial perturbations. We apply our model to several challenging temporal reasoning tasks. Our method enables a more reliable and controllable data training process than relying on randomized adversarial perturbation. Albeit simple, our method achieved significant improvements in these tasks. To facilitate further research, we will release the code and models.\",\n}\n",
    "authors": [
        "Lis Pereira",
        "Fei Cheng",
        "Wan Jou She",
        "Masayuki Asahara",
        "Ichiro Kobayashi"
    ],
    "pdf_url": "https://aclanthology.org/2024.repl4nlp-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/c9b4be72-44eb-5340-b22d-9b42fbee0e7f.pdf",
    "abstract": "We introduce a simple yet effective Prior Knowledge-Guided ADVersarial Training (PKG-ADV) algorithm to improve adversarial training for natural language understanding. Our method simply utilizes task-specific label distribution to guide the training process. By prioritizing the use of prior knowledge of labels, we aim to generate more informative adversarial perturbations. We apply our model to several challenging temporal reasoning tasks. Our method enables a more reliable and controllable data training process than relying on randomized adversarial perturbation. Albeit simple, our method achieved significant improvements in these tasks. To facilitate further research, we will release the code and models.",
    "num_pages": 7
}