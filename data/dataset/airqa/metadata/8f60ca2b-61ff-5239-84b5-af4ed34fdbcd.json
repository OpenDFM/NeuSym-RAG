{
    "uuid": "8f60ca2b-61ff-5239-84b5-af4ed34fdbcd",
    "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zhang-etal-2024-mm,\n    title = \"{MM}-{LLM}s: Recent Advances in {M}ulti{M}odal Large Language Models\",\n    author = \"Zhang, Duzhen  and\n      Yu, Yahan  and\n      Dong, Jiahua  and\n      Li, Chenxing  and\n      Su, Dan  and\n      Chu, Chenhui  and\n      Yu, Dong\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.738\",\n    doi = \"10.18653/v1/2024.findings-acl.738\",\n    pages = \"12401--12430\",\n    abstract = \"In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a [real-time tracking website](https://mm-llms.github.io/) for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.\",\n}\n",
    "authors": [
        "Duzhen Zhang",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Dan Su",
        "Chenhui Chu",
        "Dong Yu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.738.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8f60ca2b-61ff-5239-84b5-af4ed34fdbcd.pdf",
    "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing 126 MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a [real-time tracking website](https://mm-llms.github.io/) for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.",
    "num_pages": 30
}