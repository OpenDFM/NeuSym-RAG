{
    "uuid": "6d10c59f-682c-5768-ba04-3390ee4437b7",
    "title": "CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{ren-etal-2024-codeattack,\n    title = \"{C}ode{A}ttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion\",\n    author = \"Ren, Qibing  and\n      Gao, Chang  and\n      Shao, Jing  and\n      Yan, Junchi  and\n      Tan, Xin  and\n      Lam, Wai  and\n      Ma, Lizhuang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.679\",\n    doi = \"10.18653/v1/2024.findings-acl.679\",\n    pages = \"11437--11452\",\n    abstract = \"The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a new and universal safety vulnerability of these models against code input: CodeAttack bypasses the safety guardrails of all models more than 80{\\%} of the time. We find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures. Furthermore, we give our hypotheses about the success of CodeAttack: the misaligned bias acquired by LLMs during code training, prioritizing code completion over avoiding the potential safety risk. Finally, we analyze potential mitigation measures. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.\",\n}\n",
    "authors": [
        "Qibing Ren",
        "Chang Gao",
        "Jing Shao",
        "Junchi Yan",
        "Xin Tan",
        "Wai Lam",
        "Lizhuang Ma"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.679.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6d10c59f-682c-5768-ba04-3390ee4437b7.pdf",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has brought about remarkable generative capabilities but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a new and universal safety vulnerability of these models against code input: CodeAttack bypasses the safety guardrails of all models more than 80% of the time. We find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures. Furthermore, we give our hypotheses about the success of CodeAttack: the misaligned bias acquired by LLMs during code training, prioritizing code completion over avoiding the potential safety risk. Finally, we analyze potential mitigation measures. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.",
    "num_pages": 16
}