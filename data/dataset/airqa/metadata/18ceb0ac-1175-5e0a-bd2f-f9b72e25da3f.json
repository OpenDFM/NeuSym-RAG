{
    "uuid": "18ceb0ac-1175-5e0a-bd2f-f9b72e25da3f",
    "title": "Rethinking Annotation: Can Language Learners Contribute?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yoo-etal-2023-rethinking,\n    title = \"Rethinking Annotation: Can Language Learners Contribute?\",\n    author = \"Yoo, Haneul  and\n      Putri, Rifki Afina  and\n      Lee, Changyoon  and\n      Lee, Youngin  and\n      Ahn, So-Yeon  and\n      Kang, Dongyeop  and\n      Oh, Alice\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.822\",\n    doi = \"10.18653/v1/2023.acl-long.822\",\n    pages = \"14714--14733\",\n    abstract = \"Researchers have traditionally recruited native speakers to provide annotations for the widely used benchmark datasets. But there are languages for which recruiting native speakers is difficult, and it would help to get learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to the benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional resources (dictionaries and machine-translated sentences), and perform mini-tests to measure their language proficiency. We target three languages, English, Korean, and Indonesian, and four NLP tasks, sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. We find that language learners, especially those with intermediate or advanced language proficiency, are able to provide fairly accurate labels with the help of additional resources. Moreover, we show that data annotation improves learners{'} language proficiency in terms of vocabulary and grammar. The implication of our findings is that broadening the annotation task to include language learners can open up the opportunity to build benchmark datasets for languages for which it is difficult to recruit native speakers.\",\n}\n",
    "authors": [
        "Haneul Yoo",
        "Rifki Afina Putri",
        "Changyoon Lee",
        "Youngin Lee",
        "So-Yeon Ahn",
        "Dongyeop Kang",
        "Alice Oh"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.822.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/18ceb0ac-1175-5e0a-bd2f-f9b72e25da3f.pdf",
    "abstract": "Researchers have traditionally recruited native speakers to provide annotations for the widely used benchmark datasets. But there are languages for which recruiting native speakers is difficult, and it would help to get learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to the benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional resources (dictionaries and machine-translated sentences), and perform mini-tests to measure their language proficiency. We target three languages, English, Korean, and Indonesian, and four NLP tasks, sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. We find that language learners, especially those with intermediate or advanced language proficiency, are able to provide fairly accurate labels with the help of additional resources. Moreover, we show that data annotation improves learnersâ€™ language proficiency in terms of vocabulary and grammar. The implication of our findings is that broadening the annotation task to include language learners can open up the opportunity to build benchmark datasets for languages for which it is difficult to recruit native speakers.",
    "num_pages": 20
}