{
    "uuid": "550dde28-d624-5228-9e6c-2a1264898540",
    "title": "Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
    "bibtex": "@inproceedings{kulshreshtha-rumshisky-2023-reasoning,\n    title = \"Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales\",\n    author = \"Kulshreshtha, Saurabh  and\n      Rumshisky, Anna\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Neves Ribeiro, Danilo  and\n      Wei, Jason\",\n    booktitle = \"Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)\",\n    month = jun,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.nlrse-1.6\",\n    doi = \"10.18653/v1/2023.nlrse-1.6\",\n    pages = \"59--77\",\n    abstract = \"Multi-hop Question Generation is the task of generating questions which require the reader to reason over and combine information spread across multiple passages employing several reasoning steps. Chain-of-thought rationale generation has been shown to improve performance on multi-step reasoning tasks and make model predictions more interpretable. However, few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large-scale manual rationale annotation. In this paper, we introduce a new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime (8- to 128-shot). We propose to annotate a small number of examples following our proposed multi-step rationale schema, treating each reasoning step as a separate task to be performed by a generative language model. We show that our framework leads to improved control over the difficulty of the generated questions and better performance compared to baselines trained without rationales, both on automatic evaluation metrics and in human evaluation. Importantly, we show that this is achievable with a modest model size.\",\n}\n",
    "authors": [
        "Saurabh Kulshreshtha",
        "Anna Rumshisky"
    ],
    "pdf_url": "https://aclanthology.org/2023.nlrse-1.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/550dde28-d624-5228-9e6c-2a1264898540.pdf",
    "abstract": "Multi-hop Question Generation is the task of generating questions which require the reader to reason over and combine information spread across multiple passages employing several reasoning steps. Chain-of-thought rationale generation has been shown to improve performance on multi-step reasoning tasks and make model predictions more interpretable. However, few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large-scale manual rationale annotation. In this paper, we introduce a new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime (8- to 128-shot). We propose to annotate a small number of examples following our proposed multi-step rationale schema, treating each reasoning step as a separate task to be performed by a generative language model. We show that our framework leads to improved control over the difficulty of the generated questions and better performance compared to baselines trained without rationales, both on automatic evaluation metrics and in human evaluation. Importantly, we show that this is achievable with a modest model size.",
    "num_pages": 19
}