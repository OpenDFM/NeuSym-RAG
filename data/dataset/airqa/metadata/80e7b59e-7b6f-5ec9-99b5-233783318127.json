{
    "uuid": "80e7b59e-7b6f-5ec9-99b5-233783318127",
    "title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{peng-etal-2023-token,\n    title = \"Token-Level Self-Evolution Training for Sequence-to-Sequence Learning\",\n    author = \"Peng, Keqin  and\n      Ding, Liang  and\n      Zhong, Qihuang  and\n      Ouyang, Yuanxin  and\n      Rong, Wenge  and\n      Xiong, Zhang  and\n      Tao, Dacheng\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.73\",\n    doi = \"10.18653/v1/2023.acl-short.73\",\n    pages = \"841--850\",\n    abstract = \"Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g. word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal. In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data. SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach. Empirically, SE yields consistent and significant improvements in three tasks, i.e. machine translation, summarization, and grammatical error correction. Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks. Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization.\",\n}\n",
    "authors": [
        "Keqin Peng",
        "Liang Ding",
        "Qihuang Zhong",
        "Yuanxin Ouyang",
        "Wenge Rong",
        "Zhang Xiong",
        "Dacheng Tao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.73.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/80e7b59e-7b6f-5ec9-99b5-233783318127.pdf",
    "abstract": "Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g. word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal. In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data. SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach. Empirically, SE yields consistent and significant improvements in three tasks, i.e. machine translation, summarization, and grammatical error correction. Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks. Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization.",
    "num_pages": 10
}