{
    "uuid": "407bb329-d8e6-58a2-acfc-56c6c3d1f965",
    "title": "MeetingQA: Extractive Question-Answering on Meeting Transcripts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{prasad-etal-2023-meetingqa,\n    title = \"{M}eeting{QA}: Extractive Question-Answering on Meeting Transcripts\",\n    author = \"Prasad, Archiki  and\n      Bui, Trung  and\n      Yoon, Seunghyun  and\n      Deilamsalehy, Hanieh  and\n      Dernoncourt, Franck  and\n      Bansal, Mohit\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.837\",\n    doi = \"10.18653/v1/2023.acl-long.837\",\n    pages = \"15000--15025\",\n    abstract = \"With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising of questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and actively seek discussions, while the answers can be multi-span and distributed across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon.\",\n}\n",
    "authors": [
        "Archiki Prasad",
        "Trung Bui",
        "Seunghyun Yoon",
        "Hanieh Deilamsalehy",
        "Franck Dernoncourt",
        "Mohit Bansal"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.837.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/407bb329-d8e6-58a2-acfc-56c6c3d1f965.pdf",
    "abstract": "With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising of questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and actively seek discussions, while the answers can be multi-span and distributed across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon.",
    "num_pages": 26
}