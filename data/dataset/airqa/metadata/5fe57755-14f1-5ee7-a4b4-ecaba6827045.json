{
    "uuid": "5fe57755-14f1-5ee7-a4b4-ecaba6827045",
    "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{alzahrani-etal-2024-benchmarks,\n    title = \"When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards\",\n    author = \"Alzahrani, Norah  and\n      Alyahya, Hisham  and\n      Alnumay, Yazeed  and\n      AlRashed, Sultan  and\n      Alsubaie, Shaykhah  and\n      Almushayqih, Yousef  and\n      Mirza, Faisal  and\n      Alotaibi, Nouf  and\n      Al-Twairesh, Nora  and\n      Alowisheq, Areeb  and\n      Bari, M Saiful  and\n      Khan, Haidar\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.744\",\n    doi = \"10.18653/v1/2024.acl-long.744\",\n    pages = \"13787--13805\",\n    abstract = \"Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value {---} we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a *hybrid* scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at [https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness).\",\n}\n",
    "authors": [
        "Norah Alzahrani",
        "Hisham Alyahya",
        "Yazeed Alnumay",
        "Sultan AlRashed",
        "Shaykhah Alsubaie",
        "Yousef Almushayqih",
        "Faisal Mirza",
        "Nouf Alotaibi",
        "Nora Al-Twairesh",
        "Areeb Alowisheq",
        "M Saiful Bari",
        "Haidar Khan"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.744.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/5fe57755-14f1-5ee7-a4b4-ecaba6827045.pdf",
    "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value â€” we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a *hybrid* scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at [https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness).",
    "num_pages": 19
}