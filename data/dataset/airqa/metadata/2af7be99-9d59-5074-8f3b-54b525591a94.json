{
    "uuid": "2af7be99-9d59-5074-8f3b-54b525591a94",
    "title": "Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{kao-chen-2024-visualizing,\n    title = \"Visualizing Dialogues: Enhancing Image Selection through Dialogue Understanding with Large Language Models\",\n    author = \"Kao, Chang-Sheng  and\n      Chen, Yun-Nung\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.700\",\n    doi = \"10.18653/v1/2024.findings-acl.700\",\n    pages = \"11777--11788\",\n    abstract = \"For dialogue systems, the utilization of multimodal dialogue responses, as opposed to relying solely on text-only responses, offers the capability to describe different concepts through various modalities. This enhances the effectiveness of communication and elevates the overall conversational experience. However, current methods for dialogue-to-image retrieval are constrained by the capabilities of the pre-trained vision language models (VLMs). They struggle to accurately extract key information from conversations and are unable to handle long-turn conversations. In this paper, we leverage the reasoning capabilities of large language models (LLMs) to predict the potential features that may be present in the images to be shared, based on the dialogue context. This approach allows us to obtain succinct and precise descriptors, thereby improving the performance of text-image retrieval. Experimental results shows that our method outperforms previous approaches significantly in terms of Recall@k.\",\n}\n",
    "authors": [
        "Chang-Sheng Kao",
        "Yun-Nung Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.700.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2af7be99-9d59-5074-8f3b-54b525591a94.pdf",
    "abstract": "For dialogue systems, the utilization of multimodal dialogue responses, as opposed to relying solely on text-only responses, offers the capability to describe different concepts through various modalities. This enhances the effectiveness of communication and elevates the overall conversational experience. However, current methods for dialogue-to-image retrieval are constrained by the capabilities of the pre-trained vision language models (VLMs). They struggle to accurately extract key information from conversations and are unable to handle long-turn conversations. In this paper, we leverage the reasoning capabilities of large language models (LLMs) to predict the potential features that may be present in the images to be shared, based on the dialogue context. This approach allows us to obtain succinct and precise descriptors, thereby improving the performance of text-image retrieval. Experimental results shows that our method outperforms previous approaches significantly in terms of Recall@k.",
    "num_pages": 12
}