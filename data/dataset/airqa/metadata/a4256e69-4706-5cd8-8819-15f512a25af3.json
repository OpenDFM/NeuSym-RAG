{
    "uuid": "a4256e69-4706-5cd8-8819-15f512a25af3",
    "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{zeng-etal-2024-good,\n    title = \"The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation ({RAG})\",\n    author = \"Zeng, Shenglai  and\n      Zhang, Jiankun  and\n      He, Pengfei  and\n      Liu, Yiding  and\n      Xing, Yue  and\n      Xu, Han  and\n      Ren, Jie  and\n      Chang, Yi  and\n      Wang, Shuaiqiang  and\n      Yin, Dawei  and\n      Tang, Jiliang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.267\",\n    doi = \"10.18653/v1/2024.findings-acl.267\",\n    pages = \"4505--4524\",\n    abstract = \"Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs{'} training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.\",\n}\n",
    "authors": [
        "Shenglai Zeng",
        "Jiankun Zhang",
        "Pengfei He",
        "Yiding Liu",
        "Yue Xing",
        "Han Xu",
        "Jie Ren",
        "Yi Chang",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Jiliang Tang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.267.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/a4256e69-4706-5cd8-8819-15f512a25af3.pdf",
    "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMsâ€™ training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.",
    "num_pages": 20
}