{
    "uuid": "58f54684-846e-5bed-968c-6f05b61c950e",
    "title": "Generalized Category Discovery with Large Language Models in the Loop",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{an-etal-2024-generalized,\n    title = \"Generalized Category Discovery with Large Language Models in the Loop\",\n    author = \"An, Wenbin  and\n      Shi, Wenkai  and\n      Tian, Feng  and\n      Lin, Haonan  and\n      Wang, QianYing  and\n      Wu, Yaqiang  and\n      Cai, Mingxiang  and\n      Wang, Luyan  and\n      Chen, Yan  and\n      Zhu, Haiping  and\n      Chen, Ping\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.512\",\n    doi = \"10.18653/v1/2024.findings-acl.512\",\n    pages = \"8653--8665\",\n    abstract = \"Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at https://github.com/Lackel/LOOP.\",\n}\n",
    "authors": [
        "Wenbin An",
        "Wenkai Shi",
        "Feng Tian",
        "Haonan Lin",
        "QianYing Wang",
        "Yaqiang Wu",
        "Mingxiang Cai",
        "Luyan Wang",
        "Yan Chen",
        "Haiping Zhu",
        "Ping Chen"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.512.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/58f54684-846e-5bed-968c-6f05b61c950e.pdf",
    "abstract": "Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate the above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. Code and data are available at https://github.com/Lackel/LOOP.",
    "num_pages": 13
}