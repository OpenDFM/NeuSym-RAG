{
    "uuid": "8de5094f-3f78-581f-bfb4-7de363a14bad",
    "title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-noisy,\n    title = \"Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning\",\n    author = \"Wang, Ruijie  and\n      Li, Baoyu  and\n      Lu, Yichen  and\n      Sun, Dachun  and\n      Li, Jinning  and\n      Yan, Yuchen  and\n      Liu, Shengzhong  and\n      Tong, Hanghang  and\n      Abdelzaher, Tarek\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.153\",\n    doi = \"10.18653/v1/2023.findings-acl.153\",\n    pages = \"2440--2457\",\n    abstract = \"This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call label posterior) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph.\",\n}\n",
    "authors": [
        "Ruijie Wang",
        "Baoyu Li",
        "Yichen Lu",
        "Dachun Sun",
        "Jinning Li",
        "Yuchen Yan",
        "Shengzhong Liu",
        "Hanghang Tong",
        "Tarek Abdelzaher"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.153.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/8de5094f-3f78-581f-bfb4-7de363a14bad.pdf",
    "abstract": "This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call label posterior) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph.",
    "num_pages": 18
}