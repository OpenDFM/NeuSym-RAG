{
    "uuid": "0fae4050-6550-5468-ab5b-13fc489e0119",
    "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{bhagavatula-etal-2023-i2d2,\n    title = \"{I}2{D}2: Inductive Knowledge Distillation with {N}euro{L}ogic and Self-Imitation\",\n    author = \"Bhagavatula, Chandra  and\n      Hwang, Jena D.  and\n      Downey, Doug  and\n      Le Bras, Ronan  and\n      Lu, Ximing  and\n      Qin, Lianhui  and\n      Sakaguchi, Keisuke  and\n      Swayamdipta, Swabha  and\n      West, Peter  and\n      Choi, Yejin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.535\",\n    doi = \"10.18653/v1/2023.acl-long.535\",\n    pages = \"9614--9630\",\n    abstract = \"Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model{'}s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.\",\n}\n",
    "authors": [
        "Chandra Bhagavatula",
        "Jena D. Hwang",
        "Doug Downey",
        "Ronan Le Bras",
        "Ximing Lu",
        "Lianhui Qin",
        "Keisuke Sakaguchi",
        "Swabha Swayamdipta",
        "Peter West",
        "Yejin Choi"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.535.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/0fae4050-6550-5468-ab5b-13fc489e0119.pdf",
    "abstract": "Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the modelâ€™s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.",
    "num_pages": 17
}