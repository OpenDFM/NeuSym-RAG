{
    "uuid": "c5be5283-e5b6-59aa-a9a9-02b79a608e98",
    "title": "Visual Coherence Loss for Coherent and Visually Grounded Story Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
    "bibtex": "@inproceedings{hong-etal-2023-visual-coherence,\n    title = \"Visual Coherence Loss for Coherent and Visually Grounded Story Generation\",\n    author = \"Hong, Xudong  and\n      Demberg, Vera  and\n      Sayeed, Asad  and\n      Zheng, Qiankun  and\n      Schiele, Bernt\",\n    editor = \"Can, Burcu  and\n      Mozes, Maximilian  and\n      Cahyawijaya, Samuel  and\n      Saphra, Naomi  and\n      Kassner, Nora  and\n      Ravfogel, Shauli  and\n      Ravichander, Abhilasha  and\n      Zhao, Chen  and\n      Augenstein, Isabelle  and\n      Rogers, Anna  and\n      Cho, Kyunghyun  and\n      Grefenstette, Edward  and\n      Voita, Lena\",\n    booktitle = \"Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.repl4nlp-1.27\",\n    doi = \"10.18653/v1/2023.repl4nlp-1.27\",\n    pages = \"334--346\",\n}\n",
    "authors": [
        "Xudong Hong",
        "Vera Demberg",
        "Asad Sayeed",
        "Qiankun Zheng",
        "Bernt Schiele"
    ],
    "pdf_url": "https://aclanthology.org/2023.repl4nlp-1.27.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c5be5283-e5b6-59aa-a9a9-02b79a608e98.pdf",
    "abstract": "Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) character representations should enable us to find instances of the same characters and distinguish different characters. In this paper, we propose a loss function inspired by a linguistic theory of coherence for selfsupervised learning for image sequence representations. We further propose combining features from an object and a face detector to construct stronger character features. To evaluate input-output relevance that current referencebased metrics donâ€™t measure, we propose a character matching metric to check whether the models generate referring expressions correctly for characters in input image sequences. Experiments on a visual story generation dataset show that our proposed features and loss function are effective for generating more coherent and visually grounded stories.",
    "num_pages": 15
}