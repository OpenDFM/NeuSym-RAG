{
    "uuid": "194152bd-3816-5a38-8d19-49724912826e",
    "title": "Identifying and Mitigating Annotation Bias in Natural Language Understanding using Causal Mediation Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{sae-lim-etal-2024-identifying,\n    title = \"Identifying and Mitigating Annotation Bias in Natural Language Understanding using Causal Mediation Analysis\",\n    author = \"Sae Lim, Sitiporn  and\n      Udomcharoenchaikit, Can  and\n      Limkonchotiwat, Peerat  and\n      Chuangsuwanich, Ekapol  and\n      Nutanong, Sarana\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.686\",\n    doi = \"10.18653/v1/2024.findings-acl.686\",\n    pages = \"11548--11563\",\n    abstract = \"NLU models have achieved promising results on standard benchmarks. Despite state-of-the-art accuracy, analysis reveals that many models make predictions using annotation bias rather than the properties we intend the model to learn. Consequently, these models perform poorly on out-of-distribution datasets. Recent advances in bias mitigation show that annotation bias can be alleviated through fine-tuning debiasing objectives. In this paper, we apply causal mediation analysis to gauge how much each model component mediates annotation biases. Using the knowledge from the causal analysis, we improve the model{'}s robustness against annotation bias through two bias mitigation methods: causal-grounded masking and gradient unlearning. Causal analysis reveals that biases concentrated in specific components, even after employing other training-time debiasing techniques. Manipulating these components by masking out neurons{'} activations or updating specific weight blocks both demonstrably improve robustness against annotation artifacts.\",\n}\n",
    "authors": [
        "Sitiporn Sae Lim",
        "Can Udomcharoenchaikit",
        "Peerat Limkonchotiwat",
        "Ekapol Chuangsuwanich",
        "Sarana Nutanong"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.686.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/194152bd-3816-5a38-8d19-49724912826e.pdf",
    "abstract": "NLU models have achieved promising results on standard benchmarks. Despite state-of-the-art accuracy, analysis reveals that many models make predictions using annotation bias rather than the properties we intend the model to learn. Consequently, these models perform poorly on out-of-distribution datasets. Recent advances in bias mitigation show that annotation bias can be alleviated through fine-tuning debiasing objectives. In this paper, we apply causal mediation analysis to gauge how much each model component mediates annotation biases. Using the knowledge from the causal analysis, we improve the model’s robustness against annotation bias through two bias mitigation methods: causal-grounded masking and gradient unlearning. Causal analysis reveals that biases concentrated in specific components, even after employing other training-time debiasing techniques. Manipulating these components by masking out neurons’ activations or updating specific weight blocks both demonstrably improve robustness against annotation artifacts.",
    "num_pages": 16
}