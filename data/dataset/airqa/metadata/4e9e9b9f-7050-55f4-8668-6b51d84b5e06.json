{
    "uuid": "4e9e9b9f-7050-55f4-8668-6b51d84b5e06",
    "title": "Poor-Supervised Evaluation for SuperLLM via Mutual Consistency",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{yuan-etal-2024-poor,\n    title = \"Poor-Supervised Evaluation for {S}uper{LLM} via Mutual Consistency\",\n    author = \"Yuan, Peiwen  and\n      Feng, Shaoxiong  and\n      Li, Yiwei  and\n      Wang, Xinglin  and\n      Pan, Boyuan  and\n      Wang, Heda  and\n      Hu, Yao  and\n      Li, Kan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.690\",\n    doi = \"10.18653/v1/2024.findings-acl.690\",\n    pages = \"11614--11627\",\n    abstract = \"The guidance from capability evaluations has greatly propelled the progress of human society and the development of Artificial Intelligence. However, as LLMs evolve, it becomes challenging to construct evaluation benchmark with accurate labels for SuperLLMs whose capabilities approach or even surpass those of humans. To credibly conduct poor-supervised evaluation without accurate labels, we first prove that the consistency between the model under evaluation and the reference model, when their prediction distributions are independent and the sample size is infinite, can equivalently assess the true capabilities of the model to be evaluated. However, using either humans or LLMs as the reference model cannot sufficiently meet the conditions, for which we propose the PEEM algorithm. By treating all models under evaluation as reference models, PEEM alternately optimizes model weights and filters reference models based on EM algorithm to maximally alleviate the insufficiency of the conditions. Comprehensive experiments across 3 types of tasks with 16 mainstream LLMs validate the efficiency, universality, and effectiveness of PEEM. More generally, PEEM has advanced the evaluation paradigm evolution from human-centric to human{\\&}model-centric, alleviating the limitations of human capabilities for evaluating SuperLLMs.\",\n}\n",
    "authors": [
        "Peiwen Yuan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Boyuan Pan",
        "Heda Wang",
        "Yao Hu",
        "Kan Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.690.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4e9e9b9f-7050-55f4-8668-6b51d84b5e06.pdf",
    "abstract": "The guidance from capability evaluations has greatly propelled the progress of human society and the development of Artificial Intelligence. However, as LLMs evolve, it becomes challenging to construct evaluation benchmark with accurate labels for SuperLLMs whose capabilities approach or even surpass those of humans. To credibly conduct poor-supervised evaluation without accurate labels, we first prove that the consistency between the model under evaluation and the reference model, when their prediction distributions are independent and the sample size is infinite, can equivalently assess the true capabilities of the model to be evaluated. However, using either humans or LLMs as the reference model cannot sufficiently meet the conditions, for which we propose the PEEM algorithm. By treating all models under evaluation as reference models, PEEM alternately optimizes model weights and filters reference models based on EM algorithm to maximally alleviate the insufficiency of the conditions. Comprehensive experiments across 3 types of tasks with 16 mainstream LLMs validate the efficiency, universality, and effectiveness of PEEM. More generally, PEEM has advanced the evaluation paradigm evolution from human-centric to human&model-centric, alleviating the limitations of human capabilities for evaluating SuperLLMs.",
    "num_pages": 14
}