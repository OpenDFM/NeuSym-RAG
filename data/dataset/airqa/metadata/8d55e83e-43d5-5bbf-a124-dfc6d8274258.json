{
    "uuid": "8d55e83e-43d5-5bbf-a124-dfc6d8274258",
    "title": "Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{pan-etal-2024-finding,\n    title = \"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers\",\n    author = \"Pan, Haowen  and\n      Cao, Yixin  and\n      Wang, Xiaozhi  and\n      Yang, Xun  and\n      Wang, Meng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.60\",\n    doi = \"10.18653/v1/2024.findings-acl.60\",\n    pages = \"1012--1037\",\n    abstract = \"Understanding the internal mechanisms by which multi-modal large language models (LLMs) interpret different modalities and integrate cross-modal representations is becoming increasingly critical for continuous improvements in both academia and industry. In this paper, we propose a novel method to identify key neurons for interpretability {---} how multi-modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination. For rationale of our design, we provide theoretical assumption. For empirical evaluation, we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect, to shed light for future research.\",\n}\n",
    "authors": [
        "Haowen Pan",
        "Yixin Cao",
        "Xiaozhi Wang",
        "Xun Yang",
        "Meng Wang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.60.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8d55e83e-43d5-5bbf-a124-dfc6d8274258.pdf",
    "abstract": "Understanding the internal mechanisms by which multi-modal large language models (LLMs) interpret different modalities and integrate cross-modal representations is becoming increasingly critical for continuous improvements in both academia and industry. In this paper, we propose a novel method to identify key neurons for interpretability â€” how multi-modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination. For rationale of our design, we provide theoretical assumption. For empirical evaluation, we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect, to shed light for future research.",
    "num_pages": 26
}