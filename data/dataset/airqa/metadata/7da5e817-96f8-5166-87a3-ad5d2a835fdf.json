{
    "uuid": "7da5e817-96f8-5166-87a3-ad5d2a835fdf",
    "title": "Latxa: An Open Language Model and Evaluation Suite for Basque",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{etxaniz-etal-2024-latxa,\n    title = \"Latxa: An Open Language Model and Evaluation Suite for {B}asque\",\n    author = \"Etxaniz, Julen  and\n      Sainz, Oscar  and\n      Miguel, Naiara  and\n      Aldabe, Itziar  and\n      Rigau, German  and\n      Agirre, Eneko  and\n      Ormazabal, Aitor  and\n      Artetxe, Mikel  and\n      Soroa, Aitor\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.799\",\n    doi = \"10.18653/v1/2024.acl-long.799\",\n    pages = \"14952--14972\",\n    abstract = \"We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.\",\n}\n",
    "authors": [
        "Julen Etxaniz",
        "Oscar Sainz",
        "Naiara Miguel",
        "Itziar Aldabe",
        "German Rigau",
        "Eneko Agirre",
        "Aitor Ormazabal",
        "Mikel Artetxe",
        "Aitor Soroa"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.799.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7da5e817-96f8-5166-87a3-ad5d2a835fdf.pdf",
    "abstract": "We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.",
    "num_pages": 21
}