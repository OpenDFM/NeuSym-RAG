{
    "uuid": "474017ab-945f-5533-aedd-d23aba863b66",
    "title": "The Role of Global and Local Context in Named Entity Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{amalvy-etal-2023-role,\n    title = \"The Role of Global and Local Context in Named Entity Recognition\",\n    author = \"Amalvy, Arthur  and\n      Labatut, Vincent  and\n      Dufour, Richard\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.62\",\n    doi = \"10.18653/v1/2023.acl-short.62\",\n    pages = \"714--722\",\n    abstract = \"Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.\",\n}\n",
    "authors": [
        "Arthur Amalvy",
        "Vincent Labatut",
        "Richard Dufour"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.62.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/474017ab-945f-5533-aedd-d23aba863b66.pdf",
    "abstract": "Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context.",
    "num_pages": 9
}