{
    "uuid": "c283fadf-5028-5a24-9004-efb98f193534",
    "title": "Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{qian-etal-2023-sentiment,\n    title = \"Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis\",\n    author = \"Qian, Fan  and\n      Han, Jiqing  and\n      He, Yongjun  and\n      Zheng, Tieran  and\n      Zheng, Guibin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.821\",\n    doi = \"10.18653/v1/2023.findings-acl.821\",\n    pages = \"12966--12978\",\n    abstract = \"Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal behavior, SKESL conducts sentiment word masking and predicts fine-grained word sentiment intensity, so as to embed sentiment information at the word level into pre-trained multimodal representation. In addition, a non-verbal injection method is also proposed to integrate non-verbal information into the word semantics. Experiments on two standard benchmarks of MSA clearly show that SKESL significantly outperforms the baseline, and achieves new State-Of-The-Art (SOTA) results.\",\n}\n",
    "authors": [
        "Fan Qian",
        "Jiqing Han",
        "Yongjun He",
        "Tieran Zheng",
        "Guibin Zheng"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.821.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c283fadf-5028-5a24-9004-efb98f193534.pdf",
    "abstract": "Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal behavior, SKESL conducts sentiment word masking and predicts fine-grained word sentiment intensity, so as to embed sentiment information at the word level into pre-trained multimodal representation. In addition, a non-verbal injection method is also proposed to integrate non-verbal information into the word semantics. Experiments on two standard benchmarks of MSA clearly show that SKESL significantly outperforms the baseline, and achieves new State-Of-The-Art (SOTA) results.",
    "num_pages": 13
}