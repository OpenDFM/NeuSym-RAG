{
    "uuid": "d1a95159-8f9e-5f7d-869a-a144e31e2be6",
    "title": "Few-shot Spanish-Aymara Machine Translation Using English-Aymara Lexicon",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    "bibtex": "@inproceedings{tan-2023-shot,\n    title = \"Few-shot {S}panish-{A}ymara Machine Translation Using {E}nglish-{A}ymara Lexicon\",\n    author = \"Gillin, Nat  and\n      Gummibaerhausen, Brian\",\n    editor = \"Mager, Manuel  and\n      Ebrahimi, Abteen  and\n      Oncevay, Arturo  and\n      Rice, Enora  and\n      Rijhwani, Shruti  and\n      Palmer, Alexis  and\n      Kann, Katharina\",\n    booktitle = \"Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.americasnlp-1.18\",\n    doi = \"10.18653/v1/2023.americasnlp-1.18\",\n    pages = \"168--172\",\n    abstract = \"This paper presents the experiments to train a Spanish-Aymara machine translation model for the AmericasNLP 2023 Machine Translation shared task. We included the English-Aymara GlobalVoices corpus and an English-Aymara lexicon to train the model and limit our training resources to train the model in a {\\textbackslash}textit{few-shot} manner.\",\n}\n",
    "authors": [
        "Nat Gillin",
        "Brian Gummibaerhausen"
    ],
    "pdf_url": "https://aclanthology.org/2023.americasnlp-1.18.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d1a95159-8f9e-5f7d-869a-a144e31e2be6.pdf",
    "abstract": "This paper presents the experiments to train a Spanish-Aymara machine translation model for the AmericasNLP 2023 Machine Translation shared task. We included the English-Aymara GlobalVoices corpus and an English-Aymara lexicon to train the model and limit our training resources to train the model in a \\textit{few-shot} manner.",
    "num_pages": 5
}