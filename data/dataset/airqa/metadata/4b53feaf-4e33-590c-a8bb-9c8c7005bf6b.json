{
    "uuid": "4b53feaf-4e33-590c-a8bb-9c8c7005bf6b",
    "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{babe-etal-2024-studenteval,\n    title = \"{S}tudent{E}val: A Benchmark of Student-Written Prompts for Large Language Models of Code\",\n    author = \"Babe, Hannah  and\n      Nguyen, Sydney  and\n      Zi, Yangtian  and\n      Guha, Arjun  and\n      Feldman, Molly  and\n      Anderson, Carolyn\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.501\",\n    doi = \"10.18653/v1/2024.findings-acl.501\",\n    pages = \"8452--8474\",\n    abstract = \"Code LLMs have the potential to make it easier for non-experts to understand and write code. However, current CodeLLM benchmarks rely on a single expert-written prompt per problem, making it hard to generalize their success to non-expert users. In this paper, we present a new natural-language-to-code benchmark of prompts written by a key population of non-experts: beginning programmers. StudentEval contains 1,749 prompts written by 80 students who have only completed one introductory Python course. StudentEval contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. We use StudentEval to evaluate 12 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. Our analysis of student prompting strategies reveals that nondeterministic LLM sampling can mislead students about the quality of their descriptions, a finding with key implications for Code LLMs in education.\",\n}\n",
    "authors": [
        "Hannah Babe",
        "Sydney Nguyen",
        "Yangtian Zi",
        "Arjun Guha",
        "Molly Feldman",
        "Carolyn Anderson"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.501.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/4b53feaf-4e33-590c-a8bb-9c8c7005bf6b.pdf",
    "abstract": "Code LLMs have the potential to make it easier for non-experts to understand and write code. However, current CodeLLM benchmarks rely on a single expert-written prompt per problem, making it hard to generalize their success to non-expert users. In this paper, we present a new natural-language-to-code benchmark of prompts written by a key population of non-experts: beginning programmers. StudentEval contains 1,749 prompts written by 80 students who have only completed one introductory Python course. StudentEval contains numerous non-expert prompts describing the same problem, enabling exploration of key factors in prompt success. We use StudentEval to evaluate 12 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks. Our analysis of student prompting strategies reveals that nondeterministic LLM sampling can mislead students about the quality of their descriptions, a finding with key implications for Code LLMs in education.",
    "num_pages": 23
}