{
    "uuid": "be6840c2-ed43-531d-87cb-e048c4c6eb59",
    "title": "Graph-Structured Speculative Decoding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{gong-etal-2024-graph,\n    title = \"Graph-Structured Speculative Decoding\",\n    author = \"Gong, Zhuocheng  and\n      Liu, Jiahao  and\n      Wang, Ziyue  and\n      Wu, Pengfei  and\n      Wang, Jingang  and\n      Cai, Xunliang  and\n      Zhao, Dongyan  and\n      Yan, Rui\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.677\",\n    doi = \"10.18653/v1/2024.findings-acl.677\",\n    pages = \"11404--11415\",\n    abstract = \"Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.70$\\times$ to 1.94 $\\times$, significantly surpassing standard speculative decoding.\",\n}\n",
    "authors": [
        "Zhuocheng Gong",
        "Jiahao Liu",
        "Ziyue Wang",
        "Pengfei Wu",
        "Jingang Wang",
        "Xunliang Cai",
        "Dongyan Zhao",
        "Rui Yan"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.677.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/be6840c2-ed43-531d-87cb-e048c4c6eb59.pdf",
    "abstract": "Speculative decoding has emerged as a promising technique to accelerate the inference of Large Language Models (LLMs) by employing a small language model to draft a hypothesis sequence, which is then validated by the LLM. The effectiveness of this approach heavily relies on the balance between performance and efficiency of the draft model. In our research, we focus on enhancing the proportion of draft tokens that are accepted to the final output by generating multiple hypotheses instead of just one. This allows the LLM more options to choose from and select the longest sequence that meets its standards. Our analysis reveals that hypotheses produced by the draft model share many common token sequences, suggesting a potential for optimizing computation. Leveraging this observation, we introduce an innovative approach utilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This structure enables us to efficiently predict and merge recurring token sequences, vastly reducing the computational demands of the draft model. We term this approach Graph-structured Speculative Decoding (GSD). We apply GSD across a range of LLMs, including a 70-billion parameter LLaMA-2 model, and observe a remarkable speedup of 1.70× to 1.94 ×, significantly surpassing standard speculative decoding.",
    "num_pages": 12
}