{
    "uuid": "f636ff13-43ac-58d0-a68b-27870c686607",
    "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{li-etal-2023-autoconv,\n    title = \"{A}uto{C}onv: Automatically Generating Information-seeking Conversations with Large Language Models\",\n    author = \"Li, Siheng  and\n      Yang, Cheng  and\n      Yin, Yichun  and\n      Zhu, Xinyu  and\n      Cheng, Zesen  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Yang, Yujiu\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.149\",\n    doi = \"10.18653/v1/2023.acl-short.149\",\n    pages = \"1751--1762\",\n    abstract = \"Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.\",\n}\n",
    "authors": [
        "Siheng Li",
        "Cheng Yang",
        "Yichun Yin",
        "Xinyu Zhu",
        "Zesen Cheng",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Yujiu Yang"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.149.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/f636ff13-43ac-58d0-a68b-27870c686607.pdf",
    "abstract": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.",
    "num_pages": 12
}