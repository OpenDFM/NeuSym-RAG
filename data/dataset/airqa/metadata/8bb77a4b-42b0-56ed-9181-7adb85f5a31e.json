{
    "uuid": "8bb77a4b-42b0-56ed-9181-7adb85f5a31e",
    "title": "Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
    "bibtex": "@inproceedings{yan-etal-2024-modeling,\n    title = \"Modeling Complex Interactions in Long Documents for Aspect-Based Sentiment Analysis\",\n    author = \"Yan, Zehong  and\n      Hsu, Wynne  and\n      Lee, Mong-Li  and\n      Bartram-Shaw, David\",\n    editor = \"De Clercq, Orph{\\'e}e  and\n      Barriere, Valentin  and\n      Barnes, Jeremy  and\n      Klinger, Roman  and\n      Sedoc, Jo{\\~a}o  and\n      Tafreshi, Shabnam\",\n    booktitle = \"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, {\\&} Social Media Analysis\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.wassa-1.3\",\n    doi = \"10.18653/v1/2024.wassa-1.3\",\n    pages = \"23--34\",\n    abstract = \"The growing number of online articles and reviews necessitates innovative techniques for document-level aspect-based sentiment analysis. Capturing the context in which an aspect is mentioned is crucial. Existing models have focused on relatively short reviews and may fail to consider distant contextual information. This is especially so in longer documents where an aspect may be referred to in multiple ways across dispersed sentences. This work introduces a hierarchical Transformer-based architecture that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues. Experimental results show that the proposed architecture outperforms state-of-the-art methods for document-level aspect-based sentiment classification. We also demonstrate the potential applicability of our approach for long document trust prediction.\",\n}\n",
    "authors": [
        "Zehong Yan",
        "Wynne Hsu",
        "Mong-Li Lee",
        "David Bartram-Shaw"
    ],
    "pdf_url": "https://aclanthology.org/2024.wassa-1.3.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/8bb77a4b-42b0-56ed-9181-7adb85f5a31e.pdf",
    "abstract": "The growing number of online articles and reviews necessitates innovative techniques for document-level aspect-based sentiment analysis. Capturing the context in which an aspect is mentioned is crucial. Existing models have focused on relatively short reviews and may fail to consider distant contextual information. This is especially so in longer documents where an aspect may be referred to in multiple ways across dispersed sentences. This work introduces a hierarchical Transformer-based architecture that encodes information at different level of granularities with attention aggregation mechanisms to learn the local and global aspect-specific document representations. For empirical validation, we curate two datasets of long documents: one on social issues, and another covering various topics involving trust-related issues. Experimental results show that the proposed architecture outperforms state-of-the-art methods for document-level aspect-based sentiment classification. We also demonstrate the potential applicability of our approach for long document trust prediction.",
    "num_pages": 12
}