{
    "uuid": "2c220706-f6b9-5c73-88e8-c5fbdbbf1d55",
    "title": "Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{chimoto-etal-2024-critical,\n    title = \"Critical Learning Periods: Leveraging Early Training Dynamics for Efficient Data Pruning\",\n    author = \"Chimoto, Everlyn  and\n      Gala, Jay  and\n      Ahia, Orevaoghene  and\n      Kreutzer, Julia  and\n      Bassett, Bruce  and\n      Hooker, Sara\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.560\",\n    doi = \"10.18653/v1/2024.findings-acl.560\",\n    pages = \"9407--9426\",\n    abstract = \"Neural Machine Translation models are extremely data and compute-hungry. However, not all datapoints contribute equally to model training and generalization. Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significantdrop in model performance. In this paper, we propose a new data pruning technique: CheckpointsAcross Time (CAT ), that leverages early model training dynamics to identify the most relevantdata points for model performance. We benchmark CAT against several data pruning techniquesincluding COMET-QE, LASER and LaBSE. We find that CAT outperforms the benchmarks onIndo-European languages on multiple test sets. When applied to English-German, English-Frenchand English-Swahili translation tasks, CAT achieves comparable performance to using the fulldataset, while pruning up to 50{\\%} of training data. We inspect the data points that CAT selectsand find that it tends to favour longer sentences and sentences with unique or rare words.\",\n}\n",
    "authors": [
        "Everlyn Chimoto",
        "Jay Gala",
        "Orevaoghene Ahia",
        "Julia Kreutzer",
        "Bruce Bassett",
        "Sara Hooker"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.560.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2c220706-f6b9-5c73-88e8-c5fbdbbf1d55.pdf",
    "abstract": "Neural Machine Translation models are extremely data and compute-hungry. However, not all datapoints contribute equally to model training and generalization. Data pruning to remove the low-value data points has the benefit of drastically reducing the compute budget without significantdrop in model performance. In this paper, we propose a new data pruning technique: CheckpointsAcross Time (CAT ), that leverages early model training dynamics to identify the most relevantdata points for model performance. We benchmark CAT against several data pruning techniquesincluding COMET-QE, LASER and LaBSE. We find that CAT outperforms the benchmarks onIndo-European languages on multiple test sets. When applied to English-German, English-Frenchand English-Swahili translation tasks, CAT achieves comparable performance to using the fulldataset, while pruning up to 50% of training data. We inspect the data points that CAT selectsand find that it tends to favour longer sentences and sentences with unique or rare words.",
    "num_pages": 20
}