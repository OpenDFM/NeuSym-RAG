{
    "uuid": "ca267e63-8ce0-5b34-8c63-bd92a3cec88a",
    "title": "It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{subramonian-etal-2023-takes,\n    title = \"It Takes Two to Tango: Navigating Conceptualizations of {NLP} Tasks and Measurements of Performance\",\n    author = \"Subramonian, Arjun  and\n      Yuan, Xingdi  and\n      Daum{\\'e} III, Hal  and\n      Blodgett, Su Lin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.202\",\n    doi = \"10.18653/v1/2023.findings-acl.202\",\n    pages = \"3234--3279\",\n    abstract = \"Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations.\",\n}\n",
    "authors": [
        "Arjun Subramonian",
        "Xingdi Yuan",
        "Hal Daum√© III",
        "Su Lin Blodgett"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.202.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/ca267e63-8ce0-5b34-8c63-bd92a3cec88a.pdf",
    "abstract": "Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations.",
    "num_pages": 46
}