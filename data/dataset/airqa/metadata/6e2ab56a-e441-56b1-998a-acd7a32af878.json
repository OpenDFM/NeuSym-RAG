{
    "uuid": "6e2ab56a-e441-56b1-998a-acd7a32af878",
    "title": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{gao-etal-2024-two,\n    title = \"Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models\",\n    author = \"Gao, Yifu  and\n      Qiao, Linbo  and\n      Kan, Zhigang  and\n      Wen, Zhihua  and\n      He, Yongquan  and\n      Li, Dongsheng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.401\",\n    doi = \"10.18653/v1/2024.findings-acl.401\",\n    pages = \"6719--6734\",\n    abstract = \"Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM{'}s intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results on two widely used datasets demonstrate the superiority of our model.\",\n}\n",
    "authors": [
        "Yifu Gao",
        "Linbo Qiao",
        "Zhigang Kan",
        "Zhihua Wen",
        "Yongquan He",
        "Dongsheng Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.401.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/6e2ab56a-e441-56b1-998a-acd7a32af878.pdf",
    "abstract": "Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLMâ€™s intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results on two widely used datasets demonstrate the superiority of our model.",
    "num_pages": 16
}