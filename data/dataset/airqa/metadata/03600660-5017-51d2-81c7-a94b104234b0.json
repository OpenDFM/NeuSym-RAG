{
    "uuid": "03600660-5017-51d2-81c7-a94b104234b0",
    "title": "OpenRT: An Open-source Framework for Reasoning Over Tabular Data",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "bibtex": "@inproceedings{zhao-etal-2023-openrt,\n    title = \"{O}pen{RT}: An Open-source Framework for Reasoning Over Tabular Data\",\n    author = \"Zhao, Yilun  and\n      Mi, Boyu  and\n      Qi, Zhenting  and\n      Nan, Linyong  and\n      Guo, Minghao  and\n      Cohan, Arman  and\n      Radev, Dragomir\",\n    editor = \"Bollegala, Danushka  and\n      Huang, Ruihong  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.32\",\n    doi = \"10.18653/v1/2023.acl-demo.32\",\n    pages = \"336--347\",\n    abstract = \"There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.\",\n}\n",
    "authors": [
        "Yilun Zhao",
        "Boyu Mi",
        "Zhenting Qi",
        "Linyong Nan",
        "Minghao Guo",
        "Arman Cohan",
        "Dragomir Radev"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-demo.32.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/03600660-5017-51d2-81c7-a94b104234b0.pdf",
    "abstract": "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.",
    "num_pages": 12
}