{
    "uuid": "cdecf855-9e0d-52ef-b409-d85405ad5ca7",
    "title": "SummEQuAL: Summarization Evaluation via Question Answering using Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)",
    "bibtex": "@inproceedings{liu-etal-2024-summequal,\n    title = \"{S}umm{EQ}u{AL}: Summarization Evaluation via Question Answering using Large Language Models\",\n    author = \"Liu, Junyuan  and\n      Shi, Zhengyan  and\n      Lipani, Aldo\",\n    editor = \"Dalvi Mishra, Bhavana  and\n      Durrett, Greg  and\n      Jansen, Peter  and\n      Lipkin, Ben  and\n      Neves Ribeiro, Danilo  and\n      Wong, Lionel  and\n      Ye, Xi  and\n      Zhao, Wenting\",\n    booktitle = \"Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.nlrse-1.5\",\n    pages = \"46--55\",\n    abstract = \"Summarization is hard to evaluate due to its diverse and abstract nature. Although N-gram-based metrics like BLEU and ROUGE are prevalent, they often do not align well with human evaluations. While model-based alternatives such as BERTScore improve, they typically require extensive labelled data. The advent of Large Language Models (LLMs) presents a promising avenue for evaluation. To this end, we introduce SummEQuAL, a novel content-based framework using LLMs for unified, reproducible summarization evaluation. SummEQuAL evaluates summaries by comparing their content with the source document, employing a question-answering approach to gauge both recall and precision. To validate SummEQuAL{'}s effectiveness, we develop a dataset based on MultiWOZ. We conduct experiments on SummEval and our MultiWOZ-based dataset, showing that SummEQuAL largely improves the quality of summarization evaluation. Notably, SummEQuAL demonstrates a 19.7{\\%} improvement over QuestEval in terms of sample-level Pearson correlation with human assessments of consistency on the SummEval dataset. Furthermore, it exceeds the performance of the BERTScore baseline by achieving a 17.3{\\%} increase in Spearman correlation on our MultiWOZ-based dataset. Our study illuminates the potential of LLMs for a unified evaluation framework, setting a new paradigm for future summarization evaluation.\",\n}\n",
    "authors": [
        "Junyuan Liu",
        "Zhengyan Shi",
        "Aldo Lipani"
    ],
    "pdf_url": "https://aclanthology.org/2024.nlrse-1.5.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/cdecf855-9e0d-52ef-b409-d85405ad5ca7.pdf",
    "abstract": "Summarization is hard to evaluate due to its diverse and abstract nature. Although N-gram-based metrics like BLEU and ROUGE are prevalent, they often do not align well with human evaluations. While model-based alternatives such as BERTScore improve, they typically require extensive labelled data. The advent of Large Language Models (LLMs) presents a promising avenue for evaluation. To this end, we introduce SummEQuAL, a novel content-based framework using LLMs for unified, reproducible summarization evaluation. SummEQuAL evaluates summaries by comparing their content with the source document, employing a question-answering approach to gauge both recall and precision. To validate SummEQuALâ€™s effectiveness, we develop a dataset based on MultiWOZ. We conduct experiments on SummEval and our MultiWOZ-based dataset, showing that SummEQuAL largely improves the quality of summarization evaluation. Notably, SummEQuAL demonstrates a 19.7% improvement over QuestEval in terms of sample-level Pearson correlation with human assessments of consistency on the SummEval dataset. Furthermore, it exceeds the performance of the BERTScore baseline by achieving a 17.3% increase in Spearman correlation on our MultiWOZ-based dataset. Our study illuminates the potential of LLMs for a unified evaluation framework, setting a new paradigm for future summarization evaluation.",
    "num_pages": 10
}