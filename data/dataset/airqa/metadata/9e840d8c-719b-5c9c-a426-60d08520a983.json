{
    "uuid": "9e840d8c-719b-5c9c-a426-60d08520a983",
    "title": "The SETU-DCU Submissions to IWSLT 2024 Low-Resource Speech-to-Text Translation Tasks",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)",
    "bibtex": "@inproceedings{zafar-etal-2024-setu,\n    title = \"The {SETU}-{DCU} Submissions to {IWSLT} 2024 Low-Resource Speech-to-Text Translation Tasks\",\n    author = \"Zafar, Maria  and\n      Castaldo, Antonio  and\n      Nayak, Prashanth  and\n      Haque, Rejwanul  and\n      Gajakos, Neha  and\n      Way, Andy\",\n    editor = \"Salesky, Elizabeth  and\n      Federico, Marcello  and\n      Carpuat, Marine\",\n    booktitle = \"Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand (in-person and online)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.iwslt-1.12\",\n    doi = \"10.18653/v1/2024.iwslt-1.12\",\n    pages = \"80--85\",\n    abstract = \"Natural Language Processing (NLP) research and development has experienced rapid progression in the recent times due to advances in deep learning. The introduction of pre-trained large language models (LLMs) is at the core of this transformation, significantly enhancing the performance of machine translation (MT) and speech technologies. This development has also led to fundamental changes in modern translation and speech tools and their methodologies. However, there remain challenges when extending this progress to underrepresented dialects and low-resource languages, primarily due to the need for more data. This paper details our submissions to the IWSLT speech translation (ST) tasks. We used the Whisper model for the automatic speech recognition (ASR) component. We then used mBART and NLLB as cascaded systems for utilising their MT capabilities. Our research primarily focused on exploring various dialects of low-resource languages and harnessing existing resources from linguistically related languages. We conducted our experiments for two morphologically diverse language pairs: Irish-to-English and Maltese-to-English. We used BLEU, chrF and COMET for evaluating our MT models.\",\n}\n",
    "authors": [
        "Maria Zafar",
        "Antonio Castaldo",
        "Prashanth Nayak",
        "Rejwanul Haque",
        "Neha Gajakos",
        "Andy Way"
    ],
    "pdf_url": "https://aclanthology.org/2024.iwslt-1.12.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9e840d8c-719b-5c9c-a426-60d08520a983.pdf",
    "abstract": "Natural Language Processing (NLP) research and development has experienced rapid progression in the recent times due to advances in deep learning. The introduction of pre-trained large language models (LLMs) is at the core of this transformation, significantly enhancing the performance of machine translation (MT) and speech technologies. This development has also led to fundamental changes in modern translation and speech tools and their methodologies. However, there remain challenges when extending this progress to underrepresented dialects and low-resource languages, primarily due to the need for more data. This paper details our submissions to the IWSLT speech translation (ST) tasks. We used the Whisper model for the automatic speech recognition (ASR) component. We then used mBART and NLLB as cascaded systems for utilising their MT capabilities. Our research primarily focused on exploring various dialects of low-resource languages and harnessing existing resources from linguistically related languages. We conducted our experiments for two morphologically diverse language pairs: Irish-to-English and Maltese-to-English. We used BLEU, chrF and COMET for evaluating our MT models.",
    "num_pages": 6
}