{
    "uuid": "2115acea-f490-545f-aee8-a6a2e5a8991e",
    "title": "Large Language Models are not Fair Evaluators",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{wang-etal-2024-large-language-models-fair,\n    title = \"Large Language Models are not Fair Evaluators\",\n    author = \"Wang, Peiyi  and\n      Li, Lei  and\n      Chen, Liang  and\n      Cai, Zefan  and\n      Zhu, Dawei  and\n      Lin, Binghuai  and\n      Cao, Yunbo  and\n      Kong, Lingpeng  and\n      Liu, Qi  and\n      Liu, Tianyu  and\n      Sui, Zhifang\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.511\",\n    doi = \"10.18653/v1/2024.acl-long.511\",\n    pages = \"9440--9450\",\n    abstract = \"In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the {``}win/tie/lose{''} outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark{'}s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments.\",\n}\n",
    "authors": [
        "Peiyi Wang",
        "Lei Li",
        "Liang Chen",
        "Zefan Cai",
        "Dawei Zhu",
        "Binghuai Lin",
        "Yunbo Cao",
        "Lingpeng Kong",
        "Qi Liu",
        "Tianyu Liu",
        "Zhifang Sui"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.511.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/2115acea-f490-545f-aee8-a6a2e5a8991e.pdf",
    "abstract": "In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the “win/tie/lose” outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark’s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments.",
    "num_pages": 11
}