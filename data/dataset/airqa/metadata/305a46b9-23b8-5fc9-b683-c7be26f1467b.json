{
    "uuid": "305a46b9-23b8-5fc9-b683-c7be26f1467b",
    "title": "Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{he-etal-2024-enhancing,\n    title = \"Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss\",\n    author = \"He, Wei  and\n      Idiart, Marco  and\n      Scarton, Carolina  and\n      Villavicencio, Aline\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.741\",\n    doi = \"10.18653/v1/2024.findings-acl.741\",\n    pages = \"12473--12485\",\n    abstract = \"Accurately modeling idiomatic or non-compositional language has been a longstanding challenge in Natural Language Processing (NLP). This is partly because these expressions do not derive their meanings solely from their constituent words, but also due to the scarcity of relevant data resources, and their impact on the performance of downstream tasks such as machine translation and simplification. In this paper we propose an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective. Our proposed method is evaluated on a SemEval challenge and outperforms previous alternatives significantly in many metrics.\",\n}\n",
    "authors": [
        "Wei He",
        "Marco Idiart",
        "Carolina Scarton",
        "Aline Villavicencio"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.741.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/305a46b9-23b8-5fc9-b683-c7be26f1467b.pdf",
    "abstract": "Accurately modeling idiomatic or non-compositional language has been a longstanding challenge in Natural Language Processing (NLP). This is partly because these expressions do not derive their meanings solely from their constituent words, but also due to the scarcity of relevant data resources, and their impact on the performance of downstream tasks such as machine translation and simplification. In this paper we propose an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective. Our proposed method is evaluated on a SemEval challenge and outperforms previous alternatives significantly in many metrics.",
    "num_pages": 13
}