{
    "uuid": "ef80ab61-1629-5609-99a0-c7399b6f556f",
    "title": "Relying on the Unreliable: The Impact of Language Modelsâ€™ Reluctance to Express Uncertainty",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhou-etal-2024-relying,\n    title = \"Relying on the Unreliable: The Impact of Language Models{'} Reluctance to Express Uncertainty\",\n    author = \"Zhou, Kaitlyn  and\n      Hwang, Jena  and\n      Ren, Xiang  and\n      Sap, Maarten\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.198\",\n    doi = \"10.18653/v1/2024.acl-long.198\",\n    pages = \"3623--3643\",\n    abstract = \"As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence in responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47{\\%}) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans are biased against texts with uncertainty. Our work highlights new safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.\",\n}\n",
    "authors": [
        "Kaitlyn Zhou",
        "Jena Hwang",
        "Xiang Ren",
        "Maarten Sap"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.198.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/ef80ab61-1629-5609-99a0-c7399b6f556f.pdf",
    "abstract": "As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence in responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47%) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans are biased against texts with uncertainty. Our work highlights new safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.",
    "num_pages": 21
}