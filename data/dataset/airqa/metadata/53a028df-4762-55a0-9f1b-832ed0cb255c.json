{
    "uuid": "53a028df-4762-55a0-9f1b-832ed0cb255c",
    "title": "Janko at SemEval-2023 Task 2: Bidirectional LSTM Model Based on Pre-training for Chinese Named Entity Recognition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{li-etal-2023-janko,\n    title = \"Janko at {S}em{E}val-2023 Task 2: Bidirectional {LSTM} Model Based on Pre-training for {C}hinese Named Entity Recognition\",\n    author = \"Li, Jiankuo  and\n      Guan, Zhengyi  and\n      Ding, Haiyan\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.132\",\n    doi = \"10.18653/v1/2023.semeval-1.132\",\n    pages = \"958--962\",\n    abstract = \"This paper describes the method we submitted as the Janko team in the SemEval-2023 Task 2,Multilingual Complex Named Entity Recognition (MultiCoNER 2). We only participated in the Chinese track. In this paper, we implement the BERT-BiLSTM-RDrop model. We use the fine-tuned BERT models, take the output of BERT as the input of the BiLSTM network, and finally use R-Drop technology to optimize the loss function. Our submission achieved a macro-averaged F1 score of 0.579 on the testset.\",\n}\n",
    "authors": [
        "Jiankuo Li",
        "Zhengyi Guan",
        "Haiyan Ding"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.132.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/53a028df-4762-55a0-9f1b-832ed0cb255c.pdf",
    "abstract": "This paper describes the method we submitted as the Janko team in the SemEval-2023 Task 2,Multilingual Complex Named Entity Recognition (MultiCoNER 2). We only participated in the Chinese track. In this paper, we implement the BERT-BiLSTM-RDrop model. We use the fine-tuned BERT models, take the output of BERT as the input of the BiLSTM network, and finally use R-Drop technology to optimize the loss function. Our submission achieved a macro-averaged F1 score of 0.579 on the testset.",
    "num_pages": 5
}