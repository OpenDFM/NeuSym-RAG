{
    "uuid": "353478fb-a7ea-5d21-922b-c5e7dd63c153",
    "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{xu-etal-2024-knowledge,\n    title = \"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\",\n    author = \"Xu, Ran  and\n      Cui, Hejie  and\n      Yu, Yue  and\n      Kan, Xuan  and\n      Shi, Wenqi  and\n      Zhuang, Yuchen  and\n      Wang, May Dongmei  and\n      Jin, Wei  and\n      Ho, Joyce  and\n      Yang, Carl\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.916\",\n    doi = \"10.18653/v1/2024.findings-acl.916\",\n    pages = \"15496--15523\",\n    abstract = \"Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that ClinGen consistently enhances performance across various tasks by 7.7{\\%}-8.7{\\%} on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances.\",\n}\n",
    "authors": [
        "Ran Xu",
        "Hejie Cui",
        "Yue Yu",
        "Xuan Kan",
        "Wenqi Shi",
        "Yuchen Zhuang",
        "May Dongmei Wang",
        "Wei Jin",
        "Joyce Ho",
        "Carl Yang"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.916.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/353478fb-a7ea-5d21-922b-c5e7dd63c153.pdf",
    "abstract": "Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that ClinGen consistently enhances performance across various tasks by 7.7%-8.7% on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances.",
    "num_pages": 28
}