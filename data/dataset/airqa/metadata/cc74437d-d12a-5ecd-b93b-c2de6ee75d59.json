{
    "uuid": "cc74437d-d12a-5ecd-b93b-c2de6ee75d59",
    "title": "PrivaT5: A Generative Language Model for Privacy Policies",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the Fifth Workshop on Privacy in Natural Language Processing",
    "bibtex": "@inproceedings{zoubi-etal-2024-privat5,\n    title = \"{P}riva{T}5: A Generative Language Model for Privacy Policies\",\n    author = \"Zoubi, Mohammad  and\n      T.y.s.s, Santosh  and\n      Rosas, Edgar  and\n      Grabmair, Matthias\",\n    editor = \"Habernal, Ivan  and\n      Ghanavati, Sepideh  and\n      Ravichander, Abhilasha  and\n      Jain, Vijayanta  and\n      Thaine, Patricia  and\n      Igamberdiev, Timour  and\n      Mireshghallah, Niloofar  and\n      Feyisetan, Oluwaseyi\",\n    booktitle = \"Proceedings of the Fifth Workshop on Privacy in Natural Language Processing\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.privatenlp-1.16\",\n    pages = \"159--169\",\n    abstract = \"In the era of of digital privacy, users often neglect to read privacy policies due to their complexity. To bridge this gap, NLP models have emerged to assist in understanding privacy policies. While recent generative language models like BART and T5 have shown prowess in text generation and discriminative tasks being framed as generative ones, their application to privacy policy domain tasks remains unexplored. To address that, we introduce PrivaT5, a T5-based model that is further pre-trained on privacy policy text. We evaluate PrivaT5 over a diverse privacy policy related tasks and notice its superior performance over T5, showing the utility of continued domain-specific pre-training. Our results also highlight challenges faced by these generative models in complex structured output label space, especially in sequence tagging tasks, where they fall short compared to lighter encoder-only models.\",\n}\n",
    "authors": [
        "Mohammad Zoubi",
        "Santosh T.y.s.s",
        "Edgar Rosas",
        "Matthias Grabmair"
    ],
    "pdf_url": "https://aclanthology.org/2024.privatenlp-1.16.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/cc74437d-d12a-5ecd-b93b-c2de6ee75d59.pdf",
    "abstract": "In the era of of digital privacy, users often neglect to read privacy policies due to their complexity. To bridge this gap, NLP models have emerged to assist in understanding privacy policies. While recent generative language models like BART and T5 have shown prowess in text generation and discriminative tasks being framed as generative ones, their application to privacy policy domain tasks remains unexplored. To address that, we introduce PrivaT5, a T5-based model that is further pre-trained on privacy policy text. We evaluate PrivaT5 over a diverse privacy policy related tasks and notice its superior performance over T5, showing the utility of continued domain-specific pre-training. Our results also highlight challenges faced by these generative models in complex structured output label space, especially in sequence tagging tasks, where they fall short compared to lighter encoder-only models.",
    "num_pages": 11
}