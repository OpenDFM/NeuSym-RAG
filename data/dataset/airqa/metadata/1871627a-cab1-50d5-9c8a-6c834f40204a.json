{
    "uuid": "1871627a-cab1-50d5-9c8a-6c834f40204a",
    "title": "Prefix Propagation: Parameter-Efficient Tuning for Long Sequences",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{li-etal-2023-prefix,\n    title = \"Prefix Propagation: Parameter-Efficient Tuning for Long Sequences\",\n    author = \"Li, Jonathan  and\n      Aitken, Will  and\n      Bhambhoria, Rohan  and\n      Zhu, Xiaodan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-short.120\",\n    doi = \"10.18653/v1/2023.acl-short.120\",\n    pages = \"1408--1419\",\n    abstract = \"Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model{'}s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50{\\%} fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning for long-sequence language tasks.\",\n}\n",
    "authors": [
        "Jonathan Li",
        "Will Aitken",
        "Rohan Bhambhoria",
        "Xiaodan Zhu"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-short.120.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/1871627a-cab1-50d5-9c8a-6c834f40204a.pdf",
    "abstract": "Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the modelâ€™s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning for long-sequence language tasks.",
    "num_pages": 12
}