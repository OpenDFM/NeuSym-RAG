{
    "uuid": "9088c90d-1f49-5e53-9a45-9f2f207b3d03",
    "title": "A Measure-Theoretic Characterization of Tight Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{du-etal-2023-measure,\n    title = \"A Measure-Theoretic Characterization of Tight Language Models\",\n    author = \"Du, Li  and\n      Torroba Hennigen, Lucas  and\n      Pimentel, Tiago  and\n      Meister, Clara  and\n      Eisner, Jason  and\n      Cotterell, Ryan\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.543\",\n    doi = \"10.18653/v1/2023.acl-long.543\",\n    pages = \"9744--9770\",\n    abstract = \"Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can {``}leak{''} onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.\",\n}\n",
    "authors": [
        "Li Du",
        "Lucas Torroba Hennigen",
        "Tiago Pimentel",
        "Clara Meister",
        "Jason Eisner",
        "Ryan Cotterell"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.543.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/9088c90d-1f49-5e53-9a45-9f2f207b3d03.pdf",
    "abstract": "Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.",
    "num_pages": 27
}