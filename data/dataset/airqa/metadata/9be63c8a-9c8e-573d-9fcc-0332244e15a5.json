{
    "uuid": "9be63c8a-9c8e-573d-9fcc-0332244e15a5",
    "title": "Balanced Data Sampling for Language Model Training with Clustering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{shao-etal-2024-balanced,\n    title = \"Balanced Data Sampling for Language Model Training with Clustering\",\n    author = \"Shao, Yunfan  and\n      Li, Linyang  and\n      Fei, Zhaoye  and\n      Yan, Hang  and\n      Lin, Dahua  and\n      Qiu, Xipeng\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.833\",\n    doi = \"10.18653/v1/2024.findings-acl.833\",\n    pages = \"14012--14023\",\n    abstract = \"Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.\",\n}\n",
    "authors": [
        "Yunfan Shao",
        "Linyang Li",
        "Zhaoye Fei",
        "Hang Yan",
        "Dahua Lin",
        "Xipeng Qiu"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.833.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/9be63c8a-9c8e-573d-9fcc-0332244e15a5.pdf",
    "abstract": "Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters. Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models.",
    "num_pages": 12
}