{
    "uuid": "d183bf8f-52b6-5856-b21d-f5d53c6b10a0",
    "title": "Soul-Mix: Enhancing Multimodal Machine Translation with Manifold Mixup",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{cheng-etal-2024-soul,\n    title = \"Soul-Mix: Enhancing Multimodal Machine Translation with Manifold Mixup\",\n    author = \"Cheng, Xuxin  and\n      Yao, Ziyu  and\n      Xin, Yifei  and\n      An, Hao  and\n      Li, Hongxiang  and\n      Li, Yaowei  and\n      Zou, Yuexian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.608\",\n    doi = \"10.18653/v1/2024.acl-long.608\",\n    pages = \"11283--11294\",\n    abstract = \"Multimodal machine translation (MMT) aims to improve the performance of machine translation with the help of visual information, which has received widespread attention recently. It has been verified that visual information brings greater performance gains when the textual information is limited. However, most previous works ignore to take advantage of the complete textual inputs and the limited textual inputs at the same time, which limits the overall performance. To solve this issue, we propose a mixup method termed Soul-Mix to enhance MMT by using visual information more effectively. We mix the predicted translations of complete textual input and the limited textual inputs. Experimental results on the Multi30K dataset of three translation directions show that our Soul-Mix significantly outperforms existing approaches and achieves new state-of-the-art performance with fewer parameters than some previous models. Besides, the strength of Soul-Mix is more obvious on more challenging MSCOCO dataset which includes more out-of-domain instances with lots of ambiguous verbs.\",\n}\n",
    "authors": [
        "Xuxin Cheng",
        "Ziyu Yao",
        "Yifei Xin",
        "Hao An",
        "Hongxiang Li",
        "Yaowei Li",
        "Yuexian Zou"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.608.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/d183bf8f-52b6-5856-b21d-f5d53c6b10a0.pdf",
    "abstract": "Multimodal machine translation (MMT) aims to improve the performance of machine translation with the help of visual information, which has received widespread attention recently. It has been verified that visual information brings greater performance gains when the textual information is limited. However, most previous works ignore to take advantage of the complete textual inputs and the limited textual inputs at the same time, which limits the overall performance. To solve this issue, we propose a mixup method termed Soul-Mix to enhance MMT by using visual information more effectively. We mix the predicted translations of complete textual input and the limited textual inputs. Experimental results on the Multi30K dataset of three translation directions show that our Soul-Mix significantly outperforms existing approaches and achieves new state-of-the-art performance with fewer parameters than some previous models. Besides, the strength of Soul-Mix is more obvious on more challenging MSCOCO dataset which includes more out-of-domain instances with lots of ambiguous verbs.",
    "num_pages": 12
}