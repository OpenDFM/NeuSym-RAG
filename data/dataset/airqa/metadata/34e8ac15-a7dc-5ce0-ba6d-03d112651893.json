{
    "uuid": "34e8ac15-a7dc-5ce0-ba6d-03d112651893",
    "title": "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{zhang-etal-2024-text,\n    title = \"Text-like Encoding of Collaborative Information in Large Language Models for Recommendation\",\n    author = \"Zhang, Yang  and\n      Bao, Keqin  and\n      Yan, Ming  and\n      Wang, Wenjie  and\n      Feng, Fuli  and\n      He, Xiangnan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.497\",\n    doi = \"10.18653/v1/2024.acl-long.497\",\n    pages = \"9181--9191\",\n    abstract = \"When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs{'} latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences {---} a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.\",\n}\n",
    "authors": [
        "Yang Zhang",
        "Keqin Bao",
        "Ming Yan",
        "Wenjie Wang",
        "Fuli Feng",
        "Xiangnan He"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.497.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/34e8ac15-a7dc-5ce0-ba6d-03d112651893.pdf",
    "abstract": "When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs’ latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences — a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.",
    "num_pages": 11
}