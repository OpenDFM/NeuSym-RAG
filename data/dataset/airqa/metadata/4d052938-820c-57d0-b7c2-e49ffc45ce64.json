{
    "uuid": "4d052938-820c-57d0-b7c2-e49ffc45ce64",
    "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    "bibtex": "@inproceedings{fusco-etal-2023-pnlp,\n    title = \"p{NLP}-Mixer: an Efficient all-{MLP} Architecture for Language\",\n    author = \"Fusco, Francesco  and\n      Pascual, Damian  and\n      Staar, Peter  and\n      Antognini, Diego\",\n    editor = \"Sitaram, Sunayana  and\n      Beigman Klebanov, Beata  and\n      Williams, Jason D\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-industry.6\",\n    doi = \"10.18653/v1/2023.acl-industry.6\",\n    pages = \"53--60\",\n    abstract = \"Large pre-trained language models based on transformer architectureÆhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4{\\%} and 97.8{\\%} the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8{\\%} on MTOP.\",\n}\n",
    "authors": [
        "Francesco Fusco",
        "Damian Pascual",
        "Peter Staar",
        "Diego Antognini"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-industry.6.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4d052938-820c-57d0-b7c2-e49ffc45ce64.pdf",
    "abstract": "Large pre-trained language models based on transformer architectureƒhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.",
    "num_pages": 8
}