{
    "uuid": "3deeb4ee-762f-5af8-9e92-04a0901b9316",
    "title": "A Survey of Evaluation Methods of Generated Medical Textual Reports",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    "bibtex": "@inproceedings{zhou-etal-2023-survey,\n    title = \"A Survey of Evaluation Methods of Generated Medical Textual Reports\",\n    author = \"Zhou, Yongxin  and\n      Ringeval, Fabien  and\n      Portet, Fran{\\c{c}}ois\",\n    editor = \"Naumann, Tristan  and\n      Ben Abacha, Asma  and\n      Bethard, Steven  and\n      Roberts, Kirk  and\n      Rumshisky, Anna\",\n    booktitle = \"Proceedings of the 5th Clinical Natural Language Processing Workshop\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.clinicalnlp-1.48\",\n    doi = \"10.18653/v1/2023.clinicalnlp-1.48\",\n    pages = \"447--459\",\n    abstract = \"Medical Report Generation (MRG) is a sub-task of Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field.\",\n}\n",
    "authors": [
        "Yongxin Zhou",
        "Fabien Ringeval",
        "Fran√ßois Portet"
    ],
    "pdf_url": "https://aclanthology.org/2023.clinicalnlp-1.48.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/3deeb4ee-762f-5af8-9e92-04a0901b9316.pdf",
    "abstract": "Medical Report Generation (MRG) is a sub-task of Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field.",
    "num_pages": 13
}