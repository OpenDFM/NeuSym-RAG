{
    "uuid": "51cd1297-dbf4-5ab8-a4a3-28ffb0e0b9ba",
    "title": "Nonparametric Masked Language Modeling",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{min-etal-2023-nonparametric,\n    title = \"Nonparametric Masked Language Modeling\",\n    author = \"Min, Sewon  and\n      Shi, Weijia  and\n      Lewis, Mike  and\n      Chen, Xilun  and\n      Yih, Wen-tau  and\n      Hajishirzi, Hannaneh  and\n      Zettlemoyer, Luke\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.132\",\n    doi = \"10.18653/v1/2023.findings-acl.132\",\n    pages = \"2097--2118\",\n    abstract = \"Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.\",\n}\n",
    "authors": [
        "Sewon Min",
        "Weijia Shi",
        "Mike Lewis",
        "Xilun Chen",
        "Wen-tau Yih",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.132.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/51cd1297-dbf4-5ab8-a4a3-28ffb0e0b9ba.pdf",
    "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.",
    "num_pages": 22
}