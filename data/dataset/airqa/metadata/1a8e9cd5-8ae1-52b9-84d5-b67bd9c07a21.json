{
    "uuid": "1a8e9cd5-8ae1-52b9-84d5-b67bd9c07a21",
    "title": "MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{ho-etal-2024-mtp,\n    title = \"{MTP}: A Dataset for Multi-Modal Turning Points in Casual Conversations\",\n    author = \"Ho, Gia-Bao  and\n      Tan, Chang  and\n      Darban, Zahra  and\n      Salehi, Mahsa  and\n      Haf, Reza  and\n      Buntine, Wray\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.30\",\n    doi = \"10.18653/v1/2024.acl-short.30\",\n    pages = \"314--326\",\n    abstract = \"Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence high-lighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.\",\n}\n",
    "authors": [
        "Gia-Bao Ho",
        "Chang Tan",
        "Zahra Darban",
        "Mahsa Salehi",
        "Reza Haf",
        "Wray Buntine"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.30.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/1a8e9cd5-8ae1-52b9-84d5-b67bd9c07a21.pdf",
    "abstract": "Detecting critical moments, such as emotional outbursts or changes in decisions during conversations, is crucial for understanding shifts in human behavior and their consequences. Our work introduces a novel problem setting focusing on these moments as turning points (TPs), accompanied by a meticulously curated, high-consensus, human-annotated multi-modal dataset. We provide precise timestamps, descriptions, and visual-textual evidence high-lighting changes in emotions, behaviors, perspectives, and decisions at these turning points. We also propose a framework, TPMaven, utilizing state-of-the-art vision-language models to construct a narrative from the videos and large language models to classify and detect turning points in our multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score of 0.88 in classification and 0.61 in detection, with additional explanations aligning with human expectations.",
    "num_pages": 13
}