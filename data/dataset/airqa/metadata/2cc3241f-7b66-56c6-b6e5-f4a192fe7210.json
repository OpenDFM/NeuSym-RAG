{
    "uuid": "2cc3241f-7b66-56c6-b6e5-f4a192fe7210",
    "title": "CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{lei-etal-2023-ckdst,\n    title = \"{CKDST}: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation\",\n    author = \"Lei, Yikun  and\n      Xue, Zhengshan  and\n      Zhao, Xiaohu  and\n      Sun, Haoran  and\n      Zhu, Shaolin  and\n      Lin, Xiaodong  and\n      Xiong, Deyi\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.195\",\n    doi = \"10.18653/v1/2023.findings-acl.195\",\n    pages = \"3123--3137\",\n    abstract = \"Distilling knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation. However, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from machine translation. In this paper, we propose a comprehensive knowledge distillation framework for speech translation, CKDST, which is capable of comprehensively and effectively distilling knowledge from machine translation to speech translation from two perspectives: cross-modal contrastive representation distillation and simultaneous decoupled knowledge distillation. In the former, we leverage a contrastive learning objective to optmize the mutual information between speech and text representations for representation distillation in the encoder. In the later, we decouple the non-target class knowledge from target class knowledge for logits distillation in the decoder. Experiments on the MuST-C benchmark dataset demonstrate that our CKDST substantially improves the baseline by 1.2 BLEU on average in all translation directions, and outperforms previous state-of-the-art end-to-end and cascaded speech translation models.\",\n}\n",
    "authors": [
        "Yikun Lei",
        "Zhengshan Xue",
        "Xiaohu Zhao",
        "Haoran Sun",
        "Shaolin Zhu",
        "Xiaodong Lin",
        "Deyi Xiong"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.195.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2cc3241f-7b66-56c6-b6e5-f4a192fe7210.pdf",
    "abstract": "Distilling knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation. However, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from machine translation. In this paper, we propose a comprehensive knowledge distillation framework for speech translation, CKDST, which is capable of comprehensively and effectively distilling knowledge from machine translation to speech translation from two perspectives: cross-modal contrastive representation distillation and simultaneous decoupled knowledge distillation. In the former, we leverage a contrastive learning objective to optmize the mutual information between speech and text representations for representation distillation in the encoder. In the later, we decouple the non-target class knowledge from target class knowledge for logits distillation in the decoder. Experiments on the MuST-C benchmark dataset demonstrate that our CKDST substantially improves the baseline by 1.2 BLEU on average in all translation directions, and outperforms previous state-of-the-art end-to-end and cascaded speech translation models.",
    "num_pages": 15
}