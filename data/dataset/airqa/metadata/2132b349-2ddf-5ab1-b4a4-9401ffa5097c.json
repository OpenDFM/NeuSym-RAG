{
    "uuid": "2132b349-2ddf-5ab1-b4a4-9401ffa5097c",
    "title": "PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)",
    "bibtex": "@inproceedings{han-etal-2023-personapkt,\n    title = \"{P}ersona{PKT}: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer\",\n    author = \"Han, Xu  and\n      Guo, Bin  and\n      Jung, Yoon  and\n      Yao, Benjamin  and\n      Zhang, Yu  and\n      Liu, Xiaohu  and\n      Guo, Chenlei\",\n    editor = \"Sadat Moosavi, Nafise  and\n      Gurevych, Iryna  and\n      Hou, Yufang  and\n      Kim, Gyuwan  and\n      Kim, Young Jin  and\n      Schuster, Tal  and\n      Agrawal, Ameeta\",\n    booktitle = \"Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada (Hybrid)\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sustainlp-1.21\",\n    doi = \"10.18653/v1/2023.sustainlp-1.21\",\n    pages = \"264--273\",\n}\n",
    "authors": [
        "Xu Han",
        "Bin Guo",
        "Yoon Jung",
        "Benjamin Yao",
        "Yu Zhang",
        "Xiaohu Liu",
        "Chenlei Guo"
    ],
    "pdf_url": "https://aclanthology.org/2023.sustainlp-1.21.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/2132b349-2ddf-5ab1-b4a4-9401ffa5097c.pdf",
    "abstract": "Personalized dialogue agents (DAs) powered by large pre-trained language models (PLMs) often rely on explicit persona descriptions to maintain personality consistency. However, such descriptions may not always be available or may pose privacy concerns. To tackle this bottleneck, we introduce PersonaPKT, a lightweight transfer learning approach that can build persona-consistent dialogue models without explicit persona descriptions. By representing each persona as a continuous vector, PersonaPKT learns implicit persona-specific features directly from a small number of dialogue samples produced by the same persona, adding less than 0.1% trainable parameters for each persona on top of the PLM backbone. Empirical results demonstrate that PersonaPKT effectively builds personalized DAs with high storage efficiency, outperforming various baselines in terms of persona consistency while maintaining good response generation quality. In addition, it enhances privacy protection by avoiding explicit persona descriptions. Overall, PersonaPKT is an effective solution for creating personalized DAs that respect user privacy.",
    "num_pages": 10
}