{
    "uuid": "4788e740-77b6-50b5-9217-f9400d6d116e",
    "title": "Local Interpretation of Transformer Based on Linear Decomposition",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yang-etal-2023-local,\n    title = \"Local Interpretation of Transformer Based on Linear Decomposition\",\n    author = \"Yang, Sen  and\n      Huang, Shujian  and\n      Zou, Wei  and\n      Zhang, Jianbing  and\n      Dai, Xinyu  and\n      Chen, Jiajun\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.572\",\n    doi = \"10.18653/v1/2023.acl-long.572\",\n    pages = \"10270--10287\",\n    abstract = \"In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks.\",\n}\n",
    "authors": [
        "Sen Yang",
        "Shujian Huang",
        "Wei Zou",
        "Jianbing Zhang",
        "Xinyu Dai",
        "Jiajun Chen"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.572.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4788e740-77b6-50b5-9217-f9400d6d116e.pdf",
    "abstract": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks.",
    "num_pages": 18
}