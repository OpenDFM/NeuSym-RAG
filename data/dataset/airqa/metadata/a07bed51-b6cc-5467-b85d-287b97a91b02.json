{
    "uuid": "a07bed51-b6cc-5467-b85d-287b97a91b02",
    "title": "Team JUSTR00 at SemEval-2023 Task 3: Transformers for News Articles Classification",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{al-qarqaz-abdullah-2023-team,\n    title = \"Team {JUSTR}00 at {S}em{E}val-2023 Task 3: Transformers for News Articles Classification\",\n    author = \"Al-Qarqaz, Ahmed  and\n      Abdullah, Malak\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.168\",\n    doi = \"10.18653/v1/2023.semeval-1.168\",\n    pages = \"1213--1216\",\n    abstract = \"The SemEval-2023 Task 3 competition offers participants a multi-lingual dataset with three schemes one for each subtask. The competition challenges participants to construct machine learning systems that can categorize news articles based on their nature and style of writing. We esperiment with many state-of-the-art transformer-based language models proposed in the natural language processing literature and report the results of the best ones. Our top performing model is based on a transformer called {``}Longformer{''} and has achieved an F1-Micro score of 0.256 on the English version of subtask-1 and F1-Macro of 0.442 on subtask-2 on the test data. We also experiment with a number of state-of-the-art multi-lingual transformer-based models and report the results of the best performing ones.\",\n}\n",
    "authors": [
        "Ahmed Al-Qarqaz",
        "Malak Abdullah"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.168.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a07bed51-b6cc-5467-b85d-287b97a91b02.pdf",
    "abstract": "The SemEval-2023 Task 3 competition offers participants a multi-lingual dataset with three schemes one for each subtask. The competition challenges participants to construct machine learning systems that can categorize news articles based on their nature and style of writing. We esperiment with many state-of-the-art transformer-based language models proposed in the natural language processing literature and report the results of the best ones. Our top performing model is based on a transformer called “Longformer” and has achieved an F1-Micro score of 0.256 on the English version of subtask-1 and F1-Macro of 0.442 on subtask-2 on the test data. We also experiment with a number of state-of-the-art multi-lingual transformer-based models and report the results of the best performing ones.",
    "num_pages": 4
}