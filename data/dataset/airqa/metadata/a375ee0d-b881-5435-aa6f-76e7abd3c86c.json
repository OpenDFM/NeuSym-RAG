{
    "uuid": "a375ee0d-b881-5435-aa6f-76e7abd3c86c",
    "title": "SemEval-2023 Task 11: Learning with Disagreements (LeWiDi)",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{leonardelli-etal-2023-semeval,\n    title = \"{S}em{E}val-2023 Task 11: Learning with Disagreements ({L}e{W}i{D}i)\",\n    author = \"Leonardelli, Elisa  and\n      Abercrombie, Gavin  and\n      Almanea, Dina  and\n      Basile, Valerio  and\n      Fornaciari, Tommaso  and\n      Plank, Barbara  and\n      Rieser, Verena  and\n      Uma, Alexandra  and\n      Poesio, Massimo\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.314\",\n    doi = \"10.18653/v1/2023.semeval-1.314\",\n    pages = \"2304--2318\",\n    abstract = \"NLP datasets annotated with human judgments are rife with disagreements between the judges. This is especially true for tasks depending on subjective judgments such as sentiment analysis or offensive language detection. Particularly in these latter cases, the NLP community has come to realize that the common approach of reconciling{'} these different subjective interpretations risks misrepresenting the evidence. Many NLP researchers have therefore concluded that rather than eliminating disagreements from annotated corpora, we should preserve themindeed, some argue that corpora should aim to preserve all interpretations produced by annotators. But this approach to corpus creation for NLP has not yet been widely accepted. The objective of the Le-Wi-Di series of shared tasks is to promote this approach to developing NLP models by providing a unified framework for training and evaluating with such datasets. We report on the second such shared task, which differs from the first edition in three crucial respects: (i) it focuses entirely on NLP, instead of both NLP and computer vision tasks in its first edition; (ii) it focuses on subjective tasks, instead of covering different types of disagreements as training with aggregated labels for subjective NLP tasks is in effect a misrepresentation of the data; and (iii) for the evaluation, we concentrated on soft approaches to evaluation. This second edition of Le-Wi-Di attracted a wide array of partici- pants resulting in 13 shared task submission papers.\",\n}\n",
    "authors": [
        "Elisa Leonardelli",
        "Gavin Abercrombie",
        "Dina Almanea",
        "Valerio Basile",
        "Tommaso Fornaciari",
        "Barbara Plank",
        "Verena Rieser",
        "Alexandra Uma",
        "Massimo Poesio"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.314.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/a375ee0d-b881-5435-aa6f-76e7abd3c86c.pdf",
    "abstract": "NLP datasets annotated with human judgments are rife with disagreements between the judges. This is especially true for tasks depending on subjective judgments such as sentiment analysis or offensive language detection. Particularly in these latter cases, the NLP community has come to realize that the common approach of reconcilingâ€™ these different subjective interpretations risks misrepresenting the evidence. Many NLP researchers have therefore concluded that rather than eliminating disagreements from annotated corpora, we should preserve themindeed, some argue that corpora should aim to preserve all interpretations produced by annotators. But this approach to corpus creation for NLP has not yet been widely accepted. The objective of the Le-Wi-Di series of shared tasks is to promote this approach to developing NLP models by providing a unified framework for training and evaluating with such datasets. We report on the second such shared task, which differs from the first edition in three crucial respects: (i) it focuses entirely on NLP, instead of both NLP and computer vision tasks in its first edition; (ii) it focuses on subjective tasks, instead of covering different types of disagreements as training with aggregated labels for subjective NLP tasks is in effect a misrepresentation of the data; and (iii) for the evaluation, we concentrated on soft approaches to evaluation. This second edition of Le-Wi-Di attracted a wide array of partici- pants resulting in 13 shared task submission papers.",
    "num_pages": 15
}