{
    "uuid": "abd75927-1080-5439-901d-b3e04d015abb",
    "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2023-say,\n    title = \"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge\",\n    author = \"Chen, Jiangjie  and\n      Shi, Wei  and\n      Fu, Ziquan  and\n      Cheng, Sijie  and\n      Li, Lei  and\n      Xiao, Yanghua\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.550\",\n    doi = \"10.18653/v1/2023.acl-long.550\",\n    pages = \"9890--9908\",\n    abstract = \"Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as {``}lions don{'}t live in the ocean{''}, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.\",\n}\n",
    "authors": [
        "Jiangjie Chen",
        "Wei Shi",
        "Ziquan Fu",
        "Sijie Cheng",
        "Lei Li",
        "Yanghua Xiao"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-long.550.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/abd75927-1080-5439-901d-b3e04d015abb.pdf",
    "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
    "num_pages": 19
}