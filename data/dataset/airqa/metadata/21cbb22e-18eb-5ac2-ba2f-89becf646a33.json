{
    "uuid": "21cbb22e-18eb-5ac2-ba2f-89becf646a33",
    "title": "T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2023",
    "bibtex": "@inproceedings{wang-etal-2023-t2iat,\n    title = \"{T}2{IAT}: Measuring Valence and Stereotypical Biases in Text-to-Image Generation\",\n    author = \"Wang, Jialu  and\n      Liu, Xinyue  and\n      Di, Zonglin  and\n      Liu, Yang  and\n      Wang, Xin\",\n    editor = \"Rogers, Anna  and\n      Boyd-Graber, Jordan  and\n      Okazaki, Naoaki\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.findings-acl.160\",\n    doi = \"10.18653/v1/2023.findings-acl.160\",\n    pages = \"2560--2574\",\n    abstract = \"*Warning: This paper contains several contents that may be toxic, harmful, or offensive.*In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations.\",\n}\n",
    "authors": [
        "Jialu Wang",
        "Xinyue Liu",
        "Zonglin Di",
        "Yang Liu",
        "Xin Wang"
    ],
    "pdf_url": "https://aclanthology.org/2023.findings-acl.160.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/21cbb22e-18eb-5ac2-ba2f-89becf646a33.pdf",
    "abstract": "*Warning: This paper contains several contents that may be toxic, harmful, or offensive.*In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations.",
    "num_pages": 15
}