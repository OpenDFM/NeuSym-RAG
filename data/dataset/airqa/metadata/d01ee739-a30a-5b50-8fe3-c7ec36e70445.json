{
    "uuid": "d01ee739-a30a-5b50-8fe3-c7ec36e70445",
    "title": "Fine-tuning mSLAM for the SIGMORPHON 2022 Shared Task on Grapheme-to-Phoneme Conversion",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    "bibtex": "@inproceedings{garrette-2023-fine,\n    title = \"Fine-tuning m{SLAM} for the {SIGMORPHON} 2022 Shared Task on Grapheme-to-Phoneme Conversion\",\n    author = \"Garrette, Dan\",\n    editor = {Nicolai, Garrett  and\n      Chodroff, Eleanor  and\n      Mailhot, Frederic  and\n      {\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}},\n    booktitle = \"Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sigmorphon-1.31\",\n    doi = \"10.18653/v1/2023.sigmorphon-1.31\",\n    pages = \"250--250\",\n    abstract = \"Grapheme-to-phoneme (G2P) conversion is a task that is inherently related to both written and spoken language. Therefore, our submission to the G2P shared task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model pretrained simultaneously on text from 101 languages and speech from 51 languages. For fine-tuning a G2P model, we combined mSLAM{'}s text encoder, which uses characters as its input tokens, with an uninitialized single-layer RNN-T decoder (Graves, 2012) whose vocabulary is the set of all 381 phonemes appearing in the shared task data. We took an explicitly multilingual approach to modeling the G2P tasks, fine-tuning and evaluating a single model that covered all the languages in each task, and adding language codes as prefixes to the input strings as a means of specifying the language of each example. Our models perform well in the shared task{'}s {``}high{''} setting (in which they were trained on 1,000 words from each language), though they do poorly in the {``}low{''} task setting (training on only 100 words from each language). Our models also perform reasonably in the {``}mixed{''} setting (training on 100 words in the target language and 1000 words in a related language), hinting that mSLAM{'}s multilingual pretraining may be enabling useful cross-lingual sharing.\",\n}\n",
    "authors": [
        "Dan Garrette"
    ],
    "pdf_url": "https://aclanthology.org/2023.sigmorphon-1.31.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/d01ee739-a30a-5b50-8fe3-c7ec36e70445.pdf",
    "abstract": "Grapheme-to-phoneme (G2P) conversion is a task that is inherently related to both written and spoken language. Therefore, our submission to the G2P shared task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model pretrained simultaneously on text from 101 languages and speech from 51 languages. For fine-tuning a G2P model, we combined mSLAM’s text encoder, which uses characters as its input tokens, with an uninitialized single-layer RNN-T decoder (Graves, 2012) whose vocabulary is the set of all 381 phonemes appearing in the shared task data. We took an explicitly multilingual approach to modeling the G2P tasks, fine-tuning and evaluating a single model that covered all the languages in each task, and adding language codes as prefixes to the input strings as a means of specifying the language of each example. Our models perform well in the shared task’s “high” setting (in which they were trained on 1,000 words from each language), though they do poorly in the “low” task setting (training on only 100 words from each language). Our models also perform reasonably in the “mixed” setting (training on 100 words in the target language and 1000 words in a related language), hinting that mSLAM’s multilingual pretraining may be enabling useful cross-lingual sharing.",
    "num_pages": 1
}