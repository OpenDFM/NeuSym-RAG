{
    "uuid": "17305118-5263-56f8-ac46-6a96e3a72510",
    "title": "Enhancing Human Summaries for Question-Answer Generation in Education",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{gonzalez-etal-2023-enhancing,\n    title = \"Enhancing Human Summaries for Question-Answer Generation in Education\",\n    author = \"Gonzalez, Hannah  and\n      Dugan, Liam  and\n      Miltsakaki, Eleni  and\n      Cui, Zhiqi  and\n      Ren, Jiaxuan  and\n      Li, Bryan  and\n      Upadhyay, Shriyash  and\n      Ginsberg, Etan  and\n      Callison-Burch, Chris\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.9\",\n    doi = \"10.18653/v1/2023.bea-1.9\",\n    pages = \"108--118\",\n    abstract = \"We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality QG than automatic summaries. In this paper, a) we show that advances in Large Language Models (LLMs) are not yet sufficient to generate quality summaries for QG and b) we introduce a new methodology for enhancing bullet point student notes into fully fledged summaries and find that our methodology yields higher quality QG. We conducted a large-scale human annotation study of generated question-answer pairs for the evaluation of our methodology. In order to aid in future research, we release a new dataset of 9.2K human annotations of generated questions.\",\n}\n",
    "authors": [
        "Hannah Gonzalez",
        "Liam Dugan",
        "Eleni Miltsakaki",
        "Zhiqi Cui",
        "Jiaxuan Ren",
        "Bryan Li",
        "Shriyash Upadhyay",
        "Etan Ginsberg",
        "Chris Callison-Burch"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.9.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/17305118-5263-56f8-ac46-6a96e3a72510.pdf",
    "abstract": "We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality QG than automatic summaries. In this paper, a) we show that advances in Large Language Models (LLMs) are not yet sufficient to generate quality summaries for QG and b) we introduce a new methodology for enhancing bullet point student notes into fully fledged summaries and find that our methodology yields higher quality QG. We conducted a large-scale human annotation study of generated question-answer pairs for the evaluation of our methodology. In order to aid in future research, we release a new dataset of 9.2K human annotations of generated questions.",
    "num_pages": 11
}