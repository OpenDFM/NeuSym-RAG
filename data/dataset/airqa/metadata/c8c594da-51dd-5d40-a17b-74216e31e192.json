{
    "uuid": "c8c594da-51dd-5d40-a17b-74216e31e192",
    "title": "Does BERT Exacerbate Gender or L1 Biases in Automated English Speaking Assessment?",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)",
    "bibtex": "@inproceedings{kwako-etal-2023-bert,\n    title = \"Does {BERT} Exacerbate Gender or {L}1 Biases in Automated {E}nglish Speaking Assessment?\",\n    author = \"Kwako, Alexander  and\n      Wan, Yixin  and\n      Zhao, Jieyu  and\n      Hansen, Mark  and\n      Chang, Kai-Wei  and\n      Cai, Li\",\n    editor = {Kochmar, Ekaterina  and\n      Burstein, Jill  and\n      Horbach, Andrea  and\n      Laarmann-Quante, Ronja  and\n      Madnani, Nitin  and\n      Tack, Ana{\\\"\\i}s  and\n      Yaneva, Victoria  and\n      Yuan, Zheng  and\n      Zesch, Torsten},\n    booktitle = \"Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.bea-1.54\",\n    doi = \"10.18653/v1/2023.bea-1.54\",\n    pages = \"668--681\",\n    abstract = \"In English speaking assessment, pretrained large language models (LLMs) such as BERT can score constructed response items as accurately as human raters. Less research has investigated whether LLMs perpetuate or exacerbate biases, which would pose problems for the fairness and validity of the test. This study examines gender and native language (L1) biases in human and automated scores, using an off-the-shelf (OOS) BERT model. Analyses focus on a specific type of bias known as differential item functioning (DIF), which compares examinees of similar English language proficiency. Results show that there is a moderate amount of DIF, based on examinees{'} L1 background in grade band 912. DIF is higher when scored by an OOS BERT model, indicating that BERT may exacerbate this bias; however, in practical terms, the degree to which BERT exacerbates DIF is very small. Additionally, there is more DIF for longer speaking items and for older examinees, but BERT does not exacerbate these patterns of DIF.\",\n}\n",
    "authors": [
        "Alexander Kwako",
        "Yixin Wan",
        "Jieyu Zhao",
        "Mark Hansen",
        "Kai-Wei Chang",
        "Li Cai"
    ],
    "pdf_url": "https://aclanthology.org/2023.bea-1.54.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/c8c594da-51dd-5d40-a17b-74216e31e192.pdf",
    "abstract": "In English speaking assessment, pretrained large language models (LLMs) such as BERT can score constructed response items as accurately as human raters. Less research has investigated whether LLMs perpetuate or exacerbate biases, which would pose problems for the fairness and validity of the test. This study examines gender and native language (L1) biases in human and automated scores, using an off-the-shelf (OOS) BERT model. Analyses focus on a specific type of bias known as differential item functioning (DIF), which compares examinees of similar English language proficiency. Results show that there is a moderate amount of DIF, based on examineesâ€™ L1 background in grade band 912. DIF is higher when scored by an OOS BERT model, indicating that BERT may exacerbate this bias; however, in practical terms, the degree to which BERT exacerbates DIF is very small. Additionally, there is more DIF for longer speaking items and for older examinees, but BERT does not exacerbate these patterns of DIF.",
    "num_pages": 14
}