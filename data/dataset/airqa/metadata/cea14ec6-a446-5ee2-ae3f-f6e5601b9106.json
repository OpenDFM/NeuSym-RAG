{
    "uuid": "cea14ec6-a446-5ee2-ae3f-f6e5601b9106",
    "title": "DuluthNLP at SemEval-2023 Task 12: AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    "bibtex": "@inproceedings{akrah-pedersen-2023-duluthnlp,\n    title = \"{D}uluth{NLP} at {S}em{E}val-2023 Task 12: {A}fri{S}enti-{S}em{E}val: Sentiment Analysis for Low-resource {A}frican Languages using {T}witter Dataset\",\n    author = \"Akrah, Samuel  and\n      Pedersen, Ted\",\n    editor = {Ojha, Atul Kr.  and\n      Do{\\u{g}}ru{\\\"o}z, A. Seza  and\n      Da San Martino, Giovanni  and\n      Tayyar Madabushi, Harish  and\n      Kumar, Ritesh  and\n      Sartori, Elisa},\n    booktitle = \"Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.semeval-1.236\",\n    doi = \"10.18653/v1/2023.semeval-1.236\",\n    pages = \"1697--1701\",\n    abstract = \"This paper describes the DuluthNLP system that participated in Task 12 of SemEval-2023 on AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset. Given a set of tweets, the task requires participating systems to classify each tweet as negative, positive or neutral. We evaluate a range of monolingual and multilingual pretrained models on the Twi language dataset, one among the 14 African languages included in the SemEval task. We introduce TwiBERT, a new pretrained model trained from scratch. We show that TwiBERT, along with mBERT, generally perform best when trained on the Twi dataset, achieving an F1 score of 64.29{\\%} on the official evaluation test data, which ranks 14 out of 30 of the total submissions for Track 10. The TwiBERT model is released at \\url{https://huggingface.co/sakrah/TwiBERT}\",\n}\n",
    "authors": [
        "Samuel Akrah",
        "Ted Pedersen"
    ],
    "pdf_url": "https://aclanthology.org/2023.semeval-1.236.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/cea14ec6-a446-5ee2-ae3f-f6e5601b9106.pdf",
    "abstract": "This paper describes the DuluthNLP system that participated in Task 12 of SemEval-2023 on AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset. Given a set of tweets, the task requires participating systems to classify each tweet as negative, positive or neutral. We evaluate a range of monolingual and multilingual pretrained models on the Twi language dataset, one among the 14 African languages included in the SemEval task. We introduce TwiBERT, a new pretrained model trained from scratch. We show that TwiBERT, along with mBERT, generally perform best when trained on the Twi dataset, achieving an F1 score of 64.29% on the official evaluation test data, which ranks 14 out of 30 of the total submissions for Track 10. The TwiBERT model is released at https://huggingface.co/sakrah/TwiBERT",
    "num_pages": 5
}