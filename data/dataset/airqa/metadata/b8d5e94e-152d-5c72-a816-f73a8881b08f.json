{
    "uuid": "b8d5e94e-152d-5c72-a816-f73a8881b08f",
    "title": "Glossy Bytes: Neural Glossing using Subword Encoding",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    "bibtex": "@inproceedings{cross-etal-2023-glossy,\n    title = \"Glossy Bytes: Neural Glossing using Subword Encoding\",\n    author = \"Cross, Ziggy  and\n      Yun, Michelle  and\n      Apparaju, Ananya  and\n      MacCabe, Jata  and\n      Nicolai, Garrett  and\n      Silfverberg, Miikka\",\n    editor = {Nicolai, Garrett  and\n      Chodroff, Eleanor  and\n      Mailhot, Frederic  and\n      {\\c{C}}{\\\"o}ltekin, {\\c{C}}a{\\u{g}}r{\\i}},\n    booktitle = \"Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.sigmorphon-1.24\",\n    doi = \"10.18653/v1/2023.sigmorphon-1.24\",\n    pages = \"222--229\",\n    abstract = \"This paper presents several different neural subword modelling based approaches to interlinear glossing for seven under-resourced languages as a part of the 2023 SIGMORPHON shared task on interlinear glossing. We experiment with various augmentation and tokenization strategies for both the open and closed tracks of data. We found that while byte-level models may perform well for greater amounts of data, character based approaches remain competitive in their performance in lower resource settings.\",\n}\n",
    "authors": [
        "Ziggy Cross",
        "Michelle Yun",
        "Ananya Apparaju",
        "Jata MacCabe",
        "Garrett Nicolai",
        "Miikka Silfverberg"
    ],
    "pdf_url": "https://aclanthology.org/2023.sigmorphon-1.24.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/b8d5e94e-152d-5c72-a816-f73a8881b08f.pdf",
    "abstract": "This paper presents several different neural subword modelling based approaches to interlinear glossing for seven under-resourced languages as a part of the 2023 SIGMORPHON shared task on interlinear glossing. We experiment with various augmentation and tokenization strategies for both the open and closed tracks of data. We found that while byte-level models may perform well for greater amounts of data, character based approaches remain competitive in their performance in lower resource settings.",
    "num_pages": 8
}