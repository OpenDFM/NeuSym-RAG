{
    "uuid": "700f7cfd-9b05-5fbf-b56e-055498f5dae8",
    "title": "BatchEval: Towards Human-like Text Evaluation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{yuan-etal-2024-batcheval,\n    title = \"{B}atch{E}val: Towards Human-like Text Evaluation\",\n    author = \"Yuan, Peiwen  and\n      Feng, Shaoxiong  and\n      Li, Yiwei  and\n      Wang, Xinglin  and\n      Pan, Boyuan  and\n      Wang, Heda  and\n      Hu, Yao  and\n      Li, Kan\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.846\",\n    doi = \"10.18653/v1/2024.acl-long.846\",\n    pages = \"15940--15958\",\n    abstract = \"Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5{\\%} on Pearson correlations with only 64{\\%} API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.\",\n}\n",
    "authors": [
        "Peiwen Yuan",
        "Shaoxiong Feng",
        "Yiwei Li",
        "Xinglin Wang",
        "Boyuan Pan",
        "Heda Wang",
        "Yao Hu",
        "Kan Li"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.846.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/700f7cfd-9b05-5fbf-b56e-055498f5dae8.pdf",
    "abstract": "Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.",
    "num_pages": 19
}