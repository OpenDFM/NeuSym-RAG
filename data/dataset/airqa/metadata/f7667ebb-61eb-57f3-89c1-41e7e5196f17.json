{
    "uuid": "f7667ebb-61eb-57f3-89c1-41e7e5196f17",
    "title": "Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{cegin-etal-2024-effects,\n    title = \"Effects of diversity incentives on sample diversity and downstream model performance in {LLM}-based text augmentation\",\n    author = \"Cegin, Jan  and\n      Pecher, Branislav  and\n      Simko, Jakub  and\n      Srba, Ivan  and\n      Bielikova, Maria  and\n      Brusilovsky, Peter\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.710\",\n    doi = \"10.18653/v1/2024.acl-long.710\",\n    pages = \"13148--13171\",\n    abstract = \"The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts{'} lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.\",\n}\n",
    "authors": [
        "Jan Cegin",
        "Branislav Pecher",
        "Jakub Simko",
        "Ivan Srba",
        "Maria Bielikova",
        "Peter Brusilovsky"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.710.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f7667ebb-61eb-57f3-89c1-41e7e5196f17.pdf",
    "abstract": "The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune downstream models. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated textsâ€™ lexical diversity and downstream model performance. We compare the effects over 5 different LLMs, 6 datasets and 2 downstream models. We show that diversity is most increased by taboo words, but downstream model performance is highest with hints.",
    "num_pages": 24
}