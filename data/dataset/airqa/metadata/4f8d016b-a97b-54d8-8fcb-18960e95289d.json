{
    "uuid": "4f8d016b-a97b-54d8-8fcb-18960e95289d",
    "title": "Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2023,
    "volume": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    "bibtex": "@inproceedings{lyu-etal-2023-semantic,\n    title = \"Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering\",\n    author = \"Lyu, Chenyang  and\n      Ji, Tianbo  and\n      Graham, Yvette  and\n      Foster, Jennifer\",\n    editor = \"Padmakumar, Vishakh  and\n      Vallejo, Gisela  and\n      Fu, Yao\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-srw.7\",\n    doi = \"10.18653/v1/2023.acl-srw.7\",\n    pages = \"50--56\",\n    abstract = \"Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code is publicly available at \\url{https://github.com/lyuchenyang/Semantic-aware-VideoQA}.\",\n}\n",
    "authors": [
        "Chenyang Lyu",
        "Tianbo Ji",
        "Yvette Graham",
        "Jennifer Foster"
    ],
    "pdf_url": "https://aclanthology.org/2023.acl-srw.7.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2023/4f8d016b-a97b-54d8-8fcb-18960e95289d.pdf",
    "abstract": "Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code is publicly available at https://github.com/lyuchenyang/Semantic-aware-VideoQA.",
    "num_pages": 7
}