{
    "uuid": "00a3a4a7-a8ff-58c3-99de-6dabf13bc06d",
    "title": "Intrinsic Task-based Evaluation for Referring Expression Generation",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    "bibtex": "@inproceedings{chen-etal-2024-intrinsic,\n    title = \"Intrinsic Task-based Evaluation for Referring Expression Generation\",\n    author = \"Chen, Guanyi  and\n      Same, Fahime  and\n      Van Deemter, Kees\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.389\",\n    doi = \"10.18653/v1/2024.acl-long.389\",\n    pages = \"7220--7231\",\n    abstract = \"Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on WEBNLG, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in WEBNLG but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants{'} ratings more reliable and discriminable.\",\n}\n",
    "authors": [
        "Guanyi Chen",
        "Fahime Same",
        "Kees Van Deemter"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-long.389.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/00a3a4a7-a8ff-58c3-99de-6dabf13bc06d.pdf",
    "abstract": "Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on WEBNLG, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in WEBNLG but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participantsâ€™ ratings more reliable and discriminable.",
    "num_pages": 12
}