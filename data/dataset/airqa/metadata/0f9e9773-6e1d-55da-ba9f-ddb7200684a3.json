{
    "uuid": "0f9e9773-6e1d-55da-ba9f-ddb7200684a3",
    "title": "FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{lan-etal-2024-focus,\n    title = \"{FOCUS}: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models\",\n    author = \"Lan, Kaixin  and\n      Fang, Tao  and\n      Wong, Derek  and\n      Xu, Yabo  and\n      Chao, Lidia  and\n      Zhao, Cecilia\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.858\",\n    doi = \"10.18653/v1/2024.findings-acl.858\",\n    pages = \"14432--14447\",\n    abstract = \"Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to produce verbatim copies of paragraphs from their training data. This is problematic as PLMs are trained on corpora constructed by human authors. As such, there is a pressing need for research to promote the generation of original content by these models. In this study, we introduce a unique {``}self-plagiarism{''} contrastive decoding strategy, aimed at boosting the originality of text produced by PLMs. Our method entails modifying prompts in LLMs to develop an amateur model and a professional model. Specifically, the amateur model is urged to plagiarize using three plagiarism templates we have designed, while the professional model maintains its standard language model status. This strategy employs prompts to stimulate the model{'}s capacity to identify non-original candidate token combinations and subsequently impose penalties. The application of this strategy is integrated prior to the model{'}s final layer, ensuring smooth integration with most existing PLMs (T5, GPT, LLaMA) without necessitating further adjustments. Implementing our strategy, we noted a significant decline in non-original sequences comprised of more than three words in the academic AASC dataset and the story-based ROCStories dataset. Source code and scripts will be released after the paper{'}s acceptance and publication.\",\n}\n",
    "authors": [
        "Kaixin Lan",
        "Tao Fang",
        "Derek Wong",
        "Yabo Xu",
        "Lidia Chao",
        "Cecilia Zhao"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.858.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/0f9e9773-6e1d-55da-ba9f-ddb7200684a3.pdf",
    "abstract": "Pre-trained Language Models (PLMs) have shown impressive results in various Natural Language Generation (NLG) tasks, such as powering chatbots and generating stories. However, an ethical concern arises due to their potential to produce verbatim copies of paragraphs from their training data. This is problematic as PLMs are trained on corpora constructed by human authors. As such, there is a pressing need for research to promote the generation of original content by these models. In this study, we introduce a unique “self-plagiarism” contrastive decoding strategy, aimed at boosting the originality of text produced by PLMs. Our method entails modifying prompts in LLMs to develop an amateur model and a professional model. Specifically, the amateur model is urged to plagiarize using three plagiarism templates we have designed, while the professional model maintains its standard language model status. This strategy employs prompts to stimulate the model’s capacity to identify non-original candidate token combinations and subsequently impose penalties. The application of this strategy is integrated prior to the model’s final layer, ensuring smooth integration with most existing PLMs (T5, GPT, LLaMA) without necessitating further adjustments. Implementing our strategy, we noted a significant decline in non-original sequences comprised of more than three words in the academic AASC dataset and the story-based ROCStories dataset. Source code and scripts will be released after the paper’s acceptance and publication.",
    "num_pages": 16
}