{
    "uuid": "7bf346a1-d65b-59d5-8f0e-dbb587f350f1",
    "title": "AlexUNLP-STM at NADI 2024 shared task: Quantifying the Arabic Dialect Spectrum with Contrastive Learning, Weighted Sampling, and BERT-based Regression Ensemble",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of The Second Arabic Natural Language Processing Conference",
    "bibtex": "@inproceedings{sakr-etal-2024-alexunlp,\n    title = \"{A}lex{UNLP}-{STM} at {NADI} 2024 shared task: Quantifying the {A}rabic Dialect Spectrum with Contrastive Learning, Weighted Sampling, and {BERT}-based Regression Ensemble\",\n    author = \"Sakr, Abdelrahman  and\n      Torki, Marwan  and\n      El-Makky, Nagwa\",\n    editor = \"Habash, Nizar  and\n      Bouamor, Houda  and\n      Eskander, Ramy  and\n      Tomeh, Nadi  and\n      Abu Farha, Ibrahim  and\n      Abdelali, Ahmed  and\n      Touileb, Samia  and\n      Hamed, Injy  and\n      Onaizan, Yaser  and\n      Alhafni, Bashar  and\n      Antoun, Wissam  and\n      Khalifa, Salam  and\n      Haddad, Hatem  and\n      Zitouni, Imed  and\n      AlKhamissi, Badr  and\n      Almatham, Rawan  and\n      Mrini, Khalil\",\n    booktitle = \"Proceedings of The Second Arabic Natural Language Processing Conference\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.arabicnlp-1.81\",\n    doi = \"10.18653/v1/2024.arabicnlp-1.81\",\n    pages = \"735--741\",\n    abstract = \"Recognizing the nuanced spectrum of dialectness in Arabic text poses a significant challenge for natural language processing (NLP) tasks. Traditional dialect identification (DI) methods treat the task as binary, overlooking the continuum of dialect variation present in Arabic speech and text. In this paper, we describe our submission to the NADI shared Task of ArabicNLP 2024. We participated in Subtask 2 - ALDi Estimation, which focuses on estimating the Arabic Level of Dialectness (ALDi) for Arabic text, indicating how much it deviates from Modern Standard Arabic (MSA) on a scale from 0 to 1, where 0 means MSA and 1 means high divergence from MSA. We explore diverse training approaches, including contrastive learning, applying a random weighted sampler along with fine-tuning a regression task based on the AraBERT model, after adding a linear and non-linear layer on top of its pooled output. Finally, performing a brute force ensemble strategy increases the performance of our system. Our proposed solution achieved a Root Mean Squared Error (RMSE) of 0.1406, ranking second on the leaderboard.\",\n}\n",
    "authors": [
        "Abdelrahman Sakr",
        "Marwan Torki",
        "Nagwa El-Makky"
    ],
    "pdf_url": "https://aclanthology.org/2024.arabicnlp-1.81.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/7bf346a1-d65b-59d5-8f0e-dbb587f350f1.pdf",
    "abstract": "Recognizing the nuanced spectrum of dialectness in Arabic text poses a significant challenge for natural language processing (NLP) tasks. Traditional dialect identification (DI) methods treat the task as binary, overlooking the continuum of dialect variation present in Arabic speech and text. In this paper, we describe our submission to the NADI shared Task of ArabicNLP 2024. We participated in Subtask 2 - ALDi Estimation, which focuses on estimating the Arabic Level of Dialectness (ALDi) for Arabic text, indicating how much it deviates from Modern Standard Arabic (MSA) on a scale from 0 to 1, where 0 means MSA and 1 means high divergence from MSA. We explore diverse training approaches, including contrastive learning, applying a random weighted sampler along with fine-tuning a regression task based on the AraBERT model, after adding a linear and non-linear layer on top of its pooled output. Finally, performing a brute force ensemble strategy increases the performance of our system. Our proposed solution achieved a Root Mean Squared Error (RMSE) of 0.1406, ranking second on the leaderboard.",
    "num_pages": 7
}