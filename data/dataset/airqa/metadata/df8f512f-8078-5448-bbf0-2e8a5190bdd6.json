{
    "uuid": "df8f512f-8078-5448-bbf0-2e8a5190bdd6",
    "title": "The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    "bibtex": "@inproceedings{siegel-etal-2024-probabilities,\n    title = \"The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models\",\n    author = \"Siegel, Noah  and\n      Camburu, Oana-Maria  and\n      Heess, Nicolas  and\n      Perez-Ortiz, Maria\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-short.49\",\n    doi = \"10.18653/v1/2024.acl-short.49\",\n    pages = \"530--546\",\n    abstract = \"In order to oversee advanced AI systems, it is important to understand their reasons for generating a given output. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are truly capturing the factors responsible for the model{'}s predictions: the most {``}human-like{''} explanation may be different from the one that is most faithful to the model{'}s true decision making process. In this work, we introduce the correlational counterfactual test (CCT), a faithfulness metric based on counterfactual input edits that takes into account not just the binary label change, but the total shift in the model{'}s predicted label distribution. We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama-2 family on three NLP tasks. We find that these explanations are indeed more likely to mention factors when they are impactful to the model{'}s prediction, with the degree of association increasing with model size but varying significantly by task.\",\n}\n",
    "authors": [
        "Noah Siegel",
        "Oana-Maria Camburu",
        "Nicolas Heess",
        "Maria Perez-Ortiz"
    ],
    "pdf_url": "https://aclanthology.org/2024.acl-short.49.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/df8f512f-8078-5448-bbf0-2e8a5190bdd6.pdf",
    "abstract": "In order to oversee advanced AI systems, it is important to understand their reasons for generating a given output. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are truly capturing the factors responsible for the model’s predictions: the most “human-like” explanation may be different from the one that is most faithful to the model’s true decision making process. In this work, we introduce the correlational counterfactual test (CCT), a faithfulness metric based on counterfactual input edits that takes into account not just the binary label change, but the total shift in the model’s predicted label distribution. We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama-2 family on three NLP tasks. We find that these explanations are indeed more likely to mention factors when they are impactful to the model’s prediction, with the degree of association increasing with model size but varying significantly by task.",
    "num_pages": 17
}