{
    "uuid": "f57a42c1-3b89-51cf-a3e2-7cc9b12217e3",
    "title": "MoE-SLU: Towards ASR-Robust Spoken Language Understanding via Mixture-of-Experts",
    "conference_full": "Annual Meeting of the Association for Computational Linguistics",
    "conference": "ACL",
    "year": 2024,
    "volume": "Findings of the Association for Computational Linguistics: ACL 2024",
    "bibtex": "@inproceedings{cheng-etal-2024-moe,\n    title = \"{M}o{E}-{SLU}: Towards {ASR}-Robust Spoken Language Understanding via Mixture-of-Experts\",\n    author = \"Cheng, Xuxin  and\n      Zhu, Zhihong  and\n      Zhuang, Xianwei  and\n      Chen, Zhanpeng  and\n      Huang, Zhiqi  and\n      Zou, Yuexian\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Findings of the Association for Computational Linguistics: ACL 2024\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-acl.882\",\n    doi = \"10.18653/v1/2024.findings-acl.882\",\n    pages = \"14868--14879\",\n    abstract = \"As a crucial task in the task-oriented dialogue systems, spoken language understanding (SLU) has garnered increasing attention. However, errors from automatic speech recognition (ASR) often hinder the performance of understanding. To tackle this problem, we propose MoE-SLU, an ASR-Robust SLU framework based on the mixture-of-experts technique. Specifically, we first introduce three strategies to generate additional transcripts from clean transcripts. Then, we employ the mixture-of-experts technique to weigh the representations of the generated transcripts, ASR transcripts, and the corresponding clean manual transcripts. Additionally, we also regularize the weighted average of predictions and the predictions of ASR transcripts by minimizing the Jensen-Shannon Divergence (JSD) between these two output distributions. Experiment results on three benchmark SLU datasets demonstrate that our MoE-SLU achieves state-of-the-art performance. Further model analysis also verifies the superiority of our method.\",\n}\n",
    "authors": [
        "Xuxin Cheng",
        "Zhihong Zhu",
        "Xianwei Zhuang",
        "Zhanpeng Chen",
        "Zhiqi Huang",
        "Yuexian Zou"
    ],
    "pdf_url": "https://aclanthology.org/2024.findings-acl.882.pdf",
    "pdf_path": "data/dataset/airqa/papers/acl2024/f57a42c1-3b89-51cf-a3e2-7cc9b12217e3.pdf",
    "abstract": "As a crucial task in the task-oriented dialogue systems, spoken language understanding (SLU) has garnered increasing attention. However, errors from automatic speech recognition (ASR) often hinder the performance of understanding. To tackle this problem, we propose MoE-SLU, an ASR-Robust SLU framework based on the mixture-of-experts technique. Specifically, we first introduce three strategies to generate additional transcripts from clean transcripts. Then, we employ the mixture-of-experts technique to weigh the representations of the generated transcripts, ASR transcripts, and the corresponding clean manual transcripts. Additionally, we also regularize the weighted average of predictions and the predictions of ASR transcripts by minimizing the Jensen-Shannon Divergence (JSD) between these two output distributions. Experiment results on three benchmark SLU datasets demonstrate that our MoE-SLU achieves state-of-the-art performance. Further model analysis also verifies the superiority of our method.",
    "num_pages": 12
}