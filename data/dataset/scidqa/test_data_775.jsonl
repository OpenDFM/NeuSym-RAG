{"uuid": "2ebe3d38-6329-5e63-b314-35b417f98e5f", "question": "in appendix A why duplicate memory data instead of just using a smaller memory size?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c6f416eb-5f67-5c83-ad8c-8e765b4cd91d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "in appendix A why duplicate memory data instead of just using a smaller memory size?", "reference_answer": "A: Conceptually, it would be indeed be sufficient to use a smaller memory to investigate this effect; in fact the proposed results in Figure 5 begin to do this - but the authors wanted to corroborate the finding by also measuring it in a different way. For implementation reasons, the two approaches are not guaranteed to be equivalent: for example, duplicating the data that each actor adds increases the computational load on the replay server, whereas using a smaller memory size does not. During development the authors noticed that in very extreme cases, many actors adding large volumes of data to the replay memory could overwhelm it, causing a slowdown in sampling which would affect the performance of the learner and thus the overall results.\n \nIn the proposed experiments in Appendix A where the authors sought to determine whether recency of data was the reason for the proposed observed scalability results, the authors wanted to make certain that the load on the replay server in the duplicated-data experiments would be the same as in the experiments with the corresponding numbers of real actors, to ensure a fair comparison. In practice, the authors did not find that the authors were running into any such contention issues in these experiments, and the results from Figure 5 do agree with those in Appendix A. However, the authors felt that it was still helpful to include both of the results in order to cover this aspect thoroughly. The authors will add a note explaining this."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "978b5a65-4dba-5832-a401-f90df54f2451", "question": "The paper only shows results in low-dimensional domains (small graphs). Since score-based generative modelling also works in high-dimensional domains (such as images), it would be interesting to see if the method can be used for reward learning from scene images, for example.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["d66a5cd9-35b7-5db9-816b-47aaae8ae114"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The paper only shows results in low-dimensional domains (small graphs). Since score-based generative modelling also works in high-dimensional domains (such as images), it would be interesting to see if the method can be used for reward learning from scene images, for example.", "reference_answer": "A: To show the proposed method can be used for reward learning from raw-pixel images, the authors further analyse the proposed framework by training a target score network that takes the image as input and then use the trained target score network to train the proposed policies.\nThe authors compare this image-based gradient field (denoted as Ours(Image)) with 1) the state-based target score network, denoted as *Ours(State)*; and 2) a goal-conditioned baseline, denoted as *Goal(State)*. \nNote that *Ours(State)* and Goals(State) represent *Ours(SAC)* and  *Goal(SAC)* in the main paper, respectively.\nResults in [**our site**](https://sites.google.com/view/neurips2022-paper2108-rebuttal/) Sec. 5 show that *Ours(Image)* achieves comparable results with Goal(State), demonstrating the proposed framework still has good performance even though the target score network uses the raw-pixel images as input.\nThese results also indicate that raw-pixel observation is a distractor compared with the proposed key focus. Hence, the authors choose to conduct the proposed experiments in a state-based setting in the proposed main paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bba3cb2d-5ca6-5d2a-a124-5db2c79018c8", "question": "Isn't it a challenge that cosine similarity can only be applied to the scenario with at least triplet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb9a3e42-f027-5472-bfc8-5b94af5294a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Isn't it a challenge that cosine similarity can only be applied to the scenario with at least triplet?", "reference_answer": "A: The proposed methods could actually be straightforwardly applied to settings where it only contains image-text pairs."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "33f3ada9-dc9b-570a-bdc4-161cfd1e0043", "question": "Depth vs Width, how do they impact the formulation of NHK?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["46b47e0a-c219-5d2c-a609-f04a9b617ca4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Depth vs Width, how do they impact the formulation of NHK?", "reference_answer": "A: (Depth) As described in section 3.1, the definition of NHK is associated with two layers ($l$ and $l+k$) in the GNN. For implementation, authors choose $k=1$ for non-parametric NHK based on the fact that multi-layer distillation loss is equivalent to the single-layer distillation loss by changing hyper-parameters. For parametric NHK, authors choose $l+k$ as the last layer and $l$ as the first layer in order to learn a NHK that uses as much information as possible.\n**(Width)** Width may refer to the graph size or the dimension of node features as authors don’t recall mentioning the width of NHK. If this is the case, then both factors could affect the formulation of NHK by changing the definition of $\\Delta(f_\\theta, \\mathcal G)$. Empirically, the proposed method is equally effective across different settings of both factors."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "161bd179-20f0-5675-b13e-9669a45d15cc", "question": "What is the architecture used in the experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb4b14e7-457a-5b7b-bfb8-fff2970120c9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the architecture used in the experiments?", "reference_answer": "A: The authors use hashed user ID and item ID to train embeddings end-to-end,\nthen the embeddings are concatenated to form the input of a MLP,\nthe outputs of this MLP serve as embeddings of $x$.\nThe authors have included a new figure of the overall framework in the revision of the proposed work,\nand the feature encoder network is depicted in the supplementary material."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a901ce96-7370-52a2-995d-e9e2618f981b", "question": "Explain why KL-Loss was used in the paper.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cd6253e-8a5a-55f2-8bdc-77afa61d48e4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain why KL-Loss was used in the paper.", "reference_answer": "A: Following CLIP, the authors use an image-to-text loss and a text-to-image loss to supervise the model. For image-to-text loss, using KL loss is equivalent to using cross-entropy loss, as the labels for each image are all one-hot encoded. However, for the text-to-image loss, there might be several image hits for a certain label in a mini-batch. The authors follow ActionCLIP [44] to use a KL loss to supervise the text-to-image logits. Specifically, the ground-truth matrix is constructed by taking the normalized probability of each multi-hot label for the corresponding rank. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9126a156-3df0-5aa1-b5ee-328edf475391", "question": "How does Theorem 1 imply that adversarially trained models rely on spurious features?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["08dc2c19-d41c-591a-93f5-d610b59856d5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does Theorem 1 imply that adversarially trained models rely on spurious features?", "reference_answer": "A: Theorem 1 shows that the optimal parameter $\\theta$ for the adversarial objective will have non-zero weights along spurious features, as the second term in the loss function (eq 2) involves the q-norm of $\\theta$. Thus, optimizing for the adversarial objective forces a balance between having $\\theta$ be as close to $\\theta^\\text{opt}$ as possible and $\\theta$ having as low q-norm as possible. It may be necessary to diverge from the $\\theta^\\text{opt}$ (i.e. shift weight from core features to spurious ones) so that the total objective is minimized. This is further validated by the results in Figures 2 and 3 which use the characterization in theorem 1 to demonstrate that the adversarially trained model relies on the spurious features (non-zero NFS values). Note that a standardly trained model, under the proposed theoretical setting, will exactly recover $\\theta^\\text{opt}$; that is, they will make no use of spurious features. Thus, any new use of the spurious features is due to adversarial training."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4ee8c834-f378-5aa0-9c99-c5d55008a659", "question": "Why can the motion between patches I_t[x] and I_{t+1}[x] be approximated with linear transformation? Why the transformation M only depends on the displacement of the center pixel whereas different pixels in a patch I_t[x] could have different displacements?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b72f144b-ccbe-500a-862f-dcfc1b4d0f61"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why can the motion between patches I_t[x] and I_{t+1}[x] be approximated with linear transformation? Why the transformation M only depends on the displacement of the center pixel whereas different pixels in a patch I_t[x] could have different displacements?", "reference_answer": "A: One motivation of the proposed model is based on Fourier analysis: An image patch I can be expressed as I(x) = sum_k c_k e^{i<\\omega_k, x>} in a Fourier decomposition. If the authors shift it by dx, the shifted image patch J(x) = I(x - dx) = sum_k c_k e^{-i<\\omega_k, dx>} e^{i<\\omega_k, x>}. The change from the complex number c_k to c_k e^{-i<\\omega_k, dx>} corresponds to rotating a 2D vector by a 2 x 2 matrix. This is a simple example that the shift can be represented by a linear transformation in the frequency domain, as a change in phase. The proposed model does not assume Fourier basis or its localized version such as Gabor filters. The proposed model figures it out with generic vector and matrix representations. The paper assume that the motion is smooth, so that within a relative small local patch, the motion is constant. Of course, the patch size or the filter size should be related to image resolution. For images with higher resolution, the authors may want to use smaller filter size to make this assumption hold. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e4cca68-8a27-52dc-9bad-6598ef5bb164", "question": "The proposed backdoor images in Figure 9 are smoother than other images (even the clean ones). Was any image smoothing or interpolation applied?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b761f966-dde6-5d75-9704-25545a8198a2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The proposed backdoor images in Figure 9 are smoother than other images (even the clean ones). Was any image smoothing or interpolation applied?", "reference_answer": "A: The authors did not apply any image smoothing or interpolation. The trigger injection mechanism in the proposed attack is generated directly from a conditional generator, and the authors did not perform any post-processing. Thus, the smoothing effect is not expected. We've also verified the same clean and WB's backdoor images from the original samples used for Figure 9, but the observed smoothness does not exist. The authors believe this smoothness is probably caused by some accidental effects during the process of creating Figure 9 in the PDF version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9c07d2a4-983b-53ba-941f-c6c58d381e66", "question": "How does the proposed method scale to large datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["c54123c6-251b-50b5-a775-396bdfee5734"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed method scale to large datasets?", "reference_answer": "A: The authors evaluate the scalability of the proposed method on two large datasets: retail and retail-II. The results are shown in Table 2. The proposed method can scale to large datasets and achieve good performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "eadca53b-2228-5ea7-a02e-275ef43dc134", "question": "How does stochastic rounding align with the context of the mantissas rounding?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["70d23f47-63f0-5df1-88ba-b148cfca9c6a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does stochastic rounding align with the context of the mantissas rounding?", "reference_answer": "A: Following the example that you mentioned if shifted mantissa is $x= (0.01011001010101010100000)_2$, then $x_1= (0.010110)_2$ and $x_2= (0.010111)_2$. The shifted mantissa $x$ is going to be randomly rounded to either $x_1$ or $x_2$ based on the probability given on equation (13) on line 442 of the manuscript. The realization of stochastic rounding in hardware is added in the **Figure 4** of the modified manuscript. In this figure, the direction of rounding is determined by comparing a random number that is generated on-the-fly with the lower 17-bit of the mantissa."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5708f81b-aa7c-5589-9689-cdfd8f416888", "question": "What type of task characteristic makes a model most susceptible to grokking? If the mismatch between decoder learning speed and representation learning speed is key to grokking, would you expect that the four phases in the phase diagram to exist in general (e.g. for natural vision or nlp tasks)? If it's not general, what is it about the specific tasks and models studied in this work that leads to these different regions?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6e2bd28f-ad70-5fe4-a8e6-178bd931cf78"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What type of task characteristic makes a model most susceptible to grokking? If the mismatch between decoder learning speed and representation learning speed is key to grokking, would you expect that the four phases in the phase diagram to exist in general (e.g. for natural vision or nlp tasks)? If it's not general, what is it about the specific tasks and models studied in this work that leads to these different regions?", "reference_answer": "A: It is expected that four phases of learning to exist in general, but the ease of obtaining them in practice depends on specific tasks. Grokking is easy to get on datasets in which initial representation is very far from a good, final representation. From comparing (a) arithmetic datasets and (b) image classification: (a) the embeddings of numbers are initialized as random vectors, thus far from the desired, structured representations. (b) Images, although pixelization may destroy or obfuscate some semantic information, other information (e.g. topology of  the image manifold) is still preserved, making it faster to learn a good representation. In fact, authors are able to observe grokking on MNIST classification, if they manually construct a suboptimal initial representation by using a large initialization scale. The dependence on training set size and the phase diagrams are consistent with theory developed in the paper, in Appendix I."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3ea4a52e-bab6-5b56-b1d8-90e1ddd61daa", "question": "Can you explain Table 6 in more detail?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["71c0db56-6716-5fe2-8b69-ed7a4b489f99"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you explain Table 6 in more detail?", "reference_answer": "A: Table 6 compares representations learned by different pretraining strategies by measuring the error of a linear classifier trained to distinguish different sets of features. The authors observe higher error for supervised representations compared to self-supervised initializations, indicating that after supervised pretraining on ImageNet, source and target features tend to be hard to distinguish and are therefore better aligned."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b3ee6f95-94d9-5542-b7fc-9bb8a25d6730", "question": "What is a reasonable choice for $\\beta$ if no validation data is available?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["5ac229f3-30ee-5264-b149-6c0871f02d10"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is a reasonable choice for $\\beta$ if no validation data is available?", "reference_answer": "A: Choosing a constant $\\beta=0.6$ still has reasonable average performance. For the 70 trials in Table 1, the median improvement of $\\beta=0.6$ over $\\beta=1.0$ is 1.7% (absolute) accuracy points. The mean improvement is 2.1% absolute."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0f6a338d-a934-5dbf-b8f8-134468e25e9f", "question": "What are some references that support the conjecture that VAEs produce blurry images.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e544881d-455a-5477-9a3c-78a7979af303"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are some references that support the conjecture that VAEs produce blurry images.", "reference_answer": "A: The following papers clearly show the blurry images generated by VAEs. \nAutoencoding beyond pixels using a learned similarity metric\nhttps://arxiv.org/abs/1512.09300\nWasserstein Auto-Encoders\nhttps://arxiv.org/abs/1711.01558\nImplicit Discriminator in Variational Autoencoder\nhttps://arxiv.org/abs/1909.13062"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "11985174-3dd7-5fa9-928c-5db7d29d1ca5", "question": "Why is there no benchmark shown in the SARCOS figures?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e391ac18-50de-5c7d-ab70-9a7b36fb2cb0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is there no benchmark shown in the SARCOS figures?", "reference_answer": "A: The authors are not aware of established state-of-the-art performance for the SARCOS task in the literature."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "710651fb-995d-5339-b21c-e5e13fd3cc64", "question": "Which dataset is used for Figure 5?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["702daa48-ef5a-50ba-912a-30163e595c48"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which dataset is used for Figure 5?", "reference_answer": "A: Amazon-670K is used for Figure 5, we'll update the figure caption to mention this"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a3c3b2e3-9949-54c7-ad72-674ebfeff4ff", "question": "Marginal Gain. The paper mainly compares with WaNet, and from Table 2, the performance is almost the same. For example, for GTSRB and TinyImageNet, the number is exactly the same. For the MNIST and CIFAR-10, WB is only 0.01 better than WaNet. As MNIST and CIFAR-10 are small datasets, where baselines are already very high, the author should consider using more complex dataset such as ImageNet. On the other hand, on the larger Tiny ImageNet dataset, the gain is 0. Why the method performs worse on All-to-all attack than all-to-one attack?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["b761f966-dde6-5d75-9704-25545a8198a2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Marginal Gain. The paper mainly compares with WaNet, and from Table 2, the performance is almost the same. For example, for GTSRB and TinyImageNet, the number is exactly the same. For the MNIST and CIFAR-10, WB is only 0.01 better than WaNet. As MNIST and CIFAR-10 are small datasets, where baselines are already very high, the author should consider using more complex dataset such as ImageNet. On the other hand, on the larger Tiny ImageNet dataset, the gain is 0. Why the method performs worse on All-to-all attack than all-to-one attack?", "reference_answer": "A: Please note that the main contribution of this paper is to extend the concept of imperceptible backdoor from the input space to the latent representation, which significantly improves the effectiveness against the existing defense mechanisms (as the authors described in L9-11 in the Abstract). The very recent work WaNet (ICLR'21) achieved much better input-space imperceptibility than prior works, which is also the reason the authors consider WaNet as state-of-the-art in this direction of research and compare the proposed performance to it. Compared to WaNet, WB achieves as good as, if not better, than WaNet from the aspects of attack success rate  (Tables 1 and 2)  and stealthiness at the input space (Table 8 in the supplementary material). However, from the latent space, WB is much more stealthy than prior works such that the proposed method can bypass the representative defenses used for analyzing the latent space, as the authors demonstrated in Section 5.3.  \nThe reason that the authors evaluated these datasets, including the larger TinyImageNet, is to follow the similar setups and network architectures as in previous backdoor attack and defense papers for a fair comparison to the earlier methods.\nThe objective of an all-to-one attack is to manipulate all the images in the dataset to one target label. In contrast, an all-to-all attack attempts to change the labels of images at the class level (i.e., images from different original classes will have different target labels).  One common setup for all-to-all attacks is to define the target label as one-shifted (e.g., for MNIST, the target label for digit \"0\" with the backdoor is 1, the target label for digit \"1\" with the backdoor is 2, the target label for digit \"2\" with the backdoor is 3, etc.), which is used in the proposed experiments as described in Section 5.2. Thus, compared to all-to-one attacks that only have a single target label, all-to-all attacks involve multiple target labels and each target label associates to less number of poisoned images correspondingly, making such all-to-all attacks more challenging. The trend where the attack performance is worse on all-to-all attacks than all-to-one attacks is also consistent with the existing works, including BadNets and WaNet."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "321bd3b2-8ae5-5cba-b9b5-cf25c570011d", "question": "Where is the error bar of Section 4?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["837eaf5a-a9f7-55ba-99bb-7f470f0624cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where is the error bar of Section 4?", "reference_answer": "A: * Error bar of main experimental results (Table 2,3) are reported in Appendix A.5 (Table 1,2,3), due to page limitation;\n* Error bar of ablation studies (Figure 4) are visualized as black bold lines on top of each bar in Figure 4. \n\nTo calculate such error bars, the authors run each model for $5$ independent times, and calculate standard deviation. Generally, most baselines, ablation variants, and MetaTKGR produce stable performance with low variation. It is also worth noting that MetaTKGR achieves statistically significant improvements over the best baseline, with p-value smaller than 0.001 (as shown in Table 2,3)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b4d249d5-c146-5e2d-beb9-5eafb895ef5c", "question": "Which model from Dhariwal et al performs best in terms of LPIPS perceptual metric?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which model from Dhariwal et al performs best in terms of LPIPS perceptual metric?", "reference_answer": "The best perfroming model based on the table is the one with DDPM, Channel 64, Res 1 and LPIPS of 0.208."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3b38486f-39f9-50ca-8eaa-d34c079ed0d8", "question": "Is 1x1 conv in the proposed downsampling block in Figure 3 binarized or remain FP32?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4f0119d8-a664-5be3-87ab-bf4c76d488cd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is 1x1 conv in the proposed downsampling block in Figure 3 binarized or remain FP32?", "reference_answer": "A: It remains FP32 during the experiments."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "27e3737b-02a0-57aa-b414-df4641479f03", "question": "Is there any potential negative societal impact?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["85fe266a-f62a-59af-a50a-695f76ff0cb4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there any potential negative societal impact?", "reference_answer": "A: This paper focuses on class incremental learning for image recognition. If the CIL methodology is used in sensitive applications such as face recognition (private area), it may cause some potential negative societal impact."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ea2fd529-60e7-5e14-b33c-830249652f90", "question": "How does the approach we proposed in the main text differ from cross-fitting?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9889e33e-ae05-5424-80ca-a7d2f63e06a5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the approach we proposed in the main text differ from cross-fitting?", "reference_answer": "A: The approach the authors proposed in the main text uses sequential sample splitting, i.e, it estimates the outcome model for the CADR score of observation O(t) only using data O(1), ..., O(t − 1). This has the same property as cross-fitting that the fitted model is independent of the observation to which it is applied. In Appendix E2 the authors also consider a non-sequential cross-time cross-fitting, which splits the data into folds over time and excludes adjacent folds when fitting a model in order to avoid dependence and which may facilitate running fewer model fitting subroutines. Figures 2 and 3 of appendix E2 show that same conclusions regarding the benefits of CADR persist even when using this alternative fitting procedure when compared to all other baseline estimators on the 57 OpenML-CC18 datasets, 4 target policies, and linear outcome regression models."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9fbc2ca3-d69c-55e9-b03e-64662914413b", "question": "What is the difference between SimMatch and the proposed method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a4dd3c3-5cc9-5eca-8030-eebb477b53c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference between SimMatch and the proposed method?", "reference_answer": "A: SimMatch's similarity is computed between samples and samples, whereas the proposed similarity is computed between samples and super-classes.\nSimMatch's aim is to make the similarity between the strongly augmented and weakly augmented views of the image consistent, whereas the proposed aim is for the image to be apart from other super-classes and be close to its corresponding super-class.\nSimMatch focuses mainly on learning more consistency information (between strongly augmented and weakly augmented views of an image). Differently, in addition to consistent information, the proposed methods focus on learning new discriminative information (between images and super-classes)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3cf2a63c-6377-5bd3-b7bb-ca25362fa741", "question": "Can CABI still work in situations where states or actions are irreversible?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ae801009-35b9-58e7-941e-726cb84d7b54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can CABI still work in situations where states or actions are irreversible?", "reference_answer": "A: The authors acknowledge that there are cases where states or actions are irreversible, i.e., previous states cannot be inferred based on the current state (for example, the current state is an initial state and its previous state does not exist or is invalid). The authors argue that mere backward imagination may suffer from such situation. While the proposed method, CABI, can mitigate this concern with the aid of *double check*. When a state $s\\_t$ is irreversible, the disagreement between the forward model and backward model will be large. Then the generated (backward) synthetic transition from $s\\_t$ will not be added into the model buffer. One can also see such evidence in the proposed toy example (section 4.1 and Figure 4). In the proposed toy RiskWorld datasets, there exist some states that are irreversible, e.g., the states that lie in the boundary. There also exists a danger zone in the RiskWorld task, and it is invalid to have samples in this zone. The authors can see from Figure 4(c) that backward model generates many invalid transitions that lie out of the support of the dataset or lie in the dangerous zone. However, CABI guarantees a good and reliable data generation where no invalid states are included. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5c5e4e8e-172f-59fc-9555-fb0c37a59bd8", "question": "The harmonic mean (H) metric not being an accurate representation of the performance of a GZSL method, since it requires calibration, and thus can be unfair to compare non-calibrated approaches, as discussed in [Changpinyo2020]. What do you think? Provide the seen-unseen accuracies curve and AUSUC metric instead.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b450247e-0111-5e71-9a4f-e759f075be1b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The harmonic mean (H) metric not being an accurate representation of the performance of a GZSL method, since it requires calibration, and thus can be unfair to compare non-calibrated approaches, as discussed in [Changpinyo2020]. What do you think? Provide the seen-unseen accuracies curve and AUSUC metric instead.", "reference_answer": "A: The authors actually have given the non-calibration results between DPPN and related methods in Table 1 of the supplementary pdf. As shown in that results, the proposed DPPN outperforms the best one by respectively $15.3\\%$​, $8.8\\%$​, and $7.3\\%$​ for $H$​ on CUB, AWA2, aPY datasets, and obtains comparable performance on SUN dataset. Please refer to Line 5-10 of the supplementary pdf for the detailed results and analysis.\n\nThe authors conduct experiments with recent related methods reporting results on AUSUC metric, of which the results are shown below:\n|        Method         | CUB AUSUC | AWA2 AUSUC | aPY AUSUC | SUN AUSUC |\n| :-------------------: | :-------: | :--------: | :-------: | :-------: |\n| SYNC [Changpinyo2016] |   33.7    |    50.4    |     -     |   24.1    |\n|  COSMO [Atzmon2019]   |   35.7    |     -      |     -     |   23.9    |\n| EXEM [Changpinyo2020] |   36.6    |    55.9    |     -     | **25.1**  |\n|         DPPN          | **56.3**  |  **63.6**  | **33.4**  |   23.1    |\n\nAs shown in the results, the proposed DPPN outperforms the best one by respectively $19.7\\%$ and $7.7\\%$ for AUSUC on CUB and AWA2 datasets, and obtains comparable performance on SUN dataset. The robust improvements over various metrics prove that DPPN can effectively alleviate the domain bias problem in GZSL."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0dba0ff7-a650-5fc9-ad10-663d81d70071", "question": "It is claimed that the algorithm has high efficiency because evd with respect to the neighborhood is less expensive than evd of the entire network. However, the original propagation in GNN does not require any evd, and such evd needs to be computed for egonets of each node for OPEN. Hence, the efficiency claim is not justified. Can you provide explanation or experiments in terms of run time from the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9ae710df-a1a2-5163-943b-e5859d3b4f2b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is claimed that the algorithm has high efficiency because evd with respect to the neighborhood is less expensive than evd of the entire network. However, the original propagation in GNN does not require any evd, and such evd needs to be computed for egonets of each node for OPEN. Hence, the efficiency claim is not justified. Can you provide explanation or experiments in terms of run time from the paper?", "reference_answer": "A: Firstly, the authors only claim that “the EVD is efficient. ” (Line 176-177) and “the ego-network modeling is highly efficient” (Line 167 -177) in the paper instead of “algorithm has high efficiency” in the review comment. As discussed between Lines 158 -166, the complexity of EVD on one ego-network is $\\{O}( |\\mathcal{N}_v|F^2)$, and those on all ego-networks is $\\{O}( |\\mathcal{E}|F^2)$, where $|\\mathcal{E}|$ is the number of edges in the graph. Thus, the authors justify the efficiency of EVD and ego-network modeling.\nSecondly, the proposed OPEN is as efficient as GAT. Each message passing step of OPEN has the same complexity as that of GAT, i.e., $\\{O}( |\\mathcal{E}|F^2)$. The whole OPEN consists of ego-network modeling and message passing step, whose complexities are both $\\{O}( |\\mathcal{E}|F^2)$. Thus, the total complexity of OPEN is $\\{O}( |\\mathcal{E}|F^2)$, and as the same as that of GAT. The running time comparisons is shown in Table R1, where OPEN-W and OPEN-P represents the time for weight calculation and propagation, respectively. The running time of GAT and OPEN is similar. Note that the running time of GAT and OPEN is longer than that of GCN, due to their multiple-channel propagations and combinations. These additional experiments and discussions are added to Section B.4 of the Appendix.\nTable R1. Running time in seconds.\n|Method|Cora|Pubmed|Citeseer| Comput| Photo| CS| Physics|\n|------|---:|-----:|-----:|-----:|-----:|-----:|-----:|\n|GCN|9.89|6.23|5.32|16.8|6.59|19.2|21.58|\n|GAT|10.45|49.31|12.85|95.23|42.11|106.06|201.79|\n|OPEN-W|2.61|8.93|3.05|18.46|9.22|18.39|52.94|\n|OPEN-P|10.11|36.09|12.8|65.36|35.62|88.81|149.37|\n|OPEN|12.72|45.02|15.85|83.82|44.84|107.2|202.31|\nIn summary, the EVD and ego-network modeling is highly efficient and the proposed OPEN is s efficient as vanilla GAT."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "205c0707-4f16-59db-a20e-ab2a79fbe966", "question": "In Figure 3, it seems that the initialization plays a very important role in LRPCA. It is not clear that why the proposed method outperform the ScaledGD when alpha is larger.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ddb8b8b8-9a94-5a91-b339-1cb736e8c758"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 3, it seems that the initialization plays a very important role in LRPCA. It is not clear that why the proposed method outperform the ScaledGD when alpha is larger.", "reference_answer": "A: The good initialization in Figure 3 is a result of learning. ScaledGD, with the sparsification operator, cannot achieve similar initialization through hand-tuned parameters.  \n\nThe thresholding operator (in LRPCA) has two advantages over the sparsification operator (in ScaledGD): speed and no false-positives (which leads to better robustness). The speed advantage is obvious as the sparsification operator requires partial sorting in every row and every column. For robustness advantage, the authors will use an example. Recall that ScaledGD takes off the largest $\\gamma \\alpha$ fraction entries per row and per column via the sparsification operator. Let’s take the parameter $\\gamma = 1.5$ for now ($\\gamma$ must be strictly greater than $1$, and ScaledGD uses $\\gamma=2$ for analysis). When the authors have only 10% outliers, the sparsification operator takes off 15% entries---not too bad.  If the authors have 40% outliers,  the sparsification operator takes off 60% entries---lose too many good entries. In contrast, the thresholding operator (with a properly chosen thresholding value) in LRPCA takes off no more than 40% entries, and all good entries are saved. In this sense, LRPCA preserves more good redundant information than ScaledGD, especially when more outliers appear. Hence,  it is not a surprise that LRPCA outperforms ScaledGD when $\\alpha$ is larger. **The authors are happy to add a remark in the camera-ready version to clarify any ambiguity in this matter.**  \n\nIn addition, the authors add the following experiment to support the proposed claim:  \n\n*The authors generate 10 problems for each of the outlier levels (i.e., $\\alpha$) and compare"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fe8534e1-9200-5017-b9fb-04b733678c8a", "question": "Why is it better to maximize the time SOTA in Table 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["01ec1818-9fe6-516b-ab44-46377c67a0cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is it better to maximize the time SOTA in Table 1?", "reference_answer": "A: The _Time SoTA Eq_ column in Table 1 refers to the time required by CP-Optimizer to match the performance reported by the ML solution. Longer SoTA times, thus, correspond to predicted solutions of higher quality."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ff00e337-2970-5d21-9781-22cbf9ba3906", "question": "Does the tree MDP formulation have other applications besides B&B trees for MLPs?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["dd95752f-0336-50fe-a659-818f7fdb8358"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the tree MDP formulation have other applications besides B&B trees for MLPs?", "reference_answer": "A: A tree MDP model is applicable whenever one has a control problem, where the problem subdivides recursively into tasks controlled by the same policy. At minimum, this is the case for divide-and-conquer algorithms, a category to which branch-and-bound belongs, so the authors would expect it to be a good fit for modelling any problem where the objective is to control some aspect of this larger class of algorithms.\nFor example, one could imagine a robotics problem where a rover must explore interconnected rooms to perform some action, and must explore as efficiently as possible. The control task of learning to decide which rooms to explore could be modelled as a tree MDP. Another, more abstract example could be choosing the pivot elements in quicksort, a divide-and-conquer sorting algorithm. At each time step of this algorithm, the pivot is used to partition the set of elements into two, and careful selection of the pivot can have a dramatic impact on the algorithm efficiency. This is another problem that would be a good fit for a tree MDP.\nThese are only two examples, but the authors believe that the framework could be useful for many more. In each case, using a regular MDP framework would be possible, but switching to a tree MDP formulation and its associated RL algorithms could lead to better credit assignment and hence, to better sample efficiency."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7d9eeca1-d901-5f8c-bdde-0c874db92cbd", "question": "How does the performance of 2 size 32 modules compare to the performance of 1 size 64 module?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["04f187d0-d8b8-5235-8662-1907ca882b83"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the performance of 2 size 32 modules compare to the performance of 1 size 64 module?", "reference_answer": "A: For the Sparse Combo Net the authors ran an experiment on permuted sequential MNIST where the authors held the total sum of units in the network fixed at 352, but varied the number of modules these units were spread over. With all 352 units in 1 module the test performance was ~40%, which would be attributable to training of the linear feedforward input and output layers, because as you mention there is no weight updating of the RNN in this case. With 4 units each in 88 RNN modules the network was unable to learn at all, suggesting that a pure linear feedback network would be unable to do the task. The other tested modularities (11 RNNs, 22 RNNs, and 44 RNNs) all had test performance around 90% or better - see Figure S1(B) for further results."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "102adde5-9536-5c45-a729-6793c948533f", "question": "What is the conclusion of the ablation study of choosing the threshold $\\tau$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["391ef338-8caa-5642-a7e3-425413767918"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the conclusion of the ablation study of choosing the threshold $\\tau$.", "reference_answer": "A: The ablation study of $\\tau$ on VisDA and the results are shown in the following table.\n\n| $\\tau$ | Avg acc |\n| :-: | :-: |\n| 0.91 | 87.06 |\n| 0.93 | 87.27 |\n| 0.95 | 87.34 |\n| 0.97 | 87.39 |\n| 0.98 | 87.19 |\n\nAs can be seen, the performance is not sensitive to the choice of $\\tau$. Additionally, the threshold $\\tau$ in Eqn. 4 is set to 0.95 following [47].\n\n[47] Alex Kurakin, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" In NeurIPS, 2020"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7d159143-e3ed-5653-ae0d-505e48ecf686", "question": "It is not specified whether training or testing losses were used to generate the graphs in figure 5.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c5203227-69aa-500f-bc4f-d5e9a525536e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is not specified whether training or testing losses were used to generate the graphs in figure 5.", "reference_answer": "A: All the contours are training losses, it would be interesting to draw test contours.  However, the loss surface being optimized by SGD is the training loss, not the test loss, and so this is what the authors visualized."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8886f295-08f7-59c2-82da-fe23a28a74e4", "question": "How different is the analysis comparing to existing result with [R1]? Summarize the main difficulty to extend the results of this paper to multi-layer cases? \nReferences:\n[R1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["a362a82b-6398-50cc-9a84-ed4a15a95c0f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How different is the analysis comparing to existing result with [R1]? Summarize the main difficulty to extend the results of this paper to multi-layer cases? \nReferences:\n[R1] Small nonlinearities in activation functions create bad local minima in neural networks, Yun et al, 2019", "reference_answer": "A: [R1] and this paper both employ the following strategy: (a) construct a series of local minima based on a linear classifier; and (b) construct a new point with smaller empirical risk and by this way the authors prove that the constructed local minima are spurious. However, due to the differences in the loss function and the output dimensions, the exact constructions of local minima are substantially different. Meanwhile, the Stages (2) and (3) of this paper employ the transformation operation to force the data flow to go through the same series of the linear parts of the activations. The operations are carefully designed and the whole construction is novel and non-trivial. The difficulties in extending the loss function and output dimension are justified below:\n1. From squared loss to arbitrary differentiable loss: Yun et al. (2019b) calculate the analytic formations of derivatives of the loss to construct the local minima and then prove they are spurious. This technique cannot be transplanted to the case of arbitrary differentiable loss functions, because the authors cannot assume the analytic formation. To prove that the loss surface under an arbitrary differentiable loss has an infinite number of spurious local minima, the authors employ a new proof technique based on Taylor series and a new separation lemma (see Appendix A.5, Lemma 6, p. 31) to avoid the use of the analytic formulations (see a detailed proof in Appendix A.2, Step (b), pp.14-15).\n2.  From one-dimensional output to arbitrary-dimensional output: To prove the loss surface of a neural network with an arbitrary-dimensional output has an infinite number of spurious local minima, the authors need to deal with the calculus of functions whose domain and codomain are a matrix space and a vector space, respectively. By contrast, when the output dimension is one, the codomain is only the space of real numbers. Therefore, the extension of output dimension significantly mounts the difficulty of the whole proof."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a0c0d2c7-8f1e-5002-8230-d131d4454d5f", "question": "Why UniGAN achieves low IS scores on CIFAR dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["db8f6428-8da5-5415-8f58-ce4bc7a45df3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why UniGAN achieves low IS scores on CIFAR dataset?", "reference_answer": "A: Regarding the difference in IS scores between UniGAN and PGMGAN on the CIFAR dataset, in addition to being likely caused by the different generator architectures of the two models (we use an NF-based generator, while PGMGAN uses a ResBlock-based generator), it is more likely caused by the different discriminator capabilities of the two models. As the authors show in Table 3 of supplementary, the architecture of the discriminator the authors used for training on the CIFAR dataset is very simple, it consists of only a few vanilla convolutional layers and the total amount of model parameters is only 0.188M. However, the discriminator of PGMGAN consists of multiple ResBlocks, which is relatively more capable. In addition, it can be seen from supplementary that for the natural image datasets, when the authors use the powerful StyleGAN2 discriminator (see Table 4 of supplementary), the FID scores that measures the quality of generated samples can be reduced to a very low level (eg, FID<10 on CelebA dataset, and see more quantitative results in Table 12-17 of supplementary), which shows that the proposed NF-based generator can also generate high quality samples when the discriminator is powerful enough."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d3926f5e-bb8c-5231-aaa4-eb27dd7cff45", "question": "Briefly summarize a review of the generality of the attacks.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Briefly summarize a review of the generality of the attacks.", "reference_answer": "A: For the generality of the threat models, the authors conduct an additional experiment using $\\epsilon = 0.06$ which is larger than the adopted $\\epsilon=0.05$. The results are shown as follows. \n\n|                |     | ModelNet40  |                 | |       ScanObjectNN         |      | |   ModelNet10     |      |\n|    :----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |\n|           RA(%)           | PointNet   | DGCNN     | PCT       | PointNet     | DGCNN     | PCT       | PointNet   | DGCNN     | PCT       |\n| ST Baseline ($\\epsilon=0.06$)    | 0  | 0 | 0 | 0    | 0 | 0  | 0  | 0 | 0 |\n| AT Baseline ($\\epsilon=0.06$)    | 25.1±0.51  | 48.9±0.69 | 37.2±0.33 | 17.8±0.15    | 25.6±0.33 | 19.1±0.4  | 74.1±0.12  | 84.2±0.55 | 77.0±0.29 |\n| Best Finetuned ($\\epsilon=0.06$) | 43.2±0.41  | 54.1±0.75 | 39.6±0.2  | 19.3±0.23    | 33.1±0.65 | 20.5±0.37 | 75.0±0.2   | 86.4±0.48 | 77.6±0.47 |\n\nFrom this table, the authors observe that the proposed model could achieve non-trivial robustness among different epsilons (even with  $\\epsilon > 0.05$). The proposed model trained with self-supervised learning also achieves consistently stronger robustness than the AT and ST baselines. It further verifies the significance of self-supervised learning for 3D point cloud robustness. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a0e8a1e8-62fd-5ba0-b9cf-be798eaf05a4", "question": "“During image captioning, which loss was used?”", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "“During image captioning, which loss was used?”", "reference_answer": "A: Equation 2 is the only loss function the authors proposed, $L_\\text{total} = L_\\text{latent} + L_\\text{task}$. In the case of image captioning, $L_\\text{task}$ is a cross-entropy loss between the predicted tokens and the ground truth subword tokens usual for NLP tasks. As of $L_\\text{latent}$ (asymmetric latent loss), it is the squared Euclidean distance between $s$ and $g$ controlled by a hyperparameter $\\beta$ (set as 0.1 by default). This is everything about the loss function.\n\nHowever, for LSP to work, it also needs GC or GCR to encourage the convergence of $L_\\text{task}$ to its local minima. Without either $L_\\text{task}$ or GC (or GCR), the latent set prediction will not converge which is shown by the proposed convergence analysis (Section 4) and the synthetic dataset experiment (Section 5.1)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "20701636-83c8-5da7-89f0-d00142497f0a", "question": "What are the disadvantages of modeling the return distribution with a mixture of Dirac delta functions? Is this the same approach as Distributional RL with implicit quantile networks ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["15718ad6-2d24-586f-9b26-73bc071dfd0d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the disadvantages of modeling the return distribution with a mixture of Dirac delta functions? Is this the same approach as Distributional RL with implicit quantile networks ?", "reference_answer": "A: Modelling the return distribution with a mixture Dirac delta functions makes the training speed slightly slower [4] as there are many parameters to update. The authors summarize the training time of some Q value-based methods in the following. In the table below, the authors can see that RMIX is slightly slower in some scenarios. The training time is acceptable.\n\n|Scenarios |        RMIX |      QMIX | QPLEX |         VDN  |             IQL | WQMIX |\n|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|1c3s5z          |12 hours         | 9 hours         |12 hours  |7 hours          |6.5 hours | 9 hours.     |\n|MMM2              |22 hours        | 20 hours |1 day 5 hours        |18 hours        |19 hours        |20 hours|\n|5m_vs_6m        |20 hours        | 18 hours | 19 hours | 18 hours | 9 hours | 13 hours |\n|8m_vs_9m        | 8 hours        | 8 hours        | 12.5 hours        | 8 hours        | 8 hours | 8 hours |\n|10m_vs_11m        | 9 hours        | 8 hours        | 11 hours | 8 hours        | 7 hours | 9 hours |\n|corridor        | 1day 14 hours        | 24 hours        | 2day 1 hours     |   21 hours   |     22 hours   |   24 hours |\n\n\n[4] Bellemare, Marc G., Will Dabney, and Rémi Munos. \"A distributional perspective on reinforcement learning.\" International Conference on Machine Learning. PMLR, 2017."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1f1c0619-746a-5c22-afcc-8c7efdcac2d6", "question": "How did you address the distortion of perpendicular relations due to camera angles?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f1c2adf-3164-5df0-b6ba-b27f76ecd17c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How did you address the distortion of perpendicular relations due to camera angles?", "reference_answer": "A: In constructing the dataset, the authors have fixed the camera angle. Different locations of the angle will make the perpendicular relation look like different acute angles in an image. This is completely fine, as explained above, in that as long as the dataset contains concept instances with such intrinsic variation, the learned EBM is able to recognize it. This is supported by the empirical result that the classification and detection accuracy for 3D images is well above “statistics” baseline."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "61819ffc-e524-57ce-985e-17542b24d491", "question": "The performance of tNN and FC-tensor seems to be poor in CIFAR-10 compared to a vanilla FC, why is this?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4dc9acb3-9d7c-5118-b0a5-962f43ddb14e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The performance of tNN and FC-tensor seems to be poor in CIFAR-10 compared to a vanilla FC, why is this?", "reference_answer": "A: The number of parameters in tNN and FC-tensor has been largely reduced compared with conventional fully connected layers, namely around 60% compression ratio, which results in a drop of performance on CIFAR10, namely 10% classification accuracy, which is more complicated than MNIST dataset. For generally used models on ImageNet, Over-parametrization is a common issue which helps models behave well even with a reduction of parameters as shown in the proposed ImageNet experiment.\n "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "578b494e-6871-550f-90c4-20ebc3147cbb", "question": "Could the addition of self-attention in the molecule encoder prove beneficial in exploring the impact of interactions between substructures on bio-chemical properties?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d87b5532-5c5f-5940-aa2b-f17ffad843cd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could the addition of self-attention in the molecule encoder prove beneficial in exploring the impact of interactions between substructures on bio-chemical properties?", "reference_answer": "A: To verify your hypothsis, the authors supplement results of the proposed tentative exploration in the table below. To encode interactions between substructures into the final learned molecular representation, the authors utilize the permutation equivariant Set Attention Block (SAB) proposed in Set Transformer [3]. SAB takes a representation set of any size as input and outputs a representation set of equal size. SAB is able to encode pairwise and higher-order interactions between elements in input sets into outputs. The authors add such a SAB after the Substructure Encoder. For each molecule, the authors feed the representions of its substructures to SAB to obtain new substruture representations. In this way, the final molecule representation could model interactions between substructures. Due to limited time, the authors only conduct experiments on EC50-assay/scaffold/size to examine the performance of adding such a SAB. As demonstrated in the table, the authors can see that adding such a SAB further improves the proposed model on EC50-scaffold. This design is a naive attempt but brings us some valuable insights. \n\n|               |       **EC50-assay**       |     **EC50-scaffold**      |       **EC50-size**        |\n|:------------- |:--------------------------:|:--------------------------:|:--------------------------:|\n| **ERM**       |       $69.35\\pm7.38$       |       $63.92\\pm2.09$       |       $60.94\\pm1.95$       |\n| **IRM**       |       $69.94\\pm1.03$       |       $63.74\\pm2.15$       |       $58.30\\pm1.51$       |\n| **DeepCoral** |       $69.42\\pm3.35$       |       $63.66\\pm1.87$       |       $56.13\\pm1.77$       |\n| **DANN**      |       $66.97\\pm7.19$       |       $64.33\\pm1.82$       |       $61.11\\pm0.64$       |\n| **MixUp**     |       $70.62\\pm2.12$       |       $64.53\\pm1.66$       |       $62.67\\pm1.41$       |\n| **GroupDro**  |       $70.52\\pm3.38$       |       $64.13\\pm1.81$       |       $59.06\\pm1.50$       |\n| **Ours**      |  $\\mathbf{73.25\\pm1.24}$   | $\\underline{66.69\\pm0.34}$ |  $\\mathbf{65.09\\pm0.90}$   |\n| **Ours+SAB**  | $\\underline{73.15\\pm2.69}$ |  $\\mathbf{67.26\\pm1.54}$   | $\\underline{64.83\\pm1.07}$ |\n\n[1] [Open Graph Benchmark: Datasets for Machine Learning on Graphs.](https://arxiv.org/pdf/2005.00687.pdf)\n[2] [Graph Adversarial Self-Supervised Learning.](https://proceedings.neurips.cc/paper/2021/file/7d3010c11d08cf990b7614d2c2ca9098-Paper.pdf)\n[3] [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks.](http://proceedings.mlr.press/v97/lee19d/lee19d.pdf)"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d3844dbd-be22-5b48-859d-f1c1802827f2", "question": "As shown in Table 7, although MSANet contains more parameters, the running time and the FLOPs are not much. Why does this happen? Please give some explanations.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["431c48ab-9371-5daf-b349-346879e446a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As shown in Table 7, although MSANet contains more parameters, the running time and the FLOPs are not much. Why does this happen? Please give some explanations.", "reference_answer": "A: Modern multi-scale architectures usually consist of multiple stages. At the end of each stage, the feature resolution will be halved while the feature channels will be doubled. As a result, the parameters will increase due to the doubled channels, and the FLOPs and running time will decrease due to the halved resolution (Height & Width). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a1d39203-66c7-5043-a1c8-4f14645894e2", "question": "Is the formulation just a linear correlation removal?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["bb1ce27d-d4ca-5669-8dba-4302171c1a0e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the formulation just a linear correlation removal?", "reference_answer": "A: No, the proposed formulation is not just a linear correlation removal."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a8a4796d-4a09-52d0-9fe5-325c1845291f", "question": "How to train $\\Delta_k$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7bb011ff-6526-5a93-9b59-878505dc29ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to train $\\Delta_k$?", "reference_answer": "A: Eq. (6) shows the update rule of $\\Delta_k$. It is similar to Bellman equation if $|Q_k-\\mathcal{B}^*Q_{k-1}|$ was substituted with the reward. So the authors can use neural networks to represent $\\Delta_k$, just as the Q network in the Bellman equation. The training of $\\Delta_k$ is also the same as deep Q learning, with transitions sampled from the replay buffer. The source code will also be released soon for closer inspections."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "be0b4101-88e8-54fe-8b86-4d7b3573fafb", "question": "Can CQL have better performance gain with CABI compared with TD3_BC?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ae801009-35b9-58e7-941e-726cb84d7b54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can CQL have better performance gain with CABI compared with TD3_BC?", "reference_answer": "A: The performance gain upon TD3_BC or IQL is limited since the generated samples much resemble the original samples in the static dataset, which makes it hard for TD3_BC (with behavior cloning term) or IQL (that learns without querying OOD samples) to exhibit significant performance gain. The authors deem that it is interesting to investigate whether CQL can have better performance gain with the proposed CABI compared with TD3_BC. Due to the time limit, the authors can only run CQL+CABI over 4 different random seeds without tuning real data ratio $\\eta$. To be specific, the authors use real data ratio $\\eta=0.3$ for *random* datasets, $\\eta=0.7$ for *medium* and *medium-replay* datasets, and a comparatively large $\\eta=0.9$ for *medium-expert* and *expert* datasets (since they are of good quality). The forward horizon and backward horizon for rollout are set to be 3 for all of the datasets, which is consistent with the experimental setup for TD3_BC+CABI. The authors keep the original hyperparameters of CQL fixed. The authors summarize the experimental results in Table 1, where the authors observe that CQL does get large performance gain with the aid of CABI on all of the datasets. These altogether illustrate the effectiveness and benefits of the proposed data augmentation method for offline learning.\n\n| Task Name | CQL | CQL+CABI |\n| ---- | :---: | :---: |\n| halfcheetah-random | 21.7$\\pm$0.9 | **30.2$\\pm$1.4** |\n| hopper-random | 10.7$\\pm$0.1 | **13.5$\\pm$3.5** |\n| walker2d-random | 2.7$\\pm$1.2 | **7.3$\\pm$2.3** |\n| halfcheetah-medium | 37.2$\\pm$0.3 | **42.4$\\pm$0.7** |\n| hopper-medium | 44.2$\\pm$10.8 | **57.3$\\pm$12.9** |\n| walker2d-medium | 57.5$\\pm$8.3 | **62.7$\\pm$6.4** |\n| halfcheetah-medium-replay | 41.9$\\pm$1.1 | **44.6$\\pm$0.4** |\n| hopper-medium-replay | 28.6$\\pm$0.9 | **34.8$\\pm$2.4** |\n| walker2d-medium-replay | 15.8$\\pm$2.6 | **21.4$\\pm$3.1** |\n| halfcheetah-medium-expert | 27.1$\\pm$3.9 | **35.3$\\pm$4.8** |\n| hopper-medium-expert | 111.4$\\pm$1.2 | **112.0$\\pm$0.4** |\n| walker2d-medium-expert | 68.1$\\pm$13.1 | **107.5$\\pm$1.0** |\n| halfcheetah-expert | 82.4$\\pm$7.4 | **99.2$\\pm$4.5** |\n| hopper-expert | 111.2$\\pm$2.1 | **112.0$\\pm$0.2** |\n| walker2d-expert | 103.8$\\pm$7.6 | **110.2$\\pm$0.9** |\n| Total score | 764.3 | **890.4** |\n\nTable 1. Normalized average score comparison on MuJoCo \"-v0\" datasets. The results of CQL+CABI are averaged over 4 different random seeds."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cb3fe6a9-eb03-5b18-80e0-6adb4d10e469", "question": "What is the point of unbalanced OT? Is it true that all the theoretical results of the previous sections apply to formulation (14) instead of the original entropic balanced OT?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4f4bfc94-e9b3-50b0-b6d6-e554d9a67f6e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the point of unbalanced OT? Is it true that all the theoretical results of the previous sections apply to formulation (14) instead of the original entropic balanced OT?", "reference_answer": "A: Unbalanced OT extension is only introduced as a heuristic and is motivated by the practical problem of accounting for growth. However, the authors cannot claim that there is theoretical support for this extension for the moment in the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2bfb8244-d7ef-5c33-a7c9-a66c6e8c80e7", "question": "L290: Can you provide a table with the different rates given the algorithm and the setting to ease the comparison with the previous baselines?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fa13aae2-028e-579d-847c-b9e61999a1c0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "L290: Can you provide a table with the different rates given the algorithm and the setting to ease the comparison with the previous baselines?", "reference_answer": "A: The authors already have some comparisons of sample complexities in the appendix (see e.g., line 714, 738, 927). The authors will add clearer a table/discussions/pointers in the next version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b9d7e771-536a-55ea-9210-f87dbb6e67d7", "question": "The real technical contribution. Is the first contribution a very small modification of Neural-Pull?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d7ce5f53-3667-5842-aebf-36f537f678f3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The real technical contribution. Is the first contribution a very small modification of Neural-Pull?", "reference_answer": "A: The proposed novelty lies in the analysis of implicit fields which is seldom discussed in previous works. The authors did get inspiration from Neural-Pull on how to learn distance fields by moving queries. However, the nature of SDF prevents Neural-Pull from representing most real-world objects with open surfaces or geometries with inner structures, and the direct extension of Neural-Pull to UDF fails drastically as shown in Table 5. This observation drives us to design a consistency-aware learning scheme with a carefully designed loss as described in Sec.3.2 which leads to an accurate and continuous field as shown in Fig 1 of the supplementary. In Sec.3.3, the authors proposed to progressively estimate the mapping relationship between 3D queries and the approximated surface by updating the raw point cloud with well-moved queries as additional priors for promoting further convergence. Finally, previous UDF approaches fail to extract surfaces directly which greatly limits their practicability. The authors resolve this problem by introducing an algorithm for directly extracting surfaces with arbitrary topology from the gradient vector field of UDF as described in Sec.3.4."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cd0eb149-8bd8-5818-b762-6dc6f74c8457", "question": "How does the formulation of the self-supervised loss differ from NeuralPull's?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e05a33f4-2946-599c-84d1-8e554c22e021"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the formulation of the self-supervised loss differ from NeuralPull's?", "reference_answer": "A: The authors estimate the normal of the latent surface as a signed unit vector to a query point, while NeuralPull approximates the normal as $\\nabla f(\\textbf{q})/|\\nabla f(\\textbf{q})|_2$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9fcf4878-989b-5105-af76-7a52e3862c48", "question": "What is the performance of fixed LToS in prisoner?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["893be569-d272-56c9-b2c1-fa6752fbc427"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the performance of fixed LToS in prisoner?", "reference_answer": "A: The dark blue curve (better than fixed LToS) in Figure 3a is Coco-Q (it has a similar color to DQN), not DQN. Actually, the curves of DQN and DGN are always around reward=0.5 and covered by other curves. So, fixed LToS actually outperforms DQN."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "47102342-6243-566d-8a9e-1251df20a9c6", "question": "How are the models in BlenderKit generated? Are they real or synthesized objects?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e76cdd40-14a4-5e64-b8b2-c6871f4cf676"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How are the models in BlenderKit generated? Are they real or synthesized objects?", "reference_answer": "A: The models in BlenderKit are synthetic objects. As shown in Figure B.1 in Appendix B, the authors manually selected these objects that are natural and common in the real world."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "082b6d8a-3f1a-5ffe-8c80-3cbee2e7e584", "question": "The proposed method has a high computation cost, so why should it be preferred?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The proposed method has a high computation cost, so why should it be preferred?", "reference_answer": "A: The in-training algorithms do incur additional computational costs, but as the authors select a very small number of updating steps (1 for AdvLatGAN-qua and 3 for AdvLatGAN-div) in training, the additional relative overhead is small (around 20%-30%) while the performance gain is still significant. Please refer to Appendix D for the overhead experiments. Compared to the GAN's bi-level optimization process, the sampling method AdvLatGAN-z costs little, and it also outperforms peer latent sampling improvement methods in terms of cost-effectiveness in Table 8."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8023aaae-288d-5968-88e9-12dc1b11636d", "question": "To apply the attack method on an online FL system, the actual runtime of the attack method is very important. Even though the authors suggest a technique for scalability, many RL-based applications suffer from a large amount of computation and runtime. So how do the authors justify the missing analysis on actual runtime (during the overall attack process) of the proposed RL method and the baseline method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "To apply the attack method on an online FL system, the actual runtime of the attack method is very important. Even though the authors suggest a technique for scalability, many RL-based applications suffer from a large amount of computation and runtime. So how do the authors justify the missing analysis on actual runtime (during the overall attack process) of the proposed RL method and the baseline method?", "reference_answer": "A: Since the three stages of the attack overlap with each other, only comparison of the attack execution time with other baselines is required which is present below. The actual runtime varies across the FL environment, the training method used, and most importantly, the amount of computational resource available. The tables below report the numbers from the proposed current experiment settings (see Appendix D.1) and the way the simulator is implemented (clients are simulated sequentially in each FL epoch). \n\nFor MNIST and Fashion-MNIST, distribution learning takes around 100 seconds to reconstruct a batch of 32 images and the authors construct 50 batches within 2 hours. Note that multiple batches can be generated from a single gradient. The authors start policy training from the beginning of FL training, and the authors set 8 hours limit for policy training. It takes around 0.05 seconds to simulate a single FL epoch with 10 sampled clients without parallelization. Total training steps vary across defense policies as stated in the supplementary materials D.1. \n\nWith the above numbers, if the authors assume that each FL epoch takes 72 seconds to finish and there are in total of 1,000 FL epochs during FL training, then distribution learning will end before the 100th FL epoch and policy training ends by the 400th FL epochs, and the total FL training time is 20 hours. Once equipped with more powerful devices, the training time can be significantly reduced by parallelly simulating multiple clients using multiprocessing and multiple episodes using vectorized environments, which will make it possible to simulate large FL systems.  \n\nIn terms of executing time, for MNIST with clipping median defense, IPM takes around 0.25 seconds to execute an attack in each FL epoch, LMP takes around 7.7 seconds, EB takes around 0.5 seconds. The execution time of the proposed RL method varies over the action space used and it takes around 5.8 seconds with the current action space. Given that each FL epoch typically lasts a minute or longer (72 seconds in the proposed experiment), a few seconds of search time is completely acceptable. The authors observe that for defenses such as Krum, it suffices to use the gradients of the last two layers of model parameters as the action. This approach does not require any online searching and decreases the attack execution time to 0.5s. \n\n                           FL Epochs   Real Time\n     Distribution Learning   100      <= 2 hours\n     Policy Learning         400      <= 8 hours\n     Total FL Training       1000      20 hours\n\n\n                                      Real Executing Time\n      IPM                                   0.25s\n      LMP                                   7.7s\n      EB                                    0.5s\n      RL (with online search)               5.8s\n      RL (without online search)            0.5s"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "44cf3ab8-5276-5018-a642-fda4b06493c2", "question": "Would incorporating the 'pe' in existing TGNs would be at par performance of PINT+pe?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["412b382c-197c-589c-864a-4e8a67c013e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would incorporating the 'pe' in existing TGNs would be at par performance of PINT+pe?", "reference_answer": "A: Both the injective aggregation and the positional encodings are important for the performance of PINT. PE plays a major role in datasets without edge features (UCI, Enron, and LastFM). For instance, Table 1 shows that PINT's performance drops from 88.06 to 81.35 (transductive) and from 91.76 to 88.44 (inductive). As suggested by the reviewer, it is also possible to incorporate the proposed PE into existing TGNs. As a proof of concept, the authors have implemented TGN-Att with PE for the three unattributed datasets. The tables below show that TGN-Att receives a significant boost from the proposed PE. However, PINT still beats TGN-Att+PE on 5 out of 6 cases. The authors will include these additional results in the revised manuscript."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "341a479a-a466-526e-aa76-a6c3dd531893", "question": "Would the method work on a dataset that is from a different domain as compared to what was CLIP trained on?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["d21f675f-1c33-5277-9054-d795c89effbb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would the method work on a dataset that is from a different domain as compared to what was CLIP trained on?", "reference_answer": "A: Yes, the authors experiment on a Remote Sensing Image dataset BigEarth, whose domain is far from the domains of the datasets in the mainstream papers. DualCoOp consistently outperforms SARB* on BigEarth as well as the datasets in the main paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e77ed8e6-2875-535a-ab5c-7adea05b7218", "question": "A: If AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (because we sample our starting signal from a Gaussian for a 32x32x3 image and an AR process that uses a window size 3x3) (Figure 3, Left).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["94275829-60ba-5423-9225-c41722c6d35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "A: If AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (because we sample our starting signal from a Gaussian for a 32x32x3 image and an AR process that uses a window size 3x3) (Figure 3, Left).", "reference_answer": "A: Designing denoisers for autoregressive perturbations requires that the denoiser be agnostic to the exact AR process."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b010930e-fb43-549c-895e-754e9f78ffdf", "question": "What is the difference in training time between the two methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference in training time between the two methods?", "reference_answer": "A: More specifically, the authors only randomly sample one subnetwork on each step. Following the reviewer’s suggestion, the authors further show the difference in training time and memory consumption between the two methods, as shown in Table r5. For training time, ST is about 1.4 times (but less than 2 times) that of CT, since ST employs the main network and a sampled subnetwork at each step and the sampled subnetwork usually takes much less time than the main network. For memory consumption, ST and CT are basically the same, since each subnetwork is sampled from the main network."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e0604fc1-3f86-5856-9ca7-8b31d71ea838", "question": "The comparisons in Table 2 showed the advantages of the proposed method. However, it is not clear to me whether the comparison is apples-to-apples. For example, is the semantic label information used in other baselines (such as PCN)? It would be nice to explicitly mention which methods used such additional input, and which methods did not. That way, we would know where the improvement comes from (from the semantic label, or from the model architecture).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6e611069-c4c2-578c-83e3-3279cc5b3dd0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The comparisons in Table 2 showed the advantages of the proposed method. However, it is not clear to me whether the comparison is apples-to-apples. For example, is the semantic label information used in other baselines (such as PCN)? It would be nice to explicitly mention which methods used such additional input, and which methods did not. That way, we would know where the improvement comes from (from the semantic label, or from the model architecture).", "reference_answer": "A: The authors understand that, with the use of ground truth segmentation labels for the proposed SPoVT and the use of those produced by pre-trained DGCNN for the SOTAs, the comparison in Table 2 would be less informative. And, ground truth segmentation labels might not always be available during training. \nTo address and alleviate this issue, the authors conduct an additional experiment, in which segmentation labels predicted by pre-trained DGCNN are used for training the proposed SPoVT (denoted as Ours* in the updated Table 2, as listed below). From the results shown in Table 2, the authors see that while Ours* degraded the performance when compared to the original version (Ours), it still performed against SOTA methods for both completion and segmentation tasks. This suggests that the proposed model is able to utilize pre-trained segmenters for assigning point cloud labels for completion/segmentation purposes. Thus, the effectiveness and practicality of the proposed model can be verified.\n\nTable 2: Quantitative evaluation on PCN in terms of L2-Chamfer Distance (CD×$10^4$) and mIOU (%). Note that $N^{GT} = 16384$ for all methods across different categories.\n| Method          | Airplane |      |  Car |      | Chair |      | Lamp |      | Table |      | Avg. |      |\n|-----------------|:--------:|:----:|:----:|:----:|:-----:|:----:|:----:|:----:|:-----:|:----:|:----:|:----:|\n|                 |    CD    | mIoU |  CD  | mIoU |   CD  | mIoU |  CD  | mIoU |   CD  | mIoU |  CD  | mIoU |\n| PCN             |   1.26   | 67.4 | 10.8 | 38.1 |  5.77 | 79.3 | 11.4 | 62.1 |  5.22 | 76.6 | 6.88 | 64.7 |\n| PMP-Net++       |   1.80   | 70.3 | 3.82 | 48.6 |  3.42 | 75.3 | 7.93 | 66.3 |  7.87 | 59.3 | 4.97 | 64.0 |\n| VRC-Net         |   0.84   | 69.7 | 3.15 | 60.6 |  3.50 | 82.2 | 4.90 | 75.5 |  4.76 | 74.1 | 3.43 | 72.4 |\n| PoinTr          |   1.88   | 53.6 | 3.73 | 50.8 |  3.01 | 79.2 | 4.55 | 60.5 |  2.97 | 76.1 | 3.23 | 64.0 |\n| Ours*           |   0.75   | 82.1 | 2.99 | 76.9 |  2.97 | 77.0 | 4.50 | 86.1 |  3.04 | 84.1 | 2.85 | 81.2 |\n| Ours (Original) |   0.73   | 82.6 | 2.86 | 82.5 |  2.36 | 85.2 | 4.12 | 91.5 |  2.50 | 86.5 | 2.51 | 8"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9714b0ca-59d0-582c-9f41-2d559d41e24a", "question": "Can you comment on the statements that compare the output of GD (unconstrained) and the solution of the constrained problem?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["befa2892-2ca9-59fb-bd49-724e13203e60"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you comment on the statements that compare the output of GD (unconstrained) and the solution of the constrained problem?", "reference_answer": "A: Equation (4) is valid as stated. First, notice that the authors consider GD that is initialized at $w_0=0$. The authors state this right after Eq. (2), and it is a very standard assumption in the context of convex optimization."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a5a05c41-4ffd-5a67-b839-af948d88f6dd", "question": "Please compare related work on generative classifiers, e.g. Score-Based Generative Classifiers (https://arxiv.org/abs/2110.00473) in comparison to CARD.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["066d7d21-caf3-5d81-a520-23ebfe718566"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Please compare related work on generative classifiers, e.g. Score-Based Generative Classifiers (https://arxiv.org/abs/2110.00473) in comparison to CARD.", "reference_answer": "A: From the naming perspective, it might be easy to confuse CARD for classification as a type of generative classifier, as it utilizes a generative model to conduct classification tasks. However, they are two different types of generative models, as generative classifiers model the conditional distribution $p(x|y)$, while CARD models a different conditional distribution, i.e., $p(y|x)$. In fact, CARD shall be categorized as a type of discriminative classifier, by the definition in the “Score-Based Generative Classifiers” paper. Note that although both types of classifiers under image-based tasks would report NLL as one evaluation metric, they are also different, since the NLL for generative classifiers is evaluated in the space transformed from the logit space of $x$, while the NLL for discriminative classifiers is computed in the space of $y$, as the cross-entropy between true labels and predicted probabilities."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1c4b01dc-99e4-5a11-98d5-6e5ca42fda42", "question": "The SESN-B architecture resembles quite closely the SI-ConvNet architecture of Kanazawa et al. (except that that paper resized the images instead of the filters). While the proposed approach may be more computationally efficient, it's not clear what leads to the improvement in accuracy here? Can you explain the difference?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e81f1be3-4f0d-5afd-bf23-98f9fde05958"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The SESN-B architecture resembles quite closely the SI-ConvNet architecture of Kanazawa et al. (except that that paper resized the images instead of the filters). While the proposed approach may be more computationally efficient, it's not clear what leads to the improvement in accuracy here? Can you explain the difference?", "reference_answer": "A: SI-ConvNet uses image resizing in each convolutional layer of the network. It relies on the interpolation techniques which cause interpolation artifacts and lead to less stable optimization and as a results to a decreased classification accuracy."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3ee2cc47-acaf-5d51-be6a-30aaf7335342", "question": "How do equations (1) and (2) related to equations (3) and (4)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do equations (1) and (2) related to equations (3) and (4)?", "reference_answer": "A: Eq. 1 and 2 belong to the \"Adversarial samples and adversarial training\" part, while Eq. 3 and 4 belong to the \"Mode coverage by regularizing distance of generated samples\" part. They are two individual preliminaries and yet have no clear relations. Note Section 3.3 presents the logic of introducing adversarial techniques into MSGAN, obtaining the AdvLatGAN-div algorithm."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9dbd797b-1d60-5cf4-b8c0-7231b1d646b3", "question": "Are the boldfaced numbers in table 1 mistakenly written?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["5a6a3ce3-974e-5496-aea4-4b83fc6023ae"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the boldfaced numbers in table 1 mistakenly written?", "reference_answer": "A: No, the authors bold all top results that are statistically indistinguishable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7ce96f25-3059-5042-984a-a25c34c23ce7", "question": "How is the simplex constructed in the experiment section? Does it only consider nodes and edges, or also involve actual higher-order information like a 2-simplex, 3-simplex or more? If not, where does the higher-order information appear in the graph structure?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["495b8fb4-8621-5d32-bed8-ab504ae61518"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is the simplex constructed in the experiment section? Does it only consider nodes and edges, or also involve actual higher-order information like a 2-simplex, 3-simplex or more? If not, where does the higher-order information appear in the graph structure?", "reference_answer": "A: In the proposed experiments, after the construction of target spatio-temporal graphs, the authors extract 0-simplices, 1-simplices, and 2-simplices for each spatio graph $\\mathcal{G}_t$ at timestamp $t$. In order to integrate these simplicial complexes into the proposed model, the authors firstly generate Hodge-$k$ Laplacian via $\\boldsymbol{L}\\_k =\\boldsymbol{B}\\_k^{\\top}\\boldsymbol{B}\\_k+\\boldsymbol{B}\\_{k+1}\\boldsymbol{B}\\_{k+1}^{\\top}$ (where $\\boldsymbol{B}\\_k$ and $\\boldsymbol{B}\\_{k+1}$ are the $(k-1)$-simplex-to-$k$-simplex and $k$-simplex-to-$(k+1)$-simplex incidence matrices respectively); then, according to Equation 3 (see Section 4.3), the authors can obtain supra-Hodge $k$-Laplacian (here $k=1$). Finally, the authors feed the Supra-Hodge $k$-Laplacian into the proposed diffusion supra-Hodge convolution operation to to capture time-conditioned higher-order interactions on $\\mathcal{G}_t$ and to describe the dynamic diffusion of information over simplices along the temporal dimension. In the proposed experiments, the authors consider 0-simplices (nodes), 1-simplices (edges), and 2-simplices (filled triangles). \nSuggesting to explore the potential utility of 3-simplices is a great suggestion! Based on 1-simplices (edge), 2-simplices (filled triangles), and 3-simplex (tetrahedron) and the definition of Hodge-$k$ Laplacian, the authors can obtain Hodge 2-Laplacian and the corresponding Supra-Hodge $2$-Laplacian (via Equation~3 in Section 4.3). After that, the authors conduct comparison experiments between (i) ZFC-SH$^1$CN based on supra-Hodge $1$-Laplacian (i.e., utilizing information of 0-simplices, 1-simplices, and 2-simplices) and (ii) ZFC-SH$^2$CN based on supra-Hodge $1$-Laplacian (i.e., utilizing information of 1-simplices, 2-simplices, and 3-simplices). Tables 13 and 14 present the performances of ZFC-SH$^1$CN and ZFC-SH$^2$CN on PeMSD4 and COVID-19 (TX) datasets. The authors find that ZFC-SH$^1$CN consistently outperforms ZFC-SH$^2$CN.\nthe authors tend to attribute such findings to the fact that the authors observe a very low number of 3-simplices. For instance, in PeMSD4 at each timestamp the authors observe around $\\bf{29}$ 2-simplices (filled triangles) but the authors observe only around $\\bf{2}$ 3-simplices (tetrahedron). Nevertheless, the authors believe that higher order simplices, if observed in the targeted network, has a potential to boost the model performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "38b1ccd2-48cb-5838-95b5-6916893ce494", "question": "In Figure 1, for large values of N, gWOT seems to start outperforming MFL. Does this trend continue if N is chosen in the order of 10^3, 10^4...?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4f4bfc94-e9b3-50b0-b6d6-e554d9a67f6e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 1, for large values of N, gWOT seems to start outperforming MFL. Does this trend continue if N is chosen in the order of 10^3, 10^4...?", "reference_answer": "A: The authors believe that this trend indeed continues when $N$ gets larger because the proposed method estimates the marginal $\\mu_t$ more diffused than they actually are. This is mainly owing to the data-fitting term which has a finite bandwidth parameter. Theory suggests that the authors should decrease the hyperparameter ($\\lambda$ and $\\sigma^2$) as $N$ increases, but in Fig. 1 only $\\lambda$ is varied and $\\sigma^2$ is kept fixed for simplicity (see supplement for details). In contrast, gWOT uses the same support as the input samples, which implicitly gives more strength to the data rather than the prior. If both $\\lambda$ and $\\sigma^2$ were allowed to vary with increasing $N$, the authors have reason to believe that this trend would not be observed."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b8d24fdd-01a4-5a46-a104-13af399e3f2b", "question": "Representation learning. Is it feasible to design an algorithm with the proposed objective that also updates the feature representation (using some different optimization method)? In that, I guess convergence guarantees and generalization bounds may not hold but may lead to better performance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4c78b8be-a940-5979-a99f-7ece20c84cc4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Representation learning. Is it feasible to design an algorithm with the proposed objective that also updates the feature representation (using some different optimization method)? In that, I guess convergence guarantees and generalization bounds may not hold but may lead to better performance?", "reference_answer": "A: The proposed method can be easily adapted to an end-to-end automatic differentiation framework. Incorporating automatic representation learning into the proposed method is indeed highly desired because of its practical value in applications. The authors omitted the discussion of this topic in the proposed initial submission due to space limits, but will include it in the proposed revision if extra space is available.\nAlthough any representation learning model can be adopted, the authors focus on discussing the most popular one nowadays, the neural network model with end-to-end learning and automatic differentiation. The authors show how to make use of the proposed DRO method as the final loss layer in a neural network model. A network for supervised learning typically has a linear classification layer in the end without activation. Assume the penultimate layer outputs $\\boldsymbol{\\Phi}(\\boldsymbol{x}) \\in \\mathbb{R}^{k \\times d}$ for input $\\boldsymbol{x}$, the last layer will typically output  $\\boldsymbol{\\psi}(\\boldsymbol{x}) := \\boldsymbol{\\Phi}(\\boldsymbol{x}) \\boldsymbol{\\theta} \\in \\mathbb{R}^{k}$ for some $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d}$. $\\boldsymbol{\\psi}(\\boldsymbol{x})$ is sometimes called logits and yields probability distribution with a softmax layer. For example, in univariate classification, $k$ is the number of labels. In dependency parsing, $k = n^2$ with $n$ being the number of tokens in the input sentence $\\boldsymbol{x}$. Given $b$-dimensional token-wise embeddings before the penultimate layer, the biaffine layer in BiAF yields $\\boldsymbol{\\Phi}(\\boldsymbol{x}) \\in \\mathbb{R}^{n^2 \\times b^2}$ equivalently ($b^2$-dimensional feature vector for each arc). Thus $\\boldsymbol{\\psi}(\\boldsymbol{x}) \\in \\mathbb{R}^{n^2}$ is the logits for all the arcs. Note that $\\boldsymbol{\\theta}$ in the proposed formulation is naturally equivalent to the parameters of the aforementioned last linear layer. Therefore having $\\boldsymbol{\\psi}(\\boldsymbol{x})$ is sufficient for us to compute $\\mathbb{P}^*_{Y|\\boldsymbol{x}}$ and $\\mathbb{Q}^*_{Y|\\boldsymbol{x}}$. In this way, the proposed method is the loss layer without learnable parameters, which backpropagates the sub-derivative of the objective with respect to $\\boldsymbol{\\psi}(\\boldsymbol{x})$ to the linear classification layer: $\\partial{\\text{Obj}}/\\partial{\\boldsymbol{\\psi}(\\boldsymbol{x})} \\triangleq \\sum_{i = 1}^{B}  {\\mathbf q}^{(i)*} - {\\mathbf p}_{\\text{emp}}^{(i)}$, where $B$ is the batch size. Recall $\\mathbf{q}$ and ${\\mathbf p}_\\text{emp}$ are the probability vectors for $\\mathbb{Q}$ and $\\mathbb{P}^{\\text{emp}}$ respectively. The sub-derivative of the regularization term with respect to $\\boldsymbol{\\theta}$ is added to the classification layer. Although losing global convergent and provable generalization guarantees, the authors are now able to take advantage of automatic differentiation and focus on solving the minimax problem given $\\boldsymbol{\\psi}(\\boldsymbol{x})$ and groundtruth $\\boldsymbol{y}$ for training. Since the computational bottleneck lies in computing $\\boldsymbol{\\Phi}(\\boldsymbol{x}) \\boldsymbol{\\theta}$ while GPU acceleration now does it for us, the overhead of computing the adversarial loss should not be much higher than that of computing the cross-entropy loss."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b07a20cb-8d04-5ff8-bd90-0c87a4c67694", "question": "What are the modeling decisions introduced in this paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ba17e297-e254-519a-b329-6abf2c38dbe2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the modeling decisions introduced in this paper?", "reference_answer": "A: The whole story of the proposed work derives from two parts: \n\n1) The proposed first starting point is to encode the path of AST for code representation. This idea is first presented at Code2Vec and Code2Seq. The Code2seq obtains state-of-the-art of code summarization using only pairwise path information in AST. However, code2seq lacks the modelling of context, which leads us to explore combining the path representation with the source code context. Since the Transformer can model context well, the authors naturally consider integrating tree path encoding into Transformer.\n\n2) The other point comes from the proposed baselines(Great and CodeTransformer). These models design based on the Transformer, and both introduce the structural inductive bias in Transformer. Technically, they modify the positional encoding of the Transformer and use either manually designed structural edges or distance across AST to represent structural information. In this paper, the authors pursue the research line of combining the Transformer-based model with additional\nstructure information of source codes. \n\nEssentially, the difference between the proposed model and the proposed baselines is encoding path vs manually designed edges or distances. The path's different node combinations contain plenty of structure information, which is overlooked by encoding distances. To verify the benefit of learning from paths, the authors conduct ablation studies: before feeding the path into Transformer, the authors first convert all nodes in the path to the same one. In such a case, what the model learns is degenerate completely into hops across the syntax tree between two terminals.\nFor more details about ablation studies, please refer to L326-L340. \n\nPlease refer to the table below for clear comparisons.\n\n|Model|Context|Structure|\n|----|----|----|\n|Code2Seq|No|Pairwise Path|\n|Great|Yes|Manually Designed Edges|\n|XLNet|Yes|No|\n|CodeTransformer|Yes|Multiply Structural Distances|\n|TPTrans|Yes|Pairwise Path|\n\nThe pointer network enhances predictions by pointing at positions, and it has already been widely adopted in lots of seq2seq models (in both NLP and Code Intelligence). For more details about the pointer network, please refer to the A3 for y3EX."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c8f4762f-7a4d-5a31-b560-e705a33b685b", "question": "In equation (3), should it be from $t=1$ to $T$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["befa2892-2ca9-59fb-bd49-724e13203e60"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In equation (3), should it be from $t=1$ to $T$?", "reference_answer": "A: Yes"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "befc0d29-4ba0-5b7e-8adb-d6ca7b52794d", "question": "Figure 1, pane B, indicates that orthonormal bases are able to distinguish nodes that were indistinguishable in non-orthonormal bases. However, it seems to me that those points were already distinguishable before? What am I missing here? Also, do these points correspond to points in the dataset (like pane A), or are these just for illustration purposes?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 1, pane B, indicates that orthonormal bases are able to distinguish nodes that were indistinguishable in non-orthonormal bases. However, it seems to me that those points were already distinguishable before? What am I missing here? Also, do these points correspond to points in the dataset (like pane A), or are these just for illustration purposes?", "reference_answer": "A: In the non-orthonormal bases of Figure 1(b), there still exist overlaps between two groups of nodes, making them indistinguishable from a plane, while in the orthonormal bases, those two groups can be separated completely. The figure here is just for illustration purposes. Specifically, the authors randomly generate two groups of nodes with different centers based on the normal distribution, then convert the bases from non-orthonormal to orthonormal. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fdfc7f78-19c1-5504-902a-11ec67e455d3", "question": "Computational Complexities. It looks to me that the proposed layer is quite expensive. In the experiment, only one layer in ResNet is replaced by the proposed layer, and only two iterations are used in unrolling. And this already decreases the speed from 1000 to 900. Provide the results of the study on the relationship between accuracy, complexity, and iterations when all layers are replaced.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["823a2e85-8548-57a2-8256-5fdb0159c2af"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Computational Complexities. It looks to me that the proposed layer is quite expensive. In the experiment, only one layer in ResNet is replaced by the proposed layer, and only two iterations are used in unrolling. And this already decreases the speed from 1000 to 900. Provide the results of the study on the relationship between accuracy, complexity, and iterations when all layers are replaced.", "reference_answer": "A: The following table shows the comparison of SDNet-18 and SDNet-18-All on accuracy, complexity. SDNet-18-All means all convolution layers are replaced with CSC-layer. And the number of FISTA iteration is two for all CSC-layers, hence the complexity is only twice. In the new supplementary material, the authors have also conducted ablation studies on the number of iterations on ImageNet, see Table D.1.\n|                                 | Model Size     |     Top-1 Acc    |      Memory     |       Speed|\n|----------------------|-------------------|------------------|-----------------|------------|\nSDNet-18                |     11.2M          |      95.20%       |      1.2GB        |  1500 n/s |\nSDNet-18-all           |     11.2M          |      95.18%       |       2.5GB       |   720  n/s |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1352dbd2-3674-538a-990c-b47b693813b4", "question": "What are 'respective fields'?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are 'respective fields'?", "reference_answer": "A: It is a common concept in the attention mechanism, which also frequently appears in reference [A].\nAs the authors know, the final step of self-attention is to aggregate the features according to the attention score with softmax, i.e., one feature is reconstructed by weighted summation from other features. The respective fields mean the valid range that can be attended by every feature, i.e., weights after the softmax>0. The respective fields of the transformer are controlled by attention masks.\n[A] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055–4064. PMLR, 2018"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "18dc1549-343b-56ea-aa9b-cb287ae72aea", "question": "This paper is not the first work on image-based imitation learning. What are its connections to Pathak et al., Torabi et al., Liu et al.? \nReferences:\nPathak, Deepak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. \"Zero-shot visual imitation.\" In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 2050-2053. 2018.\nTorabi, Faraz, Garrett Warnell, and Peter Stone. \"Imitation learning from video by leveraging proprioception.\" In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 3585-3591. 2019.\nLiu, YuXuan, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation.\" In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1118-1125. IEEE, 2018.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f56aa6c0-7577-5284-8985-9c457597cdb3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "This paper is not the first work on image-based imitation learning. What are its connections to Pathak et al., Torabi et al., Liu et al.? \nReferences:\nPathak, Deepak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. \"Zero-shot visual imitation.\" In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 2050-2053. 2018.\nTorabi, Faraz, Garrett Warnell, and Peter Stone. \"Imitation learning from video by leveraging proprioception.\" In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 3585-3591. 2019.\nLiu, YuXuan, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. \"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation.\" In 2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1118-1125. IEEE, 2018.", "reference_answer": "A:  Broadly speaking, while these papers are related work in the sense of “imitation learning from image observations”, the exact problem settings and scope differ from those of the proposed paper, thereby making them not suitable as direct baselines. Here are some reasons:\n- Pathak et al. is specific to goal-conditioned RL, and is not applicable to tasks and environments that are are not goal-oriented (such as those considered in the experiments of this paper)\n- Torabi et al. considers a setting similar to ours, but assumes access to robot proprioception, which makes the problem substantially simpler than learning from visual inputs alone. Further, they use an on-policy model-free approach (PPO), which is not competitive in terms of sample complexity. **the authors ran such an ablation in the setting in this paper (from image observations only) and it showed little progress in 200k environment steps.**\n- Liu et al. considers a different problem setting, where there is domain shift between the demonstrator and agent. The main contributions of Liu et al. pertain to this domain shift, which is not present in the environments this paper consider, thereby making it an orthogonal contribution."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "acebb0fa-c6c0-5491-8877-6c070c6144f8", "question": "Does the last term in the second equation in (3) need to be replaced by $v_i^{\\boldsymbol{\\pi}}(s';\\boldsymbol{w},\\boldsymbol{\\phi})$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["893be569-d272-56c9-b2c1-fa6752fbc427"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the last term in the second equation in (3) need to be replaced by $v_i^{\\boldsymbol{\\pi}}(s';\\boldsymbol{w},\\boldsymbol{\\phi})$?", "reference_answer": "A: No. Here the first line is Bellman equation of the bootstrapped V-function $v_i^\\pi (s; \\boldsymbol{\\phi})$, and the second line uses V-function to define $v_i^\\pi (s; \\boldsymbol{w}, \\boldsymbol{\\phi})$ conditioned on $\\boldsymbol{w}$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "05039347-4c09-52a2-96d9-63951f3913f0", "question": "The authors propose a spatio-temporal prototype alignment method. I am considering the efficiency of this new method. Does the proposed method have the same computation budget as the old way?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b0d70329-7bd8-5c32-81c7-5743ec00d545"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors propose a spatio-temporal prototype alignment method. I am considering the efficiency of this new method. Does the proposed method have the same computation budget as the old way?", "reference_answer": "A: The proposed method is more efficient than the old way. For example, TRX aggregates the temporal information through the arrangement and combination of spatial information pairs/triplets. The complexity will increase rapidly with the number increase of the input frames. Moreover, this combination of sparse frames is not suitable for processing long videos, and its recognition ability for complex actions will also be limited (some complex human actions cannot be represented by only 2 or 3 sparse sampled frames). A simple example is that pairs/triplets-based approaches cannot distinguish whether a person hits the desk 3 times or 4 times. Because the maximum number of sampling frames is only 3, and it is difficult for the model to understand this repetitive action beyond triplet. The proposed method is to directly generate frame level spatio-temporal representation, which already contains rich temporal information, so the laborious combination operation is omitted. It can be considered as a more concise and general video feature alignment method. Here the authors make a table to show the combinatorial complexity explosion faced by the alignment method in TRX.\n\n|     # Input Frames    |     TRX [28]    |      TRX [28]    |            Ours           |\n|:---------------------:|:---------------:|:----------------:|:-------------------------:|\n|                       |      #Pairs     |     #Triplets    |     Temporal Dimension    |\n|            4          |         6       |         4        |              4            |\n|            6          |        15       |         20       |              6            |\n|            8          |        28       |         56       |              8            |\n|           12          |        66       |        220       |             12            |\n|           16          |        120      |        560       |             16            |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2a623f5b-08b3-52db-81bb-8b35e8cb1296", "question": "In theorem 1, when ζ is large, then δk shrinks fast but ηk is small, which contradicts the intuition that a large step size gives a fast convergence rate. Could you please explain this?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["957b66a3-1bd0-534c-9ef1-c3837dbdf921"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In theorem 1, when ζ is large, then δk shrinks fast but ηk is small, which contradicts the intuition that a large step size gives a fast convergence rate. Could you please explain this?", "reference_answer": "A: This theorem states that if $\\nu$ and $\\zeta$ exist such that Equation (9) holds then  $\\delta_k$ has the rate of $O(k^{-\\nu\\zeta})$. In high-level, this is because the recursion in (7) has a stationary point denoted by $r(\\eta)$ that is not necessarily placed at the origin (i.e., 0). On the other hand, by shrinking $\\eta$, the authors can move this stationary point toward zero. But there is a restriction. That is, by decreasing $\\eta$, the stationary point of the recursion will change and thus it requires some iterations to converge (or  get close enough) to this stationary point. If the step-sizes are selected too big, then the recursion cannot converge to the stationary point in a fixed small number of iterations which may result in divergence of the overall SGD. Therefore, the intuition that a larger step-size necessarily gives better convergence rate is not accurate. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d704136a-a6da-5d50-802d-76230cb797f3", "question": "In figure 2, which are the OOD plots and which are the ID plots?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6cfcaada-4caa-5df5-bc14-14bc2552e295"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 2, which are the OOD plots and which are the ID plots?", "reference_answer": "A: The dataset names are shown in the labels of the horizontal axis of each plot. For NLI task, the ID dataset is MNLI and the OOD dataset is HANS. For paraphrase identification, the ID dataset is QQP and the OOD datasets are PAWS-qqp and PAWS-wiki. For fact verification, the ID dataset is FEVER and the OOD datasets are Fever-Symmetric v1 and v2 (Symm1 and Symm2)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2fd30e16-4fc2-5acf-b163-dc64fa88d9d7", "question": "What is the precise definition for the accuracy and modularity depicted in Figure 1 ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7e085ea9-1675-54f5-a84a-34b7dd07c1e1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the precise definition for the accuracy and modularity depicted in Figure 1 ?", "reference_answer": "A: The definition of accuracy is described in line 207 in Section 4, i.e., \"Accuracy is computed as the ratio between the number of correctly predicted node labels and the number of nodes in the objects set.\" Due to limited space, the authors did not present the mathematical expression for modularity, but it can be easily found in reference [35], which the authors mention explicitly in line 209 in Section 4. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0263eb83-5ee1-5b0a-9477-70962d756984", "question": "QBR strategy is not clear. ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["14c1199a-b695-5cec-a11a-023781bfb0c5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "QBR strategy is not clear. ", "reference_answer": "A: The authors revised the caption of Figure 7 and a brief description of QBR lines 178-181. The detailed description and algorithm are in Appendix C. In addition, the authors add an analysis of QBR and w/o QBR models in Appendix C of the revised version. Please have a look."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e01c8081-b70b-5d02-87c0-e06b0cfe26c2", "question": "The overhead introduced to calculate the important masks ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The overhead introduced to calculate the important masks ?", "reference_answer": "A:  Following the suggestion, the authors count the time it takes to generate the mask in the convolution. The results are as follows:\n\n| Method / speed(ms) | KITTI (VoxelNet) | KITTI (mask time) | nuScenes (VoxelResNet) | nuScenes (mask time) |\n| ------------------ | ---------------- | ----------------- | ---------------------- | -------------------- |\n| spss topk          | 36 ms            | 1.7 ms            | 44 ms                  | 4.6 ms               |\n| sprs topk          | 33 ms            | 0.4 ms            | 44 ms                  | 0.9 ms               |\n\n**Impact on latency：** It should be mentioned that the generation of masks is based on torch.argsort(). Since PyTorch optimizations are not ideal, this part does generate additional time consumption. And this effect is more pronounced as the number of points increases. At present, the time consumption generated by the mask is still within an acceptable range as shown in the table.  the authors will use the divide and conquer algorithm to write a customized CUDA module to accelerate topk operation, which would further improve the latency. *Note that the proposed model still obtains around a 20% overall reduction in latency even with this naive implementation without sacrificing accuracy.* "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "09bae8d7-47b4-57a6-a9da-7c61db6672ee", "question": "Empirical results section can be reduced (with Table 2 in Appendix). What can we do with outliers when Condition 6 does not hold (or the matrix is close to singular)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["819b39ef-0abc-53fa-b3eb-f260fa69300f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Empirical results section can be reduced (with Table 2 in Appendix). What can we do with outliers when Condition 6 does not hold (or the matrix is close to singular)?", "reference_answer": "It would be nice to plot invertibility test results together with the estimates (PNS and causal risk difference) to ensure that those outliers are mainly due to near-singular matrices. When Condition 6 does not hold, the joint probabilities of potential outcomes are not estimable by the proposed estimation method. In addition, theoretically, as far as the matrix is not singular, Theorem 2 shows that the joint probabilities of potential outcomes are estimable. Thus, the authors did not consider the outlier problem of observed values since the authors use joint probabilities from a single dataset to estimate the joint probabilities of potential outcomes. However, as you stated, since it is very important to know the correspondence between the estimation accuracy and the singular model regarding the proposed research."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2517ec33-2653-5b67-be05-95a6e5b771ec", "question": "What is the novelty of the proposed work?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the novelty of the proposed work?", "reference_answer": "A: The authors believe that the proposed study is novel due to the following reasons.\n1. From the problem definition side, the authors are the first to study the relationship between self-supervised learning and adversarial robustness in the 3D space, which was largely unexplored. This is a new and important research problem. It contains essential differences in studying the relationship between 3D adversarial robustness and 3D SSL compared to the 2D domain. For instance, different from prior work in adversarial robustness in 2D vision that either leverages a single SSL task [1] or targets a single backbone model [2], 3D point cloud recognition using deep learning is a relatively nascent field, where finding proper universal “3D backbones” is still an active research direction. Understanding which 3d backbone is more robust in the early stage could help guide the community towards the right direction. Therefore, it is necessary to consider different types of learning architectures to study their robustness. Moreover, given the intrinsic sparsity and set property of 3D point cloud data, various 3D point cloud domain-specific attack threat models (e.g. point shifting, point adding, and point dropping) should be investigated. Therefore, in this paper, the authors conduct a much more comprehensive study along multiple 3D domain-specific dimensions to study its robustness including three 3D SSL tasks, three different types of attack threats, and three backbone architectures based on the properties of point cloud data. It is also worth noting that we, for the first time, formulate point adding (PA) and dropping (PD) adversaries into a general AT analysis framework. As also mentioned by the reviewer, the authors believe such a study could be useful to the community and future research in this domain. \n2. From the technical perspective, improving the adversarial robustness of 3D point clouds with self-supervised learning tasks is a non-trivial problem. The authors find that while some designs in 3D point cloud models work well in standard training, but they fail to provide robustness in adversarial training. The reason is that these designs introduce **randomness** in the inner maximization stage of adversarial training, causing the overall training intractable. For example, as illustrated in Section 3.2, the authors find that several components in PointNet and PCT (e.g., T-Net and farthest point sampling (FPS)) cannot work well by directly applying adversarial training. T-Net applies an overall transformation matrix to the point clouds and middle layer features, where a small perturbation in the matrix could affect all the features. Therefore, T-Net will introduce a high variance of performance in adversarial training. PCT originally leverages farthest point sampling (FPS) to sample anchors to do local clustering. Since the point cloud is dynamically changing in adversarial training, the sampled anchors are totally different in each PGD iteration, which will make models confusing. Therefore, the trained model is still not robust. The authors apply multiple modifications, including replacing the FPS operation with EdgeConv to stabilize the adversarial training (detailed in Section 3.2 and Appendix B). The capacity of EdgeConv is a superset of FPS operation since it views every point as an anchor and performs clustering so that it removes the randomness introduced by FPS. **Such a change does not modify the usage of the transformer in PCT, as shown in Appendix B, and will improve both clean and robust accuracy for the baseline.**\n3. From the experimental analysis side, the proposed analysis unveils multiple insights which are unique in 3D point cloud learning. For example, [2] shows that jigsaw SSL does not perform as well as rotation in 2D vision for robustness enhancement. However, 3D jigsaw helps connect the global and local feature learning, which improves a lot in 3D adversarial training. The authors also take the first step to analyze the robustness in the part segmentation task and provide valuable insights. \n4. The proposed ensemble methods are based on the proposed transferability analysis of different adversarially fine-tuned models that preserve different vulnerabilities. Therefore, simple but effective ensemble methods can tangibly improve the robustness. In contrast, [4] leverages different attacks/threats to adversarially train the ensemble model. The authors believe that the proposed study highlights different insights from [4]. \nThe authors believe a systematic study with insights should be considered a more critical factor, especially in the adversarial machine learning community, since fancier solutions may give a false sense of security that would be adaptively broken [3]. The proposed study further motivates future research on designing more robust operations, architectures, and SSL tasks in 3D vision.\n\n[1] Hendrycks, Dan, et al. \"Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty.\" Advances in Neural Information Processing Systems 32 (2019): 15663-15674.\n[2] Chen, Tianlong, et al. \"Adversarial robustness: From self-supervised pre-training to fine-tuning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n[3] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning. PMLR, 2018.\n[4] Florina et.al, Ensemble Adversarial Training: Attacks and Defenses, ICLR (2018)"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5d864438-a489-5829-8c1a-d2b9ae43ed13", "question": "It looks like part of the model's intuition is: I'd like to produce features (with distilled data) such that the solved weights can classify real images correctly. Is this almost quite similar to feature matching, but instead of directly matching them using standard metrics, the authors matched it with an optimization criterion? Do you think this is the reason that the proposed algorithm produces quite realistic-looking images?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It looks like part of the model's intuition is: I'd like to produce features (with distilled data) such that the solved weights can classify real images correctly. Is this almost quite similar to feature matching, but instead of directly matching them using standard metrics, the authors matched it with an optimization criterion? Do you think this is the reason that the proposed algorithm produces quite realistic-looking images?", "reference_answer": "A: It is an interesting question. Intuitively, the proposed method predicts by computing the similarity (measured by the conjugate kernel) between a real image and a set of distilled images. If the authors want this similarity measure to be accurate, making the distilled data features look closer to that of real data is better. Therefore, realistic-looking seems like a natural result of applying the proposed method. However, multiple factors can affect the appearance of the proposed distilled images. For example, Appendix C.4 shows that learning the label can play an important role; Appendix C.6 shows that the width, depth, normalization, and skip connection can also affect the appearance. Therefore, the authors also need to choose the right model architecture to generate realistic-looking images.\n\nHowever, the authors do not think the proposed method is very similar to feature matching. The key difference is that the proposed objective is indeed task-driven and requires labels to find the most discriminative feature for a particular task. However, the feature matching generally matches some feature statistics of the real and distilled data. It is likely that they only capture some general features that are useful to recover the data but not very useful for the downstream task. This is why the previous methods, like gradient matching and distribution matching, fail on the fine-grained classification task (Table 1. CUB-200), as many fine-grained classes share the same distilled features, thus providing no discriminative ability. Another difference is that feature matching objectives are not amenable for label learning, which is crucial for complex label spaces (e.g., ImageNet-1K, CUB-200). Thus, it is difficult for them to consider the class similarity information, so it becomes hard to learn the sharable information across different classes, resulting in a poor compression rate. Moreover, another drawback of surrogate objectives like gradient matching or feature matching is that it is unclear how far it is from the true objective. It is pretty tricky to measure how \"biased\" those objectives are. It is unclear whether the correct set of features (e.g., last layer or middle layer's activation or gradient) is used or whether a proper distance measure (e.g., L2 distance, cosine distance) is used. The authors conjecture that this biased objective is why previous methods do not show real-looking images. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3ba19a78-04b5-5de2-9cb0-a6e6d5dcc763", "question": "Can you discuss the computation time required for ReMERN and ReMERT? In particular, I am interested to know its comparison with DQN and PER. Is the computation of w the bottleneck of the algorithm, or that it does not affect the overall computation time significantly. ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7bb011ff-6526-5a93-9b59-878505dc29ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you discuss the computation time required for ReMERN and ReMERT? In particular, I am interested to know its comparison with DQN and PER. Is the computation of w the bottleneck of the algorithm, or that it does not affect the overall computation time significantly. ", "reference_answer": "A: Comparative experiments on Atari with DQN and PER are time-consuming and the authors here provide experiments on MuJoCo instead. Comparisons of the computation time required for SAC, DisCor, PER, ReMERN and ReMERT are shown in the following table. Experiments are conducted with one NVIDIA 2080Ti GPU on the Ant-2d environments. \n|         Algorithm        |  SAC  |  PER  | DisCor | ReMERN | ReMERT |\n| --------- | --------- | ----------- | ----------- | ----------- | ------------ | \n|    Time/h for 1M step    |  4.88 |  6.73  |  6.87  |  8.13  |  6.45  |\n|Time/h to reach reward 5k |  4.92 |  23.21   | 10.98 |  13.66 |  4.51  |\n\nAs shown by the results, prioritization methods like DisCor, ReMERN and ReMERT will indeed increase the time cost for each training step due to the neural networks, but these extra methods will not become the bottleneck of the computation time. Also, ReMERT can be more time efficient than SAC to reach a reasonable performance threshold. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ad3354c3-9a59-5df6-9f91-4a55428db668", "question": "Can you summarize why the appended information can always cut off the biased information path?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["1fc786d5-cfc8-580e-b579-c2c56e8dba87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you summarize why the appended information can always cut off the biased information path?", "reference_answer": "A: The trigger learned by the reprogram contains very strong demographic information and blocks the model from relying on the real demographic information from the input. This argument is both empirically verified by experiments (shown in Table 3) as well as theoretically proven in Sec. 3.4. Since the same trigger is attached to all the input, the uniformal demographic information contained in the trigger will weaken the dependence of the model on the true demographic information contained in the data, and thus improve the fairness of the pretrained model. Please kindly refer to the proposed response to Q1 for a brief summary of how the proposed algorithm works. The authors will move the relevant content to the main paper to improve the readability of the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d2fcb03b-b1fb-5ec8-a5f9-32f72624691d", "question": "What is the difference between the policy parameters theta and the conditioning variable z?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d93d3d23-a98b-5dbc-9827-e95bb0c84095"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference between the policy parameters theta and the conditioning variable z?", "reference_answer": "A: For context, the conditioning variable z describes the task that the policy should strive to achieve, the semantics of which grounded in each particular task being distinguishable on the basis of the state visits (this is effectively what the loss function specifies). Now to your question, the conditioning variable z is drawn from a fixed distribution (uniform on the 5-sphere) that remains constant throughout training, whereas the policy parameters theta are updated to minimize the loss function in equation 9 (through an application of the REINFORCE trick described above). That said, there are a second set of parameters, those of the variational approximation phi, that also try and minimize the same loss function, but are able to do so directly through back-propagation (the unknown environmental dynamics prevent this for the policy parameters)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1be2e9fe-15f6-581f-a633-a28d0661926b", "question": "The authors adopt Pearson correlation as the \"relation\" metric in DIST, will the performance also be significant when having other relation metrics?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6013db7e-68aa-5fd5-ae37-f6f307517d40"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors adopt Pearson correlation as the \"relation\" metric in DIST, will the performance also be significant when having other relation metrics?", "reference_answer": "A: The authors have also adopted another cosine similarity based match in the proposed experiments (see Table 10 in the proposed work), The results show that, cosine similarity can also enjoy an obvious improvements over the KD baseline. The authors believe there are more types of relations such as non-linear correlations can also benefit the performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fbd5b8dd-d3df-5335-8fa2-a7d9ab132c09", "question": "Discuss the ablation study about the sample selection threshold $\\tau$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["bc6e6c46-0eca-5f7e-94fc-528bc301819d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss the ablation study about the sample selection threshold $\\tau$.", "reference_answer": "A: (1)The sample selection the authors adopted is to estimate this clean probability of samples by modeling sample loss values with a GMM model [5,6] using the Expectation-Maximization algorithm.  If the clean sample can be distinguished according to loss values, and its estimated probability is accurate, the best threshold will be about 0.5. Hence, $\\tau=0.5$ is a typical value in related works [5,6], and the authors follow this practice in the proposed experiments.\n(2) Using the classification performance on noisy validation set as the criterion for model selection is a typical and empirically useful practice [7-10] in label-noise learning, even in the cases with instance-dependent label noise [9,10]. In this paper, the authors use mAP score on the noisy validation set as the criterion for model selection. Table 2-1 shows the ablation study about $\\tau$, which represents $\\tau=0.5$ is a good choice  both according to mAP scores on the noisy validation set and according to mAP scores on the clean test dataset.\nReferences:\n[5] Partial Multi-Label Learning With Noisy Label Identification. TPAMI 2022.\n[6] Unsupervised Label Noise Modeling and Loss Correction. ICML 2019.\n[7] DivideMix: Learning with Noisy Labels as Semi-supervised Learning. ICLR 2020.\n[8] Robustness of Accuracy Metric and its Inspirations in Learning with Noisy Labels. AAAI 2021.\n[9] Are Anchor Points Really Indispensable in Label-Noise Learning? NeurIPS 2019.\n[10] Parts-dependent Label Noise: Towards Instance-dependent Label Noise. NeurIPS 2020.\n[11] A Second-Order Approach to Learning With Instance-Dependent Label Noise, CVPR 2021."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "35ee2edd-0616-5c90-9228-73cee2169ce3", "question": "Investigate the threat models and their experimental settings.\nReferences:\n[1] Xiang, Chong, Charles R. Qi, and Bo Li. \"Generating 3d adversarial point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Liu, Daniel, Ronald Yu, and Hao Su. \"Adversarial shape perturbations on 3D point clouds.\" European Conference on Computer Vision. Springer, Cham, 2020.\n[3] Sun, Jiachen, et al. \"On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks.\" arXiv preprint arXiv:2011.11922 (2020).\n[4] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n[5] Zheng, Tianhang, et al. \"Pointcloud saliency maps.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Investigate the threat models and their experimental settings.\nReferences:\n[1] Xiang, Chong, Charles R. Qi, and Bo Li. \"Generating 3d adversarial point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Liu, Daniel, Ronald Yu, and Hao Su. \"Adversarial shape perturbations on 3D point clouds.\" European Conference on Computer Vision. Springer, Cham, 2020.\n[3] Sun, Jiachen, et al. \"On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks.\" arXiv preprint arXiv:2011.11922 (2020).\n[4] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n[5] Zheng, Tianhang, et al. \"Pointcloud saliency maps.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n", "reference_answer": "A: In the proposed study, the authors follow two principles to set up the threat model. First, the perturbation will not affect human perception much. Second, the perturbation is more than effective to break the clean trained models.\n\nPoint shifting (PS) is a well-established attack in prior literature [1,2,3]. The authors follow the experimental setups in [3] in the proposed study since [3] is the only one that also targets adversarial training-based methods. The authors believe this threat model does make sense. As Table 1 shows, the proposed L-inf norm distance $\\epsilon = 0.05$ reduces all the clean trained models’ robust accuracy to 0 or near 0. The authors have also tested that even with $\\epsilon = 0.02$, the adversary can still reduce all the models’ robust accuracy to less than 5%. On the other hand, the perturbed point clouds with $\\epsilon = 0.05$ are at the edge of correct human predictions of objects.\nAs the authors are not allowed to insert links, the authors kindly refer the reviewer to Figure 9 in [3], which also indicates the same conclusion. Numerically, $\\epsilon = 0.05$ out of [-1,1] is also similar to the commonly used  $\\epsilon = \\frac{8}{255}$ in 2D adversarial training [4]. \n\nUnlike point shifting (PS) attacks, there is no defensive analysis point adding (PA) and dropping (PD) attacks since they are bounded by L-0 distance which is not differentiable. Adding or dropping a large amount of points will also have a major effect on human perception [1,5]. In the published paper that proposes PA attack [1], they suggest adding 50 to 100 points bounding by informal Chamfer distance. Therefore, the authors adopt their experimental setting to make sure that the proposed threat model will not affect human perception. Since the attacker only has the ability to modify these newly added 100 points, it cannot reduce the accuracy to near 0. Similarly, the original paper that proposes PD has claimed dropping 200 points has great potential to even fool human perception [5]. Therefore, the authors restrict the proposed threat model to be able to drop 100 points. The proposed evaluation results are also consistent with [5] in terms of both clean and robust accuracy. \n\nTherefore, the authors believe that the proposed threat models are reasonable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0bc860b1-ece5-56b7-8075-bd582564319c", "question": "What are the errorbars over in Figure 5? Are they multiple seeds? If not, then I would like to see the figure updated with results from multiple training runs in order to properly assess variance.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0c5d282a-1541-5166-bc1b-300a30619e73"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the errorbars over in Figure 5? Are they multiple seeds? If not, then I would like to see the figure updated with results from multiple training runs in order to properly assess variance.", "reference_answer": "A: Yes. The error bars are the 95% confidence interval across 50 test runs with random initialization seeds. The authors use this setting in order to make the proposed performance directly comparable to that of PlaNet, in which the authors use this evaluation metric (though with fewer runs)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4995e534-c205-5058-9ddd-a5be08f4e963", "question": "Is the MoCo contrastive loss computed with augmentations to images or directly to latents? Does this apply during NVAE pre-training?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9b2e64f8-a989-5565-91f2-58280f79add6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the MoCo contrastive loss computed with augmentations to images or directly to latents? Does this apply during NVAE pre-training?", "reference_answer": "A: In D2C, the augmentations are performed over images (using standard augmentation techniques in self-supervised learning). This is not applied to NVAE pre-training where the authors follow the approach of the authors."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "de1332d8-1c98-5ce0-bd9d-cdfcfada66ae", "question": "To leverage the language priors with the text encoder, we treat the rank categories as words. How to choose a suitable sentence? The sentence of “a person at the age of [rj] is the best? Present the results of the ablation studies.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cd6253e-8a5a-55f2-8bdc-77afa61d48e4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "To leverage the language priors with the text encoder, we treat the rank categories as words. How to choose a suitable sentence? The sentence of “a person at the age of [rj] is the best? Present the results of the ablation studies.", "reference_answer": "A: The prompt templates for ablation are shown in the tables below. \n\n| Ctx. Ind. | Template Ctx.                                                 |\n|:---------:|---------------------------------------------------------------|\n|    0-0    | Age estimation: the age of the person is {} .                 |\n|    1-0    | Age estimation: the age of the person in the portrait is {} . |\n|    2-0    | Age estimation: the age is {} .                               |\n|    3-0    | Age estimation: the age of the face is {} .                   |\n|    0-1    | The age of the person is {} .                                 |\n|    1-1    | The age of the person in the portrait is {} .                 |\n|    2-1    | The age is {} .                                               |\n|    3-1    | The age of the face is {} .                                   |\n\nThe table below shows that different optimization start points all lead to similar convergence and performance, which suggests that the most meaningful templates work fine for this task.\n\n| Ctx. Ind.   |  0-0 |  1-0 |  2-0 |  3-0 |  0-1 |  1-1 |  2-1 |  3-1 |\n|-------------|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n| OrdinalCLIP | 2.30 | 2.31 | 2.30 | 2.32 | 2.31 | 2.32 | 2.32 | 2.31 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "36d86679-44d6-5190-bd87-755e6a5bad2b", "question": "I am wondering about the relation to soft-modularization. As far as I can tell it seems to be more general because it allows for dependency on s. The proposed method should then be a specific case of this (z does not depend on s and are shared for all m). Why should it then be less flexible (line 189.)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am wondering about the relation to soft-modularization. As far as I can tell it seems to be more general because it allows for dependency on s. The proposed method should then be a specific case of this (z does not depend on s and are shared for all m). Why should it then be less flexible (line 189.)?", "reference_answer": "A: Soft-modularization divides each layer into several groups of “modules” and then combines their outputs with “soft weights” from another “routing” network. To obtain these soft weights, the routing network takes both the task id and state as input.\n\nIn PaCo, the w-network generates a compositional vector $\\mathbf{w}$ by taking only the task id as input, and  the compositional vector  is used for combination in the parameter space ($\\boldsymbol{\\theta} =\\mathbf{\\Phi}\\mathbf{w}$).\n\nWhile “dependency on state” is one difference between PaCo and soft-modularization, another crucial difference is that PaCo interpolates in parameter space, while soft-modularization performs combination in the network output space (mentioned in Line186-187). Because of this, even if the authors incorporate s into the w-network of PaCo, it is still different from soft-modularization. As a side note, it’s easy to incorporate s into w-network, but doing so will lose the separation property of task-specific and agnostic parts, which contributes to the final performance of the proposed method (w-reset).  \n\nThe Equation in L185 of the paper was meant to help capture the connections between two methods and has to be interpreted together with the sentences in L186-187. But unfortunately, it seems that this way of presentation could be misleading if looking at the Equation itself. Because of this, the authors will remove the equation in revision and use literal descriptions for the connection and differences to avoid confusion.\n\nthe authors agree the term “less flexible” might be a bit misleading. By “less flexible in some cases”, the authors actually meant “less desirable/applicable in some cases”. For example, in cases where the authors want to conduct some task-specific operations, e.g., w-reset, it is not straightforward to do so in soft-modularization since the authors cannot find parameters that are dedicated to a specific task, because of the fact that all the networks have a “dependency on state”. The authors will clarify this point in revision."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8c13c098-866a-5cb8-a84a-bac413967d4d", "question": "Could you add these graphs for 'PointNav' and 'ObjectNav' tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5588e14f-c077-5bc8-92bf-4cc729a17980"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you add these graphs for 'PointNav' and 'ObjectNav' tasks?", "reference_answer": "A: Happy to. The authors have added Figure A2 that shows Success vs. Time for these tasks. The result is consistent with the Habitat 2.0 tasks: VER reaches a given success threshold with significantly less wall-clock time. Specifically, to reach the maximum success achieved by DD-PPO (97.4% on PointNav and 13.0% on ObjectNav), VER uses 1.6x less compute on PointNav (saving 16 GPU-days) and 4.3x less compute on ObjectNav (saving 33.4 GPU-days)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "af12cc5a-9f8b-55e9-a3d5-04ffe3a1b775", "question": "Explain about progressive attribute prototypes and comparison with APN.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b450247e-0111-5e71-9a4f-e759f075be1b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain about progressive attribute prototypes and comparison with APN.", "reference_answer": "A: The **common ground** between DPPN and APN is that both of them learn a set of attribute prototypes for a dataset. Taking CUB dataset ($312$ attributes) as an example, the attribute prototypes are trainable parameters of dim=$C\\times 312$ ($C$ is backbone feature channel), which are trained via back-propagation and shared by all images in CUB. The **different point** is that, when testing an image, APN directly uses the well-trained attribute prototypes to localize attribute-related regions for an image. Differently, DPPN first adjusts attribute prototypes according to visual contents of the testing image, and then uses these adjusted attribute prototypes to localize regions. Usually, the attribute prototypes will be adjusted several times (we found that adjusting three times obtains the best performance), thus the authors call DPPN “progressively adjusts attribute prototypes for an input image”. Based on such a progressive adjusting process, the final attribute prototypes for different images are different in DPPN, which the authors call “dynamically adjust attribute prototypes for different images”. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "da22cf6c-6311-5e03-82c7-83d492c045a3", "question": "Have you tried to compare SimCSE with “supervised models” setting?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Have you tried to compare SimCSE with “supervised models” setting?", "reference_answer": "A: The authors did this in Section 4.6 and results are shown in Table 6.  As mentioned in the paper: *the authors observe that Visual/AudioCSE can still outperform supervised SimCSE in all the tasks, but with much smaller margins. This shows that the learning signal from the high-quality negative and positive pairs of the NLI dataset are very strong (leading to a 5.32 improvement over unsupervised SimCSE) and cannot be supplemented by supervision from other modalities.''*"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6cb24b3b-c906-5fe3-8ee4-09f1427aa6a1", "question": "Evidence for the claim that OpenAI-Five, WuKong, etc. would fail with human teammates? (In particular, because of the communication gap between humans and agents?):", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["426ea220-62b5-5f9a-b6c3-7a6e2c2814ca"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Evidence for the claim that OpenAI-Five, WuKong, etc. would fail with human teammates? (In particular, because of the communication gap between humans and agents?):", "reference_answer": "A: First, as can be seen from Tables 1 and 2 (Main text) and Table 7 (Appendix), as the number of human players increases, the WR of the MC-Base (can be considered as SOTA) agent-human team decreases. While the WR of the MCC agent-human team is significantly higher than that of MC-Base, confirming the effectiveness of the meta-commands communication between humans and agents. Note that the only difference between MCC with MC-Base is the addition of human-to-agent and agent-to-human communication. \nSecond, as shown in [Table 8](https://sites.google.com/view/mcc-demo/%E9%A6%96%E9%A1%B5#h.5drjm4dzsjyw), participants gave the MC-Base agent low scores for the Reasonableness of H2A and the Overall Preference metrics, indicating that the MC-Base agent rarely collaborates with human teammates, resulting in a poor team experience. Note that, there is no communication exists in the MC-Base agent-human team."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ba3881e0-8606-5e12-b078-b2993e1cf06b", "question": "What are the effects of the hyperparameters $\\lambda_{2}$ and $\\lambda_{3}$ ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["1821bd6b-db14-5a63-b02c-4257dd291b97"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the effects of the hyperparameters $\\lambda_{2}$ and $\\lambda_{3}$ ?", "reference_answer": "A: The authors conduct an ablation experiment on $\\lambda_{2}$ and $\\lambda_{3}$. The results are shown in Figure C.4 in Appendix C.5 and below.\n\n|Model|$\\lambda_{2}=0$|$\\lambda_{2}=0.02$|$\\lambda_{2}=0.2$|$\\lambda_{2}=1$|$\\lambda_{2}=2$|$\\lambda_{2}=4$|\n|:-----|:-----:|:----:|:----:|:-----:|:----:|:----:|\n|PointNet|98.72%|98.67%|98.45%|94.22%|78.03%|69.45%|\n|PointNet++|99.69%|99.66%|99.58%|89.25%|79.82%|72.42%|\n|DGCNN|85.13%|84.82%|84.16%|69.76%|60.48%|52.46%|\n\n| Model | $\\lambda_{3}=0$ | $\\lambda_{3}=0.08$ | $\\lambda_{3}=0.8$ | $\\lambda_{3}=4$ | $\\lambda_{3}=8$ | $\\lambda_{3}=16$ |\n| :----- | :-----: | :----: | :----: | :-----: | :----: | :----: |\n| PointNet | 99.02"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3142d4d4-25d0-5f56-bb09-e02928ae67d6", "question": "How many repetitions were used for Fig. 5C?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7de55424-0d66-5dfd-9b68-f611c6c28c8c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How many repetitions were used for Fig. 5C?", "reference_answer": "A: The authors trained five repetitions for Figure 5C to reduce the noise from random variation across different model training runs."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "93776da8-ca6d-5f47-8613-f36941822aed", "question": "Can you evaluate the impact of BDETT on other application domains?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b73ceeab-4bcd-5261-8f93-2810b1cbdbab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you evaluate the impact of BDETT on other application domains?", "reference_answer": "A: The authors applied BDETT to image classification and the results show that it is effective in this vision task."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5056c429-63a5-551f-8eb1-92853987a8d8", "question": "What is the runtime memory and decoding speed during training/inference for the models and baselines in Tables 1, 2, 4 and 5? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e9212c2d-3733-5423-97bf-6bc024f6c4a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the runtime memory and decoding speed during training/inference for the models and baselines in Tables 1, 2, 4 and 5? ", "reference_answer": "A: FlashAttention brings significant memory reduction: 10-20x if just counting the attention layer (Figure 3 in the paper) and 2-4x memory reduction for the full transformer model, depending on sequence length (1.8x for BERT-large, 4x for GPT2-small). It allows us to train with longer sequences and thus improves model quality."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d4d673ac-15da-540b-85f4-873fcebd7e2c", "question": "What is the difference between the GQA programs and the collected GQA-Human-program?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["01109347-791c-5126-8638-3c915d33e900"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference between the GQA programs and the collected GQA-Human-program?", "reference_answer": "A: The results are as follows:\n\n| Metric                    | Official GQA | Collected GQA-Human-program |\n| ------------------------- | ------------ | --------------------------- |\n| Match rate                | 13.4%        | 20.1%                       |\n| Avg. length               | 3.1          | 4.6                         |\n| Avg. # filter-routines    | 1.9          | 0.6                         |\n| Ratio of complex programs | 15.2%        | 33.0%                       |\n\nThe results show that few programs can be exactly matched between GQA and the proposed collected dataset. As for the complexity, GQA-HUMAN-PROGAM is more complicated with longer and more complex programs. Note in the collection process, the authors encourage the annotators to make complex programs (see Appendix Figure D6). The authors found that the GQA-HUMAN-PROGRAM contains fewer filter routines because filter routines are often unnecessary (many of the GQA images contain one instance, so filter routines are not needed)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a78b6b14-2d63-58a1-8202-1a2b8083f7e2", "question": "Why are some latest works like latent space diffusion and VQ-Diffusion missed in the table for comparison?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9614a57f-96f6-5e08-8978-97dc94a17579"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are some latest works like latent space diffusion and VQ-Diffusion missed in the table for comparison?", "reference_answer": "A: Latent space diffusion first appeared as an unconditional generation paper, and updated a text-to-image model at the same time of the proposed work. The authors will compare it in a revised version. The authors already cited VQ-Diffusion and will add it to the table. These methods are diffusion-based and not aim to generate high-resolution images."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9b6008ae-872f-5988-a57a-c2cbb6869d5e", "question": "Performance with fine-tuning CLIP models", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["a017fa2d-2f53-5616-b5a4-2aca563cc758"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Performance with fine-tuning CLIP models", "reference_answer": "A: In Table 2, the authors compared MCM with a range of recent methods that require fine-tuning such as Fort et al. (based on ViT), MOS (based on BiT), and MSP (fine-tuned the ViT model in CLIP, same backbone as ours). Compared to the baselines, the authors show that MCM remains very competitive without fine-tuning on ImageNet-1K. \n\nDuring the proposed exploration, the authors did consider fine-tuning the entire backbone. However, the authors find that **(1)** simply fine-tuning both text and image encoders with the CLIP loss does not lead to consistent improvement for OOD detection as fine-tuning the large feature backbone without special optimization strategies can distort aligned cross-modal features learned during pre-training; **(2)** only fine-tuning the image encoder also does not yield consistent improvements compared to linear-probing. \n\nOur findings also echo a conclusion in a recent paper [1] on OOD generalization that shows fine-tuning the feature backbone leads to worse accuracy than linear-probing when the pre-trained features are good, and the distribution shift is large."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0e24ebc3-a431-5ef4-a4e1-1c689f83442c", "question": "What are the two design choices that discourage exact matching?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["615a5caa-14fa-5c4e-a066-1d3d8c093a39"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the two design choices that discourage exact matching?", "reference_answer": "A: First, the authors apply image augmentations to agent observation during training (L159-160). Thus, even if the agent navigates to the exact goal location it will never see the exact goal image, so it cannot learn exact matching. Second, the authors never process agent observations with the CLIP visual encoder -- which would make it easier to learn exact (as opposed to approximate) matching."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8288c5ac-5fe2-550b-91eb-9d5f342e9445", "question": "Figure 2, which stage is the middle two plots of Figure 2 in gradient flow? They seem to be in a very late stage of convergence, but a weird phenomenon is that the gradient flows of both DLOT and LOT exceed the target distribution firstly, and then come back. Especially when looking at those green arrows, they firstly point outside the moon, then point inside the moon. I think if you solve gradient flow correctly, it will not have this \"exceed first and then pull back\" process.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["54be9a09-b764-596d-ac07-57ee007c5fdd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 2, which stage is the middle two plots of Figure 2 in gradient flow? They seem to be in a very late stage of convergence, but a weird phenomenon is that the gradient flows of both DLOT and LOT exceed the target distribution firstly, and then come back. Especially when looking at those green arrows, they firstly point outside the moon, then point inside the moon. I think if you solve gradient flow correctly, it will not have this \"exceed first and then pull back\" process.", "reference_answer": "A: The authors run a GD scheme during 200 iterations and the authors plot in the middle the states at 50 and 100 iterations. The proposed GF considers a smaller step-size in the GD scheme and the authors present the figure in the proposed main text (please refer to Fig. 2 of the paper)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e5fde523-5a2d-56e7-84a5-eda9f0611d32", "question": "According to Algorithm 1 in the Appendix, the main difference between Phase 1 (MKD) and Phase 2 (Balancing training) is whether IIB is incorporated or not. So Phase 2 can be considered as a superset of Phase-1. So is it okay to skip Phase 1 and keep only Phase 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["85fe266a-f62a-59af-a50a-695f76ff0cb4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "According to Algorithm 1 in the Appendix, the main difference between Phase 1 (MKD) and Phase 2 (Balancing training) is whether IIB is incorporated or not. So Phase 2 can be considered as a superset of Phase-1. So is it okay to skip Phase 1 and keep only Phase 2?", "reference_answer": "A: EDBL has two training stages. The first stage trains a new model by Re-MKD and fine-tunes it with the balanced training. Because the data of the added classes are OOD, the KD training in the first stage is not a typical long tail KD training~(Long tail KD training refers to distillation with long tail data, LT-KD). Thus, the authors apply the typical RKD method to train a new model and use Re-MKD to improve knowledge transferring. After the authors obtain a new model, the second training stage becomes a typical long-tail KD training, and the authors attempt to fine-tune it by tackling LT-KD. Thus, the authors compute the IIB factor, the KD weighting factor, to re-weight the high-influenced samples in the second training stage. The authors further conducted experiments with only one training stage using IIB-KD. The results are shown in Tab. 4. From Tab. 4, the authors can find that directly using IIB-KD to train a new model performs worse than EDBL by a large margin.\nTable 4. Results of Re-MKD + CBF on CIFAR-100 with 5 phases in Base-0 protocol~(Average Accuracy on each incremental phase, %).\n        Dataset                 CIFAR-100\n    phase                       1     2       3     4      5\n    BiC                       84.8  74.02  66.7   61.5   56.5\n    BiC+Re-MKD                84.8  71.73  59.36  57.59  53.51\n    EEIL                      83.5  76.5   64.2   59.1   52.8\n    EEIL+Re-MKD               84.8  71.85  64.78  58.14  52.84\n    IIB-KD(One-stage)         83.5  69.47  60.3   53.15  48.7\n    IIB-KD(One-stage)+Re-MKD  84.8  76.7   70.93  65.73  60.51"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ba27b721-b028-5ba9-865e-2095b04f9808", "question": "Is the energy bounded in Ogbn-arxiv?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9b3f2a55-d06a-540f-ba13-7f25701fa1e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the energy bounded in Ogbn-arxiv?", "reference_answer": "A: Based on the default setting in Section A.4, the authors show the Dirichlet energies $E(X^{(k)})$ of $32$-layer EGNN here: [1.53, 1.61, 1.57, 1.53, 1.46, 1.42, 1.38, 1.33, 1.29, 1.26, 1.24, 1.23, 1.22, 1.21, 1.21, 1.20, 1.20, 1.19, 1.19, 1.19, 1.18, 1.17, 1.17, 1.16, 1.16, 1.14, 1.11, 1.09, 1.06, 0.99. 0.97, 1.09] * $10^6$. For each layer, as defined in the proposed Dirichlet energy constrained learning principle in Eq. (5), the upper limit is $E(X^{(0)}) = 1.74*10^6$, and the lower limit is $0.75 E(X^{(k-1)})$. It is observed EGNN is still bounded within the pre-defined ranges even with small $\\gamma$ in Ogbn-arxiv. \n\nAs illustrated in Figure 5 in the supplementary, EGNN is not sensitive to $\\gamma$ in Ogbn-arxiv. Once weight $W$ is properly initialized, it is regularized and updated at the vicinity of the sparse diagonal matrix. Even with small $\\gamma$, the eigenvalues of updated sparse $W$ will still be determined by the dominant diagonal values, which are close to the carefully initialized ones. Therefore, the Dirichlet energy is still bounded within the pre-defined limits. This also explains why the sparse diagonal weight works"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "91649a7a-8885-5a31-86c5-7f6e1557035a", "question": "Why are heterogeneous nodes and relations useful for this task?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6c8b72c2-b9e8-57dd-b0d3-f4e095ac803b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are heterogeneous nodes and relations useful for this task?", "reference_answer": "A: As shown in Figure 1(c), some drug traffickers always invent new tactics to evade detection. They rarely post drug descriptions or images on social media, but instead, they advertise drugs implicitly using slang and leave encrypted chat tool contact information through their comments to other users’ posts. Most of the existing works based on a single type of content feature (e.g., text or image) fail to detect drug traffickers on social media platforms. Therefore, the authors considered multi-modal features (e.g., both text and image) and the structural relationships among entities (e.g., users and posts). These foxy drug traffickers can be detected by considering the text as well as image and relationships among users and posts on social media platforms."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "51bf3fed-3df6-5e52-8159-d7c9cb25bb35", "question": "Why is it called “non-affection masks”? How does it alleviate confusion (L209)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb1f82ae-09df-5174-adbf-157ba87a8d01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is it called “non-affection masks”? How does it alleviate confusion (L209)?", "reference_answer": "A: The non-affection masks are obtained from the pseudo ground truths generated from the blind sampling strategy. The authors call them non-affection masks because they are used to mask out negative instructions from the base (or novel) class teacher model that cause the student model to wrongfully suppress novel (or base) classes from the input images as background. In other words, the authors want the teacher/student models to have no effect (i.e. no affection) on the backgrounds that contain relevant foreground information.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "08673b00-1125-54ff-9e9d-6100eb6a9d5d", "question": "In Table 4, does BLEU-5(F) denote only 5-gram precision, or is it the geometric mean of 1-5 gram overlaps?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["16903cd8-8452-50a4-b875-2e9b94e5bf87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 4, does BLEU-5(F) denote only 5-gram precision, or is it the geometric mean of 1-5 gram overlaps?", "reference_answer": "A: BLEU-5(F) denotes only 5-gram precision."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2a75fa15-4bbf-536d-897e-0d4544ef378b", "question": "What does \"responsibility minus people\" mean?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b4031cca-627d-5706-bfd4-94f394e50929"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does \"responsibility minus people\" mean?", "reference_answer": "A: A thresholded responsibility mask (capturing joint motion of hands and held objects) where pixels that are part of the Ternaus [21] person segmentation have been subtracted and set to 0.\nIglovikov, Vladimir, and Alexey Shvets. \"Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation.\" arXiv preprint arXiv:1801.05746 (2018)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "20ce4459-f0f5-5331-accf-2ad86715136e", "question": "Can you provide clarification of details and setup in Figure 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["48f9ec18-2cd0-51ff-ab26-65a64d18c681"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you provide clarification of details and setup in Figure 1?", "reference_answer": "A: Figure 1(a) is a conceptual model that illustrates the the proposed motivation, and the setup can be described as follows: Given an arbitrary state-action pair $x_0$, the model has two options of prediction, namely $s_1$ and $s_1'$. Under the old policy $\\pi_{\\text{old}}$, both options will lead the trajectory to enter regions with low value, hence $\\pi_{\\text{old}}$ is updated to $\\pi_{\\text{new}}$ to explore regions with potential high value.  Under the current policy $\\pi_{\\text{new}}$, predicting $s_1$ will result in a subsequent trajectory with significantly higher accumulative error than that of predicting $s_1'$. \n\nFigure 1(b) is an informal instance of Figure 1(a), where $x_0$ corresponds to the ant falling from the sky (executing action like adjusting the belt of the parachute), $s_1$ and $s_1'$ respectively corresponds to landing on the left/right side of the wall, and the arrows as well as the colored regions have the same meaning with the ones in Figure 1(a). \n\nthe authors have simplified Figure 1 in the introduction of the proposed revised version (Page 2). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "34ce700a-abec-5e17-971b-c9130847d168", "question": "Why is the analysis related to Figure 4 not very convincing?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the analysis related to Figure 4 not very convincing?", "reference_answer": "A: Because the same argument applied to \"pick-place\" while it is quite far in the latent space.\n\nA: Firstly, Figure 4 is the 2D PCA projection of the 10 compositional parameters $\\mathbf{w}$ for {reach, push, pick-place … } obtained by training PaCo on MT-10-rand, with ~85.4% success rate (Table 1). Actually, among all the 10 tasks, pick-place is a task that has a low success rate after training. Because of this, the $\\mathbf{w}_{\\rm pick\\-place}$ is not a representative $\\mathbf{w}$ for solving the pick-place task. The authors conjecture that this is the main reason why it is not close enough to other points with similar behavior patterns in terms of motion trajectories.\n\nA: Secondly, your question actually inspires us to think further about how to incorporate prior task similarity information (when available) into learning in a general way. In this case, for example, if the authors have the prior knowledge that \"pick-place\" is more similar to tasks such as window-open/window-close/door-open/drawer-open than some other task (e.g., button-press top-down), the authors may have an opportunity to guide the model to learn a better policy for \"pick-place\". Of course, this is the proposed current intuition only as the inter-skill similarities are typically unavailable in standard MTRL settings. Nevertheless, how to leverage the task similarities when provided or even learn it together is a very interesting direction, and the authors will explore it in future work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a9379039-652b-5013-b4d1-eaf822ba5f71", "question": "In Figure 5, it seem both the baseline WaNet and the proposed can pass the detection. What is the special advantage of the proposed method compared with WaNet then?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b761f966-dde6-5d75-9704-25545a8198a2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 5, it seem both the baseline WaNet and the proposed can pass the detection. What is the special advantage of the proposed method compared with WaNet then?", "reference_answer": "A: Figure 5 illustrates the performance against Neural Cleanse, which is a model-mitigation defense based on a pattern optimization approach at the input space as opposed to analyzing the latent space. Since the proposed objective is mainly to improve the stealthiness at the latent space while achieving similar performance from the aspects of attack success rate and stealthiness at the input space, the authors expect that WB and WaNet exhibit similar performance against Neural Cleanse. As the authors mentioned in the response above, from the latent space, WB is much more stealthy than prior works such that the proposed method can bypass the representative defenses used for analyzing the latent space, as the authors demonstrated in Section 5.3. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b85febf3-e883-578a-8c5a-d441d54f3259", "question": "Why do we need the theory part?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["52ef3784-d176-5ba3-9e33-ca99a2dd9b01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do we need the theory part?", "reference_answer": "A: As the authors discussed above, the intuition of the proposed MCQ algorithm comes from the theoretical analysis on the tabular MDP setting. The theoretical analysis provides basic insights and foundations for the proposed auxiliary loss. The authors always follow the practical application of the proposed MCB operator in the paper. For the initial version of the MCB operator, the authors cannot directly utilize it since it may be intractable to acquire the maximum over a continuous action space, and the behavior policy is often unknown. Then, the authors propose the practical MCB operator, where the authors fit an empirical behavior policy $\\hat{\\mu}$ and construct the pseudo target values based on it. The authors present theoretical analysis on the practical MCB operator in Proposition 4 and 5. Furthermore, the authors extend the practical MCB operator into the deep RL setting, and propose MCQ algorithm. In deep RL, it is challenging to figure out whether the learned policy will execute OOD actions. The authors therefore regularize all actions sampled from the learned policy. The authors deem that the whole logic of the proposed work is clear. The authors also note here that the authors actually *do not assume the prior knowledge* about the support of the in-distribution actions for the practical MCB operator (as the authors construct the pseudo target values based on the empirical behavior policy)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6738e688-c434-5639-95d8-3d45032adc34", "question": "How does the present study compare to other studies anlysing object-centric models? To what degree do your results confirm or reject previous results? E.g., Karazija et al. 2021 and Papa et al. 2022.\nKarazija, Laurynas, Iro Laina, and Christian Rupprecht. \"Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation.\" arXiv preprint arXiv:2111.10265 (2021).\nPapa, Samuele, Ole Winther, and Andrea Dittadi. \"Inductive Biases for Object-Centric Representations in the Presence of Complex Textures.\" arXiv preprint arXiv:2204.08479 (2022).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the present study compare to other studies anlysing object-centric models? To what degree do your results confirm or reject previous results? E.g., Karazija et al. 2021 and Papa et al. 2022.\nKarazija, Laurynas, Iro Laina, and Christian Rupprecht. \"Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation.\" arXiv preprint arXiv:2111.10265 (2021).\nPapa, Samuele, Ole Winther, and Andrea Dittadi. \"Inductive Biases for Object-Centric Representations in the Presence of Complex Textures.\" arXiv preprint arXiv:2204.08479 (2022).", "reference_answer": "A: _CLEVRTEX, Karazija et.al.,_: As a benchmark for unsupervised object segmentation, it shares similarities with the proposed work. Both conduct extensive experiments on the-state-of-art models on a set of benchmark datasets. However, CLEVRTEX focuses on the characteristics and comparison of different models. The proposed work, on the other hand, aims to quantify properties/inductive biases of synthetic and real-world datasets, and then discover what dataset factors incur the failure of existing models on challenging images. Notably, the authors employ real-world datasets instead of only complex synthetic datasets for systematically evaluation.\n\n_Inductive Biases.., Papa et.al.,_: This paper presents a very detailed study on the performance of MONet and SlotAtt on several synthetic datasets, so as to analyze architectural biases in the design of both models. The proposed work has similar findings on the inductive biases of MONet and SlotAtt, which can be found in the newly added Section 4.5. Apart from the study on biases of SOTA models with synthetic datasets, the proposed work also analyzes their failure on real-world datasets with extensive ablation experiments.\n\nIn summary, in the two relevant works, their experiments are still limited to synthetic images and they tend to characterize and analyze architectural designs of different models. By comparison, the proposed work benchmarks existing models on real-world datasets. Since all mentioned models fail on real-world datasets, architectural analysis is barely enough. Instead, the authors summarize and quantify inductive biases across different datasets. From the proposed experiments, the authors find that different models present different sensitivity to different dataset properties/biases, which also validates the findings of other study. More importantly, with the study of objectness biases in datasets, it is expected that better formulations of object-centric learning can be inspired in the future especially in the context of real-world applications."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8f099cd2-8299-53e1-83d6-bc312b420ea9", "question": "Why in Theorem 4 and Theorem 5, the generalization bounds depend linearly on the number of local solutions $K$? Is it from the proof technique?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["973b558e-915b-58ba-9d2a-9f46e25abc94"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why in Theorem 4 and Theorem 5, the generalization bounds depend linearly on the number of local solutions $K$? Is it from the proof technique?", "reference_answer": "A: Yes, the proposed generalization bound for non-convex problems has a linear dependence on $K$. This is indeed from the proof technique, as the authors upper bound the probability of the event “no extra local minima” as in Lemma 2. The term related $K$ in equation (17) is obtained from an application of Bonferroni’s inequality to equation (14). More details can be found in equation (111) in Appendix C.2.1. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0b0a3535-c95b-5fdf-b94b-cdeb54e9f205", "question": "Need clarification of memory aggregation for equation (3) (line 114) and experiments. Equation (3) shows that you are using most recent memory aggregation...which one are you really using?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["412b382c-197c-589c-864a-4e8a67c013e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Need clarification of memory aggregation for equation (3) (line 114) and experiments. Equation (3) shows that you are using most recent memory aggregation...which one are you really using?", "reference_answer": "A: Eq. (3) only shows the memory update for node $u$ after the event $(u, v, t)$, and the authors assume $u$ doesn't interact with any other nodes at $t$ (lines 112-113), eliminating the need for message aggregation. Given this assumption, the update in Eq. (3) doesn't imply most-recent message aggregation. However, since the authors assume that two events belong to the same batch only if they occur at the same timestamp, most-recent aggregation would result in ambiguity (multiple most-recent events). That is the reason why the authors consider mean message aggregation in the analysis section. In Table 1, models that make use of memory (i.e., PINT and TGN-Att) employ most-recent message aggregation in batches of size 200, following the original TGN paper. The authors will make this clear in the revised manuscript."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "22ed5771-9723-50df-8d4c-15c643c2cdf9", "question": "In line \"Then, we infer a configuration set X including the fragment state and the dihedral state\": What do the authors mean by state? Where can we find examples and figures to show what is Xf, Xd and Vf.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b27aca20-af64-5b2b-b51d-4b3babcca320"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In line \"Then, we infer a configuration set X including the fragment state and the dihedral state\": What do the authors mean by state? Where can we find examples and figures to show what is Xf, Xd and Vf.", "reference_answer": "A: The molecular conformation consists of the choice of 3D conformation（**fragment states**）of each fragment and the dihedral angles (**dihedral states**) between the fragments. Section 3 has more examples and figures to explain the symbols and convey the main ideas.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "773c74b1-f92b-5e70-9dd7-d98f4c193809", "question": "Why performance on Javascript is not as strong?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ba17e297-e254-519a-b329-6abf2c38dbe2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why performance on Javascript is not as strong?", "reference_answer": "A: The authors appreciate your concern about the exception of the Javascript dataset.\nFrom Table2, the authors can find that the benefit of encoding relative path for Javascript dataset is not as powerful as the performances in other datasets (while the improvement over vanilla Transformer is still significant, see Table3).\nTo investigate this phenomenon, the authors try to count the mean and variance of the length of all relative paths across four different datasets and get that:\n\n| Python | Mean | Var |  Ruby | Mean | Var |JS | Mean | Var| Go | Mean | Var|\n| ----- | ----- | ----- | ----- | ----- | -----|----- | ----- | -----| ----- | ----- | ----- |\n| Train |11.96 | 24.61| Train |9.29 | 17.62|Train |14.59 |41.46| Train |11.18 | 20.16|\n| Valid |11.81 |23.64| Valid |9.76 |18.84 | Valid |14.40| 40.02| Valid |11.56 | 22.40 |\n| Test |12.05| 25.36| Test |9.60 |18.82| Test |14.57| 40.59| Test |10.68| 18.83|\n\nIn this table, the authors can find that the path length of Javascript is not only longer but also has a significantly bigger variance than other datasets. So the authors assume that due to such property of the JS dataset, it is harder to extract useful structural information from the path than other datasets. In addition, the authors are also surprised to find that the length and variance of paths of the Ruby dataset are smaller than other datasets. Meanwhile, the gain of TPTrans over baselines on Ruby datasets are more significant than others(please refer to A3 for details). \n\nBut unfortunately, as to the outlier of path length and variance of JS languages, the authors have not yet reached an accurate conclusion. \nOne possible line of reasoning is that the inherent property of Js language results in a much complicated syntax tree. \nThe authors also suspect that this is due to the internal design of the language parser, and perhaps a better parser might alleviate this problem."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "164116b1-2be4-5174-8c47-07fcd037e641", "question": "How good are examples generated by the proposed technique in evading the recent class of adversarial example detection methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How good are examples generated by the proposed technique in evading the recent class of adversarial example detection methods?", "reference_answer": "A: The authors employ a recent detection method [7,8] to detect adversarial examples generated by different attack methods, e.g., FGSM, PGD, BIM, and ETF. All settings are the same as that used in the paper, and the results are reported in [TABLE 2-1]. The authors can see that ETF performs better than the baselines, i.e., having a high probability of evading detection methods.\n\nTABLE 2-1: Performance of adversarial detection against four attacks, metric to evaluate the detection performance can be found in [7,8].\n\n|  Mahalanobis[8]   |           |           |           |           |           |\n|---------------|-----------|-----------|-----------|-----------|-----------|\n| Method        | TNR       | AUROC     | DTACC     | AUIN      | AUOUT     |\n| BIM[9]           | 99.99%     | 99.99%     | 99.86%     | 99.86%     | 99.71%     |\n| FGSM[10]          | 98.89%     | 99.88%     | 98.89%     | 99.66%     | 99.24%     |\n| Deep*-PGD     | 97.22%     | 99.58%     | 97.92%     | 99.64%     | 99.05%     |\n| ETF           | **96.67%** | **98.73%** | **96.94%** | **98.75%** | **97.98%** |\n\n|   LID[7]  |           |           |           |           |           |\n|-----------|-----------|-----------|-----------|-----------|-----------|\n| Method    | TNR       | AUROC     | DTACC     | AUIN      | AUOUT     |\n| BIM[9]       | 99.99%     | **98.81%** | 98.33%     | 99.77%     | 99.33%     |\n| FGSM[10]      | 99.99%     | 99.99%     | 99.99%     | 99.72%     | 99.44%     |\n| Deep*-PGD | 99.99%     | 99.99%     | 99.99%     | 99.86%     | 99.72%     |\n| ETF       | **97.78%** | 99.58%     | **97.22%** | **99.51%** | **98.68%** |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5bb5765b-00ef-5f83-ba30-a42b1a60fa16", "question": "Can we utilize the base class centroids instead of the base class data? Since there exist many data for the base classes, we can easily compute the true class centroids for the base classes. Isn’t it more realistic to store the base class centroids, not the entire base class data? How does this affect performance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["48f84a36-4fdb-55ab-85d5-545e4cb455b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can we utilize the base class centroids instead of the base class data? Since there exist many data for the base classes, we can easily compute the true class centroids for the base classes. Isn’t it more realistic to store the base class centroids, not the entire base class data? How does this affect performance?", "reference_answer": "A: Since the proposed method works on the feature level, the authors need to store only the features of the base data (which are 640 dimensions in the proposed experiments) instead of the entire training data. For example, all training features of miniImagenet require only 98M storage space, which is acceptable compared to the overall overhead. \nMoreover, it is possible to access only a small subset of the training features. The authors randomly sample a small ratio(e.g., $0.01$) of features in each base class with $0.01$ storage space(0.98M), the improvements over baseline methods are still significant, as seen in the table above. Especially, when keeping one proto-type for each base class like DC, the proposed method has a small drop over using the whole training features."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "04f64b50-3540-5b79-bce0-a9d84345c11d", "question": "What are the results of CyCLIP on ImageNet-A/R/V2/Sketch datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results of CyCLIP on ImageNet-A/R/V2/Sketch datasets?", "reference_answer": "A: CyCLIP outperforms CLIP on all these datasets on zero-shot evaluation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2ce8cd5d-cac1-5080-a7c9-1ff885fb3df5", "question": "Would this approach also be applicable to NP-hard decision problems such as SAT?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["01ec1818-9fe6-516b-ab44-46377c67a0cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would this approach also be applicable to NP-hard decision problems such as SAT?", "reference_answer": "A: In principle, even in the absence of an objective function as in SAT, there may still be many feasible solutions (symmetries) per problem instance, of which only one is chosen by the solver. So a similar challenge exists here. \nThe authors believe that the proposed results would apply to SAT encodings of feasibility problems that need to be solved repeatedly for classes of inputs. For instance, it could be very useful for timetabling problems that are encoded as SAT. However, this setting presents an additional challenge in that the SAT formulation encodes the input as well and the definition of similar instances is more complex in this case. This represents an interesting challenge and the authors hope this work may enable the study of this and similar questions, paving the way to further enhanced ML methods for solving difficult optimization problems."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "91463295-4b80-5e7a-ae41-f640144e7307", "question": "How do rendering speeds compare to other frameworks like AI2Thor, iGibson, Gibson, Habitat, Habitat-2.0, etc?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["604b5dbe-a5d2-52ec-a301-aa7a4215c44b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do rendering speeds compare to other frameworks like AI2Thor, iGibson, Gibson, Habitat, Habitat-2.0, etc?", "reference_answer": "A: ProcTHOR is built within AI2-THOR and is identical in speed to AI2-THOR. The only complication here is that ProcTHOR houses can vary significantly in size and, as shown in Table 1, larger houses generally result in lower FPS. The iTHOR scenes from AI2-THOR are all one-room houses and are approximately equivalent to the \"Small\" houses from Table 1.\n\nRegarding other comparisons, this is a great question and is surprisingly challenging to answer for several reasons:\n\nDifferent simulators support different agents, each with their own action spaces and capabilities, with little standardization across simulators. AI2-THOR, and thus ProcTHOR as well, supports three different agent types: \"high-level\", \"locobot\", and \"arm\". The \"arm\" agent is often slower to simulate than the navigation-only \"locobot\" agent as it is more complex to physically model a 6 DoF arm as it interacts with objects. This is made even more complex when noting that random action sampling, the simplest policy with which to benchmark, is a poor profiling strategy as some actions are only computationally expensive in rare, but important, settings; for instance, computing arm movements is most expensive when the arm is interacting with many objects, these interactions are rare when randomly sampling but we'd expect them to dominate when using a well-trained agent.\n\nSome simulators are relatively slow when run on a single process but can be easily parallelized with many processes running on a single GPU, e.g. AI2-THOR. Thus single-process simulation speeds may be highly deceptive as they do not capture the ease of scalability.\n\nWhen training agents via reinforcement learning, there are a large number of factors that bottleneck training speed and so the value of raw simulator speed is substantially reduced. These factors include:\n\nModel forward pass when computing agent rollouts.\nModel backward pass when computing gradients for RL losses.\nEnvironment resets - for many simulators (e.g. ProcTHOR, Habitat) it is orders of magnitude more expensive to change a scene than it is to take a single agent step. This can be extremely problematic when using synchronous RL algorithms as all simulators will need to wait for a single simulator when that simulator is resetting. When training this means that, in practice, important \"tricks\" are employed to ensure that scene changes are infrequent or synchronized, without these tricks, performance may be dramatically lower.\nTo attempt to control for the above factors, the authors set up two profiling experiments, one in Habitat HM3D and one using ProcTHOR-10K, where we:\n\nUse a 2-GPU machine (GeForce RTX 2080 GPUs) where GPU-0 is reserved for the agent's actor-critic policy network and GPU-1 is reserved for simulator instances.\n\nTrain agents for the ObjectNav task (using the same LoCoBot agent with the same action space).\n\nFor both agents, use the same actor-critic policy network, the same referenced in the paper.\n\nRemove the \"End\" action so that agents always take the maximum 500 steps, this minimizes dependence on the learned policy.\n\nUse a rollout length of 128 with the same set of training hyperparameters across both models.\n\nUse a total of 28 parallel simulator processes, this approximately saturates GPU-1 memory. The authors found that Habitat instances used slightly less GPU memory than ProcTHOR instances and so the authors could likely increase the number instances for Habitat slightly, but the authors kept these equal for more direct comparison.\n\nUse a scene update \"trick\" which forces all simulators to advance to the next scene in a synchronous fashion after every 10 rollouts (e.g. after every 10 x 128 x 28 = 35,840 total steps across all simulators).\n\nThe authors ran the above profiling experiments for ~1M steps and the authors found that training with Habitat resulted in FPS ranging between 119.7-264.3 (230.5 average) and training with ProcTHOR resulted in FPS ranging between 145.5-179.4 (167.7 average). Training in ProcTHOR is thus slower than in Habitat but, for the above set up, this difference is around 1.4x rather than what the difference in single process rendering speed would suggest. While the authors did not have the time to profile Gibson, iGibson, or Habitat-2.0 in this rebuttal period, these simulators are generally stated to have single-process rendering speeds between AI2-THOR and Habitat and so the authors expect their FPS numbers between the two above ranges."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5244fa5d-461e-52a1-a36e-e2b5339a32da", "question": "How large is the validation set used in Task-agnostic search (Line 206)? It is quite surprising to me that evaluating the 256 candidate subnetworks on this set (or on MNLI) takes less than one hour.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How large is the validation set used in Task-agnostic search (Line 206)? It is quite surprising to me that evaluating the 256 candidate subnetworks on this set (or on MNLI) takes less than one hour.", "reference_answer": "A: (i) The validation set contains $300K$ instances. The authors use $128$ as the sequence length and batch size. (ii) Table 3 reports the search cost for Small-sized models (line 282) from {AutoDistil} and AutoTinyBERT. Note that this step does not require any training for {AutoDistil}. The authors compute only the self-attention relation loss for all the $256$ student subnetworks ($5x-22x$ speedups for Small-sized models) using Equation 4 with the teacher relations computed only once. The authors use Equation 6 to select the subnetwork with desired trade-off with deterministic computation of the FLOPs. The algebraic expression to compute FLOPs as a function of layers, heads, hidden size etc. is included in the submitted source code. In contrast, AutoTinyBERT performs task-specific search which requires fine-tuning the subnetworks on the task (e.g., MNLI) thereby increasing the search cost."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0585933b-a921-51d3-9fca-96e205b10ddd", "question": "Why aren't the bounds empirically studied?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["97d7c10d-2bbc-589a-8fd0-e7af45e9cc08"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why aren't the bounds empirically studied?", "reference_answer": "A: Empirical estimation of the bound is generally very computationally expensive for large models without a closed form (e.g. the expected $\\log Z$ term for classification tasks, the $\\Delta_\\lambda$ term) and subject to high variance with MCMC approximations. Therefore, almost no existing PAC-Bayes papers on meta-learning (e.g. [3, 18, 22]) that the authors know of empirically evaluate their bounds. However, compared to the previous papers, the authors do empirically study the bounds over the synthetic Sinusoid regression task by using Gaussian processes which do provide closed-form formulas (see D.1) and empirically validate the advantage of the proposed new bound in Thm 4."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f4f7be60-c9e3-5f29-8ec7-f30aac617612", "question": "Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e46ab308-c360-57b7-b04c-cc84c9fce9cc"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?", "reference_answer": "A: The complexity is linear in the number of classes, since classes are processed independently. Furthermore, text filtering is applied before cleaning, which reduces the number of images to be considered for a given class. Please also see the response R1 to reviewer1."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "12910a7b-8cb5-54d2-a5bf-497a08700c8a", "question": "The descriptions of the SPCL leaves out a few important details and intuitions. In line 144 and even in the appendix, the *a* vector is not clearly defined. The update of lambda in algorithm 1 should be more detailed. When is lambda \"small\"? and what is the formulation for updating the lambda? This sentence seems to be copied and pasted from the SPCL's original paper. In this paper, you need to provide sufficient detail so that other researchers can reproduce the results.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["122ecbbc-76f5-525c-a3fa-1ea47770e1a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The descriptions of the SPCL leaves out a few important details and intuitions. In line 144 and even in the appendix, the *a* vector is not clearly defined. The update of lambda in algorithm 1 should be more detailed. When is lambda \"small\"? and what is the formulation for updating the lambda? This sentence seems to be copied and pasted from the SPCL's original paper. In this paper, you need to provide sufficient detail so that other researchers can reproduce the results.", "reference_answer": "A: Actually, a vector is simply a n-dim vector that parameterizes a linear space. With regard to the update of lambda, in the proposed implementation it is initialized as a small number, i.e. 2 for follower and envdrop and 4 for self-monitor. Lambda will be updated by stepsize miu when it is lower than then the maximum loss for a single sample, otherwise it is updated by half of miu. The value of miu is different for different models, in the proposed implementation it is 2 for monitor and envdrop and 3 for follower. For lambda, the \"small\" or \"large\" is decided by the comparison with 25%, 50% and 75% quantile of loss. The authors say lambda is small if it is lower than the 25% quantile."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c7f386ce-b55c-5f8e-af99-d5d1440b8f08", "question": "In experiments, the authors prepared four buckets to reduce computational time in decoding. However, the inference time of a single bucket shown in Figure 2 seems practical. How did the authors decide on the bucket size in the experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2da7a4ef-eeb3-5eeb-8e54-0c08ce0556a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In experiments, the authors prepared four buckets to reduce computational time in decoding. However, the inference time of a single bucket shown in Figure 2 seems practical. How did the authors decide on the bucket size in the experiments?", "reference_answer": "A: The authors select a bucket size of 4 because the inference time drops significantly before this size while the ROUGE scores don't vary much. \nCompared with the single bucket, a bucket size of 4 decreases the latency by at least 20% (from over 0.03 to 0.024) but does not change the ROUGE score much (slightly over 20 for both settings). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "32dc288c-b59a-5ebc-bd1b-94016938a734", "question": "Although the paper claims that it is under the multimodality setting, it is strange that it misses the image modality information in the metric-based model. Please justify this. Also, how does the proposed model perform on other state-of-the-art image caption models (see list below)?\nReferences:\n[r1] Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech. ICASSP, 2021.\n[r2] The LJ Speech Dataset. https://keithito.com/LJ-Speech-Dataset/, 2017\n[r3] LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Interspeech, 2019\n[r4] RefineCap: Concept-Aware Refinement for Image Captioning. CoRR, 2021.\n[r5] Reflective Decoding Network for Image Captioning. ICCV, 2019.\n[r6] X-Linear Attention Networks for Image Captioning. CVPR, 2020.\n[r7] ClipCap: CLIP Prefix for Image Captioning. CoRR, 2020.\n[r8] Show and Tell: A Neural Image Caption Generator. CVPR, 2015.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["617d7416-f163-5ae1-a9e8-dbcf27170aa5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Although the paper claims that it is under the multimodality setting, it is strange that it misses the image modality information in the metric-based model. Please justify this. Also, how does the proposed model perform on other state-of-the-art image caption models (see list below)?\nReferences:\n[r1] Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech. ICASSP, 2021.\n[r2] The LJ Speech Dataset. https://keithito.com/LJ-Speech-Dataset/, 2017\n[r3] LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Interspeech, 2019\n[r4] RefineCap: Concept-Aware Refinement for Image Captioning. CoRR, 2021.\n[r5] Reflective Decoding Network for Image Captioning. ICCV, 2019.\n[r6] X-Linear Attention Networks for Image Captioning. CVPR, 2020.\n[r7] ClipCap: CLIP Prefix for Image Captioning. CoRR, 2020.\n[r8] Show and Tell: A Neural Image Caption Generator. CVPR, 2015.", "reference_answer": "A: The authors have evaluated their metric-based attack and feature-based attack on FastSpeech2 [r1], which is a SOTA text-to-speech (TTS) application that takes text as input and speech/audio (Mel spectrogram) as output. They randomly pick 3,000 samples from its training dataset, LJSpeech [r2], as members and 3,000 samples from another dataset, LibriTTS [r3],  as non-member samples. They use all 6,000 samples to train the multimodal feature extractor in the feature-based method. The experimental results show that the metric-based attack achieves an 86.43% success rate and the feature-based attack achieves 94.24%. They have considered SOTA image captioning models, such as RefineCap [r4] and RDN [r5]. As two studies [r4, r5] would be very time-consuming to implement without publicly available code and two works [r6, r7] are difficult to reproduce due to computing resources, the authors chose to evaluate the proposed attack on the classic encoder-decoder image captioning model [r8]. \nReferences:\n[r1] Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech. ICASSP, 2021.\n[r2] The LJ Speech Dataset. https://keithito.com/LJ-Speech-Dataset/, 2017\n[r3] LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Interspeech, 2019\n[r4] RefineCap: Concept-Aware Refinement for Image Captioning. CoRR, 2021.\n[r5] Reflective Decoding Network for Image Captioning. ICCV, 2019.\n[r6] X-Linear Attention Networks for Image Captioning. CVPR, 2020.\n[r7] ClipCap: CLIP Prefix for Image Captioning. CoRR, 2020.\n[r8] Show and Tell: A Neural Image Caption Generator. CVPR, 2015."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a9a8b70c-cf2f-5416-8164-788c13d92262", "question": "In equation 1, for the loss on the negative augmented training samples, is it $L_{neg}(B, \\tilde(B); \\theta)$ or $L_{neg}(\\tilde(B); \\theta)$? Do you use the clean dataset twice in the training? If so, what is the reason for doing so?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["13b7a9cd-5011-548f-8c52-2c24218ee25a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In equation 1, for the loss on the negative augmented training samples, is it $L_{neg}(B, \\tilde(B); \\theta)$ or $L_{neg}(\\tilde(B); \\theta)$? Do you use the clean dataset twice in the training? If so, what is the reason for doing so?", "reference_answer": "A: For uniform loss, it is $L_{neg}(\\tilde(B); \\theta)$ since the authors only need to apply uniform labels for negative examples. However, for L2 loss, the authors need the clean data information since the authors need to encourage the prediction between clean and negative examples to be far away. Therefore, the notation should be $L_{neg}(B, \\tilde(B); \\theta)$. The authors do not use the clean dataset twice and will make it more clear in the revised version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4f0207ae-b435-5d8d-bbaf-4f5c809ccfba", "question": "What are the key takeaways from the ablation study reported in the supplementary material Sec.O.1 Table 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["5809db97-e501-54dc-b37f-05d442498011"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the key takeaways from the ablation study reported in the supplementary material Sec.O.1 Table 1?", "reference_answer": "A: The authors show the comparison results on two representative tasks above, including the image/text retrieval task on Flickr30K, and the visual question answering task in VQA. Several observations can be made from the ablation: \n\n(1) All three training objectives (E2E, E2R, G2E) contribute to improving the model performance. Training the model without any single objective leads to inferior performances on downstream tasks. The authors argue that the E2E, E2R, and G2E loss promote the model from different perspectives by focusing on **semantic understanding of concepts**, **complicated relations between entities**, and **structural information.** Therefore, all three objectives are necessary for the framework and contribute to the improvement respectively.\n\n(2) By comparing the first and second row, the authors can see that simply training the CLIP model with extra time and data fails to improve the generalization performance. It also demonstrates that the improvements mainly come from the injected knowledge information rather than the continuous learning scheme."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8fe13777-8849-5e54-b7f7-372383bd4202", "question": "Figure 4: what is the takeaway message from the right figure?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["54be9a09-b764-596d-ac07-57ee007c5fdd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 4: what is the takeaway message from the right figure?", "reference_answer": "A: The right figure of Fig.4 shows two main observations: (i) that the initial point obtained using a “rank 2” or random initialization can be close to spurious and non-attractive local minima, which may trigger the stopping criterion too early and prevent the algorithm from continuing to run in order to converge towards an attractive and well behaved local minimum. (ii) When initialiazing the algorithm using kmeans methods, the authors show that the proposed stopping criterion is a decreasing function of time meaning that the algorithm converges directly towards the desired solution."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b17614c5-a92a-5452-b74f-0b8848084ea4", "question": "Unsupported claim “in experiments, we do find successful extensions from existing skill set to a new skill when the skills are similar” in appendix section 3.2.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Unsupported claim “in experiments, we do find successful extensions from existing skill set to a new skill when the skills are similar” in appendix section 3.2.", "reference_answer": "A: According to your comment, the authors now provide below some preliminary but encouraging results on the effectiveness of PaCo for continual learning without forgetting, by fixing already learnt $\\mathbf{\\Phi}$ and learning only a new $\\mathbf{w}$ vector for the continual task. For a parameter set (K=2) trained on a set of Trained-Tasks, the authors can obtain policy for a new task by reusing the same $\\mathbf{\\Phi}$ and only learning a new $\\mathbf{w}$ vector, with reasonable performance, as shown in the table below.\n\n| Trained Tasks | New Task | Success Rate on New Task (%) |\n|:---------------------------------|:--------------------:|:------:|\n| {reach, door-open, drawer-open} | drawer-close | 75 ± 9 |\n| {window-open, window-close, door-open} | door-close | 90 ± 5 |\n\nA: While encouraging, it is important to note that these are some initial, non-extensive results on some possible future extensions, and is not an essential part of the main theme of this paper, which is on the parameter-compositional MTRL method itself. Therefore, the authors will include them in the appendix in revision to avoid the distraction it might bring to the main theme of the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "05aeaeb7-c948-5456-bb13-9fd256bdc0a3", "question": "What is the empirical effect of initialization (proposed in Section 3.2.2.)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7f09e532-bc3b-56a1-96ac-6dc6c9cf7072"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the empirical effect of initialization (proposed in Section 3.2.2.)?", "reference_answer": "A: The results are in Table 5 on page 8. According to Table 5, the proposed initialization significantly outperformance random initialization."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "db618866-fc31-52d8-978e-0c8dcbc656fc", "question": "In Table 2 PNAT w/HSP seems to have amazing performance compared to other models. Could you shed some light on why this cannot be used directly? Is it because of delays due to the iterative process in extracting z_i?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2a7b14b6-33c1-505a-8a3a-d9580ad76ca7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 2 PNAT w/HSP seems to have amazing performance compared to other models. Could you shed some light on why this cannot be used directly? Is it because of delays due to the iterative process in extracting z_i?", "reference_answer": "A: HSP stands for the reference position sequence the authors used for training, it needs to take the decoder inputs and the reference as the input. The authors conduct this experiment to verify the effectiveness of the heuristics searched position. \nPNAT w/HSP indicates the oracle performance in the current setting. It is naturally cannot be used at the inference stage because it takes the reference as the input. Not due to the iterative process in extracting z_i."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b75f79b1-aa83-57bf-8d3f-ee9531937cb4", "question": "Are non-robust features the only reason for the experiment result in Table 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3c9f1151-6d66-5834-90fe-e11bbcc7e6ab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are non-robust features the only reason for the experiment result in Table 2?", "reference_answer": "A: No, the trade-off between the standard accuracy and the robust accuracy can be naturally attributed to the presence of non-robust features in the original dataset."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "41463730-7a3c-583b-a0e8-7b66a94cc252", "question": "Does PINT still fail in differing properties mentioned in Proposition 7, even the relative positional features are added?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["412b382c-197c-589c-864a-4e8a67c013e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does PINT still fail in differing properties mentioned in Proposition 7, even the relative positional features are added?", "reference_answer": "A: This is an interesting question. In fact, PINT can distinguish the constructions the authors provide in Proposition 7 (Figure 3). To see this, it suffices to note that the TCTs for, e.g., $u_1$ and $u_1^\\prime$ are no longer isomorphic when positional features are added. Consider a 2-layer model, then $r_{ w_1 \\rightarrow u_1} = [0, 1, 1]$ while $r_{w^{\\prime}_1 \\rightarrow u^{\\prime}_1} = [0, 1, 0]$, with $w_1$ and $w_1^\\prime$ as denoted in Figure 3. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0bec5b86-b11c-5139-8c20-2a4aa6300ef3", "question": "Table 1 (b): \"Exploring Diverse Expressions for Paraphrase Generation\" does not report iBLEU4, but the authors cite the number. What is this number?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Table 1 (b): \"Exploring Diverse Expressions for Paraphrase Generation\" does not report iBLEU4, but the authors cite the number. What is this number?", "reference_answer": "A: It is the relationship of these metrics is iBLEU = (1-alpha) BLEU - alpha SBLEU. Thus, the authors are able to calculate iBLEU themselves, given the reported BLEU and SLBEU. (alpha = 0.1 in their setting.)"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7fdee674-71fd-517c-a5a5-80952229bc95", "question": "Compare the runtime improvement over MSG. In Table 2 in the Supplementary Material, there is reference for only one runtime, is it for the authors' algorithm, MSG ? Is there an explanation for why RSG+ underperforms when the top k eigenvalues are dominated by the top l eigenvalues ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3c233311-ee9d-500d-912e-85eac20c42fa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare the runtime improvement over MSG. In Table 2 in the Supplementary Material, there is reference for only one runtime, is it for the authors' algorithm, MSG ? Is there an explanation for why RSG+ underperforms when the top k eigenvalues are dominated by the top l eigenvalues ?", "reference_answer": "A: Here is the table which includes the runtime of RSG+ and MSG :\n\n|            |       MNIST       | MEDIAMILL        | CIFAR                |  \n|------------|-------------------|-------------------|----------------------|\n| Time(s)    | k=1 ; k=2 ; k=4   | k=1 ; k=2 ; k=4   | k=1   ; k=2  ; k=4   | \n| RSG+(Ours) | 4.16; 4.24; 4.71  | 1.89; 1.60; 1.44  | 14.80 ;17.22 ;22.10  | \n| MSG        | 35.32;42.90;49.17 | 11.59;14.21;17.34 | 80.21 ;100.80;106.55 | \n\nTable 2 in the supplement is the runtime of [1] (not MSG), which is an earlier work on computing CCA with $d^3$ complexity. Thus the authors also compare the runtime with [1].\n\n[1] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical correlation analysis. In Proceedings of the 26th annual international conference on machine learning, pages 129–136, 2009. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dd751ecc-11b8-5457-b2a5-dd2e2ef043c9", "question": "With the projection of the gradient for the non-robust classifier, how hard is the training?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["695d1dcb-2e5c-57e5-bc1a-f8e84a82ad6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "With the projection of the gradient for the non-robust classifier, how hard is the training?", "reference_answer": "A: The authors might misunderstand the question, but there is no additional training necessary. The authors use the classifier as it is. The robust model is trained with standard adversarial training and the target classifier can be trained in any way and does not require additional training/fine-tuning. The diffusion model also remains unchanged from the original formulation. Thus, the cone-projection only appears in the final DVCE sampling and not during model training."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9306dfb7-ac58-50e7-a1a0-48cd8f87538c", "question": "“in Sec 3.2, they design a set of losses, but in the experiment, I can not find the abolition study of those losses. They only show the final model.”", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "“in Sec 3.2, they design a set of losses, but in the experiment, I can not find the abolition study of those losses. They only show the final model.”", "reference_answer": "A: The authors would like to clarify that LSP requires only one additional loss, which was defined in Equation 2. This is because LSP performed the assignment in the latent space $\\mathbb{R}^c$ that is naturally associated with the Euclidean distance. Hence, unlike other methods that require the selection of a distance metric and loss function for the assignment, LSP does not require such extra decisions to be made. The main parameters of LSP are $\\beta$, the weight for the latent loss, and $d$  in GCR.\n\nIn the proposed preliminary experiment on the CLEVR task, $\\beta$ was quite robust and setting it to 0.1 yielded good performances across datasets. The importance of $d$ in GCR was shown in an ablation study. Please note that $L_\\text{latent}$ is crucial to the convergence of LSP. Without it, LSP is not even theoretically guaranteed to converge.\n\n---"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "832613bc-11dd-55cf-83c9-c98f7058f25d", "question": "Where are the quantitative metrics reported in the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9e06e7ed-2aaf-54c1-9fa8-d5bdc49ddf6f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where are the quantitative metrics reported in the paper?", "reference_answer": "A: Authors report the density plots of all models in the appendix in Figures 9 and 10. There one can see that almost all models show similar calibrations except for two models which are described from line 199 to line 204 in the manuscript (202 to 205 in the revised manuscript). The ECE for the different models are reported in the appendix Figure 12 and Figure 13. Due to the amount of models they only reported the values without each specific name of the model. Figure 8 where they show the Precision-Recall Curve for ImageNet, the equivalent ROC curve is reported in the appendix Figure 22. Further, they report the Precision-Recall curves for CIFAR10 and CIFAR100 in the appendix Figure 14 and Figure 15. They have an evaluation on the improved downsampling and activation by inspecting the ROC curves and AUC values for these models and their comparable models in detail in Figure 20 and Table 3 in the appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "575e2554-480e-5888-ba36-c0584df06983", "question": "If classification performance is the goal then why not use simple ML models like SVM, LR on FC matrices computed via PCC? With hand-crafted features (FC matrices), ML models give better classification performance.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "If classification performance is the goal then why not use simple ML models like SVM, LR on FC matrices computed via PCC? With hand-crafted features (FC matrices), ML models give better classification performance.", "reference_answer": "A: Classification performance is not the only goal in the proposed work, as can be observed from the proposed interpretation results such as in Sec 4.4. However, following your advice, the authors have also included simple ML models such as logistic regression and SVM with the best hyper-parameters obtained through grid search on the proposed two datasets ([Code](https://anonymous.4open.science/r/BrainTransformer/baselines/lr_svm_baseline.py)). From the table below, the authors can see in both ABIDE and ABCD, that simple ML models like SVM and LR on FC matrices computed via PCC do not give better classification performance as assumed. \n\nBesides, exploring neural network models is meaningful since many SOTA technologies can be applied, like transfer learning or meta learning, which is meaningful for situations like lacking samples and labels in brain network analysis.\n\n| Method | Dataset |     AUC     |     ACC      | \n| :----: | :-----: | :---------: | :----------: | \n|   LR   |  ABIDE  | 75.97±1.09 | 68.52±1.11  | \n|  SVM   |  ABIDE  | 75.07±4.15 | 69.90±3.11  | \n|   LR   |  ABCD   | 94.06±0.26 | 87.057±0.36 | \n|  SVM   |  ABCD   | 93.52±0.54 | 86.98±0.43 | "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1402dbfb-392b-5680-9a13-8123c1133e77", "question": "Is there token-level alignment between different modals?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there token-level alignment between different modals?", "reference_answer": "A: No, the proposed learning algorithm does not require or assume such alignment.  Batches for text and images (or audio) are randomly chosen at each step. The authors do not do further alignment for text and other modals in each training step."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8859ff6d-6c21-55e5-a3c8-dbf725b313ef", "question": "What is the scope of domains for the proposed causal graph in Figure 1? Is it applicable to weakly-supervised image classification problems? Discussing some practical problems for which this causal graph is suitable would be preferable.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f4d55ffd-f72a-5334-aee2-18b43fb7bcfc"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the scope of domains for the proposed causal graph in Figure 1? Is it applicable to weakly-supervised image classification problems? Discussing some practical problems for which this causal graph is suitable would be preferable.", "reference_answer": "A: The causal graph in Figure 1 is suitable for a wide range of weakly supervised tasks where *the bag labels are determined by the labels of their instances*, such as sound event detection, object detection, and medical image analysis. For example, in histopathology medical image analysis, a whole-slide image is represented by a bag, and the cells are represented by instances. Supervision is only available at the image level, while whether a patch is cancerous or normal is unknown; however, patch level predictions are crucial for interpretability in medical applications. TargetedMIL is suitable because it accurately predicts instance labels by identifying the underlying causal factors of the cancerous cells."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c2ebca76-9c5b-5974-8091-b1cafd69b490", "question": "Are the re-evaluations in Table 1 done using the emph{exact} same architecture used to get the FRePo numbers? (i.e., is the same normalization type being used?) If not, how do the other algorithms perform using BatchNorm?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the re-evaluations in Table 1 done using the emph{exact} same architecture used to get the FRePo numbers? (i.e., is the same normalization type being used?) If not, how do the other algorithms perform using BatchNorm?", "reference_answer": "A: Yes, the authors indeed made an optimistic estimation of the previous methods. As the authors mention in Appendix A.1, the authors run four settings for previous methods, namely {original data preprocessing, FRePo ZCA processing} x {DCConv, FRePo Conv (wider, BatchNorm)} and the authors pick the setting that turns out to be the best for the previous methods. Moreover, the authors report the original paper's performance if it is better than the proposed reproducing results. In the proposed experiments, the authors observed that FRePo ZCA processing could yield better performance, but FRePo Conv does not seem to help and yields a worse performance. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e342537-8046-523a-9d26-8ff646d6f493", "question": "I'd like to see a baseline where each task has its own policy (no parameter sharing) and another baseline where the size of the parameter set (K) is the same as the number of tasks. In the K=10 case for MT10, for example, does each task essentially claim its own item in the parameter set and there is limited parameter sharing?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I'd like to see a baseline where each task has its own policy (no parameter sharing) and another baseline where the size of the parameter set (K) is the same as the number of tasks. In the K=10 case for MT10, for example, does each task essentially claim its own item in the parameter set and there is limited parameter sharing?", "reference_answer": "A: The authors have now added this baseline (Single-Task SAC). Single-Task SAC achieves 61.9% average success rate on MT10-rand under the standard setting (2M environment step/task). For more details on Single-task SAC, please refer to the reply to “**Common question on Single-Task SAC**\" above.\n\nA: K can be intuitively understood as a hyper-parameter for adjusting the strength of parameter sharing. Different values have different impacts on parameter-sharing and sample efficiency.\n- Smaller K (much smaller than task number T) will have a stronger enforcement on parameter sharing. It can be observed that with a too strong parameter sharing setting (e.g. K=3 in Table 3a) will limit its performance due to its over-constrained policy parameter space representable by $\\mathbf{\\Phi} \\mathbf{w}$.\n\n- Larger K (e.g. comparable to task number T) offers a larger representation space, but at the same time enforces less on parameter sharing, which will decrease sample efficiency (i.e., learns slower)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c6dd90b3-604c-5d34-a9bc-f1f24fe30f0e", "question": "I am wondering the opinions from the authors on the potential synergy between ClimbQ and mixed-precision quantization. From my perspective, ClimbQ can be potentially combined with mixed-precision quantization since the imbalanced class distribution can also make the range of the value representation divergent. Do you think it's feasible to potentially combine these two lines of works?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ad1b4108-8f9b-5cf4-a905-ceaa86a2a5ab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am wondering the opinions from the authors on the potential synergy between ClimbQ and mixed-precision quantization. From my perspective, ClimbQ can be potentially combined with mixed-precision quantization since the imbalanced class distribution can also make the range of the value representation divergent. Do you think it's feasible to potentially combine these two lines of works?", "reference_answer": "A: Yes, the authors also consider that the mixed-precision quantization may be applicable to the imbalanced class distributions with different ranges. The classes with larger ranges can be assigned with more bits (i.e., using more quantized values), and the classes with smaller ranges can be assigned with fewer bits (i.e., using fewer quantized values) to effectively reduce the quantization errors $|x - Q(x)|$ and avoid a significant performance degradation according to [1]. In addition to the range, the authors also think that it may also be feasible to utilize other metrics such as the Hessian matrix and eignenvalues [2, 3] to measure the contained information in separate class distributions for the decision of the assignment of bits.\n\n--Reference\n- [1] Rastegari, M., Ordonez, V., Redmon, J., & Farhadi, A. (2016, October). Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision (pp. 525-542). Springer, Cham.\n- [2] Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., ... & Keutzer, K. (2020, April). Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 05, pp. 8815-8821).\n- [3] Wang, K., Liu, Z., Lin, Y., Lin, J., & Han, S. (2019). Haq: Hardware-aware automated quantization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8612-8620)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "897d8482-989c-5045-8a75-656a810a998a", "question": "This paper only provides the experiments without pre-trained weights. I am curious about the results of the pre-trained weights with the ImageNet dataset. With this setting, can this algorithm still achieve a promising result?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["b0d70329-7bd8-5c32-81c7-5743ec00d545"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "This paper only provides the experiments without pre-trained weights. I am curious about the results of the pre-trained weights with the ImageNet dataset. With this setting, can this algorithm still achieve a promising result?", "reference_answer": "A: Because the backbone of the proposed model synchronously learns spatio-temporal features, it is a model of video understanding type. The authors can only use video datasets to train it. The authors report the performance of the proposed model after pretraining on large-scale Kinetics-400 dataset. Similarly, all models here use 8 frames as input. Through pretraining on Kinetics dataset, all models are compared under a fair pretraining condition. After that, the few-shot dataset UCF101 is used for fine-tuning and testing, and the results are shown in the following table. As shown in Table 12, the proposed model still has the best performance after pretraining on large-scale dataset. With only 1/3 FLOPs, the proposed method can surpass TRX by 2.5% and 10.8% on the UCF101 and HMDB51 dataset, respectively.\n\n|     Method             |     Pretraining     |             |     UCF101    |               |             |     HMDB51    |               |\n|------------------------|---------------------|-------------|---------------|---------------|-------------|---------------|---------------|\n|                        |                     |     Acc     |     Params    |     FLOPs     |     Acc     |     Params    |     FLOPs     |\n|     TimeSformer [2]    |     -               |     63.0    |     40.7M     |     73.35G    |     41.7    |     40.7M     |     73.35G    |\n|     TimeSformer [2]    |     Kinetics-400    |     80.5    |     40.7M     |     73.35G    |     54.2    |     40.7M     |     73.35G    |\n|     TRX [25]           |     -               |     67.0    |     25.6M     |     41.43G    |     46.4    |     25.6M     |     41.43G    |\n|     TRX [25]           |     Kinetics-400    |     85.1    |     25.6M     |     41.43G    |     60.7    |     25.6M     |     41.43G    |\n|     Ours               |     -               |     69.7    |     8.84M     |     13.76G    |     60.4    |     8.91M     |     13.65G    |\n|     Ours               |     Kinetics-400    |     87.6    |     8.73M     |     13.61G    |     71.5    |     8.75M     |     13.52G    |\n\nThe experiment results are in the Supplementary Materials, from Line 625 to Line 631. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c7d67193-4b66-51d6-af54-ca484a5f4eb7", "question": "What is the impact of the sample confidence in the ratio of reliable knowledge points (RRKP)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the impact of the sample confidence in the ratio of reliable knowledge points (RRKP)?", "reference_answer": "A: The authors conducted an experiment to show the positive relationship between sample classification confidence and the ratio of reliable knowledge points (RRKP). The sample classification confidence is quantified as the $\\log\\frac{p(y\\ =\\ c^{\\text{t r u t h}}\\ |\\ x)}{1\\ -\\ p(y\\ =\\ c^{\\text{t r u t h}}\\ |\\ x)}$. The authors measured the Pearson correlation coefficient of sample classification confidence and RRKP using the output feature of the last convolutional layer for each DNN. The following shows a positive correlation between sample classification confidence and RRKP, which indicates a positive relationship between the sample classification confidence and RRKP.\n\n| Dataset                                                      | Tiny ImageNet | Tiny ImageNet | Tiny ImageNet | COCO 2014 | CUB-200-2011 |\n| ------------------------------------------------------------ | ------------- | ------------- | ------------- | --------- | ------------ |\n| DNN                                                          | ResNet-34     | VGG-16        | MobileNet-V2  | ResNet-50 | ResNet-34    |\n| The Pearson correlation coefficient between sample classification confidence and RRKP | 0.4114        | 0.4828        | 0.4967        | 0.4039    | 0.6523       |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3971be4e-b123-564a-9a6e-4e8aecbc12dc", "question": "What is the computational cost to learn the unsigned distance field?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d7ce5f53-3667-5842-aebf-36f537f678f3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the computational cost to learn the unsigned distance field?", "reference_answer": "A: The authors make a comparison with Neural-Pull, IGR, Point2mesh on the computational cost of optimizing for a single point cloud in the following table.\n\n|methods|Neural-Pull|IGR|Point2mesh|Ours|\n|:-:|:-:|:-:|:-:|:-:|\n|Time (s)|1150|1212|4028|**667**|\n|Memory (GB)|2.2|6.1|5.2|**2.0**|\n\nThe optimization time is evaluated on a single GTX 3090 GPU. It shows that the proposed method converges faster than all the baselines. The authors will include the table in the supplementary. The authors also provided the efficiency comparison of surface generation in Table 2 of the supplementary."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "48d0f6f6-0964-52c0-96c6-b098b571a230", "question": "Can this approach be scaled up to image size of 1K? What changes need to be made, if any required?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can this approach be scaled up to image size of 1K? What changes need to be made, if any required?", "reference_answer": "A: Yes, it can, by adding one more level to the hierarchy."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8101266d-df2c-54e9-929e-f5d57d9174a3", "question": "Does PACMAC exactly match the design of MAE or DINO?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["71c0db56-6716-5fe2-8b69-ed7a4b489f99"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does PACMAC exactly match the design of MAE or DINO?", "reference_answer": "A: The authors match the SSL pretraining's general design of pulling together representations extracted from partial images, and do not imply that the authors exactly match the specifics. However as shown by the previous experiment, the authors find that exactly matching the pretraining's proxy task results indeed leads to better performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "58e7e014-1507-5c1f-a25e-5fe2e161cfeb", "question": "What is the difference between [R1] and this paper? I do not seem novelty in this part. \nReference:\n[R1] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv preprint arXiv:1911.03852, 2019.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8dedd2af-a387-54a3-a611-0b69c6963d04"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference between [R1] and this paper? I do not seem novelty in this part. \nReference:\n[R1] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv preprint arXiv:1911.03852, 2019.", "reference_answer": "A: The authors utilize weight nuclear norm to determine the bit-width of each layer in the paper and it is much easier compared to the Hessian Coefficient used in [R1]. The values of weight nuclear norm in Figure.1 shows that they are various for different layers and the experiments demonstrate the effectiveness.\n[R1] Zhen Dong, Zhewei Yao, Yaohui Cai, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. arXiv preprint arXiv:1911.03852, 2019."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8895cf48-f54b-5269-84f0-2be44587dadd", "question": "The proof in Proposition B.3. includes an assumption that $E_{x\\sim p_r}[log(D(x))]=E_{z\\sim p_z^{op}}[log(D(G(z)))]$, which means the given generator (hence the discriminator as well) for the proof must be optimal. How is it possible to ensure that all these constraints are always being met?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The proof in Proposition B.3. includes an assumption that $E_{x\\sim p_r}[log(D(x))]=E_{z\\sim p_z^{op}}[log(D(G(z)))]$, which means the given generator (hence the discriminator as well) for the proof must be optimal. How is it possible to ensure that all these constraints are always being met?", "reference_answer": "A: The proof does require $E_{x\\sim p_r}[log(D(x))]=E_{z\\sim p_z^{op}}[log(D(G(z)))]$ to be satisfied, but please note that this equation is guaranteed by the definition of $p_z^{op}(G)$ (Definition 3.3) that $p^{op}_z(G)$ satisfies if $z \\sim p^{op}_z(G)$ then $G(z) \\sim p_r$. For any given generator $G$, there will be a corresponding $p^{op}_z(G)$, and it does not require the generator to be optimal. Likewise, this equation will be satisfied for any given $D$, thus there are also no constraints for the discriminator."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "60494b71-c4d9-5a23-b1ed-695c11f3c47a", "question": "For Section 3: it’s not clear to me how these models are trained — what KD approach was used, what dataset etc?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d5ad3544-6364-57a9-b777-ab5e2cf224b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For Section 3: it’s not clear to me how these models are trained — what KD approach was used, what dataset etc?", "reference_answer": "A: (1) In Section 3, the proposed objective is to investigate how to design an efficient 3D detector, where the authors simply train the designed detectors without any knowledge distillation methods as the training schema in OpenPCDet [41] (see line 124-125). This part is agnostic to KD methods.\n\n(2) For the dataset, the authors train those models on Waymo Open Dataset with 20\\% training samples, which is also the default training schema of OpenPCDet [41] on WOD. Related clarifications can be found in: line 78, line 122-123, line 131 as well as the table header of Table 1 and Table 2 as LEVEL 2 mAPH is the specific metric of WOD."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3e8d34b5-7396-5550-9271-ccbd1a6a4bec", "question": "The assumption is novel features tend to mingle with base features, and one could find K nearest of them for task centroid estimation. Any guarantees on this? What if novel features lie in a sparse region where the closest base features are far? Would it cause instability for centroid estimation? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["48f84a36-4fdb-55ab-85d5-545e4cb455b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The assumption is novel features tend to mingle with base features, and one could find K nearest of them for task centroid estimation. Any guarantees on this? What if novel features lie in a sparse region where the closest base features are far? Would it cause instability for centroid estimation? ", "reference_answer": "A: As the features of both the base data and novel data are pre-trained through the same backbone network, it is highly unlikely that novel features lie in a sparse region where the closest base features are far. To verify this point, \nwe calculate the feature similarity between the base data and novel data in the few-shot learning setting. To make it more challenging, the authors choose the Meta-dataset where there is a very large domain gap where the novel data domains (e.g., Quickdraw and Omniglot) are prominently different from the base data (i.e., miniImageNet). In detail, \nwe define $\\lambda$ as the cosine similarity between the centroid of the support data and the base data. When the authors use $\\lambda>0.6$ as the condition to select the most similar neighborhoods, the authors see that the neighborhood size varies from 290 (when Omini is set as the novel set) to 8168 (when miniImage-test is set as the novel set). In these settings, the authors observe stable improvement over the baseline method (i.e., S2M2). \nThis suggests that the authors can find k-nearest neighbors from the base data for the centroid estimation in current FSL evaluation settings. There are two potential explanations. Firstly, realistic images (even from different domains) tend to share some common underlying characteristics (e.g., low-level cues like texture or color which are verified to be shared across domains [1]). Secondly, recent FSL methods generally pretrain a  backbone network on base data via contrastive learning to improve the generalization ability, while representations learned by contrastive learning uniformly locate on the hypersphere[2]. Thus the projection of novel data to the base data can easily find neighborhoods in dense regions.\n\n|                         | mini-test         | CoCo               | CUB              | Fungi            | Omini            | Sign            | QDraw           |\n|-------------------------|-------------------|-------------------|------------------|------------------|------------------|-----------------|-----------------|\n| baseline                | $64.63$           | $63.06$           | $47.75$          | $42.36$          | $77.28 $         | $53.50$         | $51.60$         |\n| TCPR               | $68.06_{k=10000}$ | $64.32_{k=5000}$ | $51.87_{k=5000}$ | $44.38_{k=5000}$ | $78.51_{k=100} $ | $54.83_{k=100}$ | $54.62_{k=100}$ |\n| TCPR($\\lambda>0.6$) | $67.72_{k=8168}$  | $63.81_{k=6032}$   | $51.57_{k=6437}$ | $44.27_{k=4092}$ | $78.06_{k=290} $ | $54.22_{k=890}$ | $53.56_{k=367}$ |\n\nMoreover, the proposed automatic way of choosing the value of $k$ works for all datasets, with accuracy close to the best one found by grid search. \n[1] Zhao N, et al. what makes instance discrimination good for transfer learning? ICLR 2021.\n[2] Wang T, Isola P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere[C]//International Conference on Machine Learning. PMLR, 2020: 9929-9939. ICML 2019."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "209ac906-3a70-57be-9600-834bdf5984ab", "question": "Why is there a distributional gap in adversarial joint training (AJT) between the classification and self-supervised learning tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is there a distributional gap in adversarial joint training (AJT) between the classification and self-supervised learning tasks?", "reference_answer": "A: There are two differences between the proposed AJT and [1]. \nFirst, the goal in the proposed study is different from [1]. [1] tries to adapt the backbone to different styles of point clouds. However, the proposed goal is to improve the adversarial robustness of the recognition task on the original distribution. Therefore, the generalization/adaptation to the rotation and jigsaw prediction task may distract the adversarial training on the recognition task in AJT.\nSecond, the self-supervised learning (SSL) task in [1] is to reconstruct point clouds between two different styles. As shown in Figure 1 in [1], the point cloud between two domains still align well (i.e., no transformation applied to the point cloud) Therefore, the authors believe the distributional gap is not large between two styles of point clouds. \nIn the proposed AJT experiments, the SSL task is to predict the transformation itself (i.e., rotation, patch permutation). **The spatial positions of the point cloud are totally different.** It is also hard to generalize the two distributions in the proposed problem setting. The authors compute the mean and variance of clean data, rotated data, and displaced data (for jigsaw) on ModelNet40 to show the distributional gap from one perspective:  \n\n|          | Original Data                           | Rotation $\\eta=6$                       | Rotation $\\eta=18$                     | Jigsaw k=3                 | Jigsaw k=4                 |\n|----------|-----------------------------------------|-----------------------------------------|----------------------------------------|----------------------------|----------------------------|\n| Mean  [x,y,z]   | [ 1.9201e-05, -8.8720e-05, -1."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "96e90f33-1e20-5566-b7a9-6cc7859e7dd6", "question": "Why does not the pseudo language description generated by an image caption model show significant improvements?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4fd78cd3-88da-5b98-b746-1d7a884fe23f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does not the pseudo language description generated by an image caption model show significant improvements?", "reference_answer": "A: The reason lies in the domain gap between tracking datasets and existing image caption datasets, which results in poor quality of the generated language description by image caption model (e.g., [*1]) for tracking."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "872c0096-aefc-5b2c-bd25-5c3f6fadfd49", "question": "Why are there only 5 trajectories in the experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["60f80695-0a5a-5b70-8e1c-15f4615e6138"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are there only 5 trajectories in the experiments?", "reference_answer": "A: In Table 2, the proposed results are averaged over 5 random seeds, and performance for each random seed is evaluated by averaging over 10 trajectories. i.e the proposed evaluation is an average of 50 trajectory returns, the same as OPOLO."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e1fa9557-d411-55b5-9407-83a66e60b31d", "question": "How is latency computed in Figure 7? Was the setup the same (operator fusion etc) for all models considered? Those questions raise because skipping the entire block as in Convolutional-AIG should be more efficient in terms of latency reduction.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8b287103-79c8-5ec7-8439-0b3d78ddcf2f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is latency computed in Figure 7? Was the setup the same (operator fusion etc) for all models considered? Those questions raise because skipping the entire block as in Convolutional-AIG should be more efficient in terms of latency reduction.", "reference_answer": "A: The overall latency is obtained by summing up the latency of all the blocks in a network. The latency of each block is estimated using the latency prediction model by considering the latency of both data movement and computation. More detailed description of the proposed latency prediction model is included in the updated paper and supplementary material.\n\nA: The setup of operator fusion is decided based on the averaged sparsity of a block. For example, when the sparsity is high (very few pixels are selected), the latency bottleneck would be memory access rather than computation. In this situation, it would be more efficient to conduct operator fusion. The authors calculate the averaged sparsity of each block on the ImageNet validation set and decide whether to fuse some operations. This is practical thanks to the proposed latency prediction model, which helps us to efficiently analyze the latency bottleneck.\n\nA: Although skipping the entire block as in Conv-AIG (or the proposed coarsest granularity S=56-28-14-7) is easier to implement for fast inference, it might degrade the network performance (please also refer to the proposed response to Question 5). Note that in the proposed experiments for the variant of S=56-28-14-7 (which is similar to Conv-AIG), the operator fusion is considered in the same way as other granularity settings."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cbef8f79-b668-5933-80f8-9e699ec452d5", "question": "In Equation 7, is the projection of a penultimate-layer feature via a row of the normalized parameter matrix of the last layer the corresponding logit? If yes, this equation can be further simplified.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["b761f966-dde6-5d75-9704-25545a8198a2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Equation 7, is the projection of a penultimate-layer feature via a row of the normalized parameter matrix of the last layer the corresponding logit? If yes, this equation can be further simplified.", "reference_answer": "A: Yes, the output is the normalized logits. The formulation in this equation (Equation (7)) is intended for an easier comparison to Equation (6) and to have an emphasis on the improvement (smaller and fixed number of projections) of the proposed DSWD calculation (Equation (7))  over SWD (Equation (6)). The authors will incorporate in the later revision of the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0a145789-899a-5d08-8256-b23ed13fa6c6", "question": "Is the \"dependency graph\" actually a tree, a DAG, or a cyclic graph? I'm not sure about the connection, but I felt some abstract similarity to topics such as decision diagram (DD) representation of logical functions and traversing an enumeration tree, the search space of gSpan algorithm for all subgraph patterns, with bounds as in https://doi.org/10.1109/TPAMI.2016.2567399 for example.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f2140364-3091-540e-b65a-e3025f03ef8a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the \"dependency graph\" actually a tree, a DAG, or a cyclic graph? I'm not sure about the connection, but I felt some abstract similarity to topics such as decision diagram (DD) representation of logical functions and traversing an enumeration tree, the search space of gSpan algorithm for all subgraph patterns, with bounds as in https://doi.org/10.1109/TPAMI.2016.2567399 for example.", "reference_answer": "A: The dependency graph is a DAG. DAGs are common data structures for storing subproblems in dynamic programming. Figure 10 in Lin et al., 2020 shows the graph representation of the dependency graph for GOSDT. (DD is different as it starts with a single tree and enumerates only subtrees of it - the search space is limited to the given tree. However, the proposed method has a larger search space, because it can extend to any sparse tree over all features. Note that decision trees can always be represented as decision diagrams as they are logical functions.) \nUnlike gSpan, the proposed algorithm is not considering frequency of subgraphs - that's not the proposed goal here."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2f19b288-9602-52e2-8965-c2017086b2ca", "question": "In figure 1 and in Line 251, how do the authors quantify \"best trade-off\"? What is the metric used to determine this?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 1 and in Line 251, how do the authors quantify \"best trade-off\"? What is the metric used to determine this?", "reference_answer": "A: The authors describe how to search for the optimal sub-network in Section 3.3 and Section 4.1. The \"best trade-off\" for optimal student selection is given by the sub-network with the least validation loss subject to the resource constraint as described in Eqn (6). For instance, the authors set a constraint in Eqn. (6) such that the #FLOPs of the optimal Base-sized task-agnostic compressed model is atleast 50\\% less than the teacher model (lines 244-245). Since the SuperNet training is task-agnostic, the obtained student models have to be fine-tuned on downstream tasks to report the final task performance (similar to pre-train and fine-tune paradigm of BERT-like language models)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "69dcef62-7f0f-524f-adb9-81b0ac130525", "question": "How do you deal with discrete input values (y) in this paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["bb4b14e7-457a-5b7b-bfb8-fff2970120c9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do you deal with discrete input values (y) in this paper?", "reference_answer": "A: The authors have Figure 1 to depict the calculating procedure visually.\n\nThe procedure for calculating equation (4) is:\n\n1. The CVR probability $q_{\\theta}(y|x)$ is calculated normally with one forward pass of network $q_{\\theta}$,\n   which produces the estimation of $p(y=0|x)$ and $p(y=1|x)$.\n\n2. $q_{\\phi}(a_j|x, y, \\delta_j)$ takes $x$, $\\delta_j$ and $y$ as inputs.\n   Specifically, the authors first encode $x$ with an encoding network $Encode(x)=e_x$,\n   where $e_x$ denotes an embedding of $x$. \n   Then, the authors concatenate $e_x$ with one-hot representations of $y$ ([1, 0] and [0, 1] for CVR),\n   respectively, e.g., $e_x | [1, 0]$ and $e_x | [0, 1]$ (since the authors need to take sum over different $y$).\n   The authors take $e_x | [0, 1]$ (corresponds to $y=1$) as an example, \n   this vector is then fed into a MLP with $m$ output heads that corresponding to probabilities of $m$ actions.\n\n3. The predicted probabilities are used to calculate the GDFM loss $\\mathcal{L}_{\\delta_j}$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e54f752-41c6-5f73-8fd4-46d615c4efcb", "question": "During the first stages of training when the pseudo-labels are quite noisy, is it possible for the model to get stuck?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "During the first stages of training when the pseudo-labels are quite noisy, is it possible for the model to get stuck?", "reference_answer": "A: In the beginning, both the image from the generator and the geometry extracted by GeoD are noisy, and thus it is hard for both to get stuck in a local minima. A pre-trained geometry branch gives stronger geometry guidance for the generator at the start, at which stage the primary domain classification branch does not follow up yet. This may lead to a better solution for geometry while a sub-optimal solution for RGB synthesis. The authors guess that is the reason why FID and RE of \"Trained from scratch\" are better than those of \"Pretrained\", while \"Pretrained' is better in terms of the geometry metric, SIDE."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "65ca6e35-313d-5d66-8bd2-8d5cdb2177c2", "question": "Which experiments are conducted to verify the generalization of the proposed methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["03090ab9-8bc1-52a0-9bb2-44b7a3a719f1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which experiments are conducted to verify the generalization of the proposed methods?", "reference_answer": "A: The authors have carried out experiments on Waymo Open dataset to verify the generalization of the proposed methods. The authors show the results (mAP/mAPH) in the table below, which includes base method PatchNet and the proposed PCT under IoU=0.7 on Car category.\n\n| Method         | Level                 | Overall |  0 - 30m | 30 - 50m | 50 - infinity|\n| :--------------:        | :--------------:        | :--------------: | :--------------: | :--------------: | :--------------: |\n| PatchNet        | LEVEL_1         | 0.39/0.37 | 1.67/1.63 | 0.13/0.12 | 0.03/0.03 |\n| PCT                  | LEVEL_1        | 0.89/0.88 | 3.18/3.15 | 0.27/0.27 | 0.07/0.07 |\n| PatchNet        | LEVEL_2         | 0.38/0.36 | 1.67/1.63 | 0.13/0.11 | 0.03/0.03 |\n| PCT                  | LEVEL_2        | 0.66/0.66 | 3.18/3.15 | 0.27/0.26 | 0.07/0.07 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fd124d52-9971-5020-9b51-3490d7b7dd9b", "question": "In Table 3, which rows correspond to the prior work ([1], [2], [3])?  Are they exactly the same as implemented in prior work? If not, point out what are the differences.\nReferences:\n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[3] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 3, which rows correspond to the prior work ([1], [2], [3])?  Are they exactly the same as implemented in prior work? If not, point out what are the differences.\nReferences:\n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[3] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019.", "reference_answer": "A: 1. In Table 3, [3] (goal-oriented reward) and [2] (fidelity-oriented reward based on nDTW) corresponds to model#15 and model#16 respectively. The [1] (fidelity-oriented reward based on CLS) does not correspond to any rows.   \n2. In addition, there are a few differences between the proposed reproduced methods and prior works. [2] proposes the nDTW to replace the CLS metric and uses the gain in nDTW score after taking an action as the reward signal, but the proposed experimental results show that this reward type does not perform well on Loss Number metric (LN: 5426). So the authors formulate the fidelity-oriented reward based on nDTW as 'fidelity metric + SR' (model#16, LN: 5309), which is an effective formation for fidelity-oriented rewards that was first defined in [1]. For the goal-oriented reward, since trajectories in the R2R dataset are all the shortest trajectories to the goals, [3] use the reduced distance after taking an action as an extra reward signal in addition to the SR signal. However, 44.5% of RxR trajectories are not the shortest trajectories from the start to the goal location as described in the Path Statistic section of [4]. So the authors did not include the reduced distance part in the goal-oriented reward as mentioned in footnote 1 (page 8) in the paper.  \nReferences:\n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[3] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019.\n[4] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Empirical Methods in Natural Language Processing, 2020."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3e43349e-f282-5e71-afc0-3f606025ced4", "question": "Authors seem to imply that deepnets can learn more complex functions than RKHS - is that right? If so in what sense ? Are the cases where it makes a difference significant from an application perspective?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a343c17a-4d43-5938-bf9d-875c9f30a3eb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Authors seem to imply that deepnets can learn more complex functions than RKHS - is that right? If so in what sense ? Are the cases where it makes a difference significant from an application perspective?", "reference_answer": "A: Although there are ongoing research efforts to justify the superiority of deep networks over RKHS functions (or linear estimators, more generally), empirically, deep networks work better than kernel methods when the data is structured and high-dimensional (e.g. images/text). Theoretically, deep learning is known to be superior to linear estimators including kernel ridge regression, in estimating functions with spatially inhomogeneous smoothness  (functions in Besov spaces) (Suzuki, 2019), where adaptive features are effective. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3bf5956c-d59d-5341-9b77-0745eae631f2", "question": "Should the gradient $dX/d\\theta$ always exist? Why is Erdos Goes Neural [A] missing in the discussion in Table 1? \nReference:\n[A] N. Karalias and A. Loukas, “Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs,” Advances in Neural Information Processing Systems, vol. 33, 2020.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["92c6d0ac-b67d-5898-9b75-1baeb7804479"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Should the gradient $dX/d\\theta$ always exist? Why is Erdos Goes Neural [A] missing in the discussion in Table 1? \nReference:\n[A] N. Karalias and A. Loukas, “Erdos goes neural: an unsupervised learning framework for combinatorial optimization on graphs,” Advances in Neural Information Processing Systems, vol. 33, 2020.", "reference_answer": "A: Throughout the paper, the authors use $X$ without bar to denote discrete variables and $\\bar{X}$ with bar to denote continuous variables. So, their statement “$dX/d\\theta$ is not computable” is correct. This is an argument for the case that tries to apply the theory of EGN to general CO problems, which always encounters such non-differentiable difficulty.  For the specific cases studied in EGN, EGN adopts “$d\\bar{X}/d\\theta$” (not $dX/d\\theta$), which essentially follows a relaxation as this paper suggests. However, EGN does not derive the relaxation principle as ours, although the used relaxation happens to satisfy the proposed principle. The above description implies a gap between EGN’s theory for general CO problems and its implementation for the two cases studied in [A]. In the EGN paper, the theory is only in the probabilistic sense and works for just discrete objectives (see Eq.(3) and Thm 1 in [A]).  However, the implementation in EGN for the max-clique and graph-partition problems does not strictly follow the probabilistic model but relaxes discrete variables $X$ into their continuous counterparts $\\bar{X}$. Such replacement cannot achieve the performance guarantee claimed by EGN for general CO problems, because the expectation in general does not always equal to the relaxation. Because of the above gap of EGN for general CO problems, the authors think it is tricky to well position EGN in Table 1."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "520eaea0-8cb1-5a06-95fb-a705cc6b8e4c", "question": "I wasn't able to understand precisely which information is conveyed from ZeroC1 to ZeroC2. Could you clarify this section in the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f1c2adf-3164-5df0-b6ba-b27f76ecd17c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I wasn't able to understand precisely which information is conveyed from ZeroC1 to ZeroC2. Could you clarify this section in the paper?", "reference_answer": "A: The information conveyed from ZeroC1 to ZeroC2 is the graphical structure of a hierarchical concept. For example, in Figure 3, ZeroC1 learns the graphical structure of an E shape in terms of the initial concepts and relations. The graph structure is then conveyed to ZeroC2, which enables it to classify and detect E shapes in the 3D domain. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "47dd0d74-e800-5450-a37b-2fe6a6f17617", "question": "What is the performance of directly extending Neural-Pull to unsigned distance field?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d7ce5f53-3667-5842-aebf-36f537f678f3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the performance of directly extending Neural-Pull to unsigned distance field?", "reference_answer": "A: The quantitative results obtained by directly extending Neural-Pull to UDF have been shown in ‘NP loss' of Table 5, and the simulation experiment of this extension has been shown in Fig 4. Furthermore, the visualization of the unsigned distance field learned by Neural-Pull and the proposed method has been shown in Fig 1 in the supplementary. Note that all the designs and experimental settings are kept the same as ours except for the loss. Besides, the quantity and visualization comparisons with the original Neural-Pull which learns SDF were given in Table 2, Table 4, Fig 8 and Fig 9, respectively."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "56001efc-a16d-58a7-b78d-81497676851f", "question": "In equation (11), what is the space you are taking supremum over for $D$ and $f$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["befa2892-2ca9-59fb-bd49-724e13203e60"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In equation (11), what is the space you are taking supremum over for $D$ and $f$?", "reference_answer": "A: $D$ and $f$ are taken over all valid distributions such that $f$ is convex and lipschitz. $D$ is a probability measure (w.r.t Borel $\\sigma$- algebra as is standard – the authors will note this)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e39ab074-184c-5c8e-bfc7-b1e129c14943", "question": "Weakness 6 and Question 1: Both figures 1 are non-informative and confuse the reader. In Figure 1a) what do colors represent? What are the current models and the final model?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["36883baf-d01c-533a-b3b4-376fb580e945"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Weakness 6 and Question 1: Both figures 1 are non-informative and confuse the reader. In Figure 1a) what do colors represent? What are the current models and the final model?", "reference_answer": "A: Note that the authors have explained the concept of current models and final model in Lines 36--38 of the proposed main paper. To be more specific, the CVLM setting has a sequence of 5 tasks and the models are supposed to be sequentially trained on all these tasks. Therefore, the result of the current model on Task i is obtain by evaluating the model on Task i right after trained on Task i (before it is trained on Task i+1); the result of the final model on Task i is obtain by evaluating the model on Task i after trained on all 5 tasks. Particularly, the results of the current and final models on Task 1 in Figure 1(a) show that the performance of Base-MoCo (on Task 1) drops significantly after trained on all 5 tasks. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f28701b6-db6b-5ed8-8be5-dd55a934b422", "question": "Why three images at a time?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f1c2adf-3164-5df0-b6ba-b27f76ecd17c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why three images at a time?", "reference_answer": "A: This is because in this dataset, the authors have 3 compositional concepts, and for each concept the authors show one example. For a dataset with N compositional concept, the authors will then show N images where each image corresponds to one concept."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "86c894a2-9348-5ffd-af88-040218a543e4", "question": "What are the results with RGB images? Does the method work only for grayscale images?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results with RGB images? Does the method work only for grayscale images?", "reference_answer": "A: The proposed method can also work on RGB images, and the model performance on RGB images is as follows:\n\nInput Halftone | PSNR | SSIM\n-|-|-\nRGB images | 26.20 | 0.853"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "25acf52c-3807-5dfe-9964-50cd99e69dd6", "question": "Explain the difference of the proposed method with the robust aggregation oracle proposed in [1]. It seems like it might have better defense performance against the data/model poisoning attacks. \nReference:\n[1] Pillutla, Krishna, Sham M. Kakade, and Zaid Harchaoui. \"Robust aggregation for federated learning.\" arXiv preprint arXiv:1912.13445 (2019).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain the difference of the proposed method with the robust aggregation oracle proposed in [1]. It seems like it might have better defense performance against the data/model poisoning attacks. \nReference:\n[1] Pillutla, Krishna, Sham M. Kakade, and Zaid Harchaoui. \"Robust aggregation for federated learning.\" arXiv preprint arXiv:1912.13445 (2019).", "reference_answer": "A: As mentioned in [1], noise is typically added to the gradients from a client to the server in order to prevent privacy leakage. In the setting proposed in the paper, an attacker infers the gradient by using two adjacent global models broadcasted by the server. Thus, the server needs to add noise to the global model to prevent the attacker from inferring the accurate gradient. The authors tried adding noise to the broadcasted global model and using clipping median as an aggregation rule. For MNIST, adding noise indeed decreased the quality of reconstructed images, but many of them are still recognizable and the proposed RL based method still outperforms other baselines in this setting. \n\nMNIST + Clipping Median + Noisy Gradient\n                                          Accuracy\n      No Attack                    94.84%\n      IPM                               91.58%\n      EB                                 93.29%\n      LMP                              84.74%\n      Proposed Method     16.89%"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "25782948-3d92-5ff4-9d9d-f93f9a2c1bb3", "question": "How IRLS-1 and IRLS-0.1 are sensitive to the update rules of the smoothing parameters, and is the current update rule of the smoothing parameter critical to the success of IRLS-0.1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ef620ed0-9b0c-5424-ad9f-85ae9e9c0f44"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How IRLS-1 and IRLS-0.1 are sensitive to the update rules of the smoothing parameters, and is the current update rule of the smoothing parameter critical to the success of IRLS-0.1?", "reference_answer": "A: Yes, the current update rule of the smoothing parameter is critical to the success of IRLS-0.1. To be more specific, let us recall the dynamic update rules  (5), (6), (7) respectively. In [15], rule (5) is only applied to the case p=1 (See Lines 183-196 for discussion about (5)). It was observed already in [24, Section 8.1] that IRLS-p using rule (6) does not exhibit a good global convergence behavior if $p < 0.5$ (for the compressed sensing problem), which is consistent to what the authors observe also for robust regression. \n\nA: The update rules (6) and (7) are further compared for Lp minimization in Figure 1b, and the authors see that (7) performs much better. From a theoretical point of view as well, the local superlinear convergence radius implied by rule (7) is much larger than the one implied by rule (6), as discussed in Lines 236-244.\n\nA: Note that the only difference between (6) and (7) is that (7) takes the best $\\alpha$-term approximation, while (6) takes the ($\\alpha+1$)-th largest element from the residual. Without a proof, the authors suspect that the reason that (6) performs worse is because the smoothing parameter of (6) is decreased too fast at each iteration, resulting in sub-optimality (consider a similar situation where the regularization parameter for the log-barrier function should not be increased too fast in the interior point method).\n\nA: As a summary, it can be said that both theory and empirical behavior of IRLS are sensitive to the precise choice of the update rule, and are the reason why the authors choose rule (7). While both rule (6) and (7) enjoy empirical global linear convergence for p=1 (Figure 1a), the authors are only able to prove this for (7). Finally, it should be remarked that, even though (7) is the best update rule that the authors are aware of and it is crucial for the proposed analysis, it is unclear to us how to design an optimal update rule for the smoothing parameter that can further improve IRLS."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "742c2a4c-3a08-5995-a4b5-8a7fb645c14c", "question": "What does “condition” and “target” mean in “condition to condition (C2C), condition to target (C2T), target to condition (T2C), and target to target (T2T)”?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does “condition” and “target” mean in “condition to condition (C2C), condition to target (C2T), target to condition (T2C), and target to target (T2T)”?", "reference_answer": "A: (1) condition: source image discrete codebooks \n(pose: source image tokens and pose landmarks vectors, face: sketches tokens)\n\n(2) target: target image discrete codebooks\n\nThey have been discussed in sec.3.3 as detailed implementations. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "887bee5b-9270-5dfa-a555-c635ed880f3d", "question": "Do overlapped patch embeddings suffer from aliasing?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e070ad3b-180d-5e8b-a978-d27d1aeaec36"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do overlapped patch embeddings suffer from aliasing?", "reference_answer": "A: Yes, we’ve tried to employ some overlapping patch embedding models with the proposed module and found that the problem of aliasing also existed. As originally mentioned in the proposed work, T2T-ViT in Table 4 utilizes the soft patch splits with overlapping strategy. Nevertheless, you can see that the proposed method still yields improvement on it. \n\nNote that when the authors were working on this paper, few transformers that exploit overlapping windows were available or open-sourced (including the [1,2] you’ve mentioned). To further support the observations, the authors employ the proposed method to some recently-available transformers CCT[2] and PVTv2[3]. The authors can see that both of them receive performance boosts. The results indicate that transformers with overlapped tokens still suffer from aliasing and the proposed module could produce a fix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5d526efa-e347-5111-ac0b-f6e8b5dff96f", "question": "I agree that the convergence result built upon the last iterate makes more sense, but how does the stochasticity of the gradient estimate take place in the final convergence result?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["957b66a3-1bd0-534c-9ef1-c3837dbdf921"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I agree that the convergence result built upon the last iterate makes more sense, but how does the stochasticity of the gradient estimate take place in the final convergence result?", "reference_answer": "A: In Table 1, the authors showcase the dependence on constants  $\\sigma^2$, $\\kappa$ and $\\mu$ under bounded variance assumption. It turns out that the dependency for PAGER is $\\kappa^2$, but it enters additively with $\\sigma^2$. While for SGD $\\kappa$ is multiplied with $\\sigma^2$ in the complexity."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1b4fa15f-c181-5aae-8033-6aa861926e0c", "question": "What does datasets refer to in Eq. 2? I thought the model is trained on only M3W dataset, constituting data from multiple documents. After some sifting through Appendix A, I found the relevant details in Sec. A.3.3. I think it would be a good idea to mention it in the Sec. 2.4 of main paper before introducing the equation.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["f9c136b6-1c41-543a-ad92-b5563b5e797e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does datasets refer to in Eq. 2? I thought the model is trained on only M3W dataset, constituting data from multiple documents. After some sifting through Appendix A, I found the relevant details in Sec. A.3.3. I think it would be a good idea to mention it in the Sec. 2.4 of main paper before introducing the equation.", "reference_answer": "A: The authors train on not only M3W, but Image-Text Pairs (ITP), and Video-Text Pairs (VTP) as well, as described in Sec 2.4 (L140-141). The authors compute the gradients on each of these datasets separately, accumulating these gradients to compute a single VLM parameter update. The discussion of the ITP/VTP datasets in the main paper was somewhat terse due to space limitations, and the authors will make sure to better clarify the proposed use of these in the camera-ready version with the additional content page."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "469fcbd5-fc96-5c7c-8863-d81d1bb5c378", "question": "How to pick up $\\gamma$ without accessing the transferability results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9543585e-5cb2-5d3d-be98-27c24b141ad8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to pick up $\\gamma$ without accessing the transferability results?", "reference_answer": "A: The authors have a subsection 4.4 to discuss the selection of  $\\gamma$ in practice, and an additional study on the \"transferability\" of $\\gamma$ in Appendix C. As can be seen from the proposed parameter tuning in Figure 3 and Figure 7, $\\gamma$ is more associated with the source model than the target model. The \"transferability\" of $\\gamma$ is quite good and stable. For example, given source model DenseNet-201, the highest success rate is always achieved at $\\gamma=0.5$ against all target models such as VGG19, SE154 or Inception-V3. In other words, the selection of $\\gamma$ is simple and straightforward: tune $\\gamma$ on the source model (which is known) against some random target model. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0429efa9-08be-5da0-ab2d-682a962c7607", "question": "Why was AD-DROP only applied to the first layer for STS-B (Line 175)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["18d7c29e-6555-58f2-be57-4877a5de5ec3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why was AD-DROP only applied to the first layer for STS-B (Line 175)?", "reference_answer": "A: Although smaller than CoLA, STS-B is more stable when finetuning. As shown in Table 3, the standard deviation is less than CoLA (0.5 vs. 1.9 on BERT and 0.2 vs. 0.9 on RoBERTa). Since STS-B is a regression task, the authors hypothesize that it is less likely to cause overfitting. Actually, the authors have conducted AD-DROP in all layers on STS-B and found that applying AD-DROP to the first layer can obtain better results on STS-B."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f0ada14f-31ea-59a0-8a48-1b54fb3a1e06", "question": "What is the performance of AdvLatGAN on some larger-scale image dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the performance of AdvLatGAN on some larger-scale image dataset?", "reference_answer": "A: Please refer to Line 316-329 and Table 6 for experimental results on large-scale datasets including ImageNet, CelebA and LSUN, where the proposed methods achieve significant performance gain as always (-qua+ has achieved the best performance gain on FID in SNGAN LSUN-64 setting from 11.961 to 7.285)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "48811698-7de8-5931-89df-3dadc71f9212", "question": "Is it possible to try with bi-level planning which first plans to avoid the collision?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d66a5cd9-35b7-5db9-816b-47aaae8ae114"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is it possible to try with bi-level planning which first plans to avoid the collision?", "reference_answer": "A: The Goal (ORCA) baseline in ball arrangement can be regarded as an implementation of bi-level control. The authors do not apply it in the room arrangement as the rectangle-shape furniture objects do not satisfy the circle-shape assumption made by the decentralised planner ORCA. The authors also tried to implement a centralised planning algorithm (e.g. RRT [1]). However, it is costly in time to search for a reasonable path, i.e., taking almost 1 min for 3x3 balls. If the authors increase the number of objects (i.e., from 3x3 to 3x7 balls), the authors find that it failed to search a motion path in a limited time (i.e. 10 minutes). \n\nThe authors notice that such a solution is of two main limitations: \n1. Open-loop planning:  The proposed goal and the initial state may not be reachable or far away from each other.\n2. Accessibility of the generated goal: The goal proposer ignores the environment dynamics. So the generated goal may be physically inaccessible (e.g. objects overlap with each other), as shown in Fig.4 in the supplementary.\n\nAs demonstrated in the ball arrangement experiments, the above two limits lead to weak performance : \n1. In Fig. 5 of the main paper, the likelihood curves of goal-based methods are significantly below ours.  In Table 1 of the supplementary, the average length of trajectories of goal-based methods is significantly below ours (e.g., In Circling + Clustering, the averaged state change of Ours(SAC) 48.93 +- 4.68 achieves less than half of Goal(SAC) 122.72 +- 5.93).\n2. Besides, in the room arrangement, the rectangle-shape furniture objects do not satisfy the circle-shape assumption made by the decentralised planner ORCA. The authors also tried to implement a centralised planning algorithm for ball arrangement (i.e. RRT). However, though this algorithm can find a feasible solution in one minute when the object number is less than 3*3, it failed to search a motion plan in ten minutes when the number of objects got higher. This is also the well-known curse of the dimensionality problem of the centralised planner. These aspects cause unsatisfied performance for the goal-based approach. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a0a0cb22-207d-5611-96d6-4aa3422d05cd", "question": "What are the differences between the delayed feedback problem and the heterogeneous feedback problem?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["bb4b14e7-457a-5b7b-bfb8-fff2970120c9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the differences between the delayed feedback problem and the heterogeneous feedback problem?", "reference_answer": "A: 1. **The training schema is different**.\n   Streaming training with feedback delay already differs \n   from static training without considering heterogeneous labels.\n   For example, in offline training, the missing labels will not be revealed during training,\n   and the authors do not need to consider conflict labels (e.g., negative label changes to positive label).\n\n2. **Some distinct problems arise from feedback delay**.\n   Tackling new labels is not as straightforward as it seems to be at the first glance.\n   1. Suppose a sample has been already used as a negative sample, and it converts later,\n      *how* to deal with this sample?\n      If the authors simply ignore it, then the authors have used the wrong label;\n      if the authors insert a duplicate with a positive label, then the data distribution $p(x)$\n      changes (negative samples appear once, but positive samples may appear twice),\n      and the label conflict still exists, how to repair it?\n   2. Another problem is *when* to use a sample.\n      Since the authors are not working with a static dataset passively,\n      the authors can choose the revealing time freely,\n      and this requires us to define a schedule explicitly for revealing the labels.\n\n3. **User actions play an intrinsically different role in learning with delayed feedback**:\n   In the setting of learning with heterogeneous feedback,\n   as the authors discussed in response to Reviewer fM2F Q4, user actions work more like\n   *complementary information to the conversion labels*.\n   In the setting of learning with delayed feedback,\n   the authors need to *rely on user actions to extract information related to conversions*\n   when ground-truth conversion labels have not been revealed yet.\n\nA: Q: What are the differences between GDFM and EHCF?\nA: 1. EHCF does not consider specific problems in learning with delayed feedback as\n   discussed in the previous question.\n\n2. As pointed out by Reviewer fM2F and fuKP, the proposed main contribution is providing a\n   novel probabilistic perspective to analyze the delayed feedback problem,\n   and come up with a practical method to measure the information carried by\n   user actions. These are novel points rooted in the delayed feedback problem and\n   are not considered by EHCF.\n\n3. The authors agree that in the proposed current implementation of GDFM,\n   $p(a|x)$ also relates with $p(y|x)$ linearly.\n   However, this formulation comes from the proposed probabilistic model naturally\n   with clear interpretability and establishes the base of the following analysis.\n   The linear mapping introduced by EHCF lacks such probabilistic insight.\n\n4. The training methods are different, which leads to different results:\n   GDFM learns $p(a|y)$ explicitly,\n   whereas the meaning of linear transformation learned by EHCF\n   is unclear.\n\nA: Q: Experimental comparison between GDFM and heterogeneous feedback methods such as EHCF.\nA: As analyzed in the previous questions,\nwe can not compare GDFM with EHCF directly since EHCF does not support duplicated samples and\nchanging labels in the delayed feedback setting.\n\nThe authors agree that using trainable linear layers to capture relationships between user actions\nis an applicable idea in the delayed feedback setting with user actions.\nSo the authors implemented an architecture equipped with the Transfer-based Multi-Behavior Prediction layer\nas proposed in the EHCF paper\nand used the same duplicating and revealing strategy as in GDFM to conduct a reasonable comparison.\nThe authors denote this as the \"Linear relation\" method.\nThe authors evaluate the performance on the Taobao dataset.\n\nThe performance of the Linear relation method is:\n\nAUC: 63.4±0.9%, PR-AUC: 50.1±1.5%, NLL: -470±4.6%\n\nand GDFM is:\n\nAUC: 79.4±0.5%, PR-AUC: 80.7±0.9%, NLL: 49.6±3.1%\n\nThe results support that utilizing the relationship between user actions and conversions with a proper sampling strategy\nwill improve performance on AUC and PR-AUC.\nHowever, since the Linear relation method does not consider label changing,\nthe NLL is significantly worse. And the AUC and PR-AUC metrics of GDFM are also better than EHCF based method.\n\nThe experimental results and discussion about some related heterogeneous feedback methods [1, 2, 3]\nare added to the paper. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "722a8b22-befe-5e49-8713-7a5cb042c93d", "question": "Why does ASoftmax outperform RSeR with an integer range of {−1,0,+1,+2} in 2-bit?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7f09e532-bc3b-56a1-96ac-6dc6c9cf7072"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does ASoftmax outperform RSeR with an integer range of {−1,0,+1,+2} in 2-bit?", "reference_answer": "A: You may have misunderstood Table 5, where the integer ranges of all experiment are {0, 1}. In fact, RSeR is designed for binary discretization thus only supports {0,1}. Extended ASoftmax does have the ability to turn $h(v)$ to arbitrary integer in {-2, -1, 0, 1}."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "37621d3d-c729-5a18-b9e0-31275044f1d0", "question": "Why is AFeB good for the denoising task to preserve the image details and filter unpleasant noise?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["431c48ab-9371-5daf-b349-346879e446a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is AFeB good for the denoising task to preserve the image details and filter unpleasant noise?", "reference_answer": "A: Because AFeB could learn the sampling locations and assign different weights to show how important the locations are, based on the input features."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "331c8edd-c013-5e24-9185-f133123457ca", "question": "Why are there two paths in Figure 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are there two paths in Figure 2?", "reference_answer": "A: The upper path (halftone dithering diffusion) in Figure 2 is used to generate more diverse halftones (used to improve the generalization of the model to different dithering patterns), and these generated results are used as the input of the lower path (inverse halftoning diffusion)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "71dcd017-58f3-5391-b3da-4e62947fe07c", "question": "How does the performance vary as fewer (or more) clean bits are used?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["50a93dcf-f0bd-571d-ba60-a03f6fd2e35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the performance vary as fewer (or more) clean bits are used?", "reference_answer": "A: The clean bits are only required to be able to pop a sample from the latent posterior off the BB-ANS stack, and the authors would expect this requires as many bits as the entropy of this latent posterior, q(y|s). This number depends on the dimensionality of the latent, on the choice of distribution q and on the precision of the latent discretization. It doesn't depend on the number of images to be compressed."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "71014a98-8240-5e7f-87fc-90484b5d3989", "question": "The visual language pre-training models achieve good performance on image captioning recently. Do you think the proposed new methods should apply to those methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0d97bc45-58ea-5a2c-9d6a-18b36254f00a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The visual language pre-training models achieve good performance on image captioning recently. Do you think the proposed new methods should apply to those methods?", "reference_answer": "A: The proposed Discrete Mode Learning (DML) is a general learning paradigm and does not rely on specific backbones. This is why the authors can deploy it on both Transformer and AoANet. Large-scale vision-language pretraining models are normally built based on Transformer structure so the authors believe the proposed DML can be applied to them as well. However, large-scale vision-language pretraining models generally require huge costs to train. Thus, the authors have planned to do this in further work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ea10e3c7-8562-54ae-a00d-4d2e2437a5a4", "question": "Clarify how the sub-trajectory accuracy is computed. Is this an average of the nDTW of all sub-trajectories? If it is a nDTW, then the name \"sub-trajectory accuracy\" does not seem accurate (wouldn't \"sub-trajectory nDTW\" be better?): ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Clarify how the sub-trajectory accuracy is computed. Is this an average of the nDTW of all sub-trajectories? If it is a nDTW, then the name \"sub-trajectory accuracy\" does not seem accurate (wouldn't \"sub-trajectory nDTW\" be better?): ", "reference_answer": "A: Sub-Trajectory Accuracy (SSA) is defined as the nDTW of the predicted sub-trajectory. It is reported in Table 3 as the average of the nDTW of all sub-trajectories. The authors call it Sub-Trajectory Accuracy because the authors use it to measure the navigation accuracy. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "98e82887-b009-5470-9561-19d87a0b96c6", "question": "Comparison of the learned impedance parameter functions for the 3 scenarios. Question 2: To what degree do the learned impedance parameters generalize to slight variations of the terrain?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f9d0d9cf-0cc7-5bc1-b533-aa5fc1801582"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comparison of the learned impedance parameter functions for the 3 scenarios. Question 2: To what degree do the learned impedance parameters generalize to slight variations of the terrain?", "reference_answer": "A: The authors have included Table 3 in Appendix B.3 to summarize the learned impedance parameters for the 3 scenarios. On variations of terrain. \n1)  If terrain/task change is significant (e.g., from level ground to stairs or turning), the authors will need an additional module of task planning as human joint movement profiles change significantly and thus controllers are expected to be different. \n\n2)  Existing OpenSim and human testing show that learned impedance control do generalize to slight variations such as level to small slopes or pace change [44, 46, 86]."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "52d66e32-90bc-57a6-843e-892ba694b75d", "question": "What is the overall training time for the proposed model?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["702daa48-ef5a-50ba-912a-30163e595c48"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the overall training time for the proposed model?", "reference_answer": "A: Please refer to Table 8 in the proposed response to reviewer BfTa"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "83a58a5c-29c6-55ad-a965-cb3239ff7aff", "question": "Does KD work on ImageNet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8ab99797-77c2-571c-8536-d5df28e7f652"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does KD work on ImageNet?", "reference_answer": "A: Yes, KD can improve students when using ImageNet."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "45546584-5e36-5ff3-82a8-2ae7aa58310e", "question": "How to select hyperparameters for the baselines? If the comparisons were fair?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["abd4f1be-f6b8-5e2d-8d28-ec7e944d5ab4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to select hyperparameters for the baselines? If the comparisons were fair?", "reference_answer": "A: The authors believe the proposed comparison to the BCQ and CQL methods is fair for two reasons: 1) the authors have fine-tuned hyperparameters of BCQ-MA and CQL-MA in StarCraft II. 2) the authors adopt the same neural networks structure and value-decomposition assumption as ICQ-MA. However, the authors find it is not enough to improve the performance of BCQ-MA and CQL-MA by fine-tuning hyperparameters.\nIt is not surprising that BCQ-MA and CQL-MA have poor performance in the multi-agent experimental results. In the supplementary experimental results, current offline methods are still not good enough in handling the extrapolation error in complex single-agent offline tasks, such as antmaze-medium/large and adroit-human. Based on the analysis in Section 3 and the toy example in Figure 2 in the paper, the extrapolation error is quickly accumulated with the growth of the number of agents. Multi-agent offline tasks are less tolerant of extrapolation errors than single-agent offline tasks. Therefore, most single-agent offline RL methods cannot be extended in multi-agent offline tasks.\nIn contrast, ICQ alleviates the extrapolation error by a simple yet efficient method instead of training a separate network as BCQ or using additional regularization terms, such as CQL or BRAC.  As evidence, ICQ achieves high performance in complex single-agent offline tasks such as antmaze and adroit. The state-of-the-art performance of ICQ on single-agent offline tasks lays a solid foundation for multi-agent offline tasks."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "69897a84-d858-5467-9db4-51b70254df3c", "question": "Do you think knowledge transfer methods should be our main baselines?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["a9a91f2d-c308-5f4a-bf69-66e085e155c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do you think knowledge transfer methods should be our main baselines?", "reference_answer": "A: The authors tend to disagree that knowledge transfer methods should be the proposed main baselines. The proposed approach is complementary to knowledge transfer, and it can also be used on its own in the absence of teacher networks. In any event, Table 1 and 2 already indicate that, in most cases, baseline < baseline+KD < ExpandNet < ExpandNet+KD in terms of accuracy. The ShuffleNet results above confirm that the performance of the proposed ExpandNets can be further boosted with the help of a teacher network."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "99fb813d-e869-5904-92f2-650f62dfb674", "question": "The authors design a shallow and wide model because all of their Transformer-based baselines set this configuration. The authors speculate that a wider model means that the dimensions of structured embedding vectors(edge embedding for Great, distance embedding for CodeTransformer) will also be wider, ensuring the representation capability of structured embedding vector. Does that then mean that structural encoding does not work on deeper models?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ba17e297-e254-519a-b329-6abf2c38dbe2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors design a shallow and wide model because all of their Transformer-based baselines set this configuration. The authors speculate that a wider model means that the dimensions of structured embedding vectors(edge embedding for Great, distance embedding for CodeTransformer) will also be wider, ensuring the representation capability of structured embedding vector. Does that then mean that structural encoding does not work on deeper models?", "reference_answer": "A: The authors re-design deeper TPTrans and vanilla Transformer and set 6 layers, 512 dims and 8 heads. For TPTrans, the input and output dim of GRUs are 64 and 32*2 ( *2 for bi-direction). The total parameters of the vanilla Transformer are about 49M (100M for the previous wider model). The total parameters of GRUs are 18816, and the proportion of the full model is 0.0387% (0.046% for the previous wider one). The authors repeat the experiment several times at Ruby dataset and get that:\n\n|Model|Ruby.P|Ruby.R|Ruby.F1|\n|----|----|----|----|\n|TPTrans|25.42 $\\pm$ 0.42|24.53 $\\pm$ 0.68|24.97 $\\pm$ 0.51|\n|Vanilla Transformer|21.06 $\\pm$ 0.72|19.47 $\\pm$ 0.32|20.23 $\\pm$ 0.50|\n\nFrom this table, the authors conclude that the proposed way of integrating path encodings still works for the deep and narrow model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "23b8cd28-8412-5de6-bfe2-659276c64eb1", "question": "What parts of the proposed method improve upon [A]? \nReference:\n[A] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. arXiv preprint arXiv:2012.09841, 2020.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What parts of the proposed method improve upon [A]? \nReference:\n[A] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. arXiv preprint arXiv:2012.09841, 2020.", "reference_answer": "A: Since recovering images from the discrete codebook still cause blur and artifacts in complex scenes, the authors just use discrete codebooks in masked regions as mentioned in Eq.4. For complex scenes in unmasked regions, they are recovered by encoder-decoder based CNN directly."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "39845aee-597b-5b01-bc33-d778d72ebd56", "question": "Could you elaborate more on the deficiency of the proposed method in the supervised setting (is this also true for different languages), and could that be alleviated with more unlabeled training data?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you elaborate more on the deficiency of the proposed method in the supervised setting (is this also true for different languages), and could that be alleviated with more unlabeled training data?", "reference_answer": "A: Supervised-SimCSE learns from the NLI datasets with positive (entailments) and hard-negative pairs (non-entailments). The proposed additional supervision from clustering on another modality does not come with such high quality pairs, i.e., the authors do not have hard negatives for these modalities. The mismatch in the form of losses in text and image/audio may partly explain why the improvements in supervised setting are smaller. In fact, such high quality pairs for text are also only available in English.  The authors could not evaluate supervised CSE in other languages, because the NLI dataset is not available in other languages. In fact, this is one limitation of supervised CSE, i.e., it relies on a high quality labeled dataset for training which is often not available in other languages. Since the authors think that the main problem is the lack of high quality positive / hard-negative pairs in other modalities the authors don’t believe that more unlabeled training data can further improve the supervised setting. Also, the authors find that the training of SimCSE and the proposed VisualCSE usually converge (based on validation loss) in a few thousands steps, meaning that having more unlabeled training data does not improve the performance of the proposed current framework. One possibility to leverage additional data is to increase the image batch size (then within the same number of steps, more images are leveraged). However, due to hardware limitations (GPU memory limitations), the authors set the batch size to be 48 in all the proposed experiments."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1776953e-d094-590a-b071-0c6f48565dea", "question": "What actual evidence did the authors provide to support the bold claim in Table 1 that FLO has low bias and moderate variance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["adb70bed-e3b8-5190-9fb4-e28959f2c15f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What actual evidence did the authors provide to support the bold claim in Table 1 that FLO has low bias and moderate variance?", "reference_answer": "A: The claim is based on empirical observations. For example in Figure 3, FLO is more accurate and less variable compared to other estimators. The authors also observe similar behaviors in other experiments. Prior variational MI estimation works have used numerical experiments as the authors show here to compare the variance, as there is no analytical framework that characterizes the variance theoretically. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3ac93faf-c3e0-5ee2-985f-a5d4ce3141c5", "question": "What is the split of the proposed dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the split of the proposed dataset?", "reference_answer": "A: There are a total of 8,657 images in the proposed dataset (each halftone dithered image has a corresponding continuous-tone image). The authors randomly divided around 10\\% of the images in the dataset as validation and test sets (400 images each, non-overlapping each other), and the remaining images were used as training sets (7,857 images)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2ab67345-1168-517f-9c24-ff8c62583a32", "question": "Will different encoders yield different results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["617d7416-f163-5ae1-a9e8-dbcf27170aa5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Will different encoders yield different results?", "reference_answer": "A: Different encoders in target models may yield different results. In the proposed work, the authors investigate image captioning models with two different encoders, respectively based on the structure of Resnet-152 and VGG-16. The results show that the image captioning models with Resnet encoder are slightly more vulnerable to the proposed attacks, where the attack success rate on the target model with Resnet encoder is 0.4%(in average) higher than the attack success rate on the target model with VGG encoder. The reason is perhaps, as the network structure of Resnet is deeper than VGG, the Resnet encoder may extract more representative features and thus benefits from the membership inference attack. However, the scope of the proposed current research focuses on the empirical study of membership inference attacks on multimodal models, but it is definitely worthy of diving into this area in the future.\n\nReferences:\n[r3] Investigating on Incorporating Pretrained and Learnable Speaker Representations for Multi-Speaker Multi-Style Text-to-Speech. ICASSP, 2021.\n[r4] The LJ Speech Dataset. https://keithito.com/LJ-Speech-Dataset/, 2017\n[r5] LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech. Interspeech, 2019\n[r6] RefineCap: Concept-Aware Refinement for Image Captioning. CoRR, 2021.\n[r7] Reflective Decoding Network for Image Captioning. ICCV, 2019.\n[r8] X-Linear Attention Networks for Image Captioning. CV"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9833db1e-9111-57a8-924f-72642d3651b2", "question": "How much computation time/computation resources does the proposed method need?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d7ce5f53-3667-5842-aebf-36f537f678f3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How much computation time/computation resources does the proposed method need?", "reference_answer": "A: The authors make a comparison with Neural-Pull, IGR, Point2mesh on the computational cost of optimizing for a single point cloud in the following table:\n\n|methods|Neural-Pull|IGR|Point2mesh|Ours|\n|:-:|:-:|:-:|:-:|:-:|\n|Time (s)|1150|1212|4028|**667**|\n|Memory (GB)|2.2|6.1|5.2|**2.0**|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "41731326-fabb-571c-8ed5-68a3a724421a", "question": "Whether such pruning will lead to load imblance between different CUDA threads and limit the speed up ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Whether such pruning will lead to load imblance between different CUDA threads and limit the speed up ?", "reference_answer": "A:  The calculation of Spconv is mainly divided into two parts (1) generating the index pair and (2) general matrix multiplication (GEMM). The authors analyze these two aspects separately:\n\nFirst of all, for the generation of index pairs, the authors implement it by constraining the output position based on the index mask. Specifically, the authors only need to pass the index mask into the kernel function as a parameter and use a rule to determine whether the original index pair satisfies the constraints of the index mask. The authors believe that this part does not account for a high proportion of the overall network inference time, as shown in the table in A2, the impact on CUDA threads thus can be ignored.\n\nSecondly, For GEMM, the implementation of spconv is calculated along the spatial dimensions of the kernel, eg. kernel size: 3x3x3. Different spatial locations are calculated at different iterations and will not affect each other. You might have the impression that there exists a large difference in terms of the number of points at different spatial locations, causing an imbalance in computation. However,  the authors argue that this again will not lead to load imbalance between CUDA threads because different spatial positions are mapped to independent GEMMs and each GEMM is performed in a dense manner."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b46aef70-e879-5485-9268-8ca33a151de4", "question": "Why not use dx=[f(x)+u1(x)]dt+[g(x)+u2(x)]dBt for efficient stabilization?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why not use dx=[f(x)+u1(x)]dt+[g(x)+u2(x)]dBt for efficient stabilization?", "reference_answer": "A: The authors provide a numerical experiments of different control combinations on the inverted pendulum in Appendix 7 and Figure 19.  The results imply that the proposed method can also be modified to find the deterministic control, and the introduction of stochastic control can decrease the energy cost and accelerate the stabilization process. The mixed control with both deterministic and stochastic terms is surely efficient, but here the authors focus on the stochastic term only to realize the stochastic stability because it is a novel perspective to regard the noise as a positive factor which is different from the existing methods."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "48da6813-ee91-5200-9353-590b94a323a2", "question": "Do we limit the model to only \"generate tokens that have not appeared in the previous context\"?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9c5d7aae-bbec-5e57-8114-8f24fbb8acf5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do we limit the model to only \"generate tokens that have not appeared in the previous context\"?", "reference_answer": "A: No, the proposed contrastive search is able to generate sequences containing a reasonable amount of repetitions, that are comparable to human-written texts, for high-frequency tokens as demonstrated in Table 1."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "de372e75-3be1-5bde-9785-268a4fcce073", "question": "Comparing with randomly sampled clusters in Figure 9 and random partition in Figure 13 and 14 are not reasonable as the random baselines apparently will not work well. To show the benefits of hierarchical balanced clustering, you may compare with hierarchical k-means without cluster size control.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["1d3fbc62-9a2b-564c-bbdc-716203954b0c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comparing with randomly sampled clusters in Figure 9 and random partition in Figure 13 and 14 are not reasonable as the random baselines apparently will not work well. To show the benefits of hierarchical balanced clustering, you may compare with hierarchical k-means without cluster size control.", "reference_answer": "A: Regarding \"Comparing with randomly sampled clusters in Figure 9\", the authors conducted the evaluation on the hierarchical k-means without cluster size control (HC) which is almost the same as randomly sampled centroids (we will add it into figure 9 in the final version). This might show that posting length balance and limitation is more important than center quality. For figure 13 and 14 in distributed setting, in the real business products (hundreds of billion scale), it is often required that all the partitions have roughly the same data size to maximize the resource utilization.  K-means without cluster size control usually cannot meet this requirement. \nThe authors have provided a strong baseline (balanced clustering) in figure 14 which just applied the balanced clustering technique without closure multi-cluster assignment and query-aware dynamic pruning. From figure 14, the authors can see that SPANN solution can further reduce 30% of the computation and IO cost as well as the query aggregation latency. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f1f7f594-21a0-5ac0-9275-aa8baad04117", "question": "In [R1], it is stated that attaching intermedia classifiers can affect the performance of the final predictors. Do similar phenomenons happen in the proposed method, Zero Time Waste (ZTW)? \nReference:\n[R1] Huang G, Chen D, Li T, et al. Multi-scale dense networks for resource efficient image classification. ICLR, 2018.\n", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["12a16837-7683-5205-a1fe-20f98adf46a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In [R1], it is stated that attaching intermedia classifiers can affect the performance of the final predictors. Do similar phenomenons happen in the proposed method, Zero Time Waste (ZTW)? \nReference:\n[R1] Huang G, Chen D, Li T, et al. Multi-scale dense networks for resource efficient image classification. ICLR, 2018.\n", "reference_answer": "A: The setting in [R1] optimizes the weights of the core network. The authors in this paper, however, freeze the pretrained model when training ICs (along with the BN statistics buffer, which is an easy thing to overlook). The final prediction of the original classifier is thus unaffected by the proposed ZTW method. Attaching additional ICs affects succeeding ICs because of cascading and ensembling, and as the authors show in C.1 the effect on performance is positive."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6a737442-e6f7-5b6f-8278-8a2a8a53926b", "question": "Does the proposed method improve sample efficiency during training on another task? Can the methods be used for more data efficient learning (for example in a classification task)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the proposed method improve sample efficiency during training on another task? Can the methods be used for more data efficient learning (for example in a classification task)?", "reference_answer": "A: The proposed model mainly focuses on the task of image generation. Like many other variants of GAN, the authors tend to lift the generation performance of GAN and obtain more realistic generation. The proposed methodology's novelty and promising results contribute to the community. Same thoughts may be able to be adapted to other tasks."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4eb2abb4-fc32-5f87-94b5-30f69d88b968", "question": "Discuss more on the limitations of potential bias in this dataset due to the smaller amount of objects. How were the objects selected?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e76cdd40-14a4-5e64-b8b2-c6871f4cf676"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss more on the limitations of potential bias in this dataset due to the smaller amount of objects. How were the objects selected?", "reference_answer": "A: The 100 objects were manually selected based on the following criteria. 1) They are common in the real world, including cars, street signs, etc; 2) They are easily recognizable by humans; and 3) They belong to the ImageNet classes such that the adopted visual recognition models (e.g., ResNet, ViT) can classify them from natural viewpoints with high accuracy.\nSince training NeRF for each object is computationally expensive as discussed in Appendix C.1, the number of objects in the dataset is relatively small. Thus the dataset does not contain all classes in ImageNet such as animals, which is a potential limitation of biased classes. In the revision, the authors make this clearer in Appendix B. Nevertheless, the authors think that the dataset is highly valuable for benchmarking the viewpoint robustness of visual recognition models, since it is important to understand model vulnerabilities to viewpoint changes in safety-critical applications while few efforts have been devoted to this area. It can also facilitate future research on improving viewpoint robustness. The authors will continuously enlarge the dataset in the future. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d396732e-27db-5698-be25-ee27fac03efe", "question": "What are the baseline model settings?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ce02ed28-899f-5f33-b3be-ee7d0889a007"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the baseline model settings?", "reference_answer": "A: The authors have updated the paper and the appendix to include a) what datasets are used for each baseline in Table 4, and b) the model architectures and the training objectives of those baselines in the Appendix D due to space limits. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c88a4cb7-342e-5692-8d44-3c57f44a0822", "question": "The amount of relevant/irrelevant features is unknown beforehand. In that case the proposed AUC-based method seems more adequate. Could you comment on this? Could you indicate how this size K is defined in practice? Is there a principled way to define it? What is the effect of this parameter on the performance of the proposed method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["42c36ca7-491d-543c-acb8-d9fa0c0493d2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The amount of relevant/irrelevant features is unknown beforehand. In that case the proposed AUC-based method seems more adequate. Could you comment on this? Could you indicate how this size K is defined in practice? Is there a principled way to define it? What is the effect of this parameter on the performance of the proposed method?", "reference_answer": "A: One general goal of feature-based explanations is to extract a \"compact\" set of relevant features for a given model prediction, since the most straightforward yet vacuous explanation is simply highlighting all features as relevant (which does not constitute a meaningful explanation). However, because the number of true relevant features is in general unknown beforehand (as Reviewer #1 notes), the predominant approach recent papers have considered is to output the top-K important features, for varying values for K. For example, in attribution methods such as Grad and IG, the authors could take the top-K features with the highest attribution scores. And K is usually set to varying values so that the authors generate relevant feature set explanations of different sizes. Similarly, in the proposed method, the authors allow users to set the value of K such that the proposed explanation could identify the top-K most important features to the prediction. In the proposed experiments, the authors vary the value of K such that the proposed explanations provide sets of relevant features of sizes 5%, 10%, ..., 50% of the total number of features. Then, for each of these relevant sets with differing sizes, the authors could apply the proposed evaluation criteria to evaluate their quality, which yields a single evaluation curve shown in Figure 1. \n\nSuch evaluation curves measure the quality of an explanation by considering differing sizes of relevant features, and the AUC then reflects the overall quality of the explanation. In the case where users have no knowledge about the number of relevant features, the proposed work thus suggests the use of AUC of the evaluation curve, which as the reviewer notes is indeed more than adequate as an evaluation. But to also provide a rationale for evaluations of differing values of K: it provides the quality of the relevant sets along multiple points on the evaluation curve, instead of a single numerical summary. And in some special use cases, the users might indeed be interested in a pre-defined size of relevant set, e.g. top-20% relevant features. But as the reviewer suggests, in the proposed work, the authors do recommend the use of AUC, which the authors also use to compare across different explanations, in addition to plotting the whole evaluation curves to illustrate the performances of different explanations at various sizes of relevant set."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "de37681f-d354-5ecd-9de0-ac49ad3f0f44", "question": "Regarding the generation of AR noise at the beginning inside the sliding window. Can you explain the subsequent steps?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["94275829-60ba-5423-9225-c41722c6d35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Regarding the generation of AR noise at the beginning inside the sliding window. Can you explain the subsequent steps?", "reference_answer": "A: Taking Figure 2.2 as an example, if the sliding window slides one step to the right, there is actually only one value (the next white grid cell) to be computed, $x_t$. Equation 5 is applied independently within every window. Put differently, for every window, the value $x_{t-8}$ is always at the top left corner of the window, the value $x_{t-6}$ is always the top right corner, etc. and $x_{t}$ is always the bottom right corner."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6ccbd519-7e1f-5884-9fc1-2cb28e465011", "question": "How can we verify the necessity of the network components? For example, what if we do not use the style embeddings but learn the cluster centers for identity features f_i themselves? What if we do not use the Aggregation Network and simply average the clustered features in F'?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5f4cbaf8-378f-55d3-a89e-4b2493de0fd6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How can we verify the necessity of the network components? For example, what if we do not use the style embeddings but learn the cluster centers for identity features f_i themselves? What if we do not use the Aggregation Network and simply average the clustered features in F'?", "reference_answer": "A: Here is the performance as measured in Tab.1 of the main paper. \n\n| |IJB-B TAR@FAR=1e-3|IJB-B TAR@FAR=1e-4|IJBS(avg)| \n|-|-|-|-|\n|without SIM (only $f$),  with AGN |$96.32$|$94.54$|$53.98$ |  \n|with SIM, without AGN  |$96.04$|$94.25$|$53.87$|\n|with SIM, with AGN     |$96.91$|$95.53$|$57.55$| \n\n1. __Train $f$ (without style) as an input to the Clustering Network.__\nA: As the comparison with the 1st and the 3rd row shows, style input is more effective in feature fusion. The authors explain that clustering using the learned center is made difficult with $f$ the identity feature. It lacks the quality information and characteristics that can be grouped irrespective of the identity. Therefore, SIM is crucial to feature fusion. \n\n2. __Replace AGN with a simple average.__\nA: As the comparison with the 2nd and the 3rd row shows, the role of AGN is also important. It is because the learned centers vary in their respective roles and one of the centers works as a place for bad-quality images (as shown in Fig.5). Therefore, a separate module that considers the importance of each cluster is necessary. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f271f4cc-7411-5437-bb43-1f0384e42975", "question": "Why didn't CogView2 show significant improvements over previous methods on FID-0?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9614a57f-96f6-5e08-8978-97dc94a17579"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why didn't CogView2 show significant improvements over previous methods on FID-0?", "reference_answer": "A: First, the authors didn't claim CogView2 achieve better performance than DALL-E2, while instead the authors analyze the difference in section 6.\n\nSecondly, as the authors stressed in Line 270, **the authors need to downsample the images back to 256*256** for a meaningful FID comparison, which largely reduces the usage of the proposed super-resolution method.\n\nThirdly, FID itself is not a stable metric. According to https://www.cs.cmu.edu/~clean-fid/, even jpeg quality 75/100 can create an up to 20 FID difference. The authors also find whether center-crop COCO images create a >4 FID difference on this benchmark. The authors care more about human evaluation performance, where CogView2 outperforms CogView, LAFITE et al. by a large margin. However, many text-to-image models are not open-source, so that the authors cannot include them in the evaluation. This also suggests the value of open-sourcing of CogView2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a9e7a6c8-aaf0-5602-819b-ea50f38b8155", "question": "It is indeed novel to apply multi-exit networks to RL. However, in Figure 3, it seems that individually ICs outperform performing early exiting in many cases?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["12a16837-7683-5205-a1fe-20f98adf46a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is indeed novel to apply multi-exit networks to RL. However, in Figure 3, it seems that individually ICs outperform performing early exiting in many cases?", "reference_answer": "A: In the case of the reinforcement learning experiment, the authors used only two ICs because the architecture was much smaller than in the case of image data. In consequence, one can expect that the gain of using ensemble-like approaches should be lower. However, in the case of Qbert-v0, the authors save around 45% of computations without score degradation. For Pong the authors can get a 60% reduction with a minor impact on performance. As noticed by the Reviewer, the use of a single IC alone gives better results on average in some cases. Note, however, that the standard deviations in this environment are larger, and so the difference may not be significant. The authors present results for more environments in appendix B.3.\n\n[1] Li, H., Zhang, H., Qi, X., Yang, R., & Huang, G. (2019). Improved techniques for training adaptive deep networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1891-1900).\n[2] Kaya, Yigitcan, Sanghyun Hong, and Tudor Dumitras. \"Shallow-deep networks: Understanding and mitigating network overthinking.\" International Conference on Machine Learning. PMLR, 2019.\n[3] Scardapane, Simone, et al. \"Why should the authors add early exits to neural networks?.\" Cognitive Computation 12.5 (2020): 954-966."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e1a11366-20d2-5699-b92c-b2c874cbfa03", "question": "The purpose of Sec. 4.4, and why the consistency branch is necessary to improve the synthesis quality?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The purpose of Sec. 4.4, and why the consistency branch is necessary to improve the synthesis quality?", "reference_answer": "A: The proposed key idea is the proposal of a new paradigm for 3D-aware image synthesis, which *makes the discriminator 3D-aware as well* to compete with the 3D-aware generator. Considering that the 3D evaluation mainly falls into two folds, *i.e.*, the geometry quality and the 3D consistency. Adding a geometry branch is one instantiation of the proposed idea, which can provide explicit supervision on the generator to improve the quality of the underlying geometry. In Sec. 4.4, the authors would like to show that the proposed framework can also be used to improve the multi-view consistency, by simply incorporating a *consistency branch*. The authors add this extension to show the generalizability of the proposed framework.\n\nThe newly introduced consistency branch, together with the novel view synthesis task, does not necessarily improve the synthesis quality. Instead, it helps improve the property of multi-view consistency. Concretely, although the generative neural radiance field is primarily designed for 3D-aware image synthesis, recent works (like StyleNeRF [9] and VolumeGAN [34]) introduce CNN on top of NeRF to allow high-resolution image synthesis. The CNN is performed in the 2D space, and hence fail to guarantee the 3D property. Under such a case, the proposed consistency branch could help improve the cross-view consistency, as shown in Tab. 3 in the submission."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "54376011-5a98-5a3a-aa12-6b49380cf554", "question": "What is the intuition behind adding noise as a fairness trigger, such as in patch trigger and border trigger? Does this mean demographic information is confined either in the border of the image or in a specific area of an image covered by the patch?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["1fc786d5-cfc8-580e-b579-c2c56e8dba87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the intuition behind adding noise as a fairness trigger, such as in patch trigger and border trigger? Does this mean demographic information is confined either in the border of the image or in a specific area of an image covered by the patch?", "reference_answer": "A: When an image is appended with the fairness trigger, there will be two types of demographic cues. First, the original, true demographic cues that reside in the original image; second, the false demographic cues that reside in the trigger in the border/patch. The two cues can coexist and the false cues do not need to overlie the true cues. The key is that the false cues need to be strong enough so that the neural model, when presented with the two potentially conflicting cues, will go for the false one. This is entirely possible because the neural model has not seen the fairness trigger before so it cannot learn to ignore it. This intuition is also supported by the proposed empirical analysis in Table 3, where the trigger is found to contain strong demographic cues. The authors will move Table 3 to the main paper and improve the clarity of the theoretical analysis sections."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "66d40eb8-b546-5fe2-9066-ed8cfaa733e4", "question": "Provide evaluation on the FashionMNIST and partial MNIST dataset as well as the stacked-MNIST dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["db8f6428-8da5-5415-8f58-ce4bc7a45df3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide evaluation on the FashionMNIST and partial MNIST dataset as well as the stacked-MNIST dataset?", "reference_answer": "A: The authors provide further evaluation on the mentioned two datasets, see Table 9\\&10 for quantitative results and Fig. 8 for qualitative results in the proposed revised supplementary. Similar to datasets that provide class labels such as MNIST, FashionMNIST and CIFAR, the mentioned two datasets have multiple discrete modes with each mode corresponding to one class. As the authors mentioned in Line 147-176 of supplementary, the authors adopt a conditional generation setting (ie, using $g\\left(z;y\\right)$ to generate an image, where $g$ is the generator, and $z$ and $y$ are the latent code and the class label, respectively) for datasets that provide class labels, because different classes (modes) correspond to different disjoint submanifolds, and the union of all the disjoint submanifolds cannot be homeomorphic to an continuous Euclidean latent space. Therefore, under the conditional generation setting $g\\left(z;y\\right)$, ideally, the authors can cover all the discrete modes by traversing all the class labels $y$ for $g\\left(z;y\\right)$. In the proposed experiments, for the model trained on each dataset, the authors first randomly sample 10000 class labels $y^{\\left(i\\right)}$ and latent codes $z^{\\left(i\\right)}$, then obtain generated samples $\\left\\\\{x^{\\left(i\\right)}=g\\left(z^{\\left(i\\right)};y^{\\left(i\\right)}\\right)\\right\\\\}_{i=1}^{10000}$ for evaluation. The proposed model can cover all 11 modes of the FashionMNIST and partial MNIST dataset and most of the 1000 modes of the stacked-MNIST dataset."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1937bf65-5098-5de2-9b71-eb9a45ebcd56", "question": "Have you considered tasks other than image classification?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Have you considered tasks other than image classification?", "reference_answer": "A: Yes, the authors conducted zero-shot and fine-tuned cross-modal retrieval experiments on the standard Flickr30K 1K and MSCOCO 5K test set."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "84a7f307-43ad-5035-ad4b-a1d8edfa01b4", "question": "In Table 1, why are the results of ReWatt better than RL-S2V? Since there are more constraints (i.e., smaller action space) in ReWatt than RL-S2V, RL-S2V could be easier to fool GCNs. Could you explain more about the results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6962924b-a586-5127-9274-5f2b61990acb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1, why are the results of ReWatt better than RL-S2V? Since there are more constraints (i.e., smaller action space) in ReWatt than RL-S2V, RL-S2V could be easier to fool GCNs. Could you explain more about the results?", "reference_answer": "A: The authors agree that RL-S2V has a larger action space, which means the optimal solution it can achieve is as good or better than the one the proposed method can find. However, both methods are not guaranteed to always find the optimal solution in the given action space. The authors list some potential reasons to explain why ReWatt can outperform RL-S2V as follows:\n1) When performing an adding/deleting edge action in RL-S2V, it chooses two nodes sequentially. Then it decides to add an edge between two nodes if they are not connected, otherwise, the edge between them is removed. Since most graphs are very sparse, the RL-S2V algorithm is, by design, biased to adding an edge. On the other hand, ReWatt removes an edge and then add another edge. The adding/deleting edge operations are more balanced. \n2) The reward design in ReWatt is different from RL-S2V. In RL-S2V, a non-zero reward is only given at the end of an attacking session. Specifically, at the end of an attacking session, a positive reward of $1$ is given if the attack succeeded, otherwise a negative reward $-1$ is given. All the intermediate steps get $0$ reward. In ReWatt, the reward is given after each action. A positive reward is given once an action leads to a successful attack. A negative reward is penalized to take each action if it does not directly lead to a successful attack, which encourages the attacker to make as few actions as possible. Furthermore, the authors also proposed an adaptive negative reward design, which determines the value of the negative reward according to the size of each graph. In fact, the design of this adaptive negative reward has shown to be very effective and important to the ReWatt framework. As shown in Table 1, ReWatt-n (which is a variant of ReWatt without the adaptive negative reward design) performs much worse than ReWatt. Specifically, if the authors apply ReWatt-n in the same setting of RL-S2V (with fixed actions), its performance is not as good as RL-S2V in REDDIT-MULTI-12K and REDDIT-MULTI-5K datasets. The performance of ReWatt-n on REDDIT-MULTI-12K is [11.26%; 14.7%; 18.02] while RL-S2V achieves [9.46; 18.5% 21.1%]. On the REDDIT-MULTI-5K, the performance of ReWatt-n is [4.49%; 5.62%; 6.74%] while RL-S2V archives [4.49%; 16.9%; 18.0%]. Hence, the design of the proposed adaptive negative reward could be an important reason why ReWatt can perform better than RL-S2V."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f8129119-877c-59ee-beab-9240b457b97d", "question": "Why is G-ESTT considered a meaningful baseline to compare to?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b16dcd7f-b871-5927-bf5b-b6fba78c46ee"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is G-ESTT considered a meaningful baseline to compare to?", "reference_answer": "A: A big advantage of G-ESTT over G-ESTS and all other explore-then-commit-type algorithms is that it could reuse the arms and response in the exploration stage again in Stage 2 (line 195). This helps obtain a consistent and promising estimate at the very beginning of Stage 2, while other explore-then-commit-type algorithms still need some warmup when switching to Stage 2. This advantage is also validated in the experiments in the paper. Specifically, from Figure 1 (c),(d) and Figure 2 (c), (d) (Appendix I.2) the authors can see that G-ESTT could yield more robust performance when switching to Stage 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "997fc57f-0932-5b85-8e52-582d46a04453", "question": "To what extent do datasets in machine learning really have a fractal structure? I feel like this paper is somewhat split between completely artificial examples (like the snowflake made of snowflake images) and natural image datasets where the existence of fractal structure is non-obvious. It would be nice if there were more datasets with a fractal structure which is somewhat organic. Perhaps satellite images of weather patterns?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8c872a12-f8c8-5947-8865-90659c926e57"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "To what extent do datasets in machine learning really have a fractal structure? I feel like this paper is somewhat split between completely artificial examples (like the snowflake made of snowflake images) and natural image datasets where the existence of fractal structure is non-obvious. It would be nice if there were more datasets with a fractal structure which is somewhat organic. Perhaps satellite images of weather patterns?", "reference_answer": "A: Great suggestion! In fact, the proposed original application for this work was aerial imagery of farmlands or other locations with repeated patterns - also the reason behind the proposed choice to utilize high-resolution aerial images for the compression task with Neural Collages in Section 4.2. \nthe authors would like to highlight a subtle point about self-similarity. Consider for example a simulated landscape generated using a fractal algorithm (or even a \"classical\" fractal such as the Mandelbrot set), and then images of it taken from different perspectives. \"Fractalness\" of the object does not necessarily imply that the resulting perspectives would be well-suited to self-similarity-based methods such as Neural Collages. In particular, Neural Collages exploit self-similarity at a tile level (in pixel space!), which can be present even when the object in the image is not fractal. The authors strongly agree that there are many stimulating questions at the intersection of self-similarity methods and deep learning, certainly far too many to answer in a single paper. The proposed goal with this work and method is to provide compelling evidence that Neural Collages and self-similarity can have impact on various deep learning applications - the authors hope to have convinced you of the same!"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5ac67647-d9aa-5391-a838-6bdb002ac2f0", "question": "What do the upper and lower boundaries in the figures mean? Is it the standard deviation? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ca0919cf-d751-5b4e-ba01-39f71c9c35d2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What do the upper and lower boundaries in the figures mean? Is it the standard deviation? ", "reference_answer": "A: No, it denotes the confidence intervals."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6365697d-3a4c-5771-ae4c-69a232ecccbf", "question": "Why is the q function q_w cannot be simply replaced by the logit function f_w?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the q function q_w cannot be simply replaced by the logit function f_w?", "reference_answer": "A: The Bellman equation indeed draws the connection between the reward and the q-value function. \n\nIn the RL setting, the reward is given, so the authors can derive the optimal q-value function by the Bellman optimality equation. \n\nIn the IRL setting, the reward is not given. Therefore, it is possible to freely optimize the q-value function, and then use the Bellman optimality equation to derive the underlying reward. The proposed contribution lies in deriving the q-value function from the policy (based on a common assumption [1, 2, 3] stated in Assumption 1) and then deriving the reward function for text generation, following the IRL setting."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "64d5ce48-fcf5-5ae2-990a-347962830f0e", "question": "Processing point clouds (or point sets) with MLPs has a few drawbacks that are already well-known, such as imposing an ordering of the points and requiring a fixed cardinality for the point set. While the distance function used in Eq. 4 allows point sets of different sizes, the MLP inputs point clouds of fixed size (set as 2048 in the experiments). Briefly discuss how to take point clouds of differing sizes to compute the scene flow would be useful -- especially for the case when computing over the sequence of M point sets.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["55b8b220-92ee-568e-ae6f-add043bfce90"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Processing point clouds (or point sets) with MLPs has a few drawbacks that are already well-known, such as imposing an ordering of the points and requiring a fixed cardinality for the point set. While the distance function used in Eq. 4 allows point sets of different sizes, the MLP inputs point clouds of fixed size (set as 2048 in the experiments). Briefly discuss how to take point clouds of differing sizes to compute the scene flow would be useful -- especially for the case when computing over the sequence of M point sets.", "reference_answer": "A: Interesting observation. In the experiments in Table 1, the input pair of point clouds have the same number of points (randomly sampled during data loading). In the experiments of section 4.3 (estimating scene flow from large point clouds), the number of points in the input pair is arbitrarily different as the authors used the full raw lidar point clouds. The proposed method runs an optimization per scene, and the MLP regularizer is automatically instantiated at the beginning. Please keep in mind that a single point is an input to the MLP. Therefore, the proposed method can naturally accommodate input pairs of point clouds with different sizes.\nIf one would use the proposed objective function to train a self-supervised model, then having equal size point clouds would be more practical for batch learning. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0c9db003-8414-5525-b850-7a2591f4548e", "question": "Is the proposed method only applicable to computer vision tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["94275829-60ba-5423-9225-c41722c6d35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the proposed method only applicable to computer vision tasks?", "reference_answer": "A: The authors only develop perturbations for images, but an AR perturbation can be crafted for any continuous signal. The authors speculate that the proposed method could work for audio classification as well."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ddb45257-679a-5c70-a805-50095ce373a1", "question": "Why do you think positions can be predicted in a NAR manner? Isn't it just shifting the burdens to the position predictor? (Since in transformers if it's able to learn positions then it should be trivial to reorder based on those positions)", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2a7b14b6-33c1-505a-8a3a-d9580ad76ca7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do you think positions can be predicted in a NAR manner? Isn't it just shifting the burdens to the position predictor? (Since in transformers if it's able to learn positions then it should be trivial to reorder based on those positions)", "reference_answer": "A: It is motivated by the practice of learning syntax position (also called syntax distance). Shen et al[2] have shown that syntax position of words in a sentence could be predicted by neural networks in a non-autoregressive fashion, which even obtained top parsing accuracy among strong parser baselines. The authors transfer this scenario to the position predictions in NATs. As shown in Table 3, experiments have shown that the results predicted using the NAR manner are not good enough (20.81 BLEU), which may validate your point of view. Despite this, it also achieved performance beyond the baseline (16.71 BLEU), so the authors believe that it is still valuable for exploration.\n[1] Ma X, Zhou C, Li X, et al. FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow. EMNLP 2019.\n[2] Shen Y, Lin Z, Jacob A P, et al. Straight to the tree: Constituency parsing with neural syntactic distance. ACL 2018."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f272d9e2-14b7-5397-ab2c-f2b4e4051f39", "question": "Does the variance of different runs have an impact on the validation of the proposed theory?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2e95e384-0486-5889-9b83-6dd486270ed6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the variance of different runs have an impact on the validation of the proposed theory?", "reference_answer": "A: The variance of different runs comes from the stochastic gradient in each iteration (equations (3) and (4)). The proposed proposed theory already takes the variance into account. Specifically, under the variance bounded assumption (Assumption 4), the authors show that the potential function is monotonically decreasing (Lemma 1). Based on this, the authors prove the convergence of the proposed alternating stochastic gradient descent ascent algorithm. Moreover, the proposed experiment validates the proposed computational theory. The plotted curves in Figure 1 are average reward obtained by multiple independent evaluations of the learned policy in the environment. The authors see that the plotted curves are well concentrated around its average performance, despite the variation in each trajectory. After sufficiently many iterations, the average reward converges, which corroborates Theorem 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c313c064-2204-5244-b5b4-1b6f92234741", "question": "Why is 2.4 min the success criteria for solving a maze?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f9de7728-3595-5e1b-b94f-4a2a9933b230"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is 2.4 min the success criteria for solving a maze?", "reference_answer": "A: There has to be some upper limit on the duration of a navigation trial, and the authors chose 5000 simulation steps in this work. Plots in Figure 5 show the success rate as a function of episode duration for durations less than this maximum threshold."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "536fb566-ff3d-52b5-a976-a6d61bcbe550", "question": "What is the reason the filter bank strategy performs better than convolution filter?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e070ad3b-180d-5e8b-a978-d27d1aeaec36"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the reason the filter bank strategy performs better than convolution filter?", "reference_answer": "A: Compared to the normal feature maps, self-attention maps in transformers vary across different images and heads, containing much more high frequency parts. The authors evaluate the variance of input features versus their attention maps. variance: $0.14$ vs $0.39$. You can see that attention representations have a much higher variance. Meanwhile, different attention heads may contain totally different regional information. It’s extremely difficult to learn the weights spatial convolution and hard to optimize. In contrast, the low-rank constraints on the generated filters provide smoothed candidates of these high-frequency attention maps. Then it’s much easier for the spatially-variant coefficient estimator to learn the combination weights. From the proposed perspective, the aliasing reduction on attention maps requires more redundant smoothness choices, while it’s easy to only obtain non-sense features and poor convergences using only convolution. That’s the reason why the filter bank performs better."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9055a0a6-80ab-5c13-b43b-f6ea4e45d929", "question": "Why are the NL-CG method starting at higher position than the other methods in figure 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["fb775f36-5ad4-5b12-9a52-f3e4a657b8d7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are the NL-CG method starting at higher position than the other methods in figure 2?", "reference_answer": "A: The first point is the performance after training with around 20K samples. NL-CG can already learn something using these samples. The authors further show results on the predator-prey task in the revised paper. Similarly, the proposed method requires very few (20K-30K) samples to achieve DCG's performance after converges."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bed469e3-e919-5d35-a914-36810744606f", "question": "Why is it that the proposed method appears to considerably underperform prior work with RCM (Wang et al, 2019, Jain et al, 2019, Ilharco et al, 2019)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is it that the proposed method appears to considerably underperform prior work with RCM (Wang et al, 2019, Jain et al, 2019, Ilharco et al, 2019)?", "reference_answer": "Our proposed method can effectively improve the local cross-modal alignment ability and outperform prior works [3, 4, 5]. All these works only reported their results on a different dataset (R2R). On Landmark-RxR and en-RxR datasets, the authors have reproduced the goal-oriented reward [5] and fidelity-oriented reward [4], and compared the proposed method with them under a fair setting. As illustrated in Table 3, the proposed soft focal-oriented reward (model#17) outperforms the goal-oriented reward with 1.0% dropped on Loss Number and 1.0% improved on SR, and outperforms the fidelity-oriented reward with 0.7% dropped on Loss Number. The proposed hard focal-oriented reward outperforms the goal-oriented reward with 2.1% dropped on Loss Number and 2.2% improved on SR and outperforms the fidelity-oriented reward with 1.8% dropped on Loss Number and 1.2% improved on SR. The significant drop on the Loss Number metric indicates that the focal-oriented rewards make the model have a better local cross-modal alignment ability and make fewer mistakes during navigation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "42848fa4-3ba0-5576-8327-08ae7ce9a245", "question": "List where all in the paper can we find discussion and comparisons with respect to the prior work.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "List where all in the paper can we find discussion and comparisons with respect to the prior work.", "reference_answer": "A:  1. As mentioned in the Introduction and Related Work section, the authors discuss prior works about cross-modal alignment, reward shaping and their disadvantages.  \n2. As mentioned in Section 3.2 and Part C in the supplementary material, the authors compare the proposed Landmark-RxR with coarse-grained and fine-grained datasets.  \n3. As mentioned on Lines 96-97, the authors point out that the proposed work focuses on using fine-grained supervision to benefit the cross-modal alignment ability.    \n4. As mentioned in Section 6.2, the authors compare the proposed focal-oriented rewards with the goal-oriented and fidelity-oriented rewards in Table 3 and analyze the results.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "af505be1-e542-5554-86ec-3b71b5eea476", "question": "For training, do you need the data to be arranged in certain task-specific order for it to generalize to the tasks in few-shot manner during evaluation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f9c136b6-1c41-543a-ad92-b5563b5e797e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For training, do you need the data to be arranged in certain task-specific order for it to generalize to the tasks in few-shot manner during evaluation?", "reference_answer": "A: If the question is about the order in which the authors provide the different datasets at training time (M3W, ITP and VTP), the answer is no as the authors accumulate gradients over all tasks. It's effectively equivalent to building a single batch containing a fixed number of elements from each task at each training step.\n\nIf the question is more about the order of images in the interleaved samples from M3W, then no specific effort is made to make it better for downstream tasks: the authors simply order the images and the text as they appear in the webpage (see Appendix A.3.2 for details)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b2b02626-e7bb-5931-8ad1-f27309f0260b", "question": "Only evaluated on traditional ViTs, how about more recent ViTs? Will more intense use of convolution layers alleviate such overfitting issues mentioned?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["13b7a9cd-5011-548f-8c52-2c24218ee25a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Only evaluated on traditional ViTs, how about more recent ViTs? Will more intense use of convolution layers alleviate such overfitting issues mentioned?", "reference_answer": "A: The proposed work mainly focuses on the traditional ViTs as this is the newest building block for image models and has been comparatively studied much less than CNNs. It would be an interesting future research direction to study if other ViT-based architectures share similar problems. Intuitively, the authors do not think that more convolutional layers can alleviate the problem because the authors empirically observe a hybrid architecture (including both convolutional layers as well as ViT) suffer from the same problem."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a57bafdf-6f19-52df-9876-48bb93b6db48", "question": "Can we add more data to the pre-training dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can we add more data to the pre-training dataset?", "reference_answer": "A: Yes, the authors can add 1M image-text pairs from SBU dataset or even 12M data from CC12M."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "362399a8-7c93-509a-8581-b8ab1eb06ca6", "question": "Why do you think TD3+BC seems to be better for expert-level demonstrations (for most tasks)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["52ef3784-d176-5ba3-9e33-ca99a2dd9b01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do you think TD3+BC seems to be better for expert-level demonstrations (for most tasks)?", "reference_answer": "A: The authors summarize the performance comparison of the proposed MCQ against TD3+BC on *medium-expert* and *expert* datasets in Table 2. The authors find that MCQ is actually competitive to TD3+BC on most of the datasets that contain expert demonstrations. MCQ achieves the better average score on 3 out of 6 datasets, and is also better in terms of the mean score. TD3+BC behaves naturally well on expert-level datasets with the aid of the behavior cloning (BC) term (BC itself can behave well on expert datasets). While MCQ can achieve competitive performance against TD3+BC by properly training OOD actions."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ba9e445-c7cd-535b-9668-11854b5376c3", "question": "How is 2D convolution used as encoders in the voxel space?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2cb98599-ddd1-5ea4-ba9d-a65c36c82757"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is 2D convolution used as encoders in the voxel space?", "reference_answer": "A: For the Conv2D setting in Table 2 of the main paper, the authors process each layer of the voxel space along the axis $Z$ using 2D convolution."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "34703006-c034-5af9-95bf-8cba0a360c39", "question": "Present the results of ablation of L_C in terms of FID, MSE and Accuracy from the paper.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9b2e64f8-a989-5565-91f2-58280f79add6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Present the results of ablation of L_C in terms of FID, MSE and Accuracy from the paper.", "reference_answer": "A: The authors performed an additional ablation study on this topic, where the authors train a D2 model (without the contrastive learning component), which applies a diffusion model over the latent variables. The experiment is identical to what Reviewer 1 asked for (although their focus is over the generative performance of the D2 model).\n\nThis table shows the FID score of the generated images with a different number of diffusion steps.\n\n|        |       | CIFAR10 |       |       | CIFAR100 |        |\n|-------|-------|-------|-------|-------|-------|-------|\n| Steps | 10    | 50  | 100   | 10    | 50       | 100   |\n| D2      | 22.3 | 15.8 | 15.1  | 28.35 | 19.81    | 19.85 |\n| D2C   | 17.71 | 10.11| 10.15 | 23.16 | 14.62    | 14.46 |\n\nThis table shows the MSE, FID and latent representation accuracy comparisons between D2, D2C and NVAE.\n\n|        |       | CIFAR10 |       |       | CIFAR100 |       |\n|-------|-------|---------|-------|-------|----------|-------|\n|        | FID   | MSE     | Acc   | FID   | MSE      | Acc   |\n| D2    | 15.1  | 0.24    | 40.6  | 19.85 | 0.48     | 17.89 |\n| D2C   | 10.15 | 0.76    | 76.02 | 14.62 | 0.44     | 42.75 |\n| NVAE  | 36.4  | 0.25    | 18.8  | 42.5  | 0.53     | 4.1   |\n\nCompared with the performance of NVAE (36.4 on CIFAR10 and 42.5 on CIFAR100), even D2 is significantly better. Additionally, D2C is even better than D2 in terms of unconditional generation performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a997e504-175e-58aa-9e64-ceee986200d0", "question": "What is the number of real/fake samples for FID score in Table 4?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the number of real/fake samples for FID score in Table 4?", "reference_answer": "A: Evaluation metrics are calculated over 50k real samples and 50k fake samples. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7cf5f072-536a-5058-b866-97dc3eec13a1", "question": "How were the perceptron learning experiments conducted?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7de55424-0d66-5dfd-9b68-f611c6c28c8c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How were the perceptron learning experiments conducted?", "reference_answer": "A: The authors have added a detailed description of all perceptron learning experiments in Appendix B. Furthermore, the updated manuscript contains a link to a google Colab with code to reproduce all of the theory figures and associated perceptron learning experiments throughout the paper (https://colab.research.google.com/drive/1in35C6jh7y_ynwuWLBmGOWAgmUgpl8dF?usp=sharing)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e1c86bc1-d3b1-51a7-b92f-1825d7cc88a0", "question": "Does it mean the decoder is the main component of DDSP?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["64d8ae22-e06f-5a32-92a0-40150b3dbb8e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does it mean the decoder is the main component of DDSP?", "reference_answer": "A: The main components of DDSP are the differentiable signal processing components controlled by decoder outputs, used in all models (the yellow components in Figure 2). As shown in Figure 2, the authors denote the decoder as the neural network that controls these components. The authors draw this distinction to highlight that the DDSP components are agnostic to model architectures and loss function (spectral, adversarial, waveform), as long as they provide the appropriate control signals. For clarity, the authors note that prior to this work, such digital signal processing components have not been implemented in a differentiable form, and could not be trained end-to-end in the manner described here."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "63bd2a47-18d2-5360-b31e-adff883f0d25", "question": "What is the optimization process of the mixed-precision? I have a little confusion about the Eq.(18) and what is the meaning of the Y in Eq.(18)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["8dedd2af-a387-54a3-a611-0b69c6963d04"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the optimization process of the mixed-precision? I have a little confusion about the Eq.(18) and what is the meaning of the Y in Eq.(18)?", "reference_answer": "A: There are lots of candidate bit-width configurations in the search space, so the authors utilize a Pareto Frontier approach to find the bit-width configuration with the minimal $\\Omega$ as described in the following equation (Eq.(18) in the paper).\n$\\Omega = \\sum_{i=1}^L \\Omega_i=\\sum_{i=1}^L \\sum_{j=1}^{m}\\sigma_j(\\textbf{Y})\\cdot\\|\\widehat{\\textbf{Y}}-\\textbf{Y}\\|^2_2.$\nwhere L is the number of layers and m is the number of singular values. \nThe Y in Eq.(18) represents the output feature of MLP module and attention map for the MSA module."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fcdb7099-b16c-523f-9cdd-19ead14ca2ef", "question": "How is the camera pose estimated from the input image? TODO: Add the ref in answer.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4cbd7cbb-7b71-57e9-9b12-e7a19df3dff5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is the camera pose estimated from the input image? TODO: Add the ref in answer.", "reference_answer": "A: Other mainstream NeRF-style methods [NeRF 2020] were followed to estimate the pose using COLMAP."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1c218f45-2ac5-551c-be8d-2c423cfd9d1a", "question": "Should the experiments on CIFAR be compared to fully data-augmented CNNs?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f000c838-1733-5b65-b8ff-3ec832a960a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Should the experiments on CIFAR be compared to fully data-augmented CNNs?", "reference_answer": "A: The proposed motivation for reporting the results on CIFAR without data augmentation in the proposed work is to highlight the data efficiency property of the proposed model.  \nthe authors conduct additional experiments on CIFAR with data augmentation and report the results in Table I. As the authors can see, under the setting using data augmentation, $E^4$-Net still significantly outperform G-CNNs."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4418751b-374f-5e4c-a76a-0c51f7972747", "question": "Comment on the practical usage of the method, if it cannot deal with general observation operators or not robust to model mismatch.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4899724f-eebf-5cd2-8f14-5e4cad622d8a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comment on the practical usage of the method, if it cannot deal with general observation operators or not robust to model mismatch.", "reference_answer": "A: Actually, the model can be adapted to general observation operators and model mismatch is not a concern for the setting described in the paper. Furthermore, the parameter estimation problem as the authors have formulated, is widely used in a diverse collection of settings in the physical sciences."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9095f1a9-daf0-5938-89ed-869fe6adf202", "question": "Where the data to be used in D_{unlabel} comes from (L152-166)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb1f82ae-09df-5174-adbf-157ba87a8d01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where the data to be used in D_{unlabel} comes from (L152-166)?", "reference_answer": "A: As mentioned in L244-245 under the experimental setup section of the proposed work, the authors use MS COCO and Open Images datasets as D_{unlabel}. The authors deferred the mention of the exact datasets for D_{unlabel} to the experimental setup section to keep the descriptions of the proposed approach general."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "37f5ec9c-c1b1-5929-b82f-8c0a554ac268", "question": "What are some other datasets that are more challenging and cleaner?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are some other datasets that are more challenging and cleaner?", "reference_answer": "A: Caltech101, CIFAR10, CIFAR100, Flowers102, Food101, ImageNet1K, OxfordIIITPet, StanfordCars."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "12b05dd3-2690-5e2c-8cad-ea7f14cbc244", "question": "How is FRePo's cross-architecture performance when trained on Conv-IN like all the other methods in Table 2 (aside from KIP)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is FRePo's cross-architecture performance when trained on Conv-IN like all the other methods in Table 2 (aside from KIP)?", "reference_answer": "A: As shown in the Table below or Table 15 in Appendix C.6, the distilled data trained with Conv-IN transfers less well to architectures than the distilled data trained by Conv-BN. However, the distilled data generated by FRePo (Conv-IN or Conv-BN) still outperforms the previous methods on ResNet, VGG, and AlexNet.\n\n| | | Conv | Conv-NN | ResNet-DN | ResNet-BN | VGG-BN | AlexNet |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| DSA | Conv-IN | 53.2+-0.8 | 36.4+-1.5 | 42.1+-0.7 | 34.1+-1.4 | 46.3+-1.3 | 34.0+-2.3 |\n| DM | Conv-IN | 49.2+-0.8 | 35.2+-0.5 | 36.8+-1.2 | 35.5+-1.3 | 41.2+-1.8 | 34.9+-1.1 |\n| MTT | Conv-IN | 64.4+-0.9 | 41.6+-1.3 | 49.2+-1.1 | 42.9+-1.5 | 46.6+-2.0 | 34.2+-2.6 |\n| KIP | Conv-NTK | 62.7+-0.3 | 58.2+-0.4 | 49.0+-1.2 | 45.8+-1.4 | 30.1+-1.5 | 57.2+-0.4 |\n| FRePo | Conv-IN | 59.2+-0.3 | 56.2+-0.2 | 51.1+-0.8 | 50.8+-0.2 | 51.8+-0.3 | 55.3+-0.8 |\n| FRePo | Conv-BN | 65.5+-0.4 | 65.5+-0.4 | 58.1+-0.6 | 57.7+-0.7 | 59.4+-0.7 | 61.9+-0.7 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9448e302-48a4-5311-bc4a-8539fbc18d22", "question": "In the ablation it appears that without human controller input the model learns slower, but converges to a similar level of performance. Some discussion on this would be useful to better understand the impact of including the additional information.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f9d0d9cf-0cc7-5bc1-b533-aa5fc1801582"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the ablation it appears that without human controller input the model learns slower, but converges to a similar level of performance. Some discussion on this would be useful to better understand the impact of including the additional information.", "reference_answer": "A:  This is an optimal control problem and thus the objective is to minimize regulation cost, which is unlike game problems to achieve maximum scores. As such, learning convergence is based on the same criteria in Table 2, Appendix A.4. The authors thus can compare learning and success rates by reaching the same convergence level.\n\n2) Please see general point G3 for a detailed description and interpretation of each of the proposed learning performance metrics, including learning rate."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "32d37103-de0b-533d-a90c-57030878bbb8", "question": "Why have previous XMC papers such as SiameseXML and DeepXML been not compared with?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["702daa48-ef5a-50ba-912a-30163e595c48"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why have previous XMC papers such as SiameseXML and DeepXML been not compared with?", "reference_answer": "A: DeepXML numbers are reported in Table 1 under the name of Astec since the DeepXML paper refers \"DeepXML\" name as the framework, and the method as \"Astec\". The authors don’t compare with SiameseXML because it uses additional label features which most of the standard XMC methods don’t use nor do the standard XMC datasets have these label features (Amazon-670K, Wikipedia-500K, Amazon-3M)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "17cb0782-6ee7-5b85-a7f8-a054b577cb6a", "question": "While I like the continuous (in time) part of the paper, I believe that discretization matters very much when the method is actually implemented. For example, the formulation of the method is summarized inbetween Line 189 and 190, but how is du/dt estimated? After the continuous dynamics is learned, how is prediction done?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["95771315-90ab-58db-a1fd-77e62b9a63d6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "While I like the continuous (in time) part of the paper, I believe that discretization matters very much when the method is actually implemented. For example, the formulation of the method is summarized inbetween Line 189 and 190, but how is du/dt estimated? After the continuous dynamics is learned, how is prediction done?", "reference_answer": "A: The authors agree that discretization certainly matters, and combination of the proposed model and the structure-preserving methods, such as symplectic integrators, variational integrators and also energy-preserving integrators, surely improves the results. However, in order to focus on learning the symplectic form, which is the main proposal, the authors prefer not to bias the focus of the discussion towards discretization methods since there are too many integrators to be tested as listed above. Hence, the authors consider simple situations where the data of du/dt are given by the continuous true models and the classical Runge-Kutta method (ode45) is used for prediction."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "93882705-3772-5b36-a971-fa5ae1e6edd3", "question": "What is the open-domain setting for TriviaQA experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["c0fcc52a-376c-5e4b-aee0-e864fc1c9671"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the open-domain setting for TriviaQA experiments?", "reference_answer": "A: The authors directly use the code released by Lin et al. to generate the data and the statistics the authors reported are based on their code’s output. In Table 4, all the baselines and the proposed method indeed consider the same open-domain setting, where a retrieval module is required to collect the paragraphs. Lee et al. (ORQA) also consider this setting despite the fact that they used a different retrieval model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e1b54678-3f89-5344-8961-3a3f862164af", "question": "Whic experiments are conducted in the paper to study/show the effectiveness of -z?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Whic experiments are conducted in the paper to study/show the effectiveness of -z?", "reference_answer": "A: Ablation studies in Table 4, 5 and 6 by the authors. Please refer to the paper for the experimental setting: \"-z\" is the sampling improvement method; \"-qua\" and \"-div\" are the GAN training methods; \"-qua+\" and \"-div+\" are achieved by integrating \"-qua\" and \"-z\" and integrating \"-div\" and \"-z\". Taking Table 4 as an example, for each backbone, the authors report the results of both \"-qua\" and \"-qua+\". The comparison between the \"-qua\" column and the \"bare\" column shows the effectiveness of \"-qua\", while the comparison between the \"-qua+\" column and the \"-qua\" column shows the effectiveness of \"-z\"."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8a3ffb23-dbd1-52a9-902c-960df11e66fb", "question": "What is the architectural search space?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7bc1a198-5694-53e6-b66d-adace5537478"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the architectural search space?", "reference_answer": "A: The proposed architectural search space is shown in Table 1. The coarse-to-fine selection (described in Sec 3.2 and Sec 4.4) is a simple and essential component in the proposed NAS method to identify better child models under different resource budgets. The proposed work is novel as a simple, unified and effective approach to scale up neural architecture search, and the coarse-to-fine selection, compared with other search methods, is simple and effective and delivers good results."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "604a3c57-adbc-5e5f-9a33-452a313df007", "question": "What is the significance of the discussion of immutability and separability?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["8a4dd3c3-5cc9-5eca-8030-eebb477b53c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the significance of the discussion of immutability and separability?", "reference_answer": "A: The authors propose immutability and separability to better understand the reasons for the failure of semi-supervised models in BSL. In the proposed model, the authors use two types of loss to encourage the model being immutability and separability.\n\n- immutability--learning consistency loss\nImmutability is obtained in the classical SSL framework by learning consistency information between strongly and weakly augmented images, so do the proposed model.\n\n- separability--learning discriminative loss\nAs the authors mentioned in the previous question, the key to improving SSL models is to learn discriminative information so that they maintain separability even under BSL. The authors use clustering to learn the similarity/dissimilarity relationship between samples and super-classes in order to learn discriminative information so that the model keeps separability.\n\nUnfortunately, as immutability and separability are only the insights the authors propose, there is no standard way of measuring them. The authors propose the following approach for measurement. First, 10 images were selected from each category in the test set, for a total of 100 images.\n\n- evaluate immutability\n\nThe 100 images were randomly perturbed and the model was tested to see if the predictions of these perturbed images were consistent with the original images. After testing (the model obtained after training on 10 CIFAR-10 labels), FixMatch and the proposed model were correct at 100% and 98% respectively, which indicates that the immutability of the model can be satisfied under BSL. \n\n- evaluate separability\n\nthe authors calculate the confusion matrix for the predicted results of these 100 images. \n\n|class index|1|2|3|4|5|6|7|8|9|10\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|1|10|0|0|0|0|0|0|0|0|0|\n|2|8|1|0|0|0|0|0|1|0|0|\n|3|10|0|0|0|0|0|0|0|0|0|\n|4|8|0|0|2|0|0|0|0|0|0|\n|5|7|0|0|0|2|1|0|0|0|0|\n|6|10|0|0|0|0|0|0|0|0|0|\n|7|10|0|0|0|0|0|0|0|0|0|\n|8|9|0|0|0|0|0|0|1|0|0|\n|9|7|1|0|0|0|0|0|0|2|0|\n|10|9|0|0|0|0|0|0|0|0|1|\n\nTable R3-d. FixMatch's confusion matrix (10 labels on CIFAR-10 with seed=1)\n\n|class index|1|2|3|4|5|6|7|8|9|10\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|1|9|0|0|0|0|0|0|0|1|0|\n|2|0|10|0|0|0|0|0|0|0|0|\n|3|0|0|7|0|0|1|2|0|0|0|\n|4|1|0|0|7|0|2|0|0|0|0|\n|5|2|0|0|0|8|0|0|0|0|0|\n|6|0|0|0|0|0|7|0|3|0|0|\n|7|0|0|1|0|0|0|9|0|0|0|\n|8|0|0|0|0|0|0|0|10|0|0|\n|9|4|0|0|0|0|0|0|0|6|0|\n|10|0|1|0|0|0|0|0|0|0|9|\n\nTable R3-e. The proposed model's confusion matrix (10 labels on CIFAR-10 with seed=1)\n\nSeparability refers to the ability of the model to distinguish between different classes of images. Obviously, the confusion matrix of the proposed model indicates better discriminative power (i.e., separability) of the proposed model with small amount of mis-classification shown on the off-diagonal parts."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cc126d64-cb71-5a56-bd66-6e5c46b83cb1", "question": "Prompt Tuning prior experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2b1a1c48-bc94-563f-bc5c-e60979b04d68"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Prompt Tuning prior experiments?", "reference_answer": "A: The authors list the prior experiments of prompt tuning in the table below. The convergence step is defined as the step that achieves the highest performance on the dev set.\n\n|                   | **CoLA** | **SST-2** | **MRPC** | **QQP** | **STSB** | **MNLI** | **QNLI** |\n|-------------------|----------|-----------|----------|---------|----------|----------|----------|\n| Convergence Steps | 20900    | 23100     | 4950     | 22850   | 17950    | 33250    | 27550    |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0393fa36-00a9-5dd6-844d-e32309e91f01", "question": "What do authors think of sparse/dense distillation? Can we still do similar distillation on sparse models?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d5ad3544-6364-57a9-b777-ab5e2cf224b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What do authors think of sparse/dense distillation? Can we still do similar distillation on sparse models?", "reference_answer": "A: Focal loss is a defacto selection in 3D object detection to solve foreground/background imbalance and is already equipped in the supervised training objective in all the proposed trained models.\n\nA: As far as the authors know, focal loss is not widely employed as a distillation loss for 2D object detection as shown in Mimicking [22], FG [42], FGD [G], etc. Still, the authors implement a focal distillation loss similar to the supervised loss. The experimental results are shown in the following table. The PP logit KD is around 0.7\\% higher than focal loss on CP-Voxel-XS. As for CP-Pillar-v0.64, since the capability difference between teacher and student are large, focal loss even suffers performance degradation compared to vanilla KD, while the proposed PP logit KD consistently brings performance boost."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "12dad983-9f2a-5c93-934d-a38ef2cba555", "question": "I am not clear about the MLM baseline. Does it mean the SuperLM is directly trained on MLM objective? Is the \"random sample a subnetwork\" step still used?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am not clear about the MLM baseline. Does it mean the SuperLM is directly trained on MLM objective? Is the \"random sample a subnetwork\" step still used?", "reference_answer": "A: \"MLM\" indicates that the SuperLM is trained with masked language modeling (MLM) loss instead of using Equation 4 for self-attention distillation loss. The remaining steps including the random sampling of subnetworks are the same. Please refer to Section 4.1.3 and Table 4 for the result comparison."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7d1e26ae-2740-5d3d-ba55-4330b5f691a9", "question": "How does the proposed \"baseline\" experimental results correspond to the published DPA implementation? Does it take out the data augmentation already in the implementation and compare against that?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["86b0a7a9-c402-5373-bc10-3ce3edc5e031"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed \"baseline\" experimental results correspond to the published DPA implementation? Does it take out the data augmentation already in the implementation and compare against that?", "reference_answer": "A: The baseline results are consistent with the published DPA implementation. DPA_baseline uses the very **same** augmentations and hyperparameters as the published DPA implementation and the results in the proposed Figure 2 matches the corresponding settings reported in the original paper of DPA. The authors do **not** take out the augmentation already in the implementation. It is surprising but one can indeed double or triple the reported robustness of one of SOTAs! This is because the proposed community has not put much effort into improving base learners and the potential from more data-efficient base learners remains undiscovered."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ff1984e0-7900-5810-9ff5-2a4b344f47dc", "question": "Why are SNC, SM, QSM and SC interesting properties? Can you provide some real problems that are proven to satisfy these conditions?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["efe4823c-cf07-5450-83eb-e1d8ddca17d4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are SNC, SM, QSM and SC interesting properties? Can you provide some real problems that are proven to satisfy these conditions?", "reference_answer": "A: The eigenvalues of the Jacobian around the equilibrium of GAN games have been theoretically studied by Mescheder et al. [2018] and Nagaran and Kolter [2017] and practically by Berard et al. [2018]. Certain bounds on the eigenvalues imply some local monotonicity properties. For instance, if $\\Re(\\lambda)>\\mu$ for all $\\lambda$ an eigenvalue of the Jacobian around the equilibrium then the operator $F$ is locally strongly monotone around the equilibrium (see, for instance see Azizian et al. [2019]).  One high-level conclusion that can be drawn from Mescheder et al. [2018] and Nagaran and Kolter [2017], and Berard et al. [2018] is that for some GAN formulations, some of the SNC, SM, QSM, and SC hold (at least locally)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "34c41f49-da91-589e-8933-8464fdf90054", "question": "Does the model work better with some halftoning algorithms than others?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the model work better with some halftoning algorithms than others?", "reference_answer": "A: In order to explore the performance of the model on different halftoning algorithms, the authors can look at the results of the experiments on images generated by 9 halftoning algorithms, namely Floyd-Steinberg Dithering, Jarvis-Judice-Ninke Dithering, Stucki Dithering, Atkinson Dithering, Burkes Dithering, Sierra Dithering, and several of their variants (Lau and Arce, 2018). The experimental results are as follows:\n\nMethod | PSNR | SSIM\n-|-|-\nFloyd-Steinberg | 24.46 | 0.735\nSimple Floyd-Steinberg | 24.01 | 0.692\nJarvis-Judice-Ninke | 24.42 | 0.749\nStucki | 24.53 | 0.749\nAtkinson | 23.08 | 0.710\nBurkes | 24.69 | 0.746\nSierra | 24.49 | 0.750\nSierra Lite | 24.40 | 0.733\nTwo row Sierra | 24.54 | 0.741\n\n\nThe authors can observe that the proposed method achieves similar results on different halftoning algorithms, which also verifies the good robustness of the proposed method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dd02a0e4-0bf5-5e01-be9a-8cee3d44df73", "question": "What does the percentage in Table 1 mean?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["12a16837-7683-5205-a1fe-20f98adf46a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does the percentage in Table 1 mean?", "reference_answer": "A: It is the fraction of total inference cost measured in floating point operations."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f4eb9aa3-f73b-55df-b083-3820d9a7c3de", "question": "In the main theorem, can we have $\\epsilon \\rightarrow 0$, so we only need one layer message passing? How does the distinguishing error in Theorem 1 (i.e., the term $1-o(1)$) depend on $\\epsilon, n, r...$? … Please quantify the dependence of the  bound in the result to  to allow comparison and help readers. Also, please provide a few sentences as proof sketch right after the main result.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["928d74ad-4c97-537a-b49e-b215f66676f0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the main theorem, can we have $\\epsilon \\rightarrow 0$, so we only need one layer message passing? How does the distinguishing error in Theorem 1 (i.e., the term $1-o(1)$) depend on $\\epsilon, n, r...$? … Please quantify the dependence of the  bound in the result to  to allow comparison and help readers. Also, please provide a few sentences as proof sketch right after the main result.", "reference_answer": "A: Yes. The authors just need one layer message passing, which is also demonstrated empirically in Appendix C, Figure 3 (right). \n\nThe more subtle distinguishing error follows  $n^{-0.5 + \\epsilon}$ + $n^{3/2-(\\epsilon^2/3) \\log n / \\log\\log n}$. In practice, the authors choose a fixed $\\epsilon$ and enlarge n to infinity, and then there will be almost no error. An empirical demonstration of this result is given in Appendix C Figure 3. The authors did not provide a proof sketch because of the page limitation. In the final version, if more pages are allowed, the authors can definitely provide some proof sketch. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e46d0b72-7db9-5e8a-b063-e8e1f6e6717d", "question": "In legend Figure 3.a), are $T_{\\text{dyn}}$ and $T_{\\text{plas}}$ fixed? If so, to which values?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["1ff74430-3f62-56d5-ab39-6e60abb348b1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In legend Figure 3.a), are $T_{\\text{dyn}}$ and $T_{\\text{plas}}$ fixed? If so, to which values?", "reference_answer": "A: In Fig. 3.a), $T_{\\text{dyn}}$ is about 400/10=40 time steps as measured by the time for which the fixed point between two consecutive complex $\\beta$ is reached (Fig 3.b)). Fig 3.a) also shows how $T_{\\text{plas}}$ could be chosen as $10T_{\\text{osc}}$ so that the weight update optimally follows the gradient."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1b98961d-eb93-583c-ac12-771343b5080e", "question": "Summarize the abbreviations in a look-up table, for the color ablation datasets.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Summarize the abbreviations in a look-up table, for the color ablation datasets.", "reference_answer": "A: The ablation abbreviation look-up table is summarized here. This is present in the appendix:\n\n| ablation  | object-level | scene-level | color-related | shape-related | Object Color Gradient | Object Shape Concavity | Inter-object Color Similarity | Inter-object Shape Variation |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| C  | &check; |  | &check; |  | &check; | | | |\n| S  | &check; |  |  | &check; |  | &check; | | |\n| T  |  | &check; | &check; |  |  | |&check; | |\n| U |  | &check; |  | &check; |  | | | &check;|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a766eaed-4b78-5016-aab7-19e325d04f2d", "question": "What's the reason behind choosing NFNet for visual encoder? Did you try transformers (e.g. ViT) as a vision encoder where you can directly utilise tokens as inputs to Perceiver Resampler?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f9c136b6-1c41-543a-ad92-b5563b5e797e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What's the reason behind choosing NFNet for visual encoder? Did you try transformers (e.g. ViT) as a vision encoder where you can directly utilise tokens as inputs to Perceiver Resampler?", "reference_answer": "A: Note that the outputs of NFNet and ViT are of essentially similar flavors: NFNet outputs a HxWxD 2D spatial grid of features, while ViT outputs can be viewed as (HW)xD features, where H and W are determined by the patching in the first ViT layer (which can be viewed as a convolution). The similarity may be more apparent once the authors flatten NFNet's HxWxD spatial grid to HWxD for input into the PerceiverResampler, matching the \"raw\" outputs of ViT. That said, NFNet proved to be faster on the proposed hardware, and the authors ran into stability issues with ViT during the contrastive pretraining phase. The authors did experiment with CLIP ViT as an encoder; these results can be found in the proposed ablations Table 3 \"CLIP ViT-L/14\". It outperforms the smaller NFNet-F0, but performs substantially below the proposed main result with NFNet-F6."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5d590536-2699-5d80-8527-a2e1d74b4dba", "question": "How do different initial parameters of the seed shape affect the generated results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do different initial parameters of the seed shape affect the generated results?", "reference_answer": "A: In Appendix section 2.2, the authors discuss the influence of different types of seed shapes on the model performance. Compared with using the entire pocket directly, using a seed shape achieves a better binding affinity. The results indicate that the seed shape can capture protein's structural information more moderately. In section 3.5, the authors also discuss how the number of molecular shapes sampled with the seed shape affects the method's performance. In Figure 11, the authors find that increasing the number gives us a performance rise, which implies comprehensive explorations of pockets benefits model performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8efa3e79-87aa-5bd9-9eb8-201e3f0602a4", "question": "How is this method applicable to more challenging CIL problems where there is distribution shift and out-of-distribution data?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4dab9e04-b50a-53b7-a04b-d87fc3c041b3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is this method applicable to more challenging CIL problems where there is distribution shift and out-of-distribution data?", "reference_answer": "A: Class-IL is quite challenging if not storing old samples. When distribution shift and out-of-distribution (OOD) data are added, the difficulty of the problem will be further increased. The authors hypothesize that an effective Class-IL method with good OOD robustness could perform well when there are distribution shift and out-of-distribution data. Since the Class-IL ability of the proposed method has been verified in the paper, here the authors conduct standard OOD detection experiments following [Hendrycks, et al., 2017] and [Lee, et al., 2018] to show that the proposed classAug can also enhance the OOD robustness of the original model, as shown in the following table (ResNet-18 on CIFAR-10 (In-distribution); OOD datasets: MNIST, Fashion-MNIST, LSUN, Tiny-ImageNet).\n\n|Metric|Method|MNIST|Fashion|LSUN|Tiny-ImageNet|\n|---|---|---|---|---|---|\n|**AUROC**|**baseline**|87.02|90.28|88.50|88.49|\n|-|**classAug**|**94.99**|**94.40**|**93.90**|**93.92**|\n|**AUPR-In**|**baseline**|79.89|86.18|83.48|83.84|\n|-|**classAug**|**93.05**|**92.43**|**91.08**|**91.77**|\n|**AUPR-Out**|**baseline**|92.26|94.26|92.92|92.70|\n|-|**classAug**|**97.20"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "517e05ff-5474-5dfb-8021-537a5ff64f31", "question": "Scene editing is not clearly defined in the context of this paper. Why is the physics simulator necessary for scene editing? Can't we edit the scene in an interactive manner?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6c995533-fb04-547f-9d46-27b6ce8f709d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Scene editing is not clearly defined in the context of this paper. Why is the physics simulator necessary for scene editing? Can't we edit the scene in an interactive manner?", "reference_answer": "A: Scene editing in the proposed work includes adding, deleting, moving, deforming objects, or even simulating deformation using different physics parameters in an existing video. The authors show the examples in the proposed supplementary video and NEW Figure 5 and 6."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0a9e2e9a-6164-5a36-a673-5e270b863fb6", "question": "Equation 8 smoothes out the effect between particles of different distances. How sensitive is the final performance of the model to the specific smoothing formulation? Is it possible to learn a reweighting function instead of hardcoding?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7566656d-1495-5b20-92c3-535a544492ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Equation 8 smoothes out the effect between particles of different distances. How sensitive is the final performance of the model to the specific smoothing formulation? Is it possible to learn a reweighting function instead of hardcoding?", "reference_answer": "A: The type of the window function influences the performance. The authors cannot (yet) backpropagate to the window function but this is a reasonable extension. The authors added a comparison with a triangular window to the appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5ee925ea-0876-56b8-8340-7e8e20eaf781", "question": "Isn't this evaluation too simple? There isn't an agent that rearranges these objects, instead the objects all rearrange themselves. This is a good initial evaluation that shows the method provides a reasonable reward signal, but it leaves a lot of questions since this reduces the time-horizon, isn't representative of how the objects would be moved by a single agent (one object moves at a time), and removes the initial exploration to find the objects.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["d66a5cd9-35b7-5db9-816b-47aaae8ae114"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Isn't this evaluation too simple? There isn't an agent that rearranges these objects, instead the objects all rearrange themselves. This is a good initial evaluation that shows the method provides a reasonable reward signal, but it leaves a lot of questions since this reduces the time-horizon, isn't representative of how the objects would be moved by a single agent (one object moves at a time), and removes the initial exploration to find the objects.", "reference_answer": "A: The authors argue that multiple objects moving together is also a practical setting. The authors agree there are cases where objects should be moved one by one, but the proposed framework still has the potential to meet this setting. To this end, the authors design a bi-level approach (denoted as *Ours + Planner*) for object arrangement: The high-level policy determines which object to move according to the trained target score network (e.g. choosing the object with the largest gradient component). The low-level policy leverages the target score network and ORCA planner to output the action. \n\nThe authors compare this approach with another heuristic-based bi-level planner (denoted as *Goal + Planner*): The high-level planner first generates goals for each object and chooses the object with the farthest distance to the goal to move. The low-level planner is the same as *Ours + Planner*.\n\nAs shown in Figure. 3 in  [**our site**](https://sites.google.com/view/neurips2022-paper2108-rebuttal/), *Ours + Planner* is better than *Goal + Planner* in efficiency. This shows the effectiveness of the proposed methods in handling the scenario where the agent can move one object at a time."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f068b455-d360-5bfa-a2a9-2f75fa2d0df1", "question": "In the experiments, the R_nDTW and R_SR rewards are not fully explained. How do these rewards correspond to what was introduced in prior work? The combination of R_nDTW and R_SR seem to work well (comparable with the proposed method).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the experiments, the R_nDTW and R_SR rewards are not fully explained. How do these rewards correspond to what was introduced in prior work? The combination of R_nDTW and R_SR seem to work well (comparable with the proposed method).", "reference_answer": "1. The R_nDTW (model# 14) reward takes the nDTW metric as a reward directly. It is the original form before the authors modify it to the soft focal-oriented reward. No prior work has only used it as a reward. The authors report R_nDTW in Table 3 to conclude that only considering the global alignment between instructions and trajectories makes the agent just concern about the similarity of trajectories but not the locations that instructions really concern during navigation.  \n2. As shown in the caption of Table 3 and Line 285, the R_SR reward is the goal-oriented reward [5] which uses the SR metric as a reward signal.   \n3. As mentioned in the caption of Table 3, the combination of R_nDTW and R_SR is exactly the fidelity-oriented reward [4]. \nReferences:\n[4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n[5] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for visionand-language navigation. Advances in Neural Information Processing Systems, 2018."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4137937c-2de2-5191-89a2-e32465013659", "question": "Given the difficulty of modeling similar classes, how does such a method fair on fine-grained classification (e.g., on CUB-200)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Given the difficulty of modeling similar classes, how does such a method fair on fine-grained classification (e.g., on CUB-200)?", "reference_answer": "A: The authors have evaluated the proposed method on CUB-200-2011 (rescale to 32x32) and added the result to the main text (See the visualization in Appendix E.2). As shown in the table below, the proposed method achieves significantly better performance than all other methods and real data, especially when the authors distill only one image per class. The authors observe that methods like DSA and DM that work reasonably well for coarse-grained image classification tasks fail to outperform the real data baseline on the fined-grained classification task. This is because DSA and DM learn the distilled data independently and fail to capture the similarity among different classes. In contrast, the proposed method works well as the authors take into account the class similarity by considering all classes together and learning the label. The learned label also plays an important role at test time. Similar to the teacher label in knowledge distillation (https://arxiv.org/abs/1503.02531), the proposed distilled label is soft and not only contains knowledge for the most likely class but also contains “dark knowledge” like class similarity for all other classes. As a result, training a model from scratch on the proposed distilled data achieves much better performance than the previous methods that do not learn the label.\n\nTable: Test accuracies of models trained on the distilled data from scratch on CUB-200-2011 (http://www.vision.caltech.edu/datasets/cub_200_2011/), a fine-grained image classification dataset. Test accuracy on the full dataset is $21.74\\pm0.64$.\n\n|Img/Cls|Real|DSA|DM|MTT|FRePo|\n|--|--|--|--|--|--|\n|1|1.43$\\pm$0.11|1.29$\\pm$0.09|1.61$\\pm$0.06|2.16$\\pm$0.05| 12.41$\\pm$0.20|\n|10|5.36$\\pm$0.31| 4.54$\\pm$0.26 |4.38$\\pm$0.16| OOM |16.84$\\pm$0.12|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a8f6af99-8763-57e1-b9e7-116650629162", "question": "Comparisons with recent works?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a017fa2d-2f53-5616-b5a4-2aca563cc758"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comparisons with recent works?", "reference_answer": "A: For the Energy score, please refer to Appendix F.1 for a detailed discussion where the authors investigate the effectiveness of Energy score based on CLIP. For GradNorm, as suggested, the authors provide the results as follows. For reference, the authors also paste the results reported in the original paper (Table 1) [1] based on ResNetv2-101 trained on ImageNet (numbers are FPR95/AUROC).\n\n| Model                   | iNaturalist | SUN         | Places      | Texture     | Average     |\n| ----------------------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n| GradNorm (ResNetv2-101) | 50.03/90.33 | 46.48/89.03 | 60.86/84.82 | 61.42/81.07 | 54.70/86.31 |\n| GradNorm (CLIP-B)       | 68.35/79.53 | 40.74/91.11 | 49.64/87.31 | 48.37/87.51 | 51.77/86.37 |\n| MSP (CLIP-B)            | 40.89/88.63 | 65.81/81.24 | 67.90/80.14 | 64.96/78.16 | 59.89/82.04 |\n| MCM (CLIP-B)            | 32.08/94.41 | 39.21/92.28 | 44.88/89.83 | 58.05/85.96 | 43.55/90.62 |\n\nA: Given the same feature backbone (CLIP-B), when linear probed on ImageNet-1k, GradNorm indeed improves the average performance compared to the classic MSP score (59.89\\% vs. 51.77\\% in FPR95); GradNorm (CLIP-B) achieves comparable and even better performance compared to GradNorm (ResNetv2-101 trained from scratch on ImageNet) due to better feature representations as a result of large-scale pre-training. For example, the average FPR95 is improved from 54.70\\% to 51.77\\%; Finally, MCM (CLIP-B) still outperform GradNorm by a large margin (43.55\\% vs. 54.70\\% in FPR95) across most OOD test sets, which is encouraging as MCM is zero-shot and training free. \n\n[1] Huang et al., On the Importance of Gradients for Detecting Distributional Shifts in the Wild, NIPS 2021"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d77bf634-489b-59ef-ae8b-420489eb0cc0", "question": "Can you provide the MAP performance for the feature-based MLC datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["26a3ec8c-e13f-54f6-a36a-eff4c9f61425"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you provide the MAP performance for the feature-based MLC datasets?", "reference_answer": "A: Yes, the table below shows the mean average precision for various models.\n### MAP Performance for feature-based MLC datasets\n\n|                   |  Use of samples | **bibtex** | **delicious** | **genbase** | **cal500** | **eurlexev** | **expr_fun** | **spo_fun** | **Average** |\n|-------------------|---|:----------:|:-------------:|:-----------:|:----------:|:------------:|:------------:|:-----------:|-------------|\n| **cross-entropy** | x |     54.95 |        37.24 |      75.61 |     50.59 |       47.39 |      * **47.42** |      40.13 |    50.47   |\n|    energy only    |   |            |               |             |            |              |              |             |             |\n| **SPEN**          | x |     35.07 |    **25.36** |      42.75 | **36.93** |   **38.25** |   **40.05** |  **30.83** | **35.61**  |\n| **DVN**           | x | **36.68** |        17.57 |  **72.13** |     31.53 |       20.02 |       17.85 |      14.03 | 29.97      |\n| **NCE**           | o |      6.81 |         4.99 |      10.98 |     27.22 |        0.13 |       15.16 |       7.03 | 10.33      |\n|    SEAL-Static   |   |            |               |             |            |              |              |             |             |\n| **margin**        | x | **56.15** |    **39.77** |      66.21 |     50.96 |       47.45 |       47.07 |  **39.79** | 49.63      |\n| **regression**    | x |     54.40 |        34.31 |      98.80 |     50.58 |   * **47.65** |   **47.24** |      38.84 |  53.12 |\n| **NCEranking**    | o |     54.55 |        36.36 |  **98.94** | **51.49** |       47.53 |       46.63 |      39.29 |  **53.54** |\n|        SEAL-Dynamic       |   |            |               |             |            |              |              |             |             |\n| **margin**        | x |     55.06 |        36.63 |      98.82 |     49.07 |       40.17 |       46.42 |      37.60 | 51.97      |\n| **regression**    | x |     56.62 |        38.84 |      98.98 |     51.15 |       45.44 |   **47.33** | * **40.17** |  54.08 |\n| **regression-s**  | o | * **56.67** |    * **40.25** |      98.90 | * **51.51** |   **47.16** |       46.56 |      37.76 |  * **54.11** |\n| **NCEraking**     | o |     56.65 |        37.76 |      98.91 |     47.33 |       44.84 |       46.32 |      37.76 | 52.80      |\n| **ranking**       | o |     54.37 |        39.36 |  * **99.05** |     43.36 |       45.75 |       47.16 |      39.29 | 52.62      |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f4c077b4-6225-5188-9d68-d28b1d839242", "question": "How is the bounds related to equation 2 and 3?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7bb011ff-6526-5a93-9b59-878505dc29ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is the bounds related to equation 2 and 3?", "reference_answer": "A: The authors use these bounds to derive a lower bound of $w_k$, so that the authors may down-weight some transitions but never up-weight a transition by mistake. Concretely, $|Q_{k-1}-\\mathcal{B}^\\ast Q_{k-2}|\\leq c_2$ implies $\\gamma P^{\\pi_{k-1}}\\Delta_{k-1}+c_2$ is an upper bound of $|Q_k-Q^*|$. Together with $2-\\pi_k(a|s)\\geq 1$ and $|Q_{k-1}-\\mathcal{B}^\\ast Q_{k-2}|\\geq c_1$ we"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2b30bb75-1e75-5698-9d58-a08554063bae", "question": "The behavior of the phase diagram is qualitatively different between the setting in [1] and the toy model, namely that instead of models going from memorization to comprehension to grokking, in the real task setting it is going from memorization to grokking to comprehension. Provide a sufficient explanation to bridge this gap.\n[1] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6e2bd28f-ad70-5fe4-a8e6-178bd931cf78"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The behavior of the phase diagram is qualitatively different between the setting in [1] and the toy model, namely that instead of models going from memorization to comprehension to grokking, in the real task setting it is going from memorization to grokking to comprehension. Provide a sufficient explanation to bridge this gap.\n[1] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.", "reference_answer": "A: The phase diagram's main point is to show that grokking is pathological, a result of improperly tuned hyperparameters.  Going from X to Y implies choosing a path on the plane, and depending on the choice the results are different. In that sense, one can almost always find a way in which everything is adjacent to everything else, as long as they have a common border. Note also that the phase diagrams only show a part of the plane!"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "54be3714-e3dc-550f-8c91-f99f50d17d0c", "question": "How to generate the pairwise similarity for pointset experiment?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["af7926a7-2c80-50e5-8644-13eaab8fdafe"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to generate the pairwise similarity for pointset experiment?", "reference_answer": "A: The authors generated the similarities used in the pointset experiments using the formula $\\mathsf{sim}(u,v) = 1/(1 + \\mathsf{dist}(u,v))$. Then the similarities were reweighted so that the similarities lie in [0, 1] by dividing by the maximum similarity."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5ba3db81-a2e5-5789-96bb-4e941cc349ad", "question": "In the experiments, a nearest-neighbor type of graph is used, where each agent is connected to a few neighboring agents in its physical vicinity. However, such a geometric and symmetric graph construction method might be suboptimal in practice. How should the network graph be constructed and is an automatic way to do so?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["893be569-d272-56c9-b2c1-fa6752fbc427"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the experiments, a nearest-neighbor type of graph is used, where each agent is connected to a few neighboring agents in its physical vicinity. However, such a geometric and symmetric graph construction method might be suboptimal in practice. How should the network graph be constructed and is an automatic way to do so?", "reference_answer": "A: In networked MARL, the graph is typically assumed to be given or simply structured by vicinity [1,2,3], since the graph structure can be too complex [4]. As for the study on graph density, the authors have experiments in *jungle*,  the authors choose the number of neighbors to be 1, 2, 3, and 4. As illustrated Figure 8 in the revision of Appendix,  the number of neighbors indeed affects the performance. By now the authors choose to consider the number of neighbors as a hyperparameter to tune as [2] do, and #neighbors=3 is the best in *jungle*. \n\n[1] Value propagation for decentralized networked deep multi-agent reinforcement learning, C. Qu et al., NeurIPS 2019.\n[2] Intention propagation for multi-agent reinforcement learning, Qu et. al., 2020. \n[3] Scalable multi-agent reinforcement learning for networked systems with average reward, G. Qu et al., NeurIPS 2020.\n[4] Self-Organized Polynomial-Time Coordination Graphs, Yang et al., ICML 2022."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2d1d4431-f270-5758-aaeb-0bb1966b3a76", "question": "Summarize the quantitative analysis of the relationship between the estimated regional importance $w^{(r)}$ and the Shapley value $\\phi^{(r)}$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Summarize the quantitative analysis of the relationship between the estimated regional importance $w^{(r)}$ and the Shapley value $\\phi^{(r)}$.", "reference_answer": "A: Authors design a quantitative metric to analyze the relationship between $w^{(r)}$ and $\\phi^{(r)}$. Given a certain input image, the authors measured the Pearson correlation coefficient between $w^{(r)}$ and $\\phi^{(r)}$ over different regional features. The authors conducted **an additional experiment** to analyze their correlation by evaluating the correlation between $w^{(r)}$ and $\\phi^{(r)}$ through all input images for each DNN. The table below shows that there was a positive relationship between $w^{(r)}$ and $\\phi^{(r)}$. This demonstrated that the estimated importance $w^{(r)}$ could objectively reflect the importance of each region.\n\n| Dataset                                                      | Tiny ImageNet                  | Tiny ImageNet                  | Tiny ImageNet                  | COCO 2014                      | CUB-200-2011                   |\n| ------------------------------------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ | ------------------------------ |\n| DNN                                                          | ResNet-34                      | VGG-16                         | MobileNet-V2                   | ResNet-50                      | ResNet-34                      |\n| The Pearson correlation coefficient between $w^{(r)}$ and $\\phi^{(r)}$ through all images | $0.8943{\\scriptsize\\pm0.0994}$ | $0.6307{\\scriptsize\\pm0.1831}$ | $0.8658{\\scriptsize\\pm0.1432}$ | $0.8814{\\scriptsize\\pm0.1623}$ | $0.8561{\\scriptsize\\pm0.1680}$ |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "58e9937a-bc8a-53c4-b859-a299d850d586", "question": "It is not clearly stated why magnitude is the key to discriminate foreground and background points？", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is not clearly stated why magnitude is the key to discriminate foreground and background points？", "reference_answer": "A:**Visual Analysis：** To better understand points pruned by the proposed magnitude criterion, the authors visualize point clouds before and after pruning. Note that the point clouds used for visualization are randomly chosen from the nuScenes dataset. The comparison results are shown in the link [[visual](https://drive.google.com/drive/folders/1aoQOrYRB57tKGHymMg3IuS2DRoLh00wR?usp=sharing)], the authors provide the original image and the pruned image with the file names _raw.png and _im.png respectively. And the authors roughly annotate the positions of cars (red) and pedestrians (yellow). The authors observe that most of the foreground points are preserved. For the background areas, points that fall in vertical structures, such as light, poles, and trees, are also preserved  as they tend to be hard negatives, and easily confused with foreground objects. These points require a deep neural network with a certain capability to process in order to recognize them as background.  In contrast, background points in flat structures such as road points are largely removed because they are easily identifiable redundant points.\n**Why foreground points with high feature magnitude?**  To gain more insights into why high feature magnitude corresponds to the above patterns, the authors conjecture that this is caused by the training objective in 3D object detection. When training a 3D object detection model, the focal loss is adopted as default in 3D object detection. When the authors look closer at the focal loss, it will incur a loss on positive samples and hard negatives while easy negatives are removed from the loss. Thus, this will generate gradients in the direction that can incur an update of features for areas with positive samples and hard negatives. This can eventually make a difference in their feature magnitudes in comparison with areas for easy negatives which are less frequently considered in the optimization objective."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5bcde0df-c0d8-5865-b244-2695cd799a92", "question": "could we use the product of CD and EMD (or sqrt(CD) * sqrt(EMD)) to get the same behavior?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["1ba9b611-9a76-5d01-8476-dade426b3784"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "could we use the product of CD and EMD (or sqrt(CD) * sqrt(EMD)) to get the same behavior?", "reference_answer": "A: That formulation has several drawbacks: first, the physical meaning is not clear and it cannot be viewed as a distance metric with the clean formulation. And then, this is not efficient and the time consumption is even heavier than pure EMD. But the authors will consider adding formulations like this that ensembles CD and EMD as an extra baseline."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f32449b7-2c41-524b-8194-1e788f82d207", "question": "5: Where is $x'$ defined in the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "5: Where is $x'$ defined in the paper?", "reference_answer": "A: $x'$ denotes the perturbed version of source image $x$ in Eq. (3) and Eq. (6)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b479dd01-6342-5770-8f1b-54d291d05d1e", "question": "Why does the proposed method (last row, Table 1) beat the baseline (row 1, Table 1)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["bb1f82ae-09df-5174-adbf-157ba87a8d01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does the proposed method (last row, Table 1) beat the baseline (row 1, Table 1)?", "reference_answer": "A: The baseline in Row 1 is trained on training data without base and novel class co-occurrence to make fair comparison with the proposed approach without co-occurrence (\"w/o co-occur\"). It can be seen from Row 6 that the proposed approach without class overlap in the in-the-wild data (“w/o category) did not outperform the baseline in Row 1. In contrast, the proposed method in the last row is trained on data with co-occurrence (\"w co-occur\") and with class overlap in the in-the-wild data (\"w category\") to make a fair comparison with [27] and [40]. The authors will indicate “w/o co-occur” in Row 1-3 for clarity in the final paper. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "38beb000-d186-520b-8aff-e9729a1a8da7", "question": "It was unclear to me why in section 3.2 DP-SGD was insufficient?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["cac0c042-64d2-5835-b21b-ffd703d46fbe"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It was unclear to me why in section 3.2 DP-SGD was insufficient?", "reference_answer": "A:  Assuming that you are asking why training only student models with DPSGD is not sufficient. Before the authors answer this question, let us recall the basics of analysis of DPSGD. In DPSGD, the authors add noise to clipped per-sample gradients and hence every iterate of DPSGD is private. In other words, after each iteration t of DPSGD, model weights W_t can be assumed to be public information. Now, in iteration t+1, gradients of samples are computed with respect to W_t, and hence privacy only depends on gradients belonging to the samples in a single batch. This is crucial for applying amplification by subsampling theorems in privacy analysis. \nNow consider a framework where the teacher models are trained using SGD on the dataset D and the student models are trained with DPSGD while minimizing the Equation 1 on dataset D. Such an algorithm does not output a differentially private compressed student model. This is due to the distillation loss term H(y_true, P_S) in Equation 1. Here, P_S is a function of the entire dataset as the teacher was not trained with DP. Therefore,  gradients of samples are now functions of entire dataset D, which forbids us from applying subsampling theorems in privacy analysis. The proposed solution to circumvent this was to make P_S DP as well by training the teacher model with DP on dataset D. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b58865f4-14fd-5fe8-8799-971999cf5a15", "question": "Do assumptions 2 and 3 hold even for the SE kernel?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["6ab57824-ded6-5585-87a5-538d0defdada"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do assumptions 2 and 3 hold even for the SE kernel?", "reference_answer": "A: First of all, the squared-exponential kernel is given as $k(z,z_*)=\\sigma_f^2\\exp({-\\frac{1}{2\\ell^2}}||z-z_*||^2)$. Based on the distance definition $D(z,z_*)=||z-z_*||$, the authors have that $k(z,z_*)=\\kappa(D(z,z_*))$ where $\\kappa(D(z,z_*))=\\sigma_f^2\\exp({-\\frac{1}{2\\ell^2}}D(z,z_*)^2)$. Hence the decomposition property in Assumption 2 is satisfied.\nSecond, it can be seen that $\\kappa(D(z,z_*))$ is a monotonically decreasing function with regard to $D(\\cdot,\\cdot)$. When $D(z,z_*)=0$, the authors have $\\kappa(0)=\\sigma_f^2$.\nThird, please refer to equation (6.5) on page 131 of reference [A8], the function can be written as $f(x)=\\sum_{i=1}^{n}\\alpha_ik(z_*,z_i)$ with $z_i\\in\\mathcal{Z}$ and $\\alpha_i\\in\\mathbb{R}$. Since $k(z_i,z_*) = \\sigma_f^2\\exp({-\\frac{1}{2\\ell^2}}||z_i-z_*||^2)$ is Lipschitz continuous [A8],[A9], then the authors can conclude that Assumption 3 holds even for the SE kernel."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4c1fe5ca-344d-5cb5-a286-b6ef327db40b", "question": "The cost function in Equation 3 only regulates the penultimate-layer features (Fc, Fb) but not the inputs (x, T(x)). How can the proposed method achieve imperceptible backdoor in the input space, as shown in the Supplementary PDF?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table", "formula"], "anchor_pdf": ["b761f966-dde6-5d75-9704-25545a8198a2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The cost function in Equation 3 only regulates the penultimate-layer features (Fc, Fb) but not the inputs (x, T(x)). How can the proposed method achieve imperceptible backdoor in the input space, as shown in the Supplementary PDF?", "reference_answer": "A: The imperceptibility in the input space is achieved via the backdoor injection function with a conditional noise generator (Equation (2)), which adds artificially imperceptible noise (as the trigger) to the image. The magnitude of this noise is controlled by the parameter $\\epsilon$ (please see the values used in the proposed experiments in Table 6 in the supplementary material). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "616f1f3b-e297-5d1c-9d81-f4d04e4a32b0", "question": "How easy would it be to apply this method to panoptic segmentation? In particular, the mask encoding work well for stuff segments? Would it be easy to merge the predictions from different masks (similar to the pixel-wise argmax done in DETR)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a31b1f08-c3ff-54a1-bb1e-e4d1f878a710"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How easy would it be to apply this method to panoptic segmentation? In particular, the mask encoding work well for stuff segments? Would it be easy to merge the predictions from different masks (similar to the pixel-wise argmax done in DETR)?", "reference_answer": "A: SOLQ may fail to directly work for stuff segments, which are usually of very large size. It will result in large information loss when compressing them into low-dimension vectors by mask encoding. One can encode the stuff segments by dividing stuff segments into several patches and conduct mask encoding for each patch. It works well for us on pure semantic segmentation task (For example, mIoU=77.73% on Cityscapes dataset with Swin Tiny backbone). In this way, the authors can simply merge the predictions from different masks.\n\n[1] End-to-End Video Instance Segmentation with Transformers, Wang et al \n[2] Boundary IoU: Improving Object-Centric Image Segmentation Evaluation, Cheng et al \n[3] PointRend: Image segmentation as rendering, Kirillov et al \n[4] Boundary-preserving Mask R-CNN, Cheng et al"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "28b05d56-63f6-52b2-87a5-494f42af6f35", "question": "What is the correlation between EI (grayscale) and EI (rotation). Intuitively, they are strongly correlated; if so, which of the two is the most important source of invariance for generalization?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["65d774b3-6bb6-5475-926a-0dfa3a2e9100"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the correlation between EI (grayscale) and EI (rotation). Intuitively, they are strongly correlated; if so, which of the two is the most important source of invariance for generalization?", "reference_answer": "A: The authors observe that they are indeed strongly correlated. In the proposed experiment, the Spearman's rank correlation $\\rho$ is 0.947, 0.950, and 0.965 on ImageNet-Val, ImageNet-S, ImageNet-R, respectively. It suggests that the network simultaneously gains rotation and grayscale invariance. Regarding which invariance is more important for generalization, the proposed correlation studies (Figures 2 and 3) show that rotation invariance generally has a stronger correlation with accuracy than grayscale invariance (5 out of 6 test sets). The only case for grayscale to have a stronger correlation is ImageNet-R, which is featured by style shift. The authors think under style shift, the model probably has more incentives to be invariant to color changes. In the real world, images often exhibit diverse geometric and color variations. To measure generalization in these scenarios, the authors think both rotation and grayscale invariance are critical."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b3989ba0-0b2d-5ce1-a8bf-76f8e4b31f04", "question": "Figure 4: what is the x-axis \"operations\"?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["54be9a09-b764-596d-ac07-57ee007c5fdd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 4: what is the x-axis \"operations\"?", "reference_answer": "A: The x-axis corresponds to the total number of algebraic operations."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a57ada32-1924-5dfb-b650-1fc45eeeb226", "question": "Why it is necessary to multiply the feature with the magnitude mask?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why it is necessary to multiply the feature with the magnitude mask?", "reference_answer": "A: (1) why multiplying magnitude mask: For this problem, the proposed initial purpose is to let the magnitude mask as a bridge to provide additional gradients for supervising the feature norm, further enhancing the difference between important and non-important features. As the network is end-to-end optimized for the object detection task, the additional gradient will not interfere with the original gradient but instead try to make areas that are important for detection have a larger magnitude.\n\n(2) Necessity of the multiplication operation: the authors do further investigation on whether this multiplication is necessary. The authors observe that it only brings marginal performance gains as shown in Table below. This further confirms that without any additional guidance, the magnitude of features from a detection network is sufficient to serve as a good criterion for deciding important vs unimportant regions. This strengthens the proposed initial claim and echoes the proposed motivation of using magnitude as a selection criterion.\n\n| Method (KITTI)           | Easy  | Moderate | Hard  |\n| ------------------------ | ----- | -------- | ----- |\n| SPSS-Conv                | 89.22 | 84.36    | 78.83 |\n| SPSS-Conv (not multiply) | 89.02 | 84.13    | 78.81 |\n\n| Method (nuScenes)        | mAP   | NDS   |\n| ------------------------ | ----- | ----- |\n| SPSS-Conv                | 58.48 | 66.11 |\n| SPSS-Conv (not multiply) | 58.27 | 66.01 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "95156b61-5a53-5c0a-8a3a-fa79daeaef01", "question": "How do you measure the performance of subnetworks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do you measure the performance of subnetworks?", "reference_answer": "A: When measuring the performance of subnetworks, the authors will employ batch-norm re-calibration for each sampled sub-network, following [4]. (As shown in Appendix C.2 of supplementary). Note that both common and stimulative training keep the same testing settings. Since batch-norm re-calibration can be considered as the necessary correction to ensure a fair evaluation, the authors don’t retrain the last logits layer or upscale the signal in the original paper. In this response, the authors also provide some experimental results after retraining the last logits (with 10/100 epochs) and upscaling the signal (following [5]), as shown in Table r1. On the one hand, retraining the last logits can further improve the subnetworks after CT, but subnetworks from (ST + retrain the last logits) still perform much better than that from (CT + retrain the last logits). Besides, the authors show that retraining the last logits with different epochs has no effect on the relative ranking of subnetworks, and the performance of retraining 10 epochs is similar to that of retraining 100 epochs. On the other hand, the authors can see that upscaling the signal has little impact on the performance, the main reason is that batch-norm re-calibration can correct the signal automatically.\n\n[4] Jiahui Yu and Thomas S Huang. Universally slimmable networks and improved training techniques. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1803–1811, 2019.\n[5] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15(1) (2014) 1929–1958\n\n**Table r1: Top 1 Accuracy (%) after retraining or upscaling**\n||Subnet1  | Subnet2      | Subnet3     |\n|:--------------|:----------- |:------ |:-----------|\n|**Method**|[1,1,1,1,1]|[2,2,2,1,1]|[2,3,3,2,2]|\n||(96.69M)|(129.07M)|(192.65M)|\n| CT | **28.48** | **36.13**| **65.25**|\n| CT + retrain the last logits (10) |  61.44 |66.87 |74.95 |\n| CT + retrain the last logits (100)| 63.54| 66.7 |75.2 |\n| CT + upscale the signal | 28.48 | 36.9 |65.16 |\n| ST | **77.85** | **79.43** | **80.61** |\n| ST + retrain the last logits (10) | 77.97 | 79.28 | 80.52|\n| ST + retrain the last logits (100) | 78 | 79"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "42db44a5-0b13-50de-97e2-dea8c776e007", "question": "For Appendix A.2, the bound from equation (18) could become arbitrarily large as $M^q$ and $M^q_v$ are now dependent on the norm of $X$. Does the bound still makes sense in that case?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["70d23f47-63f0-5df1-88ba-b148cfca9c6a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For Appendix A.2, the bound from equation (18) could become arbitrarily large as $M^q$ and $M^q_v$ are now dependent on the norm of $X$. Does the bound still makes sense in that case?", "reference_answer": "A: Although both quantities $M^q$ and $M^q_v$, depends on the norm of $X$, given the ReLU  activation and the batch-norm layer, the norm of $X$ is controlled. As a matter of fact, the training procedure fails to converge even in the floating-point setup if the norm of $X$  increases arbitrarily."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a7fb28ca-8f08-5795-9590-30b27748be45", "question": "What is the exact formula used to compute ensemble variance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["ae801009-35b9-58e7-941e-726cb84d7b54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the exact formula used to compute ensemble variance?", "reference_answer": "A: The authors take the ensemble rejection in forward dynamics model as an example. The authors train an ensemble of forward dynamics models, $f\\_1(s\\_{t+1}|s\\_t), f\\_2(s\\_{t+1}|s\\_t),\\ldots,f\\_N(s\\_{t+1}|s\\_t)$. For a given current state $s\\_t$, the authors can then get an ensemble of next state $(\\hat{s}\\_{t+1}^1, \\hat{s}\\_{t+1}^2,\\ldots,\\hat{s}\\_{t+1}^N)$. The authors then randomly pick one next state while recording the variance in the ensemble at the same time. The authors  then reject the generated next state if the variance in the ensemble is large. That is, the authors evaluate the variance of $(\\hat{s}\\_{t+1}^1, \\hat{s}\\_{t+1}^2,\\ldots,\\hat{s}\\_{t+1}^N)$, i.e., $Var = \\mathbb{E}\\_{i=1}^N [(\\hat{s}\\_{t+1}^i - \\mathbb{E}[\\hat{s}\\_{t+1}^i])^2]$. The authors sort the transitions in a batch by their calculated variance, and only trust the 20\\% transitions that have the smallest *ensemble variance*. The authors will add this detail in the appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3e0e490e-a48f-55ff-b485-41cafe685ab5", "question": "What causes the differences between SVF and WS' or S'W? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["eb805361-8dc3-514d-857b-40fb116fd5b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What causes the differences between SVF and WS' or S'W? ", "reference_answer": "A: In this question, the authors try to provide the proposed understanding of what causes the superior performances of SVF over WS' and S'W. The authors conjecture that this may be related to the context that S or S' can access when fine-tuning the parameters. Assume that W has the shape of [M, N]. S and S' are diagonal matrices. S has the shape of [Rank, Rank], and S' has the shape of [M, M] or [N, N]. When optimizing the parameters, S' only has relations on dimension M or dimension N in a channel-wise manner, while S can connect all channels on both dimension M and dimension N, as S is in the singular value space. This differences can affect the received gradients when training S or S', which results in different performance. To give more evidences, the authors design more variants of SVF and provide their results in the table below.\n\n| Mehod  | Backbone  |Expression of weight |Fine-tune param | Fold-0  |  Fold-1 | Fold-2  | Fold-3  |  Mean |\n| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |\n| baseline  | ResNet-50  | W  | -  | 65.60| 70.28| 64.12| 60.27| 65.07 |\n|  baseline |ResNet-50  |USV$^T$|S|  67.42  | 71.57  | 67.99  | 61.57  | 67.14  |\n|  baseline | ResNet-50 |USS'V$^T$|S'|  67.16 | 71.58  | 68.59  | 61.08  | 67.10  |\n|  baseline | ResNet-50 |USS'V$^T$| S + S'|  66.42  | 71.73  | 67.23  | 61.12  | 66.63  |\n\nThe authors find that given S and S' are lie in the singular value space, all variants can outperform the freezing backbone baseline."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aa0db94f-f818-5a0e-ac52-8794e2df086d", "question": "Tables 1 and 2: are the reported prediction errors evaluated on the training or test set?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["01ec1818-9fe6-516b-ab44-46377c67a0cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Tables 1 and 2: are the reported prediction errors evaluated on the training or test set?", "reference_answer": "A: Both results on JSP and OPF are compared against state of the art industrial solvers (IBM CP-Optimizer for JSPs and COIN-OR IPOPT for OPFs). A modern constraint solver will eventually (given enough solving time) surpass the quality of solutions produced by ML models trained on both the Standard and OD datasets. The authors find that the results based on standard datasets are not competitive and are surpassed within a short time by real solvers, which partially motivates this work. To surpass results coming from OD-trained models on scheduling, for instance, CP-Optimizer can take anywhere from a few minutes to several hours. Comparing with the SoTA runtime makes it possible to compare the proposed approach to the performance of highly optimized industrial solvers. The paper is motivated by contexts in which the same problem is solved repeatedly for different outputs, often in operational settings. This is the case in manufacturing, supply chains, logistics. and energy optimization where the OPF is solved every five minutes,"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d44f23da-d374-53cd-ba92-0b35609255d2", "question": "Why focus on image editing rather than image inpainting?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why focus on image editing rather than image inpainting?", "reference_answer": "A: The authors would like to take the image inpainting as the future work to explore."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "78901cb2-34da-5d0a-8ddc-a066f894d74c", "question": "The SLF performance of the supervised WideResNet-28-10 is much lower than the end-to-end trained supervised model? And SLF vs. AFF?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e3b47ecc-a9c3-57b5-b46f-f70d9e702ea4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The SLF performance of the supervised WideResNet-28-10 is much lower than the end-to-end trained supervised model? And SLF vs. AFF?", "reference_answer": "A: (a) Let us first clarify the notations of pre-training methodologies used in Table S1, where all pre-trained models are finetuned using SLF as the authors stated in the previous response. Thus,  `Supervised` in Table S1 refers to the method of `Supervised AT pretraining + SLF`. \n\n(b) Based on the proposed best understanding of the reviewer's comment, the authors assume that the following holds:\n\n(b1) The `SLF performance of the supervised WideResNet-28-10` is associated with the method of `Supervised AT pretraining + SLF`, namely, the  `Supervised` row of Table S1. \n\n(b2) The `end-to-end trained supervised model` refers to the model acquired  using  the conventional end-to-end supervised AT (without using  pretraining + finetuning). \n\n**the authors hope that the proposed understanding of your comment in the above points (a) and (b) is correct. If not, please feel free to correct us, especially for (b2). The proposed response to your question is unfolded below.**\n\n**(R1)** First, the authors want to point out that the SLF performance of the `supervised WideResNet-28-10` (namely, the  `Supervised` row of Table S1) is comparable  to the `end-to-end trained supervised model` (using AT) reported in related publications, as shown  in **3rd row of Table 1 in [1]; 2nd row of Table 1 in [2]** (see reference details at the end of response): The `end-to-end trained supervised model` under WideResNet-28-10 (without using extra data) achieves 47.10% RA and 86.43% SA on CIFAR-10  in [1] (similarly found in [2]), which is quite close to the SLF performance of the  `Supervised` row of Table S1, with  46.26% RA and 85.95% SA, as shown in **Table S3**.\n\n\n**(R2)** Next,  the authors list the performance of the end-to-end Supervised AT model (reported in [1]),  the Supervised AT pretrained model, and the proposed AdvCL pretrained model evaluated under both SLF and AFF settings, with different architectures on CIFAR-10. Note that as discussed in the *\"Sec. B. Implementation Details\"* of the supplement, the authors use the TRADES-type robust cross-entropy loss for AFF, following [3]. \n\n**Table S3.** Performance of the end-to-end Supervised AT model (reported in [1]),  the Supervised AT pretrained model, and the proposed AdvCL pretrained model evaluated under both SLF and AFF settings, with different architectures on CIFAR-10.\n\n| Method | Backbone | RA(%)  | SA(%)|\n| ----------- |:-----------:|:-----------:|:-----------:|\n|Supervised AT End-to-end [1] |ResNet-18|45.60|78.38|\n|Supervised AT + SLF  |ResNet-18|44.40|79.77|\n|Supervised AT + AFF |ResNet-18|49.89|79.86|\n|AdvCL(ours) + SLF|ResNet-18|50.45|80.85|\n|AdvCL(ours) + AFF|ResNet-18|52.77|83.62|\n|Supervised AT End-to-end [1] |WideResNet-28-10|47.10|86.43|\n|Supervised AT + SLF  |WideResNet-28-10|46.26|85.95|\n|Supervised AT + AFF |WideResNet-28-10|52.80|86.85|\n|AdvCL(ours) + SLF|WideResNet-28-10|53.75|86.71|\n|AdvCL(ours) + AFF|WideResNet-28-10|55.18|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "536f5317-584d-5f09-9288-190cf4e5b8fc", "question": "Why is the formulation \"is a Markov equilibrium (up to function approximation).\" not adapted?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["1c199ede-d2ee-591f-b6af-40cf0994de63"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the formulation \"is a Markov equilibrium (up to function approximation).\" not adapted?", "reference_answer": "A: The authors meant “Up to function approximation” in the sense of “will converge to the equilibrium in the limit of low function approximation error”. Perhaps the authors should be more precise in the proposed language here."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "52a106b0-7660-589b-ae6b-1e818b87262e", "question": "What is the major difficulty that prevents the proposed method from being applied to large images, e.g., 256x256 images? If one uses a small resolution setting to extract synthesized images (e.g., 64x64) and uses techniques like GAN to up-sample it larger (e.g., 128x128), does it bring good performance on large datasets (e.g., ImageNet)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the major difficulty that prevents the proposed method from being applied to large images, e.g., 256x256 images? If one uses a small resolution setting to extract synthesized images (e.g., 64x64) and uses techniques like GAN to up-sample it larger (e.g., 128x128), does it bring good performance on large datasets (e.g., ImageNet)?", "reference_answer": "A: Optimization is the main difficulty in scaling the algorithm to high-resolution images, and it becomes increasingly challenging to capture the correlation between pixels and different examples. t is a good idea to use the GAN approach to up-sample the data, but the main focus of the proposed work is to propose a novel training objective. The authors follow the previous evaluation protocols and architectures closely. The authors will leave the generator approaches for future work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "55b5cb9d-5b3c-53b2-8e25-0b9e9645b746", "question": "In Table 1 the authors compare their method to BiAF in terms of compute time per gradient descent step. How do the total training times compare?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4c78b8be-a940-5979-a99f-7ece20c84cc4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1 the authors compare their method to BiAF in terms of compute time per gradient descent step. How do the total training times compare?", "reference_answer": "A: In the proposed experiments, the authors observed that all the methods take about 150-300 steps to get to the optimal performance on the validation set. So for 200 training samples, the marginal approach is as fast as BiAF while the stochastic approach is 7 times slower than BiAF. Note that BiAF only involves computing a linear combination of features and a summation for backpropagation whereas the stochastic and game DRO methods have to solve a saddle-point problem with iterative methods per gradient step. However, if representation learning is enabled, the computational cost is likely to be dominated by backpropagation in the backbone network. In this regard, the additional cost of replacing the smooth surrogate loss with the proposed method is not significant.\n[1] Stoyanov, Veselin, and Jason Eisner. \"Minimum-risk training of approximate CRF-based NLP systems.\" In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 120-130. 2012.\n[2] Mensch, Arthur, and Mathieu Blondel. \"Differentiable dynamic programming for structured prediction and attention.\" In International Conference on Machine Learning, pp. 3462-3471. PMLR, 2018.\n[3] Gormley, Matthew R., Mark Dredze, and Jason Eisner. \"Approximation-aware dependency parsing by belief propagation.\" Transactions of the Association for Computational Linguistics 3 (2015): 489-501."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d18e68eb-dd6f-5d58-a54a-044598924e70", "question": "For table 1, were the surrogate model trained using labels or in a contrastive manner?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For table 1, were the surrogate model trained using labels or in a contrastive manner?", "reference_answer": "A: All surrogate models except those used in Table 4 (in the paper) are trained via an instance discrimination task, i.e., using labels."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "149d57e9-cb02-546a-92ab-71dec91012ef", "question": "Please provide more details about Equation 2, which defines the graph dynamic programming algorithm.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["b27aca20-af64-5b2b-b51d-4b3babcca320"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Please provide more details about Equation 2, which defines the graph dynamic programming algorithm.", "reference_answer": "A: The objective described by Eq(2) involves searching for a solution P to partition a molecule into fragments with the lowest total intra-fragment degrees of freedom (DoF). The authors define the DoF of each molecular fragment as the maximum RMSD between all possible pairs in the fragment conformation vocabulary V(f). The authors rewrote the vocabulary construction section to help the readers better understand the proposed processing pipeline.\n- Graph dynamic programming algorithm: First search for a substructure from the fragment collection, then split the molecule into several substructures to minimize the mean of DoF of all substructures. Every substructure is  also a nested sub-problem until the substructure can no longer be further split. The authors add the detailed algorithm in Appendix D."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "24a63c7e-9fe7-556e-950d-33068c601ec7", "question": "What is the method performance on out-of-distribution (OOD) samples?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the method performance on out-of-distribution (OOD) samples?", "reference_answer": "A: The authors conducted the following experiment to evaluate the method performance on OOD samples. Specifically, in the current problem setting, OOD samples are considered as adversarial samples obtained via the PGD attack. The method performance is quantified as the value of $KL[P(X_2|X_1)\\Vert Q_{\\Lambda}(X_2|X_1)]$ (in Eq. (7)), which measures how well the projected regional features $h^{(r)}$ reflects sample-wise similarities. A smaller KL divergence indicates better performance. Thus, if the value of $KL[P(X_2|X_1)\\Vert Q_{\\Lambda}(X_2|X_1)]$ on normal samples is similar to that on OOD samples, the authors can consider the method performance is good on OOD samples. The authors compared the method performance between normal samples and OOD samples. OOD samples were used in the learning of the projection matrix $\\Lambda$, and the authors calculated the value of $KL[P(X_2|X_1)\\Vert Q_{\\Lambda}(X_2|X_1)]$ based on the conv_53 layer feature in VGG-16 for normal samples and OOD samples. The table below shows that the value of $KL[P(X_2\\vert X_1)\\Vert Q_{\\Lambda}(X_2\\vert X_1)]$ on normal samples was similar to that on OOD samples. This indicated that the method performance on OOD samples was good.\n\n|                                                      | on normal samples | on OOD samples |\n| ---------------------------------------------------- | ----------------- | -------------- |\n| $KL[P(X_2\\vert X_1)\\Vert Q_{\\Lambda}(X_2\\vert X_1)]$ | 0.8491            | 0.8619         |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ed5db17d-912b-5f6d-b276-df45ab7aa573", "question": "Don't you just have to solve MNIST on the nodes' images and then solve edge covering?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["92c6d0ac-b67d-5898-9b75-1baeb7804479"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Don't you just have to solve MNIST on the nodes' images and then solve edge covering?", "reference_answer": "A: Although the ground-truth cost is a simple sum of some node-feature-based edge weights, the authors do not assume that the authors know such a specific form in prior. The authors also do not assume the authors know any labels of the MNIST node features. So, the reviewer’s suggested pipeline is not applied."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "13b4924e-1398-5a02-bac8-b33a51ab8f97", "question": "How is a 3D image input represented?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f1c2adf-3164-5df0-b6ba-b27f76ecd17c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is a 3D image input represented?", "reference_answer": "A: The 3D image in Section 3.2 is represented as a 2D matrix with RGB channels, similar to how CLEVR is represented as a 2D image of a 3D scene."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3c4efa0d-0b65-5372-8a8b-a9866f61f095", "question": "Why is DiffPure a complicated defense?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["916943a1-d7b2-5f15-9fce-d4ccf3637a9c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is DiffPure a complicated defense?", "reference_answer": "A: DiffPure has a complicated solver of stochastic differential equations (SDE) and requires high-end GPUs with 32 GB of memory."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "08d52141-b64c-5353-aa4e-21caed4e53e5", "question": "How did the authors handle the kernel computation in equation 2? Do they use mini-batches instead?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How did the authors handle the kernel computation in equation 2? Do they use mini-batches instead?", "reference_answer": "A: In practice, the authors use all the synthetic data and sample a minibatch from the real dataset to compute the meta-gradient (Algorithm 1). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "032d383e-0ddd-5830-ad19-cd7e50d4dbc1", "question": "“Lemma 1: How to compute the iterate $w_{S,k}^{\\*}$ since the guarantee is on this iterate? The definition of eq. (12) requires knowing $w_{k}^{\\*}$ which are local minima. Is there a way to get a result on a realistic iterate that does not require knowledge of local minima?”", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["973b558e-915b-58ba-9d2a-9f46e25abc94"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "“Lemma 1: How to compute the iterate $w_{S,k}^{\\*}$ since the guarantee is on this iterate? The definition of eq. (12) requires knowing $w_{k}^{\\*}$ which are local minima. Is there a way to get a result on a realistic iterate that does not require knowledge of local minima?”", "reference_answer": "A: The authors do not need to get any of $w_{S,k}^{\\*}$ or $w_{k}^{\\*}$. As the authors have clarified in line 214, the stability bound in Theorem 1 can be applied to any infeasible algorithm. Thus, the authors construct the infeasible auxiliary iterates in equation (13) to derive the generalization error on $w_{S,k}^{\\*}$ via bounding the stability of the auxiliary iterates. This is a purely theoretical analysis, and does not require knowing any of $w_{S,k}^{\\*}$ or $w_{k}^{\\*}$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7ad61a7d-f11d-54c4-8c35-5acc75703e63", "question": "How the Q function is estimated, and also how the integral over is computed in practice?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["a1d44a36-ee0d-593c-b65d-3937420a8bab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How the Q function is estimated, and also how the integral over is computed in practice?", "reference_answer": "A: The authors apologize for the confusion, and the authors have added additional details in the proposed work for clarity in the proposed revision: \n\n\"where $Q$ is the state-action value function estimated using the normal Bellman equation and proposed surrogate reward function\"\n\n\"In practice, Eq. (15) can be resolved via reparameterization trick. However, this can be easier in deterministic environments with deterministic expert data, where the expert state transition is a simple Dirac distribution and thus does not require the extra sampling step but can be computed directly via the output of the state transition predictor\"."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "729f3de9-9535-53d4-8d5e-67b189a56d1e", "question": "Could the second term of Eqn. 7 be further simplified?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d87b5532-5c5f-5940-aa2b-f17ffad843cd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could the second term of Eqn. 7 be further simplified?", "reference_answer": "A: Yes. Mathematically, the second term in Eqn. 7 in the proposed work and $\\beta\\frac{1}{|\\mathcal{G}|}\\sum_{(G,y)\\in\\mathcal{G}}-\\log q_\\theta(y|G)$ are both equivalent to $\\beta\\mathbb{E}_{(G,y)}[-\\log q_\\theta(y|G)]$. \n\nFor practical implementation, the two formulas are slightly different. The expectation $\\mathbb{E}_{(G,y)}[-\\log q_\\theta(y|G)]$ is hard to calculate directly, thus Monte Carlo estimation is applied to approximate this value. The proposed implementation first uses the samples under each specific environemt for approximating the environment-specific risk and then calculate the average across different enviroments. The second term in Eqn. 7 is exactly what the authors have done in the proposed implementation. Therefore, the authors kept this form in the paper instead of using the simplified one to stay consistent with the proposed implementation.\n\nIt should be mentioned that there is a absolute value symbol $\\vert\\cdot\\vert$ in the first term of Eqn. 7. Hence, even if the second term of Eqn. 7 is simplified, the two terms are still completely different."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "31af77e4-b8a0-535a-841c-e0a7870f456f", "question": "In eq (2), should $y_{nk}$ be $y_{nl}$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4d52f085-ca43-524f-bc59-fcf55a9f5859"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In eq (2), should $y_{nk}$ be $y_{nl}$?", "reference_answer": "A: The correct log likelihood function\nunder the proposed prior setting (Eq. (2) in the main paper) should be:\n\n<$$\\ln p({\\bf Y}|{\\bf X})=\\ln \\int \\int \\sum_{Z}\\prod_{n} \\prod_{k} \\prod_{l} p( {\\bf f}^{(k)}| {\\bf X})p(\\theta_{kl})  p(z_{nk}| {\\bf f}_n)  p(y_{nl}|z_{nk},\\theta_{kl}) dF d\\Theta$$>\n    \nThe above equation also illustrates how the labels are generated given\nthe mixture model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d93b9e08-758c-59e5-bb0b-a4f6c01bdba0", "question": "Using MAGs: I believe modeling the causal graphs using MAGs is a major weakness of the work when considering interventions. MAGs are attractive under Markov equivalence [27,28] and under soft interventions [10,14] because the true causal diagram (DAG with latents or ADMG) is not differentiable from its equivalent MAG. This is not true under hard interventions as considered in this work. For instance, consider the causal graph $G=\\\\{ A\\rightarrow B\\rightarrow C, B\\leftarrow L \\rightarrow C \\\\}$ and the corresponding MAG $M=\\\\{ A\\rightarrow B\\rightarrow C, A\\rightarrow C \\\\}$. Both graphs are Markov equivalent, yet they are differentiable under $do(B)$. The challenges discussed in the paper are limitation in atomic interventions rather than a justification for adopting MAGs. Please explain.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["417f2603-5c9f-5a8c-ba54-87cae5475819"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Using MAGs: I believe modeling the causal graphs using MAGs is a major weakness of the work when considering interventions. MAGs are attractive under Markov equivalence [27,28] and under soft interventions [10,14] because the true causal diagram (DAG with latents or ADMG) is not differentiable from its equivalent MAG. This is not true under hard interventions as considered in this work. For instance, consider the causal graph $G=\\\\{ A\\rightarrow B\\rightarrow C, B\\leftarrow L \\rightarrow C \\\\}$ and the corresponding MAG $M=\\\\{ A\\rightarrow B\\rightarrow C, A\\rightarrow C \\\\}$. Both graphs are Markov equivalent, yet they are differentiable under $do(B)$. The challenges discussed in the paper are limitation in atomic interventions rather than a justification for adopting MAGs. Please explain.", "reference_answer": "A: When not all variables of interest can be measured, DAGs between these observed variables are not sufficient to represent the observed distribution, since latent variables may introduce confounding effects between the observed variables. In these cases, it is very common to model the observed variables through MAGs and it comes with many desirable properties (see e.g., Richardson et al. 2002, ‘Ancestral Graph Markov Models’).  The example pointed out in the question is correct. Using single vertex (atomic) hard interventions, the authors can differentiate MAGs from DAGs for some specific graphs. However, it is not generally true for any pair of MAG and a DAG. Consider the DAG $G=$ { $A \\rightarrow B \\rightarrow C, A \\leftarrow L1 \\rightarrow C, A \\leftarrow L2 \\rightarrow B, B \\leftarrow L3 \\rightarrow C $} with latents $L1$, $L2,$ and $L3$; and a MAG $M=$ { $A \\rightarrow B \\rightarrow C, A \\rightarrow C $}. The authors can observe that both these causal graphs are Markov equivalent. However, unlike the example mentioned by the reviewer, the authors cannot distinguish these two graphs using any single vertex interventions. Such examples can be constructed for distinguishing two DAGs as well (Fig. 4). As described in the paper, for practical reasons, the proposed choice of interventions is restricted to atomic interventions (non-atomic interventions are just too hard to implement in practice). However, the authors note that the proposed choice of using MAGs is not based on the choice of the interventional setup (atomic or not), but rather the fact that the MAGs are an attractive way to model the causal structure in presence of latents. The proposed setup does benefit from the fact the learning of MAGs is compatible with atomic interventions. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6fd6d10b-93cf-544d-995c-4592f134b11f", "question": "In Table 1 I find it difficult to understand how fair the comparisons are. Are these models similar in terms of number of layers? Or number of parameters? Given BrainNetTF seems to have had some sort of hyperparameter search, it is probably unfair to compare with out-of-the-box models?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1 I find it difficult to understand how fair the comparisons are. Are these models similar in terms of number of layers? Or number of parameters? Given BrainNetTF seems to have had some sort of hyperparameter search, it is probably unfair to compare with out-of-the-box models?", "reference_answer": "A: Sure, the authors calculate the number of parameters for each model and show the results below. It is shown that the proposed model is larger than GNN and CNN models but has a similar size to other transformer models. \n\n|Dataset|Method|#Para|\n| :----: | :-----: | :---------: |\n|ABIDE|BrainNetTF|20.2M|\n|ABIDE|BrainNetCNN|0.93M|\n|ABIDE|FBNetGen|0.55M|\n|ABIDE|VanillaTF|15.6M|\n|ABIDE|SAN|57.7M|\n|ABIDE|Graphormer|1.23M|\n|ABIDE|BrainGB|1.08M|\n| :----: | :-----: | :---------: |\n|ABCD|BrainNetTF|45.1M|\n|ABCD|BrainNetCNN|0.93M|\n|ABCD|FBNetGen|1.18M|\n|ABCD|VanillaTF|32.7M|\n|ABCD|SAN|186.7M|\n|ABCD|Graphormer|1.66M|\n|ABCD|BrainGB|1.49M|\n\nAs for the hyperparameter tuning, the comparison is fair since grid search is applied for hyper-parameter tuning for the proposed model as well as important parameters in all other baselines. To be specific, for BrainGB, the readout function is searched from {mean, max, concat} and the message-passing function is searched from {Edge weighted, Node edge concat, Node concat}. For BrainGNN, the learning rate is searched in {0.01, 0.005, 0.001} and the feature dimension is searched in {100, 200}. For FBNetGen, different encoders {1D-CNN, GRU} are tested with different hidden dimensions {8, 12, 16}. For BrainNetCNN, dropout rate is selected from {0.3, 0.5, 0.7}. For VanillaTF, the number of transformer layers is searched from {1, 2, 3} with the number of headers from {2, 4, 6}. For SAN, the authors test LPE hidden dimension from {4, 8, 16}, the number of LPE and GT transformer layers from {1, 2}, and the number of headers from {2, 4} with 50 epochs of training. For Graphormer, the number of encoder layers is selected from {1, 2} and the embed dimension is from {256, 512}."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1206d41e-b405-5582-93bd-b43804d8f1b8", "question": "In figure 1, since MNLI performance is the criteria and also the y-axis, why is the selected models (red) not always the best performing models?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 1, since MNLI performance is the criteria and also the y-axis, why is the selected models (red) not always the best performing models?", "reference_answer": "A: Note that the proposed objective is to minimize the \\#FLOPs and maximize the accuracy (e.g., on MNLI) with the trade-off determined by the resource constraint (see A5) for different partitions (Base, Small, Tiny). Given a gallery of compressed models from AutoDistil with variable FLOPs and performance, the authors use A5 for optimal model selection. Another potential reason why red is always not the best model is that the authors use the heldout validation set from the unlabeled training corpus (Wikipedia + BooksCorpus) for student selection and then evaluate them on MNLI (see Section 3.3) which may not be optimal due to sample differences in the two datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "add1ae49-ca3a-5ff1-aa5a-6bd60408bf77", "question": "How does the convergence speed of the proposed model compare to the baseline methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["46b47e0a-c219-5d2c-a609-f04a9b617ca4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the convergence speed of the proposed model compare to the baseline methods?", "reference_answer": "A: The table of convergence epochs shows that the convergence speed of the proposed models are on par with other KD baselines."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2ebd3051-d57f-5c58-bfbd-f869dccb3b67", "question": "Did you include the full noise-free data in Figure 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["480f038b-c554-5f51-ab95-3dc71882eab4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Did you include the full noise-free data in Figure 2?", "reference_answer": "A: Yes, the authors have now included the full noise-free data in Figure 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "33f61d84-71f5-5879-a4ca-01fd28397afa", "question": "How does node perturbation with weight normalization perform in more complex datasets and architectures?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e391ac18-50de-5c7d-ab70-9a7b36fb2cb0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does node perturbation with weight normalization perform in more complex datasets and architectures?", "reference_answer": "A: In newly added Figure S5, the authors investigated NP learning in a convolutional neural network solving CIFAR-10. \nAs expected, vanilla NP learning becomes unstable in the middle of learning when the learning rate is too large (Fig. S5A). \nHowever, by adding weight regulation via weight decay, NP learning becomes stable even at a large learning rate (Fig. S5B), supporting the applicability of the proposed results for complex networks and tasks. \nHere, the authors applied weight decay instead of weight normalization, because an implementation of the weight normalization in a convolutional network was somewhat tricky.\nthe authors explained this result briefly at the end of section 5 in the main text. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4091b2b1-3ca3-5f79-89bc-b444d25f4c38", "question": "Why the two expressions in Eq.7 could be equivalent, i.e., Spatial vs Time?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["46b47e0a-c219-5d2c-a609-f04a9b617ca4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why the two expressions in Eq.7 could be equivalent, i.e., Spatial vs Time?", "reference_answer": "A: The equivalence of two equations in Eq.7 is based on recent works [1-5] that built a connection between the heat equation and GNN. The main result of these works is: \"By treating node features H as signal X (corresponding to $x(u,t)$ in heat equation Eq.2) on the graph, solving the heat equation with Euler scheme yields the formulation of a GNN layer.\" In other words, the GNN can be seen as the discretizations of the continuous diffusion process described by the heat equation. Correspondingly in Eq.7, the left equation is a general GNN layer (discretized diffusion process), and the right equation is directly derived from Eq.2 (continuous diffusion process). Moreover, different definitions of Laplace-Beltrami operator $\\Delta$ yield different GNNs (such as SGC and linear GAT). Unfortunately, not all GNNs have a simple form of $\\Delta$, and for some of them, whether there exists such $\\Delta$ is an open research question. Therefore, the operator is written as $\\Delta(f_\\theta, \\mathcal G)$ to associate it with model $f_\\theta$ and use equivalence in Eq.7 as an analytical assumption. \n\nReferences:\n[1] GRAND: Graph Neural Diffusion, in ICML’21; \n[2] Beltrami Flow and Neural Diffusion on Graphs, in NeurIPS’21; \n[3] Dissecting the Diffusion Process in Linear Graph Convolutional Networks, in NeurIPS’21; \n[4] PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations, in NeurIPS’21; \n[5] Graph Neural Networks as Gradient Flows, arxiv’22"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a73440b3-1785-5520-8353-273cf50ae4e1", "question": "Explain why in figure 3, the RL curve converges to random guess and has no oscillation? Is it NaN in gradient?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain why in figure 3, the RL curve converges to random guess and has no oscillation? Is it NaN in gradient?", "reference_answer": "A: In this case, the RL attack drives the model accuracy to a very low level (~10%) due to the loss being extremely large. However, it is not due to a NaN in the gradient vector, as the authors adopt a NaN detection technique in the experiments, and the attacker will take a less ambitious action if a NaN is encountered (see Appendix D.1 Attack execution setting). As they observed in the experiments, the RL attack can quickly lead the server to a ‘bad’ model, while each gradient it sends is still legal. This again shows the advantage of the RL attack over myopic attacks, i.e., finding a shortest path (multiple steps into the future) towards a target model instead of finding a one-step gradient (after aggregation) that points to a bad model. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "87caf23d-04b6-53ff-a3a4-84095b413586", "question": "Typically, the FISTA algorithm requires hundreds of iterations to converge so my expectation is that the reconstructions x=Az  with only 2 iterations are not high fidelity (e.g., terms of PSNR). This is supported by the visualization in Appendix B2 which shows that feature maps only encode contours or high-level information about the input. The authors mention that increasing the number of FISTA iterations can boost the classification performance a bit. Have the authors’ studied how increasing the number of FISTA iterations affects the model’s robustness to noise or can they provide intuition about it?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["823a2e85-8548-57a2-8256-5fdb0159c2af"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Typically, the FISTA algorithm requires hundreds of iterations to converge so my expectation is that the reconstructions x=Az  with only 2 iterations are not high fidelity (e.g., terms of PSNR). This is supported by the visualization in Appendix B2 which shows that feature maps only encode contours or high-level information about the input. The authors mention that increasing the number of FISTA iterations can boost the classification performance a bit. Have the authors’ studied how increasing the number of FISTA iterations affects the model’s robustness to noise or can they provide intuition about it?", "reference_answer": "A: The following table shows how the number of FISTA iterations affects the model’s robustness to noise. The model is trained on the ImageNet dataset. The “natural accuracy” column is the accuracy tested on the validation set of ImageNet, the columns “Gaussian”, “Shot”, and “Impulse” are three different noises from ImageNet-C. The authors report the top-1 accuracy results with adaptive lambda. While using more iterations slightly increases the model performance on both natural accuracy and robust accuracy. \n\n|# of FISTA iterations       |natural accuracy  | Gaussian  | Shot       |  Impulse|\n|----------------------------|---------------------|-------------|-----------|-----------|\n|2                                     | 69.47%                |  29.16%   |  27.59% |  22.01%|\n|4                                     | 69.51%                |  29.69%   |  28.15% |  24.15%|\n|8                                     | 69.79%                |  30.91%   |  29.87% |  26.69%|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "307de5d3-b853-56a5-84c2-1d8d7ea2fb02", "question": "Can ClimbQ help with the inference performance by exploiting class imbalance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ad1b4108-8f9b-5cf4-a905-ceaa86a2a5ab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can ClimbQ help with the inference performance by exploiting class imbalance?", "reference_answer": "A: The authors have conducted experiments to compare the inference time (sec./batch) and throughput (#images/sec.) of ClimbQ with the quantization baselines in the paper. The results are presented in the linked document: https://www.dropbox.com/s/nqxps1p3sjbeo9c/response_to_reviewer2_additional_Q2.pdf?dl=0.\n\nIt can be observed that ClimbQ has fewer time costs in inference, i.e., with smaller latency compared with other approaches. In addition, the throughput of ClimbQ is higher, i.e., more images can be processed in a fixed time span. The better efficiency of ClimbQ in inference than that of the compared approaches is mainly due to a simple function adopted (see Eq. (1)) for the scaling and projection of class distributions and the uniform quantization (see Eq. (2)) without other additional operations used in the compared approaches, such as clipping functions and transformations. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d12ccdd2-2618-5826-9900-7a5a056c6acc", "question": "What is the image size?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the image size?", "reference_answer": "A: For Super-Resolution, the proposed input is $32\\times32$ and the proposed output size is $512\\times512$. For all other tasks, the input and target resolution are $256\\times256$ and the authors downsample the input to the corresponding operating resolution at each level of the hierarchy. The authors will include this in the camera-ready. Regarding scaling up to image size of 1K, one can simply add an additional level in the hierarchy to reach that resolution.    "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ece0b31-3129-5c0d-be49-e43b16c4c782", "question": "The baseline navigation models used in the paper are sort of out-dated. How about some recent advanced VLN models such as VLN-BERT and Transformers? Would the CL work for them or not?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["122ecbbc-76f5-525c-a3fa-1ea47770e1a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The baseline navigation models used in the paper are sort of out-dated. How about some recent advanced VLN models such as VLN-BERT and Transformers? Would the CL work for them or not?", "reference_answer": "A: For VLN-BERT, this work aims to use web-scraped vision-and-language materials to learn visual groundings that is transferable to VLN tasks. As stated in the paper, the training of VLN-BERT contains a generalized curriculum learning process, i.e. from language-only data, to web image-text pairs and finally to path-instruction pairs from the VLN dataset. Since the proposed work focuses more on the curriculum inside path-instruction pairs from the VLN dataset, the authors did not make a direct comparison. The authors believe that using VLN-BERT as the backbone and adopting the proposed method for fine-tuning can improve the result. The additional training data used by VLN-BERT does not conflict with the proposed method (which does not need extra data). The authors think the agent's performance can benefit from both. The authors will supplement experiments in the next version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b63b0d47-078f-57cd-a06f-bf42e3a1144f", "question": "Can the proposed approach be applied to larger colored image datasets such as CIFAR-10? If yes, summarize the results.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can the proposed approach be applied to larger colored image datasets such as CIFAR-10? If yes, summarize the results.", "reference_answer": "A: The authors found that it is possible to recover a batch of 4 images from CIFAR-10 using the method of Inverting Gradients, which is not very effective for the purpose of distribution learning. However, using gradient leakage to recover training images is a growing area and there are more works trying to recover a large batch of images. For example, GradInversion [2] can recover data from a larger batch (8-48 images) of ImageNet data for ResNets. On the other hand, since the authors consider an insider attack in this work, the attackers’ local data can be used to build the simulator even without distribution learning. The table below shows that when the attackers use 500 real images from CIFAR-10 (<1% of total data) owned by themselves to train a policy, the proposed RL based method still outperforms other baseline attacks.\n\nCIFAR10 Clipping Median\n                                          200 FL Epochs      600 FL Epochs      1000 FL Epochs\n            No Attack                       35.38%                45.38%                 53.7%\n            IPM                                  28.83%                36.85%                 42.98%\n            EB                                    31.96%                43.45%                 10%\n            LMP                                 14.53%                31.73%                 10%\n            Proposed Method         10.05%                10.28%                 9.52%"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9113fe8f-50a9-5277-8d92-00789831eb13", "question": "What is the difference in definition of uniformly stable from work [1]?\nReference:\n[1] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, 2016.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["973b558e-915b-58ba-9d2a-9f46e25abc94"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the difference in definition of uniformly stable from work [1]?\nReference:\n[1] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning, 2016.", "reference_answer": "A: The uniformly stable in Definition 2 of this paper, is different from the one in (Hardt et al., 2016), which does not take expectation over training sets $S$ and $S^{\\prime}$. The authors adopt the new definition because the proposed proof of generalization is based on such new uniformly stability. Theoretically speaking, the additional expectation taken over the training set here is used to upper bound the probability of good events as in equation (49) in the Appendix, so that the authors can upper bound the generalization error.   "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "62345067-14b2-5c94-8b52-3598fdd47466", "question": "4: What does $\\Delta_s$ mean in eq. 6 and 9?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "4: What does $\\Delta_s$ mean in eq. 6 and 9?", "reference_answer": "A: $\\Delta_s$ and $\\Delta_g$ stand for the data space perturbation applied to the source and guide images, respectively."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bd0cc93a-7646-5ad0-803b-b3bb94997aee", "question": "What is the intuition behind the derived reward function in Eq 7? It looks kind of like an advantage function.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the intuition behind the derived reward function in Eq 7? It looks kind of like an advantage function.", "reference_answer": "A: The intuition is that, given a q-value function (which is derived from the policy by Assumption 1), the authors will be able to derive a reward function as Eq 7 by rearranging the Bellman optimality equation (Eq 2). \n\nIn RL, the (optimal) advantage function is defined as\n$A(s,a) = q(s, a) - v(s) = q(s,a) - \\max_{a’} q({\\color{red}s}, a’)$\n\nBut the proposed derived reward is \n$r(s,a) = q(s,a) - \\max_{a’} q({\\color{red}s+[a]}, a’)$\n\nThe difference is highlighted in red. In other words, there is a one-step shift in the second term. \n\nMore importantly, the advantage is defined to have a relative comparison among different q-values given a state (e.g., for actor-critic training), where the reward is typically assumed to be well-defined and given.\n\nthe authors instead derive the reward from a given q value function. Therefore, the authors believe they are not related although they appear similar. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "951215a3-3851-5ec0-91b9-0741dfddb0c1", "question": "Does the technique also work in other domains like NLP?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the technique also work in other domains like NLP?", "reference_answer": "A: Following much of previous works [3,4,5,6], the authors conduct experiments in the area of image classification. The authors also believe that it is an exciting problem to study the effectiveness of ETF in the field of NLP, but it remains challenging to use ETF for NLP. For instance, it is unclear in the NLP domain whether critical differences exist between those models learned from a few data and those learned from extensive training data, which is beyond the scope of this work. The authors sincerely appreciate your comment and will explore such an interesting problem in the future. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "eb52cc36-ebc3-5df4-8232-f9032676ca75", "question": "On Experiments: In the experiments you use the setting from [1] but you did not compare against the algorithms from [1]. Was there any particular reason for this? In addition i believe that parameter alpha and beta were never properly defined in the main paper but they are referenced extensively in the experiments related to generalized OGD\nReference:\n[1] T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5c82eb89-b12e-5a4b-884a-105abda1008d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "On Experiments: In the experiments you use the setting from [1] but you did not compare against the algorithms from [1]. Was there any particular reason for this? In addition i believe that parameter alpha and beta were never properly defined in the main paper but they are referenced extensively in the experiments related to generalized OGD\nReference:\n[1] T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. arXiv preprint arXiv:1906.00331, 2019.", "reference_answer": "A: The proposed experiment is based on the [2], not [1]. Stochastic smoothed AGDA [2] is introduced as the fastest single loop algorithm for NC-SC theoretically, and this was the reason the authors chose [2] to compare this algorithm with OGDA in practice. Moreover, the authors already compared the proposed algorithm with all algorithms in [2] as it is shown in Figure 1(a). The authors acknowledge that experiment in [2] is actually based on [3], however the problem setup and algorithms in [3] are totally different than ours as they considered variance reduction, and Hamilton gradient descent, while the proposed problem setup is clearly different. Also, note that even [2] does not compare with the algorithms in [3]. Moreover,  the parameters $\\alpha$ and $\\beta$ are supposed to represent the ratio between the correction term, and the current gradient for generalized OGDA algorithm. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8bb0c54f-0298-5c88-8b7e-eef980686ded", "question": "As all methods almost completely fail on original real-world data, are there any substantial findings that are directly related to real images?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As all methods almost completely fail on original real-world data, are there any substantial findings that are directly related to real images?", "reference_answer": "A: This is great question. In addition to the proposed analysis and findings for each of the four models in the newly added Section 4.5, the authors further conduct the following generalization experiments to investigate how the real images impact the models. In particular, the authors use the well-trained model from dSprites dataset to directly test on three fully-ablated real-world datasets, i.e., removing all four factors. The quantitative results are as follows:\n\n\n| dataset  | AIR | MONet | IODINE | SlotAtt |\n| ------------- | ------------- | ------------- | ------------- | ------------- |\n|   | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec |\n| YCB - C+S+T+U | 21.0 / 25.4 / 42.2 / 37.1 | 69.5 / 56.4 / 64.1 / 77.0 | 87.2 / 65.5 / 80.6 / 89.7 | 67.5 / 50.3 / 58.5 / 75.4 |\n| ScanNet - C+S+T+U  |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7c4cee7d-1d4d-5052-9e91-22b981427d10", "question": "is vec{u} a vector of one voters' utilities per candidate, or a vector of utility functions of all voters?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2878f2ff-8f3e-5f98-9ab1-2e6e62a416c3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "is vec{u} a vector of one voters' utilities per candidate, or a vector of utility functions of all voters?", "reference_answer": "A: $\\vec{u} \\in \\mathbb{R}^{n\\times m}$ is the utility profile, or the utility functions of all voters (line 76). In Figure 1, the social welfare function takes the utility functions of all voters as an argument. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b25a7b7e-69ec-59f0-b030-0e54ab374ed8", "question": "Why it is necessary to have a rigidity network in addition to a motion network, given the magnitude of motion represents rigidity?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6c995533-fb04-547f-9d46-27b6ce8f709d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why it is necessary to have a rigidity network in addition to a motion network, given the magnitude of motion represents rigidity?", "reference_answer": "A: The rigidity network can disambiguate moving foreground vs. static background. An intuitive alternative is to use the magnitude of the motion field as a criterion to separate dynamic foreground, i.e. large motion area corresponds to dynamic objects (and the converse). However, in some frames, dynamic parts might only contain small offsets from the canonical frame. For example, a cyclic bouncing ball could overlap with the canonical position and thus have offset values of zero, even though it should be classified as a dynamic area. \n\nNEW Figure 9 (a, b) filters the scene using the motion magnitude. It is not a good criterion, since a large portion of the background is still left, while the ball is already incomplete in (b). The separation using the rigidity map in Figure 9 (d) is much better. The reason is that the rigidity map collects the motion information from all frames and it is not restricted to a single frame."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6079fd1c-2471-5a3d-b1a3-5f13870e3911", "question": "Why is the explanation of Lambda in Figure 1 too short to be understandable?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7566656d-1495-5b20-92c3-535a544492ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the explanation of Lambda in Figure 1 too short to be understandable?", "reference_answer": "A: The authors extended the explanation in the revision and added the detailed definition of the function to the appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8368db85-a891-5896-927d-8c003fff0a41", "question": "Practical value of gradient bias in contrastive learning/Is gradient bias a real problem?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["47568b20-d6be-558d-a292-b109717b3b01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Practical value of gradient bias in contrastive learning/Is gradient bias a real problem?", "reference_answer": "A: The authors argue that gradient bias indeed is indeed a real problem worth investigating, for the following reasons. The authors will incorporate these arguments into the final revision (which has more space) for clarification.\n1. Gradient bias intrinsically inherits the non-decomposability of the contrastive loss due to the negative samples in the denominator. In principle, since contrastive learning defines a set of transformation functions, meaning one data sample in theory should be associated with an infinite number of negative samples by taking different transformations of other data. This is defined as the generalization loss. Using minibatch optimization on a finite training data defines the empirical loss. One difference of contrastive learning compared to standard supervised learning is that the gradient, when simply doing empirical risk optimization with minibatches, is a biased version of the true gradient. Thus, simply using empirical loss optimization with bias gradients to approximate the generalization loss optimization might not be a good idea, which can lead to potential sub-optimal solutions.\n2. A: The authors demonstrate the performance gap with extensive experiments, all suggesting performance improvements when correcting the gradient bias with the proposed algorithm. Remarkably, the proposed solution to this issue only needs minimal modifications to the original contrastive learning, but can lead to consistent performance improvement under different settings (from small scale to large scale problems, and from single modal data to multi-modal data), with little computation overhead (empirically around 10% in the proposed experiments compared to standard contrastive learning).\n3. A: In the cases of small minibatches, since the sum of negative similarity scores will typically be more noisy than that with larger minibatches, it is expected that the gradient bias will be a little more dominating. However, even if using large minibatches, gradient bias still exists. This can be seen from the performance gap between the proposed method and the standard contrastive learning. Please see the gaps of large batchsizes in Table 1 and 2. Even though the gaps tend to become smaller, they are still significant enough compared to the standard contrastive learning (in the large models and data setting such as the ImageNet experiments in Table 2, the authors believe an improvement around 0.5 is considered significant in the community, e.g., please see Table 2 in the UniCL paper: https://arxiv.org/pdf/2204.03610.pdf).\n4. A: The authors agree increasing batch sizes can mitigate the issue to some extent. However, this is not considered to be principled and economic for this problem. The authors believe one goal of research is to develop better and more efficient solutions for large problems, and the authors do not think it is wise to stop exploring other efficient solvers for a problem if one only can solve it in a resource-heavy way (such as using large minibatches). Increasing batch size indicates the need to use expensive and higher-performance computing machines. For example, MoCo-v3 uses the most powerful TPU servers for large batch-size experiments, which is unaffordable to most researchers. The proposed method tries to address the problem in a more principled way by correcting the gradient bias, which the authors believe can be further improved with acceleration techniques such as variance reduction from standard stochastic optimization literatures (which the authors leave as interesting further work).\n5. A: The authors believe, in the near future, the scale of data will increase much faster than that of the computational power. In other words, the largest batch setting achieved today is still considered small given an extremely large dataset, especially in the multi-modal setting. So investigating scaling up small minibatch training is still an important problem.\n6. A: Furthermore, the proposed work provides one potential explanation for the common question of “why contrastive learning needs much larger batch sizes compared to standard supervised learning?”. The authors can explain it from the perspective of gradient bias, i.e., smaller batch sizes could induce more gradient bias thus it is more difficult to control the quality of stochastic gradients, leading to worse solutions.\n7. A: Using other tricks such as feature normalization can mitigate the problem to a certain degree. However, the bias could still exist in theory. Moreover, the proposed method is orthogonal to these tricks so can be combined to get better solutions."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "afce3dd8-18ba-51dd-9492-738e964929b7", "question": "The training time and memory complexity of v5 is not provided in Table 5. Is it similar to the basic version? What is the trade-off between the efficiency and effectiveness of v5?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["42636461-55ba-5d99-a3d8-e7949e4cef52"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The training time and memory complexity of v5 is not provided in Table 5. Is it similar to the basic version? What is the trade-off between the efficiency and effectiveness of v5?", "reference_answer": "A: Actually, the time refers to inference time in Table 5, and for CTR prediction the authors care more about the online inference efficiency. As mentioned in Section 3.2, over-parameterization does not introduce any additional latency or memory cost to inference. It means the time and memory cost is similar to V4. It is true that adding over-parameterization will bring additional cost to training. But considering the efficiency during inference, for the CTR prediction tasks, it is willing to adopt over-parameterization."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "47e5e653-2df2-5132-9094-7fa128dad9e6", "question": "How does the method compare to traditional numerical inverse solvers?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table", "formula"], "anchor_pdf": ["1eb3000c-95d6-590d-a4af-4fcbb2e5cd06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the method compare to traditional numerical inverse solvers?", "reference_answer": "A: In theory, traditional iterative solvers can surpass the network predictions in accuracy but at the cost of much higher run-time. Table 1 shows this for the proposed Navier-Stokes experiment. The proposed domain-specific scale-invariant solver reaches the same accuracy as the network after 7 iterations while a standard optimizer will take thousands of iterations and not find the best solution. The authors also tested the gradient descent and L-BFGS-B optimizers on the heat equation. Figure 13 in the revised PDF shows the optimization curves. As expected, L-BFGS-B performs better than gradient descent, matching the prediction from the neural network trained with Adam after 500 iterations. However, both optimizers fail to reach the accuracy of the near-instantaneous (64 ms) predictions of the SIP network within a reasonable time frame. Running L-BFGS for 1000 iterations took 102 seconds. The results are visualized in Fig 10 top."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1acb7c86-f673-51b1-8a0e-9e105ae9e6c8", "question": "P2.2 The proposed formulation seems specific to the setup: if the task becomes walking up/down stairs, or traversing stepping stones, having desired and commanded velocity from the human controller might be insufficient.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f9d0d9cf-0cc7-5bc1-b533-aa5fc1801582"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "P2.2 The proposed formulation seems specific to the setup: if the task becomes walking up/down stairs, or traversing stepping stones, having desired and commanded velocity from the human controller might be insufficient.", "reference_answer": "A:  The authors believe there may be a misunderstanding here. The proposed control design does not require \"desired and commanded velocity from human controller\". Note that the \"desired\" gait trajectory or velocity is up to the human as shown in the states of the robot control system. The robot joint motion is to follow the human's. The reviewer may have mixed up how simulations were setup vs a real human in experiment. In OpenSim, the authors have to set up a controller for human intact knee as the default setup only offers a normative knee joint movement, a case that does not reflect realistic human-prosthesis walking. This same setup has been reported in previous studies using OpenSim simulations [70] and real human experiments [85].\n\n2)  If the authors change terrain or task significantly, which includes traversing stepping stones, the authors will need a \"task planner\", which is not the focus of this study. Please also refer to P1.5."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fef618a6-ccf4-5ba9-8ef0-b6f2e1ffb96b", "question": "Why not try adopting qua and div at the same time?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why not try adopting qua and div at the same time?", "reference_answer": "A: The authors indeed had already tried with different weights to promote the combination, but the improvement is minor and unstable. Note there hardly exists successful efforts in explicitly modeling and unifying these two aspects. The difficulty lies in the contradiction that whether to cover more modes or to generate high-fidelity samples for generating a single image, which has already been discussed in the main paper (Line 33-38). The authors leave this nontrivial task for future work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "754a3661-893d-5f07-9389-e2ae50fcae7c", "question": "Was BrainNetTF tested with other readout functions?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Was BrainNetTF tested with other readout functions?", "reference_answer": "A: Yes, the authors have equipped BrainNetTF with other readout functions, such as MEAN, MAX, SUM, SortPooling, and Concat. The results can be found in the VanillaTF column of Table 2. Since MHSA+Concat=VanillaTF and MHSA+OCRead=BrainNetTF, testing VanillaTF with different readout functions is equal to testing BrainNetTF with different readout functions. Besides, the authors have also added experiments for an additional readout function where the VanillaTF is equipped with DiffPool [1]. The results are summarized in the following table.\n\n| Dataset | Readout | VanillaTF|\n|:-------:|:--------------:|:---------:|\n|ABIDE|MEAN | 73.4±1.4|\n|ABIDE|MAX | 75.6±1.4|\n|ABIDE|SUM | 70.3±1.6|\n|ABIDE|SortPooling| 72.4±1.3 |\n|ABIDE|DiffPool| 62.9±7.3 |\n|ABIDE|CONCAT| 76.4±1.2 |\n|ABIDE|OCRead | 80.2±1.0 |\n|:-------:|:--------------:|:---------:|\n|ABCD|MEAN |91.3±0.7 |\n|ABCD|MAX | 94.4±0.6|\n|ABCD|SUM | 91.6±0.6|\n|ABCD|OCRead | 89.9±0.6|\n|ABCD|DiffPool | 83.9±1.3 |\n|ABCD|CONCAT | 94.3±0.7 |\n|ABCD|OCRead | 96.2±0.4|\n\n\n[1] Ying, Zhitao, et al. \"Hierarchical graph representation learning with differentiable pooling.\" Advances in neural information processing systems 31 (2018)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "77152a6b-7c04-5326-afb3-8fa43acb5cb0", "question": "Is there a theoretical basis for using the intersection of a seed shape and a pocket shape to obtain a molecule shape?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there a theoretical basis for using the intersection of a seed shape and a pocket shape to obtain a molecule shape?", "reference_answer": "A: As the authors mentioned in section 1, DESERT is not baseless. The authors design the intersection strategy based on two principles: a) Structure determines properties. [3] [4] [5] show a drug candidate would have satisfactory bio-activity to a target pocket if their shapes are complementary. b) Ligand often attaches tight to a pocket. As the authors mentioned in line 69 and Figure 1, the authors have conducted several preliminary studies, which show the average distance between ligands and pockets is $1.52A$, even less than the length of C-C bond, i.e., $1.54A$, in a molecule itself. Based on these principles, the proposed desired molecular shapes should satisfy the property, i.e., complementary to the pocket, to achieve good bioactivity. The intersection method makes the sketched molecular shape meet the requirement."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "37d7bf49-8b03-5e78-a170-7883ae44c3d7", "question": "In figure 14, why is HeSBO not compared?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f3072a1b-c092-53ce-8230-337ce10fcf33"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 14, why is HeSBO not compared?", "reference_answer": "A: The results show that HeSBO has similar performance to LA-MCTS-TuRBO and is worse than MCTS-VS and TuRBO. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b4f16c39-d965-5132-bd7c-0bc07dc253d5", "question": "Figure 2: Why does the error of the proposed approach with the median estimator improve between s = 200 and s = 300?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cbf43ca-f2ed-58e7-a653-10b0139e4509"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 2: Why does the error of the proposed approach with the median estimator improve between s = 200 and s = 300?", "reference_answer": "A: The parameter $[\\mathbf{p}^\\star]_k$ is identifiable only when less than a half of local distributions $\\mathbf{p}^t$ are misaligned with $\\mathbf{p}^\\star$ at their $k$-th entry.\nIf $s > \\frac{d}{2}$, then by the pigeonhole principle, the aforementioned property is violated. So it is beyond the scope of the proposed theoretical predictions, though the authors included $s > \\frac{d}{2}$ in the proposed experiments for completeness. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c4123373-658e-5d6a-adbf-f1323df08d4a", "question": "Where do you compare the RL algorithms with each other? What are the findings?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c2e3506d-d2fb-5103-9abe-9952a29a548d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where do you compare the RL algorithms with each other? What are the findings?", "reference_answer": "A: Figure 7 in Appendix the authors compare PPO with REINFOCE, DQN, and categorical DQN on all three optimization problems and shows that PPO performs best. PPO has better exploration properties than REINFORCE, which tends to converge too soon to a local optimum. The poor performance of DQN and CatDQN can be explained by the sparse reward (the reward is only non-zero at the terminal state), such that the Bellman error and training loss for updating the Q network are zero in most states. The authors also found the performance of DQN and CatDQN to be sensitive to the choice of the epsilon greedy rate and Boltzmann temperature for trading-off exploration and exploitation and increasing diversity."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "06639899-b8d1-5a9a-bf88-3fbcbb927cc9", "question": "Regarding Table 2, which dataset has been used for rows that do not have a tick for R2R?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Regarding Table 2, which dataset has been used for rows that do not have a tick for R2R?", "reference_answer": "A: For Table 2, all rows use R2R for validation. For row 2 and row 3 that do not have a tick for R2R, row 2 uses sub-instruction and sub-trajectory pairs from Landmark-RxR as training data and row 3 uses complete instruction and trajectory pairs from en-RxR as training data. The authors will refine the proposed descriptions. \n\n[1] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Empirical Methods in Natural Language Processing, 2020. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4ff04861-538f-5584-889e-2331d88ff0eb", "question": "What is the text consistency metric?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the text consistency metric?", "reference_answer": "A: The text consistency metric measures the proportion of the captions for which the authors retrieve the correct image and one of the four similar captions simultaneously over the whole dataset. This is similar to Equation 2 in the proposed work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "164016b4-2495-5420-9e49-5cb8cb28727d", "question": "VGG is easy to attack, how is the results for different network (maybe adversarially trained, more robust)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "VGG is easy to attack, how is the results for different network (maybe adversarially trained, more robust)?", "reference_answer": "A: The authors have  conduct experiments on different networks. The authors analyzed four DNNs trained on the Tiny ImageNet dataset, including the normally trained VGG-16, the normally trained ResNet-34 (Line 235-236), the adversarially trained VGG-16 based on [cite 1], and the distilled VGG-16 (the student DNN distilled based on the output feature of the conv_53 layer in the normally trained VGG-16, following settings in Line 316-317), to analyze the change of regional features' reliability and importance in adversarial attacks. The authors used $\\Delta_{\\text{orientation}}=E_x[E_r(1-\\cos(h_{\\text{ori}}^{(r)},h_{\\text{adv}}^{(r)}))]$ and  $\\tilde\\Delta_{\\text{strength}}=E_x\\left[E_r\\left(\\frac{|\\Vert {h_{\\text{ o r i}}}^{(r)}\\Vert_2-\\Vert {h_{\\text{ a d v}}}^{(r)}\\Vert_2|}{\\Vert {h_{\\text{ o r i}}}^{(r)}\\Vert_2}\\right)\\right]$(Line 304-306) to measure the change of regional features' reliability and importance in the attack, respectively. Note that for fair comparison between different DNNs, the authors used the strength of regional features $\\Vert h_{\\text{ori}}^{(r)}\\Vert_2$ for normalization when computing the change in regional features' strength $\\tilde\\Delta_{\\text{strength}}$, instead of $\\Delta_{\\text{strength}}$ in the answer to Q1. In this experiment, the authors analyzed regional features in the last convolutional layer in each DNN. The authors calculated $\\Delta_{\\text{orientation}}$ and $\\tilde\\Delta_{\\text{strength}}$ of the regional features after the attack. The following table compares the change of regional features among the four"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c192d203-5557-5aab-8e03-c82ef94e96fb", "question": "Why is the MRR in fig 1 so low?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["65edc536-d32b-58f2-9db7-78f17419bac4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the MRR in fig 1 so low?", "reference_answer": "A: According to Equation (5) of the proposed work, many factors can influence the expected MRR, such as the answer number and the density of datasets. A higher density means that this KG is closer to the closed world, so the MRR will also be higher."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3c77f6b3-c5b6-59a8-8240-916901fb7881", "question": "In table 2 and the relevant text, what test is used and how is p-value computed?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ef019d45-8e82-523d-996c-fc36fa7f2222"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In table 2 and the relevant text, what test is used and how is p-value computed?", "reference_answer": "A: Following Pop Music Transformer (Huang et. al., 2020), the authors use the Wilcoxon signed rank test, and compare Museformer with each of the baseline models based on the overall scores to calculate the p-value, which is described in detail in the Appendix C."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "467e62a8-5c64-5fdd-9a35-3f02b1375b02", "question": "Why sample with probability in view transform (Figure 3)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2cb98599-ddd1-5ea4-ba9d-a65c36c82757"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why sample with probability in view transform (Figure 3)?", "reference_answer": "A: Because the authors cannot get the real depth of each image in the camera-based setting (with the camera only). Therefore, the authors need to estimate the depth of each pixel when the view is transformed to the voxel space. There are actually three ways in the process **(1)** projecting each pixel like a ray with the same prob, **(2)** using estimated discrete depth, **(3)** using estimated depth distribution. For **(1)**, projecting pixels with the same prob cannot reflect the object structure in 3D space, which brings semantic ambiguity with much inferior performance in the proposed experiments. For **(2)**, estimating discrete depth relies heavily on a pre-trained accurate depth estimator, which damages the end-to-end framework design in the proposed UVTR. Thus, the authors adopt **(3)** to estimate the depth distribution $\\mathrm{D}_I$ for efficient view transform, which guarantees a high recall rate in depth and can be optimized in an end-to-end manner. The authors will make this clear in the revision."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "059ad50d-f0b9-556a-aa4a-52c98ad43662", "question": "I am little confused about the use of SegFormer and DeepLab, which are according to the authors, \"aggregate spatial-wise object location features\" for the former, and give \"intrinsic representations of each object category\" for the latter. However, these two networks, to my knowledge, are two semantic segmentation networks. I might misunderstand the differences of usage of these two networks, but can you clarify?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0e6ba89c-14a5-5176-aa62-ec09a52d9717"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am little confused about the use of SegFormer and DeepLab, which are according to the authors, \"aggregate spatial-wise object location features\" for the former, and give \"intrinsic representations of each object category\" for the latter. However, these two networks, to my knowledge, are two semantic segmentation networks. I might misunderstand the differences of usage of these two networks, but can you clarify?", "reference_answer": "A: The main reason to employ SegFormer for spatial dimension is due to its capability of capturing long-range dependencies. Under this set up, spatial features in every corner of the image can be attended and correlated. A further theoretical support for this point can be found in [Ref1], which shows vision transformers (e.g., Segformer) retain more spatial information than ResNet. \nOn the other hand, for ResNet backbone from DeepLabV3-ResNet50 was adopted for semantic feature extraction due to the lightweight capacity, it serves as an auxiliary semantic context aggregator while introducing insignificant compute requirements. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ed075f00-cd3a-5aa1-9271-5869c26aae6b", "question": "Briefly compare the proposed method with other previous works referenced below.\nReferences:\n[r1] Hu, Weipeng, Wenjun Yan, and Haifeng Hu. \"Dual face alignment learning network for NIR-VIS face recognition.\" IEEE Transactions on Circuits and Systems for Video Technology 32, no. 4 (2021): 2411-2424.\n[r2] Hu, Weipeng, and Haifeng Hu. \"Orthogonal modality disentanglement and representation alignment network for NIR-VIS face recognition.\" IEEE Transactions on Circuits and Systems for Video Technology 32, no. 6 (2021): 3630-3643.\n[r3] Zhao, Jian, Lin Xiong, Panasonic Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Panasonic Sugiri Pranata, Panasonic Shengmei Shen, Shuicheng Yan, and Jiashi Feng. \"Dual-agent gans for photorealistic and identity preserving profile face synthesis.\" Advances in neural information processing systems 30 (2017).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["17e87869-0815-5e88-9ed2-3cb8faaf55ec"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Briefly compare the proposed method with other previous works referenced below.\nReferences:\n[r1] Hu, Weipeng, Wenjun Yan, and Haifeng Hu. \"Dual face alignment learning network for NIR-VIS face recognition.\" IEEE Transactions on Circuits and Systems for Video Technology 32, no. 4 (2021): 2411-2424.\n[r2] Hu, Weipeng, and Haifeng Hu. \"Orthogonal modality disentanglement and representation alignment network for NIR-VIS face recognition.\" IEEE Transactions on Circuits and Systems for Video Technology 32, no. 6 (2021): 3630-3643.\n[r3] Zhao, Jian, Lin Xiong, Panasonic Karlekar Jayashree, Jianshu Li, Fang Zhao, Zhecan Wang, Panasonic Sugiri Pranata, Panasonic Shengmei Shen, Shuicheng Yan, and Jiashi Feng. \"Dual-agent gans for photorealistic and identity preserving profile face synthesis.\" Advances in neural information processing systems 30 (2017).", "reference_answer": "A: DA-GAN [3] reveals that high-quality profile view synthesis could facilitate the face recognition task. But DA-GAN is proposed for the VIS face recognition task while the proposed work is for NIR-VIS face recognition. DFAL [2] and OMDRA [1] focus on domain-invariant face features extraction. Both methods do not involve any facial image generation with new identities."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1e4472db-ea73-5d26-a377-e52956af6672", "question": "Can the left inclusion of Equation (1) in Theorem 1.1 be replaced by $[l, u]$ or is $[l + \\delta, u - \\delta]$ needed because the network $n$ is an approximation of the function $f$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["8917c30c-c58f-5e07-a859-55f510122e0c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can the left inclusion of Equation (1) in Theorem 1.1 be replaced by $[l, u]$ or is $[l + \\delta, u - \\delta]$ needed because the network $n$ is an approximation of the function $f$?", "reference_answer": "A: Yes, the authors need the lower bound to be $[l+\\delta, u-\\delta]$ because the network $n$ is an approximation of $f$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b39da115-0919-5654-9ad2-b4623cc04f56", "question": "Have the authors considered the interplay between finetuning and catastrophic forgetting? Prompt tuning (Lester et al.) has previously been shown to be effective as an alternative to full model finetuning, and I imagine it would have some benefits over few-shot prompting without pitfalls of finetuning. \nReferences:\nLester et al. 2021 The Power of Scale for Parameter-Efficient Prompt Tuning.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7fdb5c7c-8e2c-5a6d-801d-6317beaec91e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Have the authors considered the interplay between finetuning and catastrophic forgetting? Prompt tuning (Lester et al.) has previously been shown to be effective as an alternative to full model finetuning, and I imagine it would have some benefits over few-shot prompting without pitfalls of finetuning. \nReferences:\nLester et al. 2021 The Power of Scale for Parameter-Efficient Prompt Tuning.", "reference_answer": "A: Yes, the authors have considered prompt tuning. The authors believe prompt tuning could be a useful tool in the context of length generalization. There are, however, a couple of reasons why they didn’t prioritize prompt tuning in their experiments:\n* Despite the innocuous-looking number of parameters that get updated during prompt tuning, even a single prompt can be tuned to match the performance of full-model finetuning on nontrivial tasks. (see Figure 3 in Lester et. al. (2021)) In addition, sometimes heavy regularization in form of l2 regularization and dropout is needed to prevent overfitting to the training set when one uses prompt tuning. These all suggest that the behaviour of prompt tuning might be qualitatively more similar to fine-tuning than prompting by hand.\n* Initialization plays a very important role in prompt tuning (see the discussion in Section 7 in Lester et. al.), indicating that the optimization landscape is highly non-convex. In the context of prompt tuning for algorithmic tasks, one needs to be careful while picking prompt initializations.\n* Dyer et. al. (2021) has shown that scale goes a very long way towards fixing catastrophic forgetting. The authors find in their experiments that even 50b models show practically the same generalization trends as 1000x smaller models.\n* The authors did do a bit of exploratory automated prompt tuning (in token space) for the chain-of-thought parity task by varying the randomizable elements in the prompt. While the authors did see a spread in performance, no single prompt significantly overperformed the others. The authors didn’t use the highly tuned prompt in our results in order to avoid overclaiming.\n\nReferences:\nLester et al. 2021 The Power of Scale for Parameter-Efficient Prompt Tuning.\nRamasesh, Vinay Venkatesh, Aitor Lewkowycz, and Ethan Dyer. \"Effect of scale on catastrophic forgetting in neural networks.\" In International Conference on Learning Representations. 2021.\n"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "101873ec-612b-540d-8c06-5b6f894a7657", "question": "(1.4 b) What is the number of scales in interaction in this experiment compared to the number of scales in S? And the same question for the plot on the right in Figure 2.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e81f1be3-4f0d-5afd-bf23-98f9fde05958"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "(1.4 b) What is the number of scales in interaction in this experiment compared to the number of scales in S? And the same question for the plot on the right in Figure 2.", "reference_answer": "A: In this experiment the authors used an interaction of 2 scales. And the total number of scales is equal to 3. In Figure 3, the total number of scales is 5 and the number of scales in interaction is represented on the horizontal axis."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e591ed7c-b55e-5a80-aa91-99823b3d60b2", "question": "Why is rendered($v^{\\ast}$) always harder to classify than the real($v^{\\ast}$)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e76cdd40-14a4-5e64-b8b2-c6871f4cf676"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is rendered($v^{\\ast}$) always harder to classify than the real($v^{\\ast}$)?", "reference_answer": "A: This is because the authors generated the adversarial viewpoint based on the rendered image $\\mathcal{R}(\\mathbf{v})$ as shown in Eq. (2). Thus the rendered image from the adversarial viewpoint is more likely to fool the model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ba08ef21-f78e-5671-afe7-d21fa86912cc", "question": "What is the novelty of RLQP?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f503fab8-64be-5f89-ada1-aafcee9e2f54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the novelty of RLQP?", "reference_answer": "A: Yes, RLQP is the first to formulate the RL training as a single-policy multi-agent RL in this context."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fc5af9ef-336c-5361-a7ec-e9394bd79cfb", "question": "Are the symmetry groups chosen interesting?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f000c838-1733-5b65-b8ff-3ec832a960a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the symmetry groups chosen interesting?", "reference_answer": "A: Here, the authors further consider the p8 group, which is composed of planar rotation of angles that are multiples of $\\pi/4$. Including the p8 group, the proposed $E^4$ layer is more or less the same as the p4 case. The authors conduct experiments on CIFAR with data augmentation. The network architecture and other training settings are kept the same as section 5.2 of the proposed work. Results are listed in Table III. As shown in the table, incorporating more rotational symmetries further improve performance of the proposed model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5eda0346-b1cc-522e-bc21-c2a113cd305e", "question": "What is the originality? Is there anything substantial to the derivation of DPM-Solver than directly using the variation-of-constants?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["77c357a7-98f0-5bd4-8db6-d7cd3c94bf3e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the originality? Is there anything substantial to the derivation of DPM-Solver than directly using the variation-of-constants?", "reference_answer": "A: The variation-of-constants is a classical idea for solving the semi-linear ODE, so it is natural to combine it with diffusion ODEs which have a semi-linear structure. However, to the best of the proposed knowledge, the connection between the variation-of-constants and DPMs has not been revealed in the prior literature, so the combination of each other is also the proposed contribution. Further, compared to directly using the variation-of-constants, the authors emphasize that the change-of-variable for $\\lambda$ is highly nontrivial and can bring much more insights. The authors discuss the difference between directly using the variation-of-constants and the proposed DPM-Solver below to demonstrate that **the change-of-variable for $\\lambda$ is also critical and is the proposed novel contribution**.\n\nFor the original diffusion ODE, the exact solution can be formulated by the variation of constants:\n$$ x_t = e^{\\int_s^t f(\\tau)\\mathrm{d}\\tau}x_s + \\int_s^t \\left(e^{\\int_\\tau^t f(r)\\mathrm{d} r}\\frac{g^2(\\tau)}{2\\sigma_\\tau} \\epsilon_\\theta(x_\\tau,\\tau)\\right)\\mathrm{d}\\tau. $$\nIf the authors directly approximate the above integral, the authors need to discretize and approximate the coefficients related to $f(t)$ and $g(t)$ (i.e., the noise schedule). Such computation is complicated and may involve other discretization errors. Instead, the authors propose the following equivalent but much simpler formulation:\n$$ x_t = \\frac{\\alpha_t}{\\alpha_s}x_s - \\alpha_t \\int_{\\lambda_s}^{\\lambda_t} e^{-\\lambda} \\hat\\epsilon_\\theta(\\hat x_\\lambda,\\lambda)\\mathrm{d}\\lambda. $$\nWhat the authors want to emphasize is that, the above integral is **invariant to the noise schedule** between $\\lambda_s$ and $\\lambda_t$ because the integrand contains no functions about $f(t)$ and $g(t)$ and only related to $\\epsilon_\\theta$. Such formulation can unify the sampling of diffusion models for different types of noise schedules. Therefore, the proposed DPM-Solver is also invariant to the noise schedule and is a general solver for all types of diffusion models. The authors have updated Appendix A in the revision to discuss more such invariance properties. Therefore, DPM-Solver is a customized solver for DPMs, and its derivation is more than directly using the variation-of-constants. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b92af39a-7a87-511f-b7cb-361684152ba4", "question": "On Figure 1 and Figure 5, what is the input to the model? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "On Figure 1 and Figure 5, what is the input to the model? ", "reference_answer": "A: Yes, the inputs to the model on Figure 1 and Figure 5 are both halftone images."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "636b4744-7295-5004-a50d-13e1c36383b3", "question": "Why not use bigger datasets to support their method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why not use bigger datasets to support their method?", "reference_answer": "A: The authors would like to point out that the proposed modified MNIST dataset is not trivial as the name may suggest. As shown in appendix C.1 or [(click to see image)](https://i.ibb.co/yNZJg2q/2564-08-05-18-22-53-lsp-v3-full-pdf-Adobe-Acrobat-Pro-DC.png), a single image can contain as many as 50 digits with a lot of overlap and variations in orientations, brightness, contrast, and sharpness. The prediction scores also indicated that this task is not trivial even for a ResNet-50 DETR model (Table 4)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "96ee7494-9b21-560f-8cbb-9e8aa48854d7", "question": "How the generated molecules could have so good Vina scores without any protein pocket information leveraged in the generation process?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How the generated molecules could have so good Vina scores without any protein pocket information leveraged in the generation process?", "reference_answer": "A: Actually, as shown in Figure 3, the pocket information is used in the generation process. When the authors design molecules based on a given pocket, the authors sample the molecular shape from the pocket, which contains the geometric information of the pocket. As the authors reported in section 3.2, the shape helps DESERT produce high-quality molecules."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f674c07b-c662-58ea-b8dc-b936a0e854a4", "question": "How would the results in Table 1 and Table 2 look like if $T^{full}$ is a model based on early stopping rather rather than a model after 120 or 200 epochs?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["0f14464e-91f1-5e40-bc55-949e27ca57a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How would the results in Table 1 and Table 2 look like if $T^{full}$ is a model based on early stopping rather rather than a model after 120 or 200 epochs?", "reference_answer": "A: Overall, training the teacher models on CIFAR for 200 epochs and ImageNet for 120 epochs does not lead to obvious overfitting. The numbers of training epochs are not significantly affected by using early stopping. The authors have tested the common early stopping strategy (patience=10) on all teacher models. Table 1 shows the numbers of training epochs for teacher models with or without early stopping. The authors also show the curves of validation accuracy versus epoch for all teacher models in the supplementary material (see section 3, Figure 3). It shows that whetherthe authors use early stopping has no effect on the results of the proposed work. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "30a0e5ff-2291-5e0d-9164-d87d894fcf83", "question": "In Figure 6, it does not make sense to have discrepancy distance and training success rate curves share the same y-axis. Could you please clarify?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["6b867db9-1840-5d83-aef2-f93a489f17b6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 6, it does not make sense to have discrepancy distance and training success rate curves share the same y-axis. Could you please clarify?", "reference_answer": "A: The discrepancies $\\mathbf{d}=[d_1,...,d_T]$is normalized with equation: $\\frac{\\mathbf{d} -min(\\mathbf{d})}{max(\\mathbf{d})-min(\\mathbf{d})}$, so that they are both in range [0, 1] and can share the same y-axis."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fc4b3f9d-a302-5eb1-9ee8-cc59bde7c288", "question": "Are there any results for cases when predictions of multiple almost equivalent models disagree for a user, how do the explanations disagree in this setting ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["28ac8153-a6b6-5c41-bba9-62b11855630c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there any results for cases when predictions of multiple almost equivalent models disagree for a user, how do the explanations disagree in this setting ?", "reference_answer": "A: The proposed results are not limited to points/users where the model predictions agree. So this question is probably best answered with Figure 5 (right) and Figure 10 in the Appendix. The x-axis shows epistemic uncertainty (which the authors use to quantify predictive disagreement), the y-axis shows a measure of explanatory agreement (which the authors will clarify–see the proposed response to Reviewer UP6W).\nIf the authors only consider the points/users on the right, where epistemic uncertainty is high, and hence the models disagree, the authors can get a sense of the distribution of explanatory agreement. The authors can then contrast this to the points/users on the left, where epistemic uncertainty is low. By doing so the authors see that epistemic uncertainty (predictive disagreement) is not indicative of explanatory agreement. There is some very weak correlation, but it is not consistent. The authors will clarify this in the Results section."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bcdbbd97-0aa8-52cf-89e9-ea4191507039", "question": "In figure 4c, why is it that the RL histogram has weird performance, especially 0.3. Why does it have no standard deviation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 4c, why is it that the RL histogram has weird performance, especially 0.3. Why does it have no standard deviation?", "reference_answer": "A: The authors observe that when the loss of the global model is beyond a certain value, model accuracy will be constant or close to a low point. Thus, when the proposed attack dramatically damages the FL training, the final accuracy will be similar and low. This explains why the variance of RL results is low or close to 0 in Figure 4 (c). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f969792f-1991-5e66-a2de-21b9271bdd7e", "question": "Why is it difficult to understand Eqs. 58-60?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e391ac18-50de-5c7d-ab70-9a7b36fb2cb0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is it difficult to understand Eqs. 58-60?", "reference_answer": "A: While it is true that the authors used both approximations (dropping the higher-order terms and replacing the trace of a product with the product of the traces) in Eqs. 59 and 60, in all equations, the authors first dropped the higher-order terms, then approximated the trace of a product with the product of the traces. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "67e5a234-941d-5743-bd34-0f3378440dc1", "question": "Why only 8 bit and 2 bit in Figure 7?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["14c1199a-b695-5cec-a11a-023781bfb0c5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why only 8 bit and 2 bit in Figure 7?", "reference_answer": "A: Under different budgets, orange curve arrows in the four corners of the figure mean the adjustment scale of the precision value from 2 bit to 8 bit and 8 bit to 2 bit, not just 2bit and 8bit."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0fb35084-db0b-5242-9ecf-87f754d7f64c", "question": "Do you think that the experimental setups might be flawed? While the authors did show improvement in robustness, there are some caveats in the setup. The authors used a single $\\epsilon$ value for the PGD attack ( 0.005) based on a rejected ICLR’21 submission [21]. Published works (like [12,19,73]) usually report several $\\epsilon$ values for PGD attack or run on C&W formulation to make sure that the defense/attack methods work on generic attacks and not for that specific $\\epsilon$ ( that might be too small for the attack to work).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table", "formula"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do you think that the experimental setups might be flawed? While the authors did show improvement in robustness, there are some caveats in the setup. The authors used a single $\\epsilon$ value for the PGD attack ( 0.005) based on a rejected ICLR’21 submission [21]. Published works (like [12,19,73]) usually report several $\\epsilon$ values for PGD attack or run on C&W formulation to make sure that the defense/attack methods work on generic attacks and not for that specific $\\epsilon$ ( that might be too small for the attack to work).", "reference_answer": "A: The following points are in favour of the experimental setups:\n1. First, the authors would like to kindly highlight that the authors utilize $\\epsilon = 0.05$ instead of 0.005, which is incorrectly indicated in the question., \n2. Additionally, the authors argue that $\\epsilon = 0.05$ is already a very large perturbation. As Table 1 shows, the proposed attack with $\\epsilon = 0.05$ reduces all the clean trained models’ robust accuracy to 0 or near 0. The authors have also tested that even with $\\epsilon = 0.02$, the adversary can still reduce all the models’ robust accuracy to less than 5%. On the other hand, the perturbed point clouds with $\\epsilon = 0.05$ are at the edge of correct human predictions of objects. As the authors are not allowed to insert links, the authors kindly refer the reviewer to Figure 9 in [1], which also indicates the same conclusion. Numerically, $\\epsilon = 0.05$ out of the range [-1,1] is also similar to the commonly used  $\\epsilon = \\frac{8}{255}$ in 2D adversarial training [2].  \n3. The authors use the experimental guidelines in [1] because it is the only existing work that leverages adversarial training analysis in 3D point clouds. \nThe authors follow the reviewer’s suggestions to evaluate the proposed adversarially trained model on more attacks including PGD attacks with $\\epsilon = 0.02/0.04/0.06$ and C&W attack. \nThe results of PDG attack with different  $\\epsilon = 0.02/0.04/0.06$  as shown as follows. \n\n|                |     | ModelNet40  |                 | |       ScanObjectNN         |      | |   ModelNet10     |      |\n|    :----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |\n|           RA(%)           | PointNet   | DGCNN     | PCT       | PointNet     | DGCNN     | PCT       | PointNet   | DGCNN     | PCT       |\n| AT Baseline ($\\epsilon=0.02$)    | 63.2±0.33  | 80.9±0.22 | 78.8±0.15 | 43.6±0.31    | 54.9±0.55 | 54.1±0.33 | 91.9±0.21  | 93.3±0.19 | 92.1±0.13 |\n| Best Finetuned ($\\epsilon=0.02$) | 76.0±0.19  | 84.3±0.19 | 79.4±0.17 | 47.4±0.23    | 62.0±0.51 | 56.1±0.49 | 92.1±0.25  | 96.2±0.26 | 94.6±0.16 |\n| AT Baseline ($\\epsilon=0.04$)    | 46.2±0.3   | 70.9±0.3  | 61.3±0.11 | 28.5±0.19    | 39.6±0.45 | 35.1±0.21 | 84.5±0.36  | 91.0±0.37 | 86.2±0.29 |\n| Best Finetuned ($\\epsilon=0.04$) | 61.1±0.15  | 76.7±0.25 | 62.9±0.13 | 31.8±0.4     | 48.9±0.6  | 37.4±0.39 | 85.2±0.26  | 92.9±0.44 | 88.4±0.41 |\n| AT Baseline ($\\epsilon=0.06$)    | 25.1±0.51  | 48.9±0.69 | 37.2±0.33 | 17.8±0.15    | 25.6±0.33 | 19.1±0.4  | 74.1±0.12  | 84.2±0.55 | 77.0±0.29 |\n| Best Finetuned ($\\epsilon=0.06$) | 43.2±0.41  | 54.1±0.75 | 39.6±0.2  | 19.3±0.23    | 33.1±0.65 | 20.5±0.37 | 75.0±0.2   | 86.4±0.48 | 77.6±0.47 |\nFrom this table, the authors observe that the proposed model could achieve non-trivial robustness among different epsilons (even with  $\\epsilon > 0.05$), and it is expected that the adversarially trained models perform better with $\\epsilon < 0.05$. The proposed model trained with self-supervised learning also achieves consistently stronger robustness than the AT and ST baselines. It further verifies the significance of self-supervised learning for 3D point cloud robustness. \nIn the following table, the authors show the robustness of the proposed best fine-tuned model against target C&W attack. For each test sample, the authors randomly select a class as a target. The authors also compare it with two baselines, including standard training (ST) baseline and adversarial training (AT) baseline. Note that the numbers here are the attack success rate since the authors use target attacks (the lower, the more robust). From the result, the authors could observe that the proposed method still achieves consistently higher robustness compared to the two baseline methods.  It also verifies the significance of self-supervised learning for 3D point cloud robustness. \n|                |     | ModelNet40  |                 | |       ScanObjectNN         |      | |   ModelNet10     |      |\n|    :----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |:----:   |\n| ASR(%)         | PointNet   | DGCNN | PCT  | PointNet     | DGCNN | PCT  | PointNet   | DGCNN | PCT  |\n| ST Baseline    | 98.3       | 98.1  | 99   | 100          | 100   | 100  | 95.3       | 96.2  | 95.1 |\n| AT Baseline    | 11.2       | 7.6   | 9.8  | 35.9         | 24.4  | 39.7 | 5.9        | 5.5   | 6    |\n| Best Finetuned | 6.93       | 5.21  | 5.68 | 30.1         | 20    | 30.4 | 5.5        | 4.2   | 5.5  |\n\n[1] Sun, Jiachen, et al. \"On Adversarial Robustness of 3D Point Cloud Classification under Adaptive Attacks.\" arXiv preprint arXiv:2011.11922 (2020).\n[2] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n[3] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning. PMLR, 2018.\n[4] Zhou, Hang, et al. \"Dup-net: Denoiser and upsampler network for 3d adversarial point clouds defense.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n[5] Ma, Chengcheng, et al. \"Towards Effective Adversarial Attack Against 3D Point Cloud Classification.\" 2021 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2021."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "eda8f014-2926-5610-a03e-be47184ea1f7", "question": "Did the authors try to retrieve the most similar images in the real dataset to make sure it's not actually selecting/copying key exemplars from the dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Did the authors try to retrieve the most similar images in the real dataset to make sure it's not actually selecting/copying key exemplars from the dataset?", "reference_answer": "A: It is a great question, which is why the authors provide four videos to visualize the distillation process in the Appendix. The distilled data is the product of the optimization procedure, and it converges to a similar point whether it is initialized from the real image or random noise. As you can see in the videos, the distilled data is far away from its initialization. Intuitively, the distill data can be considered the \"principle component\" of a dataset. Instead of selecting or copying the exact images from the dataset, the proposed method synthesizes the images that best reflect the most representative pattern in the original dataset. \n\nAnother evidence that \"learning\" is happening is that a pair of distilled images and labels are highly correlated. Suppose you interpolate the label value between two different classes and only learn the image. You can see the learned image interpolates between the two classes as well, which suggests the proposed method tries to synthesize the best image to reflect its label."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a707e4d5-cc2b-526f-815f-7b4d6afba46b", "question": "It is not clear why heuristic search would work here. Is any pretraining required? Otherwise, since there's no gradient signal for the positions, I'm not sure how the model figures it out.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["2a7b14b6-33c1-505a-8a3a-d9580ad76ca7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is not clear why heuristic search would work here. Is any pretraining required? Otherwise, since there's no gradient signal for the positions, I'm not sure how the model figures it out.", "reference_answer": "A: Heuristic searched position actually provides an association between the source representation and the target word.  PNAT learning to reinforce this word-to-word translation to satisfy this connection. The authors do not use any pretraining process here. The parameters of heuristic searching are the encoder and the final word predictor.  The encoder needs to reasonably represent the source input, while the word predictor needs to predict the words from the representation. The experiment in Section 4.4 has verified this assumption. As shown in Table 3, PNAT w/ HSP completes word-to-word generation well, achieved a very high BLEU score (more than 15.0 BLEU compare to Transformer) which verified that word-to-word connections established through HSP are acceptable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0c59d2c6-0d12-506c-a309-c07757f7cac4", "question": "Which experiment justifies the effectiveness of the proposed method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["3959ec3f-8c34-50e6-8515-ca5c61e8020c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which experiment justifies the effectiveness of the proposed method?", "reference_answer": "A: The authors have conducted the following experiments to justify their framework:\nIn Table 5 in Appendix H, they remove the components of MIW, regularization and model rollouts respectively. And the authors can see that when removing each component proposed in the framework, the performance drops compared with the proposed Main algorithm, which shows the necessity of each component in the framework. They use KL divergence as the original theorem suggested, and the empirical result of using KL divergence is not as good as that of using JSD. It could be due to the practical difficulty in the implementation for using KL. They plot the MIW distributions for different MIW training methods (GenDICE, DualDICE, VPM and theirs) in Figure 3. As the authors can see, their proposed method indeed produces stable MIW estimates, showing the efficacy of the proposed MIW training method, so that the MIW can be used for model training effectively. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "811fc699-a864-5145-8266-96c355b13b30", "question": "What is the standard Flickr30K 1K and MSCOCO 5K test set?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the standard Flickr30K 1K and MSCOCO 5K test set?", "reference_answer": "A: Image-to-text retrieval requires each image to retrieve one of the five relevant captions in its top K closest predictions. In contrast, the text-to-image retrieval requires each caption to retrieve the correct image (only one possible) in its top K closest predictions."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "103c1cc5-3de2-563d-a939-5639125da16a", "question": "Compate OdrinalCLIP with a Linear probe baseline.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cd6253e-8a5a-55f2-8bdc-77afa61d48e4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compate OdrinalCLIP with a Linear probe baseline.", "reference_answer": "A: The authors conducted experiments with the Linear probe solution on all tasks. The results are presented below. \n\n**Table R1-1. The MAE results on four benchmarks. The lower, the better.**\nDataset | MORPH II  | Adience |  Image Aesthetics  | Historical Image Dating\n---|:---:|:---:|:---:|---\nLinear probe | 4.70 | 0.64 | 0.487 | 0.86\nOdrinalCLIP | **2.32** | **0.47** | **0.280** | **0.67**\n\n**Table R1-2. The Accuracy results on three benchmarks. The higher, the better.**\nDataset | Adience |  Image Aesthetics  | Historical Image Dating\n---|:---:|:---:|:---:\nLinear probe |  51.8% | 61.60% | 41.07%\nOdrinalCLIP |  **61.2%** | **72.85%**| **56.44%**\n\nIt can be observed that OdrinalCLIP method consistently outperforms the Linear probe method on all datasets, which demonstrates the effectiveness of the method. It is worth pointing out that since most SOTA methods use VGG-16 as the vision encoder, the authors simply follow this setting for a fair comparison. Moreover, the specific choice of vision encoder does not affect OdrinalCLIP method and conclusion."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e82ce41-c4c5-5139-8fd7-13cda721b552", "question": "Non-planar reflector. How would the proposed method work when the reflector is not planar?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4cbd7cbb-7b71-57e9-9b12-e7a19df3dff5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Non-planar reflector. How would the proposed method work when the reflector is not planar?", "reference_answer": "A: Most reflection-removal-related problems assume a piece of planar glass. The authors follow this assumption in this paper and do not specifically consider the influence of non-planar reflectors in the proposed experiments. The authors will clarify this assumption in the final version. Since the authors capture images in the real world, some examples in Figure 3 of the proposed work are captured through a piece of glass with slightly curved areas, and the proposed method still shows its robustness. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4b69f843-4bd5-58bd-bd2f-e87bff4f0572", "question": "Can you clarify the novel contributions of this paper and how the method compares to a combination of [A]+[B], which appear conceptually very similar? \nReferences:\n[A] K. Shen, R. Jones, A. Kumar, S. M. Xie, J. Z. HaoChen, T. Ma, and P. Liang, “Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation,” 2022. \n[B] V. Prabhu, S. Khare, D. Kartik, and J. Hoffman, “Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8558–8567, 2021.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["71c0db56-6716-5fe2-8b69-ed7a4b489f99"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you clarify the novel contributions of this paper and how the method compares to a combination of [A]+[B], which appear conceptually very similar? \nReferences:\n[A] K. Shen, R. Jones, A. Kumar, S. M. Xie, J. Z. HaoChen, T. Ma, and P. Liang, “Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation,” 2022. \n[B] V. Prabhu, S. Khare, D. Kartik, and J. Hoffman, “Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8558–8567, 2021.", "reference_answer": "A: PACMAC and SENTRY [B] both use selective self-training on reliable instances identified via predictive consistency, and that PACMAC makes use of in-domain self-supervised pretraining proposed in Shen et al. [A]. However, PACMAC differs from a combination of [A]+[B] in 2 important ways, which leads to improved performance:\ni) **PACMAC proposes a novel proxy task for identifying reliable target instances**: predictive consistency across partial image inputs generated via masking. By doing so, PACMAC approximately matches the design of its selection strategy to its SSL pretraining (MAE [E] and DINO [D], which learn to reconstruct / learn invariance to partial inputs respectively), in contrast to SENTRY, which measures consistency across random image augmentations.\nii) **PACMAC incorporates model knowledge in its selection strategy** by using attention-conditioning to focus on salient image regions, rather than random augmentations sampled from a manually pre-defined set.\nUnlike a naive combination of [A]+[B], PACMAC thus explicitly couples its SSL pretraining with its selection strategy, and further improves this selection by leveraging the Vision Transformer (ViT) attention mechanism. \nThe authors demonstrate that such coupling improves performance. First, they ablate PACMAC by replacing its selection strategy with SENTRY’s: they exactly match hyperparameters, and select target instances based on predictive consistency across 3 image augmentations, generated via RandAugment [C] with N=3 and M=2.0, and use majority voting. Shown below are target accuracies averaged over all 12 shifts in OfficeHome:\n\n|     | MAE | DINO |\n| ----------- | ----------- |----------- |\n| SENTRY selection    |  66.1   |   67.4     |\n| PACMAC selection    | **66.8**      |   **69.6**     |\n\nAs seen, PACMAC selection outperforms SENTRY selection in both cases: +0.7 (MAE init.) and +2.2 (DINO init.). They compare directly against a combination of Shen et al.[A]+ and SENTRY [B]: The authors note that the full SENTRY method uses additional diversity regularizers and entropy maximization losses. For a fair comparison, the authors add these losses to their method and call it PACMAC*. Shown below are target accuracies comparing [A]+[B] with PACMAC*, averaged across 12 OfficeHome shifts with a DINO initialization:\n\n|     | acc.(%) |\n| ----------- | ----------- |\n| Shen et al. [28] + SENTRY [9]    | 69.6       |\n| PACMAC*   | **70.6**    |\nIn this case as well, PACMAC* outperforms [A]+[B]. \nFinally, authors compare the effectiveness of SENTRY’s selection strategy against their on the Cl->Pr shift from OfficeHome. To do so, they measure reliability precision (how often is a target instance marked as reliable, actually correctly classified?), and reliability recall (what fraction of correctly classified target instances are selected via each method?), and compute the F1 score. Averaged across epochs, they observe the following (from the detailed plot in Sec 2.4 of supplementary):\n\n|     | avg. F1 score |\n| ----------- | ----------- |\n| SENTRY selection    |  84.0      |\n| PACMAC selection   |  **85.0**    |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b98420ba-434c-5f80-8045-e6295b4efbf0", "question": "Why are the improvements not significant when using our method on deeper and wider networks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["a9a91f2d-c308-5f4a-bf69-66e085e155c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are the improvements not significant when using our method on deeper and wider networks?", "reference_answer": "A: As shown in Appendix A.4, Table 9, where the authors investigate the use of the proposed Expand-CK on AlexNet with different number of channels, the authors found that the benefits decrease as the compact model size increases. This, the authors believe, further evidences that the benefits of the proposed approach are due to over-parameterization."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ae6a467-e1a0-5835-8a93-89dd1304ce04", "question": "The performance on standard few-shot classification datasets are actually not comparable to SOTA. E.g., according to the [leaderboard](https://few-shot.yyliu.net/miniimagenet.html), with the standard inductive setting, many methods can achieve over 54% with simple Conv-4 architecture on miniImageNet 5way 1shot. While in-domain few-shot classification is obviously less challenging, why is it that the proposed method performs poorly?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7752490b-b213-5da8-a3dd-bf6ce698e939"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The performance on standard few-shot classification datasets are actually not comparable to SOTA. E.g., according to the [leaderboard](https://few-shot.yyliu.net/miniimagenet.html), with the standard inductive setting, many methods can achieve over 54% with simple Conv-4 architecture on miniImageNet 5way 1shot. While in-domain few-shot classification is obviously less challenging, why is it that the proposed method performs poorly?", "reference_answer": "A: In Table 3, the authors show the in-domain performance comparison with similar training and test set and similar evaluation protocol for the methods considered for cross-domain few-shot learning. First, the authors want to clarify that the proposed goal is not meta-learning for in-domain few-shot evaluation. The proposed approach is about having a stronger pretraining if some unlabeled target-related data are available, which is not the evaluation protocol of the leaderboard. Moreover, the proposed method needs unlabeled data from novel classes, which results in the different test set for the evaluation than the leaderboard uses. Thus the results are not comparable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e8a638e-d295-54f7-bfeb-04634337f9d0", "question": "Where are $Exp$ and Riemannian gradient update concepts disccussed in the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3c233311-ee9d-500d-912e-85eac20c42fa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where are $Exp$ and Riemannian gradient update concepts disccussed in the paper?", "reference_answer": "A: The authors formally define this with the closed-form expression for the manifolds specifically used in this work in Table 1 in the supplement."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e44af8d8-0289-5c25-8f99-6238e0ca532a", "question": "Seeing how image labels were used for SupCon, is there a correlated experiment in the audio domain to incorporate labels?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Seeing how image labels were used for SupCon, is there a correlated experiment in the audio domain to incorporate labels?", "reference_answer": "A: Based on your suggestion, the authors have run additional loss ablation experiments for the audio domain. The results can be seen in the table below which has been added to Appendix G in the proposed revision. The authors see a similar trend in audio as the authors did with images.\n\n\n| Model | STS12 | STS13 | STS14 | STS15 | STS16 | STS-B | SICK-R |  Avg. |\n|:---------|:---------:|:---------:|:---------:|:----------:|:---------:|:---------:|:----------:|:-------:|\n| BERT (SimCLR) | 70.98 | 83.03 | 75.74 | 83.03 | 78.10 | 78.33 | 69.74 | 77.00 |\n| BERT (SupCon) | 71.65 | 84.27 | 76.69 | 83.22 | 78.69 | 79.94 | 70.49 | 77.85 |\n| RoBERTa (SimCLR) | 68.88 | 83.74 | 74.69 | 82.46 | 82.02 | 81.52 | 70.65 | 77.71|\n| RoBERTa (SupCon) | 68.44 | 83.96 | 75.77 | 82.38 | 82.07 | 81.63 | 70.56 | 77.83 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "10a56479-855b-55e0-8f2f-2bf92a2b24cf", "question": "Does \"few-shot\" in the method mainly mean splitting the search space into K=3 subspaces?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does \"few-shot\" in the method mainly mean splitting the search space into K=3 subspaces?", "reference_answer": "A: Yes, please refer to Table 1 for search space partition and lines 136-167 in paper for more details. The authors adopt the \"few-shot\" NAS terminology from prior work in the CV domain [1].\n[1] Zhao, Y., Wang, L., Tian, Y., Fonseca, R., & Guo, T. (2021, July). Few-shot neural architecture search. In ICML."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f6af30f3-8527-5b39-9006-05d321836bc3", "question": "Is the derivative operator an extension of the Lie-derivative operator? The derivative operator here is related to Ito’s lemma from stochastic DDE literature.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the derivative operator an extension of the Lie-derivative operator? The derivative operator here is related to Ito’s lemma from stochastic DDE literature.", "reference_answer": "A: The derivative operator in this paper is derived from Ito's formula and it can be understood as the Lie-derivative operator in the stochastic version, but in the existing literature, mathematicians do not use 'Lie-derivative' to denote this operator.  Actually, compared to the tranditional Lie-derivative, additonal terms, induced by the stochastic configuration, are included in this operator."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "db1f5107-0512-5f9d-b7ce-33eb94e333f8", "question": "Is it the conclusion that actor-critic is better than Q-learning?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6962924b-a586-5127-9274-5f2b61990acb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is it the conclusion that actor-critic is better than Q-learning?", "reference_answer": "A: The authors agree that RL-S2V has a larger attack space, which means the optimal solution it can achieve is as good or better than the one the proposed method can find. However, both methods are not guaranteed to always find the optimal solution in the given attack space. The authors list some potential reasons to explain why ReWatt can outperform RL-S2V as follows:\n1) When performing an adding/deleting edge action in RL-S2V, it chooses two nodes sequentially. Then it decides to add an edge between two nodes if they are not connected, otherwise, the edge between them is removed. Since most graphs are very sparse, the RL-S2V algorithm is, by design, biased to adding an edge. On the other hand, ReWatt removes an edge and then add another edge. The adding/deleting edge operations are more balanced. \n2) The reward design in ReWatt is different from RL-S2V. In RL-S2V, a non-zero reward is only given at the end of an attacking session. Specifically, at the end of an attacking session, a positive reward of $1$ is given if the attack succeeded, otherwise a negative reward $-1$ is given. All the intermediate steps get $0$ reward. In ReWatt, the reward is given after each action. A positive reward is given once an action leads to a successful attack. A negative reward is penalized to take each action if it does not directly lead to a successful attack, which encourages the attacker to make as few actions as possible. Furthermore, the authors also proposed an adaptive negative reward design, which determines the value of the negative reward according to the size of each graph. In fact, the design of this adaptive negative reward has shown to be very effective and important to the ReWatt framework. As shown in Table 1, ReWatt-n (which is a variant of ReWatt without adaptive negative reward design) performs much worse than ReWatt. Specifically, if the authors apply ReWatt-n in the same setting of RL-S2V (with fixed actions), its performance is not as good as RL-S2V in REDDIT-MULTI-12K and REDDIT-MULTI-5K datasets. The performance of ReWatt-n on REDDIT-MULTI-12K is [11.26%; 14.7%; 18.02] while RL-S2V achieves [9.46; 18.5% 21.1%]. On the REDDIT-MULTI-5K, the performance of ReWatt-n is [4.49%; 5.62%; 6.74%] while RL-S2V archives [4.49%; 16.9%; 18.0%]. Hence, the design of adaptive negative reward could be an important reason why ReWatt can perform better than RL-S2V.\nAlso, please note that RL-S2V cannot be implemented with actor-critic by simply replacing $p_{fir}*p_{thi}$ with $p(add/remove|e_t)$ in the framework of ReWatt. This is because the action of ReWatt is different from RL-S2V as described in 1). The edge $e_t$ chosen by ReWatt is an existing edge in the graph, therefore the authors can only delete it from the graph and can not add it to the graph. Hence, $p(add/remove|e_t)$ cannot be performed in practice. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0bfb947e-95a1-583c-8d5a-4e515c39843d", "question": "Clarify the assumption behind p(y_T ) = N (fφ (x), I ) (6). Please explain whether this applies to real-world datasets, e.g. ImageNet. \"where $f_{\\phi}(x)$ is pre-knowledge of the relation between x and y0, e.g., pre-trained with D to approximate E[y|x], or 0 if we assume the relation is unknown.\". Does this mean $f_{\\phi}(x)$ can be computed by a DNN, e.g. ResNet50 for classification?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["066d7d21-caf3-5d81-a520-23ebfe718566"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Clarify the assumption behind p(y_T ) = N (fφ (x), I ) (6). Please explain whether this applies to real-world datasets, e.g. ImageNet. \"where $f_{\\phi}(x)$ is pre-knowledge of the relation between x and y0, e.g., pre-trained with D to approximate E[y|x], or 0 if we assume the relation is unknown.\". Does this mean $f_{\\phi}(x)$ can be computed by a DNN, e.g. ResNet50 for classification?", "reference_answer": "A: Exactly – in the context of classification, $f_{\\phi}(x)$ would represent a probability prediction for class label. The functional form can be chosen as one sees fit; and a deterministic deep neural network is a preferred choice by us, as when properly trained it can already obtain a satisfying accuracy. For the proposed experiments in CIFAR-10 (and FashionMNIST, whose results are now placed in Appendix A.3), the authors apply a pre-trained ResNet18 network; for Noisy MNIST dataset, the authors apply a DNN with the same DNN architecture."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ad49b6fd-bb68-5ed8-b341-2ad897604351", "question": "Why there is a $\\lambda$ in (6) and (7) when the formulation directly depends on $\\nu$, which blocks $\\lambda$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["4d52f085-ca43-524f-bc59-fcf55a9f5859"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why there is a $\\lambda$ in (6) and (7) when the formulation directly depends on $\\nu$, which blocks $\\lambda$?", "reference_answer": "A: The reviewer is correct\nthat $\\nu$ blocks $\\lambda$ so $z_{nk}$ does not depend on $\\lambda$.\nThis can also be read from the graphical model presented in Figure 3. We\nwill update the equation accordingly."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2321f099-2a12-588d-b6f9-d951a5e35499", "question": "Random Projection appears to routinely outperform the other two methods … the paper can be improved if the random matrix used in projection can be adaptive?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5a61440f-7209-5976-84ba-2fccfdfd2b62"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Random Projection appears to routinely outperform the other two methods … the paper can be improved if the random matrix used in projection can be adaptive?", "reference_answer": "A: Great question! Certainly, Random Projection outperforms Top Outputs and Random Sampling on the vast majority of datasets. However, there are cases where Random Projection shows slightly worse performance than other approaches; see, e.g., the results for Delicious in Figure 1 in the Supplement. Therefore, if one has sufficient resources and model performance plays an important role, the authors would recommend testing all three methods. If the resources are limited, according to the proposed numerical study, it is better to use Random Projection. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "41846ac4-40a1-5fb6-bce8-6b1cb7a7e1f7", "question": "What is the text consistency metric?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the text consistency metric?", "reference_answer": "A: This is similar to Equation 2 in the proposed work. The authors find that CyCLIP outperforms CLIP on fine tuning for both the datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b2961cbc-e47c-5521-bc66-ed4196b41c41", "question": "Why is the proposed method considered dynamic?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["7752490b-b213-5da8-a3dd-bf6ce698e939"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the proposed method considered dynamic?", "reference_answer": "A: The term 'dynamic' refers to the momentum teacher, as the parameters of the teacher network are updated during training from the parameters of the student network. The authors provided ablation on the importance of the momentum update in Table 11 in the supplementary material, which shows that the authors get around 1.47% average improvement over fixed teacher for 5-way 5-shot evaluation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8f196b79-eb66-5967-b034-045cecce4980", "question": "Why do the authors choose the Pearson correlation coefficient, and what are the advantages of this metric?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["8dedd2af-a387-54a3-a611-0b69c6963d04"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do the authors choose the Pearson correlation coefficient, and what are the advantages of this metric?", "reference_answer": "A: The Person correlation coefficient for conducting the proposed method is identical to the normalized cosine similarity. The formulation can be presented as:\n\n$Cosine(x,y)=\\frac{\\sum_i x_i y_i}{\\sqrt{\\sum_i x_i^2}\\sqrt{\\sum_i y_i^2}}$\n\n$Person(x,y)=\\frac{\\sum_i (x_i-\\overline{x}) (y_i-\\overline{y}) } {\\sqrt{\\sum_i (x_i-\\overline{x})^2} \\sqrt{\\sum_i (y_i-\\overline{y})^2}} = Cosine(x-\\overline{x},y-\\overline{y})$\n\nIt can be seen that Cosine similarity is not invariant to shifts so the Person correlation coefficient is more precise for evaluating the similarity since the mean value can be corrected by the bias correction. The experimental results in the following table demonstrate the analysis and the authors will add the analysis and the experimental results in the manuscript.\n\n| 8-bit ViT-B  | Euclidean distance | Cosine similarity | Pearson correlation |\n| :--: | :--: | :--: |  :--: |\n| Top-1 Accuracy | 75.42% | 75.57% | 75.81% |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fe1f7c98-ad33-586e-9aa6-3f717bc4ee63", "question": "What is the relationship between proposed Shape2Mol and existing shape-based molecular methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the relationship between proposed Shape2Mol and existing shape-based molecular methods?", "reference_answer": "A: The authors didn't claim CogView2 achieve better performance than DALL-E2. The authors stressed in the paper, that they need to downsample the images back to 256*256** for a meaningful FID comparison, which largely reduces the usage of the proposed super-resolution method. FID itself is not a stable metric. According to https://www.cs.cmu.edu/~clean-fid/, even jpeg quality 75/100 can create an up to 20 FID difference. The authors also find whether center-crop COCO images create a >4 FID difference on this benchmark. The authors care more about human evaluation performance, where CogView2 outperforms CogView, LAFITE et al. by a large margin. However, many text-to-image models are not open-source, so that the authors cannot include them in the evaluation. This also suggests the value of open-sourcing of CogView2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "14890a99-7a6c-535a-9e07-8c427deffa78", "question": "What are the speed-up claims supported by?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["5588e14f-c077-5bc8-92bf-4cc729a17980"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the speed-up claims supported by?", "reference_answer": "A: Results in Table 3."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "66cbe5c8-2e90-5749-b7d9-12c496a522d9", "question": "Why do not you compare with G-SA neural nets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f000c838-1733-5b65-b8ff-3ec832a960a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do not you compare with G-SA neural nets?", "reference_answer": "A: The authors construct the model p4-SAR18 by replacing the second group convolution layer in each Res-Block of p4-R18 with G-SA layer. The authors carry out the experiments on CIFAR10 and CIFAR100 in the same setting of p4-$E^4$R18. Results are listed in Table IV. The authors can see that for p4-SAR18 the computation cost is more expensive and the performance is weaker than standard G-CNN, which is consistent with results shown in [24].\n\nTable IV: Results of G-SA layer on CIFAR.\n\n | Model | error on CIFAR10 (%) | error on CIFAR100 (%) | params | flops |\n | --- | --- | --- | --- | --- |\n | p4-R18 |7.53 | 27.96 | 11M | 2.99G |\n | p4-$E^4$R18 |6.42 | 26.59 | 5.8M | 1.85G |\n | p4-SAR18 | 12.8 | 36.3 | 10.8M |3.67G |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0a45f811-ff85-56d5-8a3c-df0ba5fd6294", "question": "Why is the sign of $\\sigma z$ flipped in Eq. 46?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e391ac18-50de-5c7d-ab70-9a7b36fb2cb0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the sign of $\\sigma z$ flipped in Eq. 46?", "reference_answer": "A: Because $z$ is a zero-mean Gaussian random variable, $y^* = Ax + \\sigma z$ can be replaced with $y^* = Ax - \\sigma z$, as long as the subsequent equations are consistent."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "73263a0f-0741-531d-a7a3-245a031dc7d5", "question": "Are there any results evaluating the human player impression from pairing up with AI team-mates?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["426ea220-62b5-5f9a-b6c3-7a6e2c2814ca"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there any results evaluating the human player impression from pairing up with AI team-mates?", "reference_answer": "A: In the Human-AI Game Test, the authors only show the objective metrics: the WR and the RR. In fact, during the Human-AI Game Test, after completing each game test, the testers gave scores on several subjective preference metrics to evaluate their agent teammates, including the Reasonableness of H2A (how well agents respond to the meta-commands sent from testers), the Reasonableness of A2H (how reasonable the meta-commands sent from agents), and the Overall Preference for agent teammates. The authors present the results of and discussion on human subjective preference metrics [here](https://sites.google.com/view/mcc-demo/%E9%A6%96%E9%A1%B5#h.5drjm4dzsjyw) and included these results in Appendix A.10.3. [Table 8](https://sites.google.com/view/mcc-demo/%E9%A6%96%E9%A1%B5#h.5drjm4dzsjyw) shows that **humans are satisfied with teaming up with MCC agents and gave the MCC agent the highest score on all three metrics**, which is consistent with the objective metrics results (Tables 1 and 2 in the main text and Table 7 in the appendix). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "66ad44e4-1697-53c1-b190-6a76a66bfdfa", "question": "It would be helpful to speculate a bit more on why the difference would be so large here - is the paper suggesting that training on supervised protein structure tasks might degrade zero-shot performance on fitness prediction tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["89b0a51b-8b50-586a-bab7-2259a051e049"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It would be helpful to speculate a bit more on why the difference would be so large here - is the paper suggesting that training on supervised protein structure tasks might degrade zero-shot performance on fitness prediction tasks?", "reference_answer": "A: The authors explain this issue by adding new results in Figure 4 and explanation in Section 4.3 with red color. The authors trained ESM1b (ESM-PDB-88M with 88M parameters, similar size as Evoformer) using the similar training dataset as AlphaFold, including both the original PDB dataset and these used during self-distillation process, around 650K in total. The authors found the results of ESM-PDB-88M showed very bad results, similar as Evoformer. This confirmed the proposed conjecture that one key reason for Evoformer's poor results are simply because of insufficient training data. Please see the proposed work's detailed analysis."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "54f083ab-c5fb-5d3c-866b-f52ea185fcb5", "question": "How is epistemic uncertainty computed here? Are Rashomon sets or underspecification sets used to compute it ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["28ac8153-a6b6-5c41-bba9-62b11855630c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is epistemic uncertainty computed here? Are Rashomon sets or underspecification sets used to compute it ?", "reference_answer": "A: Epistemic uncertainty is computed on both the Rashomon sets and the underspecification sets. Whenever the authors discuss/report the epistemic uncertainty for a set of approximately equivalent models, it was computed over the models in that set, and using Equation 2. The authors can clarify this. The authors can also add some sample calculations in the Appendix, since it is a little unclear how Equation 2 should be applied to a set of models. \n\nIf the authors want the epistemic uncertainty at input $x$ the authors solve Equation 2 for the epistemic term. \nLet $f_n(x)$ be the output probability of $y=1$ (a \"bad\" outcome) from model $n$ for an input/user $x$. \nLet $H(f_n(x))$ be the entropy of that output probability. Then the epistemic uncertainty at $x$ over a set of $N$ models is computed as: $H(\\frac{1}{N}\\sum_n f_n(x)) - \\frac{1}{N}\\sum_n H(f_n(x))$. The entropy of the mean minus the mean of the entropies. A set of four models having output probabilities [0.52, 0.51, 0.49, 0.48] at $x$ would have low epistemic uncertainty. While a set having outputs [0.99, 0.98, 0.02, 0.01] at $x$ would have high epistemic uncertainty."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9b7710a4-57e2-568e-8872-d063947f82fa", "question": "Do the authors only provide statistics on off-the-shelf checkpoints?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9e06e7ed-2aaf-54c1-9fa8-d5bdc49ddf6f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do the authors only provide statistics on off-the-shelf checkpoints?", "reference_answer": "A: The authors not only provide statistics on off-the-shelf checkpoints (see lines 154ff, Experimental Setup in the paper). They access checkpoints of different adversarially trained models from RobustBench, which they call “robust”. Thus, they understand a model to be robust, if it shows robust accuracy on RobustBench of more than 41.44 % on CIFAR10, 18.95% on CIFAR100 and 25.32 on ImageNet (listed on RobustBench) accuracy. All non-robust models are self-trained and have 0% robust accuracy! To facilitate the presented analysis, they train all 71 architectures with the respective training schemes to high clean validation accuracies as seen for example in Figure 1. Their paper facilitates the analysis of the behavior of robust versus non-robust models by providing direct comparison of 71 models."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5512bb7b-09a7-5055-82ea-891d47ee0c44", "question": "Have the authors compared to the GraphHopper kernel?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["52a61f8b-4dd7-5f34-9268-a1a379353951"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Have the authors compared to the GraphHopper kernel?", "reference_answer": "A: Several papers have used label-based similarities with RW kernels, e.g. [14, 40, 22]. The proposed kernel groups walks at their starting nodes, increasing expressiveness and accuracy over standard formulations. This grouping technique is neither used by the classical shortest-path kernel nor by the GraphHopper kernel but could also be useful for these kernels. Please find below the results of the GraphHopper kernel, which performs clearly worse than the proposed approach (NCW) on the considered datasets. \n\n|              | Mutag        | Nci1         | Nci109       | Ptc-Fm       | Enzymes      | Proteins     | ImdbBin      | \n|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n| GraphHopper  | 85.4$\\pm$1.9 | 72.8$\\pm$0.2 | 71.7$\\pm$0.3 | 57.8$\\pm$1.2 | 33.9$\\pm$1.0 | 68.1$\\pm$0.5 | 52.6$\\pm$0.8 |\n|  NCW         | 86.9$\\pm$0.9 | 85.5$\\pm$0.2 | 85.9$\\pm$0.2 | 63.4$\\pm$1.2 | 54.8$\\pm$1.0 | 74.8$\\pm$0.5 | 70.4$\\pm$0.8 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b4d5a525-269d-570c-b0b2-d88cce709e31", "question": "For fine-tuning a backbone network as a goal, why not compare with the methods of meta-learning, adaptor, bias tuning, or domain adaptation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["eb805361-8dc3-514d-857b-40fb116fd5b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For fine-tuning a backbone network as a goal, why not compare with the methods of meta-learning, adaptor, bias tuning, or domain adaptation?", "reference_answer": "A: Here are the results of comparing the SVF with Adapter and Bias Tuning. For quick check, the authors conduct experiments on Pascal-5$^i$ with the 1-shot setting. The details for adapter and bias tuning are given below:\n\n- Adapter: Adapter is proposed in transformer-based models. When applying it into CNN-based backbone (ResNet), the authors make simple adjustments. The authors follow [C] to build the adapter structures and add them after the stages in the ResNet.\n- Bias Tuning: In the ResNet backbone, the convolution layers do not contain bias term. The bias terms that can be used for tuning is the ones in BN layers. The authors fine-tune the bias terms in all BN layers in this method.\n\nThe experimental results are given in the table below. It shows that **SVF outperform Adapter and Bias Tuning by large margins**. Moreover, the authors find that the introduction of Adapter will directly lead to over-fitting, while Bias Tuning reduces performance of the baseline model.\n\n| Method  | fine-tune method  | Fold-0  | Fold-1  | Fold-2  | Fold-3  | Mean  |\n| ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ |\n| baseline  |  Freeze Backbone | 65.60  | 70.28  | 64.12  | 60.27  | 65.07  |\n| baseline  |  SVF | 67.42  | 71.57  | 67.99  | 61.57  | **67.14**  |\n| baseline  |  Adapter | 18.41  | 20.21  | 26.62  | 17.62  | 20.71  |\n|  baseline |  Bias-Tuning |  61.62 |  70.10 | 64.80  | 55.19  | 62.93  |\n\nFor meta-learning and domain adaptation:\n- In the few-shot segmentation, meta-learning is applied in the segmentation head to learn the knowledge in support images but not in the backbone, posing challenges in directly comparing SVF with meta-learning methods.\n- In addition, domain adaptation is another research direction whose setting differs from the setting in few-shot segmentation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "89c236e1-1b37-599a-bd13-780463570d49", "question": "Discuss loss weight $\\lambda_{dis}$ in detail.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8a4dd3c3-5cc9-5eca-8030-eebb477b53c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss loss weight $\\lambda_{dis}$ in detail.", "reference_answer": "A: As suggested, the authors conducted experiments with the same seed and the results are shown below.\n\n|loss weight $\\lambda_{dis}$|0.5|0.7|1.0|1.5|2.0|\n|-|:-:|:-:|:-:|:-:|:-:|\n|accuracy|80.64|80.95|81.28|79.54|78.90|\n\nTable R3-c. Results of different $\\lambda_{dis}$. (seed = 1)\nOur model is insensitive to loss weights. In fact, the values of discriminative distribution loss and consistency loss are very close in order of magnitude, both  between 0.1 and 0.3. So the authors set the weight to 1.0."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9cdc7191-a2cd-54c7-9520-0da4dbd803d3", "question": "If $\\|E\\|\\_{F}^{2} \\geq 1$, does Equation (37) imply that $\\lambda\\_{k+1}\\left(X^{T} X\\right)$ is large whenever $\\sin \\left(\\theta_{k}\\right)$ is large?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e72b7a5f-e9de-515b-a42f-530e48d8a04e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "If $\\|E\\|\\_{F}^{2} \\geq 1$, does Equation (37) imply that $\\lambda\\_{k+1}\\left(X^{T} X\\right)$ is large whenever $\\sin \\left(\\theta_{k}\\right)$ is large?", "reference_answer": "A: No."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3a1b376c-8786-5a93-958a-36b434eadb5e", "question": "Are the goals provided by visual images in some of the proposed experiments?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3c8c8518-08a4-546e-98e5-a844ae34bf9c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the goals provided by visual images in some of the proposed experiments?", "reference_answer": "A: Yes, the goals are provided by visual images in some of the proposed experiments."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6342012e-57df-570f-87be-c0548b02dd3b", "question": "In figure 5, how can DCG be compared with the method since they should use different Qtot (linear vs non linear)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["fb775f36-5ad4-5b12-9a52-f3e4a657b8d7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 5, how can DCG be compared with the method since they should use different Qtot (linear vs non linear)?", "reference_answer": "A: Although DCG and NL-CG uses different network structures and optimization methods, they are learning under the same environments, and thus the same reward settings. The maximum expected accumulated rewards should be the same."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "68c8f345-2b40-5300-bc0d-d9d473f8c47e", "question": "About the correlation, I did not get how to explore the correlation, could you give more details?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c0de57d0-1c93-5a02-86ea-b3e371ea1078"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "About the correlation, I did not get how to explore the correlation, could you give more details?", "reference_answer": "A: The authors define _correlation_ of an observed point and a target point as _the learned attention score between them_ in NIERT model, namely $\\alpha(q_i,k_j)$ decribed in [A3], since it represents the proportion of information passing from an observed point to a certain target point.\n\nthe authors showed such correlation of each observed point on the entire domain, namely $\\alpha(\\cdot,k_j)$ (Figure 5 of revised paper & Figure 16 of revised Supplementary material). For each observed point, the authors extract the head with the highest response from the last multi-head partial self-attention layer of NIERT.\n\nThese results show that the correlation is very similar to Gaussian RBF, that is, each observed point only affects the area near it. Meanwhile, unlike Gaussian, it is non-centrosymmetric and adaptive. These implies the interpretability of NIERT decribed in [A3]."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2e74d02c-284d-5bce-8e57-9b95ca2d7731", "question": "What are the results of applying the method to Pyramid Vision Transformer?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["99754856-542e-5463-8e6a-6104b605cdef"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results of applying the method to Pyramid Vision Transformer?", "reference_answer": "A: The results are shown in the table."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "70ff00d8-9de5-5e60-b20b-8e306488c4d0", "question": "What are the results of the ablation experiments for Modality Mixer?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4fd78cd3-88da-5b98-b746-1d7a884fe23f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results of the ablation experiments for Modality Mixer?", "reference_answer": "A: In specific, the ablations analyze the cross-modal channel attention and the residual connection in ModaMixer. The experiments are conducted with the \"template\" setting and results are shown in Table #2. From Tab. #2, the authors can see that, when using only cross-modal channel attention (i.e., VLT_SCAR w/o Residual Connection), the performance is increased by 9.0%/7.0% from 52.1%/40.7% to 61.1%/47.7% in SUC on LaSOT and TNL2K, showing the effectiveness of multimodal fusion. In addition, when adding residual connection (i.e., VLT_SCAT by default), the performance is further improved by 2.8%/2.1% from 61.1%/47.7% to 63.9%/49.8%, which verifies the importance of residual connection in ModaMixer. Based on this ablation analysis, the authors argue that final improvement by ModaMixes can be attributed to both multimodal fusion and the usage of residual connection, along with ASS (see ablation experiment in Tab. 3 of the manuscript). \n\n**Table #2**: Ablation studies on ModaMixer.\n| # | Method | Setting | LaSOT | LaSOT | TNL2K | TNL2K |\n| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n|  |  |  | SUC (%) | P (%) | SUC (%) | P (%) |\n| 1 | VLT_SCAR | w/o. Cross-modal Channel Attention and Residual Connection  | 52.1 | 50.6 | 40.7 | 40.2 |\n| 2 | VLT_SCAR | w/o. Residual Connection | 61.1 | 63.6 | 47.7 | 48.1 |\n| 3 | VLT_SCAR | default | 63.9 | 67.9 | 49.8 | 51.1 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "746be8f8-9760-5c74-ac61-ced523253e0e", "question": "The relations between Equation 2 and 6.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["c58494c7-6057-56bd-8a87-46f043ca173d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The relations between Equation 2 and 6.", "reference_answer": "A: The loss function in Equation 2 shows that the convergence of G-CapsNet can be guaranteed mathematically, just like standard neural networks. In contrast, the CapsNet in (Sabour et al., 2017) can not ensure convergence mathematically since the computation of coupling (routing) coefficients is not part of the optimization. For example, the best routing number for MNIST is 3, as suggested in (Sabour et al., 2017). The authors found that if the routing number is 4 or larger, the performance degraded. The loss function in Equation 6 gives details. You are right that the relation between Equation 2 and 6 is not clear, the authors will add an explanation. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "48bbfccb-3780-587f-bc25-b24a910dbf4f", "question": "For efficiency experiments presented in Table 1, the authors used sketch ratio $c/n=0.1$, which may be an optimistic scenario. What does the efficiency comparison looks like with settings as used in the prediction performance comparison in Table 3, where higher sketch ratios are used?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["455acdcb-5389-550d-852f-da08c44e150e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For efficiency experiments presented in Table 1, the authors used sketch ratio $c/n=0.1$, which may be an optimistic scenario. What does the efficiency comparison looks like with settings as used in the prediction performance comparison in Table 3, where higher sketch ratios are used?", "reference_answer": "A: The authors want to highlight that for the efficiency comparison, setting the sketch ratio $c/n=0.1$ is not necessarily an optimistic scenario for Sketch-GNN (the authors didn't cherry-pick but just selected a random compression ratio to report). No matter what sketch ratio $c/n$ is used, the efficiency comparison to the baselines is still fair (the full-graph training case is not a baseline) because the authors made sure to keep the size of the graph fed into the GNN model to be the same for all baselines. Apart from the \"full-graph training\" case, there are two types of baselines: mini-batch sampling methods (GraphSAINT, VQ-GNN) and compression methods (graph coarsening). For sampling baselines, the authors have set the (average) subgraph size $b=0.1n$, and for compression methods, the authors have set the size of coarsened graph to be $0.1n$ too. Under this setup, the effective number of nodes fed to the model in sketch-GNN and the other baselines are all proportional to the sketch ratio $c/n$, no matter what sketch ratio the authors choose. If the authors increase $c/n$, the memory usage and epoch time of all methods will increase. The authors will post added efficiency evaluation results to the appendices in future updates."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ebf665f-3fff-53fb-8cbc-02e86ec6eab2", "question": "Why are the ablation studies only with synthetic noisy label data?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3715fde5-bc6f-5912-9e72-d652f8221393"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are the ablation studies only with synthetic noisy label data?", "reference_answer": "A: Table 11 & 12 show results for various settings of p on WebVision - a real-world dataset with noisy labels. It would be impossible to measure the amount of “real-world noise” let alone control the amount in an ablation study. The authors therefore conducted experiments on synthetic data in order to precisely control the amount of noise. The same methodology is used in both the MentorNet paper and Ren’s ICML18 paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8dd0d814-fbca-5720-8c9e-1793c71a3e31", "question": "Why do the results in Figure 6 do not suggest that there is an increase in sample diversity?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why do the results in Figure 6 do not suggest that there is an increase in sample diversity?", "reference_answer": "A: Figure 6 tends to show Eq. 8's effectiveness in obtaining pairs that tend to collapse (i.e. $\\mathbf{z}$ distant in latent space and $G(\\mathbf{z})$ close in pixel space) as the authors aim to regularize $G$ more purposefully through the aforementioned hard samples (sample pairs that tend to collapse) in training to improve $G$ (Line 219-222, 240-244). On the contrary, Eq. 8-inv tends to obtain pairs with much different generation results. Using \"diverse\" to describe this pair is imprecise, instead it can be described as \"leads to more different generation\"."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a4eb15ab-1430-5c2f-883c-42b90100d921", "question": "On the $k$-clique constraint satisfaction experiments why didn’t you compare against the Erdős model from Table 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["93650c95-c6a5-5f93-8455-e4f0258cfaa4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "On the $k$-clique constraint satisfaction experiments why didn’t you compare against the Erdős model from Table 1?", "reference_answer": "A: The goal of the experiment is to show that SFEs can incorporate problem knowledge."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c1c1d829-84b2-586d-a0aa-fae933e588a6", "question": "What is the main reason of using the restart strategy? How to compare the step size η with the classic one in SGD? Is this choice of the learning rate critical to show the convergence?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["957b66a3-1bd0-534c-9ef1-c3837dbdf921"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the main reason of using the restart strategy? How to compare the step size η with the classic one in SGD? Is this choice of the learning rate critical to show the convergence?", "reference_answer": "A: There are two main motivations to consider restarting strategy for SGD. The first motivation is practical. Notice that the step-size schedule in the restarted SGD is essentially a popular step-decay (piecewise constant) strategy. Such technique is commonly used in practice and often outperforms the classical diminishing step-size schedule for SGD [Krizhevsky et al, 2012], [He et al, 2016]. Therefore, the authors believe it is important to provide solid theoretical foundations for such piecewise constant step-size strategy. \nThe second motivation comes from the analysis perspective. Notice that the restart strategy is crucial for obtaining $\\varepsilon^{-\\frac{2}{\\alpha}}$ sample complexity of PAGER. The authors provide discussion about why restarting is important and the intuition why SGD is not sufficient in Appendix C. Therefore, it becomes interesting to find out if restarts help to improve the sample complexity of SGD (as it is the case for variance reduced method). \nNotice that setting $T=1$, the proposed restarted SGD reduces to the standard SGD and the correspondence for the classical step-size becomes evident. Here, the authors analyze a more flexible step-size schedule, than a classic one (allowing to set arbitrary $T\\geq 1$). \nOverall, the authors did the proposed best to extensively analyze SGD from different angles (using minibatch, restarting, constant/varying step-sizes); however, the proposed conclusion is that at least following this type of analysis, the improvement over $\\varepsilon^{-\\frac{4-\\alpha}{\\alpha}}$ cannot be obtained. \n\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a6036805-668f-5d51-a0e1-f5ceabc3ab49", "question": "What is the performance of MCQ on random and medium datasets with different $\\lambda$ values?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["52ef3784-d176-5ba3-9e33-ca99a2dd9b01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the performance of MCQ on random and medium datasets with different $\\lambda$ values?", "reference_answer": "A: See Table 1 for the normalized average score of MCQ over different choices of $\\lambda$ on MuJoCo \"-v2\" datasets. The results are averaged over 4 different random seeds."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fe326f7e-7b5b-5543-adaf-d640de9d5a82", "question": "Similar to SOS (sum of squares) methods, are there methods that don’t use deep learning to generate V. Is it possible to compare to these?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Similar to SOS (sum of squares) methods, are there methods that don’t use deep learning to generate V. Is it possible to compare to these?", "reference_answer": "A: There exist some quadratic program (QP) based methods that utilize the V function in SDE to dynamically find the control, but they always fix some candidate V functions and focus on generating the control. Here the authors provide a numerical comparison with these methods, including HDSCLF and BALSA, in Figure 6. And the authors can see that the proposed learning control outperforms these methods in those experiments. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8db33ac8-7b79-55d8-9451-7b490e3b4e37", "question": "Why was ACGAN not included as one of the backbones in the STL-10 experiment?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why was ACGAN not included as one of the backbones in the STL-10 experiment?", "reference_answer": "A: ACGAN requires label input to generate. However, the experiment is based on the unlabeled part of STL-10 as it contains more images [1]. Thus ACGAN does not work on unlabeled STL-10, and the authors do not include it as one of the backbones in the STL-10 experiment. \n[1] An Analysis of Single Layer Networks in Unsupervised Feature Learning AISTATS, 2011."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cfcb6786-58a5-544f-9f06-02dd87430854", "question": "What is G?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["25c2eb12-a2ce-59c8-b48b-438ef66d9bca"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is G?", "reference_answer": "A: G is the on-policy graphical model defined in Figure 2, (b)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4886f17c-0102-5839-a010-a4998ed4108d", "question": "How does FRePo perform on the original architecture?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does FRePo perform on the original architecture?", "reference_answer": "A: See Appendix C.6 Table 16 and Table 17. The authors observe that DCConv works reasonably well when distilling a small number of images (~100). The performance degrades a lot when distilling 1000 images from CIFAR100 because the KRR component needs a larger feature dimension to perform well when the authors distill more data."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a36e145f-2678-5341-8dbb-47e97b9b5664", "question": "Please also show a table of comparing the proposed approach with the previous \"SOTA\" ...", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["70d23f47-63f0-5df1-88ba-b148cfca9c6a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Please also show a table of comparing the proposed approach with the previous \"SOTA\" ...", "reference_answer": "A: The authors have Table.4 and also its corresponding paragraph in lines 297-302:\n\n|**Model**                 | **Dataset**              | **Ours** | **[1]** | **[2]** | **[3]** | **[4]**|\n|---                | ---              | --- | --- | --- | --- | ---\n**MobileNetV2**  | **ImageNet**    | 72.8 | 70.5    | 71.9    | 71.2    | 72.6    \n**ResNet18**     | **ImageNet**    | 69.3 | -       | 70.2    | 69.7    | 71.1    \n**DeepLab-V1**   | **VOC**         | 74.7 | 69.9    | -       | -       | -       \n**Faster R-CNN** | **COCO**        | 37.4 | -       | 37.4    | 34.9    | -       \n  \n*Comparison with state of the art:  Table 4 provides a comparison between the proposed training method and state of the art across different experiments. There are some important differences between the proposed method and other works: (i) the proposed integer training method uses a fixed-point batch-norm layer where both forward and back propagation is computed using integer arithmetic, (ii) the proposed integer training method uses an integer-only SGD,  (iii) in the proposed training method, no hyper-parameter is changed while they have changed hyper-parameters or used gradient clipping.*"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "243f491c-5f53-5dd8-816d-2f386f318624", "question": "In Table 1, the cEP estimate used in the noisy regime uses 1 point when beta is zero and 10 for non-zero beta. This in a very biased negative update but unbiased positive update. What will be the results if 5 points were used for the two phases to reduce this \"bias imbalance\"? Would this improve the quality of the cEP estimate in the noisy regime?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["1ff74430-3f62-56d5-ab39-6e60abb348b1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1, the cEP estimate used in the noisy regime uses 1 point when beta is zero and 10 for non-zero beta. This in a very biased negative update but unbiased positive update. What will be the results if 5 points were used for the two phases to reduce this \"bias imbalance\"? Would this improve the quality of the cEP estimate in the noisy regime?", "reference_answer": "A: Running the classic EP simulations with averaging also over the free equilibrium, makes classic EP perform slightly worse, since the befThe authors agree that this comparison is now more fair and the authors updated all results accordingly. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "456b9c0c-fb5b-5329-9c85-70552366c1d4", "question": "What is the definition of anisotropy?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9c5d7aae-bbec-5e57-8114-8f24fbb8acf5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the definition of anisotropy?", "reference_answer": "A: The anisotropic nature of language models was first investigated by [1]. The authors' original definition of anisotropic token distribution was based on token-level cosine similarity measurement [1]. In the proposed study, the authors follow the same method as [1] and illustrate the language model's anisotropy from token-level measurement as demonstrated in Figure 1. Please refer to the original paper [1] for more details."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dfa0b7d1-9c77-5f6f-8e60-94afd97e4a9d", "question": "Why use only N=5 images for DTU and not all available ones?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["052b92f6-35e8-59ed-bb9e-0ab6269d29da"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why use only N=5 images for DTU and not all available ones?", "reference_answer": "A: This is an interesting question. To illustrate the influence of $N$, the authors perform an ablation study in Tab. 5 of the supplementary material. As shown in the results, the optimal $N$ for DTU dataset is 5 in the proposed case. Here the authors give the proposed analysis for this ablation study.\nSince the authors are performing experiments on DTU test set, where the camera distribution is quite sparse, the problem of occlusion becomes severer as the number of input views increases. Similar results are also observed in TransMVSNet [3] and PatchmatchNet [26]."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "babf822b-f584-5f6a-8231-b45c44c0863b", "question": "17: In the experiments, the R_nDTW and R_SR rewards are not fully explained. How do these rewards correspond to what was introduced in prior work? The combination of R_nDTW and R_SR seem to work well (comparable with the proposed method): A17:", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "17: In the experiments, the R_nDTW and R_SR rewards are not fully explained. How do these rewards correspond to what was introduced in prior work? The combination of R_nDTW and R_SR seem to work well (comparable with the proposed method): A17:", "reference_answer": "1. As shown on Lines 273-274, the R_nDTW (model# 14) reward takes the nDTW metric as a reward directly. It is the original form before the authors modify it to the soft focal-oriented reward. No prior work has only used it as a reward. The authors report R_nDTW in Table 3 to conclude that only considering the global alignment between instructions and trajectories makes the agent just concern about the similarity of trajectories but not the locations that instructions really concern during navigation.  \n2. As shown in the caption of Table 3 and Line 285, the R_SR reward is the goal-oriented reward [5] which uses the SR metric as a reward signal.   \n3. As mentioned in the caption of Table 3, the combination of R_nDTW and R_SR is exactly the fidelity-oriented reward [4]. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c681a7fe-feb7-58f6-822a-7abd6a3394a0", "question": "Many biased datasets are tabularly represented. How does this work apply on tabular data? Appending additional dimension of vector directly?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["1fc786d5-cfc8-580e-b579-c2c56e8dba87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Many biased datasets are tabularly represented. How does this work apply on tabular data? Appending additional dimension of vector directly?", "reference_answer": "A: Fairness reprogramming can be applied to tabular data. For reprogramming, there are many ways to design triggers according to different tasks and requirements. Unlike NLP, where the authors append the trigger to the input or embeddings, the model for tabular data is sensitive to input size. As the tabular data have a fixed input size, the authors can directly apply the **additive trigger** to the input data to keep the input dimension unchanged (i.e., adding a perturbation on the original input), just as the authors adopted in image domains (Figure 1). To verify the argument, the authors applied the proposed method to the tabular data and conducted additional experiments on the UCI Adult dataset with a two-layer MLP model, and the results are shown in this **[Figure](https://ibb.co/ssNyK7v)**. The results suggest that the proposed method could effectively improve model fairness for tabular data. The proposed method achieves comparable debiasing performance with the post-processing adversarial training method without modifying any model parameters."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b525bd9a-392d-5920-8c54-e81cb4870be7", "question": "Its unclear reading the figure 1 and the description, as to how to achieve exponential scaling. Given a fixed alpha_total and ability to prune by a fraction f, is the idea to decrease f such that for large f (less training data), use easy examples but for small f (more training data) use hard examples?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7de55424-0d66-5dfd-9b68-f611c6c28c8c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Its unclear reading the figure 1 and the description, as to how to achieve exponential scaling. Given a fixed alpha_total and ability to prune by a fraction f, is the idea to decrease f such that for large f (less training data), use easy examples but for small f (more training data) use hard examples?", "reference_answer": "A: The key idea again is that alpha_total is not fixed in Figure 1, as explained in the proposed “Parsing Fig. 1” explanation.   Also f is the fraction of examples kept so all else held equal, reducing f would reduce training set size.   Fig. 1 shows the possibility of finding a sequence of datasets of increasing size alpha_prune obtained by increasing alpha_total (i.e. collecting more data) but decreasing f (keeping less of it) such that test error E as a function of pruned dataset size alpha_prune falls of exponentially with alpha_prune.  The practical setting in which this is relevant are scenarios in which new data is collected daily (i.e. new text generated on the web each day, i.e. new interactions generated on a social network each day).  In such a setting data grows without bound over time and there is no sense in which alpha_total is a fixed quantity.  In such settings the authors can become increasingly selective about which data the authors train on (i.e. reduce the fraction of data f that is kept as a function of all data alpha_total generated so far).  Then alpha_prune = f * alpha_total is the actual training set size.    "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5da4e144-f54d-525b-84c1-2b29d4edad5d", "question": "Are the localization network replaced by image similarities in pixel space or some image descriptor space?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f9de7728-3595-5e1b-b94f-4a2a9933b230"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the localization network replaced by image similarities in pixel space or some image descriptor space?", "reference_answer": "A: Yes, they are replaced by image similarities in pixel space or some image descriptor space."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1ae8977c-25c8-54dd-bf67-6bf59e28e5fd", "question": "How many floating point values are unknown to the victim?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["94275829-60ba-5423-9225-c41722c6d35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How many floating point values are unknown to the victim?", "reference_answer": "A: If AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (because we sample our starting signal from a Gaussian for a 32x32x3 image and an AR process that uses a window size 3x3) (Figure 3, Left)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "659c2863-0f57-5cab-83f6-eabff46e0f1f", "question": "Summarize and thoroughly discuss with empirical comparison of the proposed method with previous methods for likelihood-free inference.\nReferences:\nGeorge Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 837–848. PMLR, 2019\nYanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville, and Zhanxing Zhu. Neural approximate sufficient statistics for implicit models. arXiv preprint arXiv:2010.10079, 2020.\nJan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke. Likelihood-free inference with emulator networks. In Symposium on Advances in Approximate Bayesian Inference, pages 32–53. PMLR, 2019.\nGeorge Papamakarios and Iain Murray. Fast E-free inference of simulation models with bayesian conditional density estimation. Advances in neural information processing systems, 29, 2016.\nDavid Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In International Conference on Machine Learning, pages 2404–2414. PMLR, 2019.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4899724f-eebf-5cd2-8f14-5e4cad622d8a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Summarize and thoroughly discuss with empirical comparison of the proposed method with previous methods for likelihood-free inference.\nReferences:\nGeorge Papamakarios, David Sterratt, and Iain Murray. Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 837–848. PMLR, 2019\nYanzhi Chen, Dinghuai Zhang, Michael Gutmann, Aaron Courville, and Zhanxing Zhu. Neural approximate sufficient statistics for implicit models. arXiv preprint arXiv:2010.10079, 2020.\nJan-Matthis Lueckmann, Giacomo Bassetto, Theofanis Karaletsos, and Jakob H Macke. Likelihood-free inference with emulator networks. In Symposium on Advances in Approximate Bayesian Inference, pages 32–53. PMLR, 2019.\nGeorge Papamakarios and Iain Murray. Fast E-free inference of simulation models with bayesian conditional density estimation. Advances in neural information processing systems, 29, 2016.\nDavid Greenberg, Marcel Nonnenmacher, and Jakob Macke. Automatic posterior transformation for likelihood-free inference. In International Conference on Machine Learning, pages 2404–2414. PMLR, 2019.", "reference_answer": "A: To suit the scenario presented in this paper, i.e., estimating multiple $\\boldsymbol{\\phi}_i$ for multiple different observations $\\mathbf{Z}_i$ at test time (as discussed in A1-1), and for a fair comparison, authors compare with the non-adaptive counterpart of the algorithms referenced in these papers (i.e., they have a fixed set of training samples used to train a system used for all test samples). For instance, they set the round number of SNL+ to be 1. \n\nNext, while this paper's problems are in a high-dimensional regime (e.g., $\\rm{dim}(\\mathbf{Z}) = 396,000$ for Lorenz 96), sequential neural likelihood estimation (SNL) in Papamakarios et al. [2019] does not scale to the high-dimensional setting, which is recognized as a limitation and future direction by the authors. The later work in Chen et al. [2020] SNL+ addresses this limitation by learning sufficient statistics (embeddings) of the data based on the infomax principle. Detailed empirical comparisons are shown in Tables 1 and 2 for SNL+.\n\nClosely related to Papamakarios et al. [2019], Lueckmann et al. [2019] tries to ``emulate'' the simulator likelihood and advocates the use of active learning to choose the simulations in the next round based on uncertainty in the posterior. This work is unable to compare with this method as Lueckmann et al. [2019] does not appear to have available code and also lacks evidence that it can scale to high-dimensional data settings (their high-dimensional setting is of dimension $O(10^3)$ while this one has $O(10^6)$). \n\nFinally, sequential neural posterior estimation (SNPE-A in Papamakarios and Murray [2016], SNPE-C in [Chen et al., 2020, Greenberg et al., 2019]) trains a neural network to approximate the posterior distribution directly. SNPE-C gains an advantage against SNPE-A in that it has fewer restrictions regarding the form of prior and posterior by leveraging neural conditional density estimation [Papamakarios et al., 2017]. The detailed empirical comparisons regarding non-sequential SNPE-C (i.e., NPE-C) are in Tables 1 and 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "14523ad1-f339-5349-9cac-030b4af4867a", "question": "Compare and discuss similar work such as:\nKarazija L, Laina I, Rupprecht C. ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation, NeurIPS Datasets&Benchmarks 2021. Proposes a benchmark and an analysis of a similar set of methods. The motivation for introducing a new dataset is based on the same intuition: current real-world datasets are too difficult. Additionally it comes to some of the same conclusions (e.g., texture vs. lighting vs. plain colors). \nWeis MA, Chitta K, Sharma Y, Brendel W, Bethge M, Geiger A, Ecker AS. Benchmarking Unsupervised Object Representations for Video Sequences. J. Mach. Learn. Res.. 2021 Analysis of similar concepts for video segmentation methods with a focus on occlusions. Many of the investigated methods are video extensions of the approaches in this paper.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare and discuss similar work such as:\nKarazija L, Laina I, Rupprecht C. ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation, NeurIPS Datasets&Benchmarks 2021. Proposes a benchmark and an analysis of a similar set of methods. The motivation for introducing a new dataset is based on the same intuition: current real-world datasets are too difficult. Additionally it comes to some of the same conclusions (e.g., texture vs. lighting vs. plain colors). \nWeis MA, Chitta K, Sharma Y, Brendel W, Bethge M, Geiger A, Ecker AS. Benchmarking Unsupervised Object Representations for Video Sequences. J. Mach. Learn. Res.. 2021 Analysis of similar concepts for video segmentation methods with a focus on occlusions. Many of the investigated methods are video extensions of the approaches in this paper.", "reference_answer": "A: The authors have a separate paragraph \"Related Work\" in Section 1 page 2 which does this comparison. \n\n_CLEVRTEX, Karazija et.al.,_: As a benchmark for unsupervised object segmentation, it shares similarities with the proposed work. Both conduct extensive experiments on the-state-of-art models on a set of benchmark datasets. However, CLEVRTEX focuses on the characteristics and comparison of different models. The proposed work, on the other hand, aims to quantify properties/inductive biases of synthetic and real-world datasets, and then discover what dataset factors incur the failure of existing models on challenging images. Notably, the authors employ real-world datasets instead of only complex synthetic datasets for systematically evaluation.\n\n_Benchmarking Unsupervised.., Weis et.al.,_: This work targets at object-centric learning on sequential data instead of single images. It shows detailed architectural analysis and comparison between a set of models. Similar to the proposed work, they also consider properties of datasets. Specifically, several challenging cases such as occlusions are discussed. However, they are still limited to synthetic datasets, and its dataset properties are generally descriptive rather than quantitative. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aae6307c-4710-5035-a15d-ecbc3f8e2fde", "question": "Is equation 20 is wrong? I believe it s wrong because it is calculated by solving Eq 12 and Eq 19. The authors cancel out the normalization terms from Eq 12 and Eq 19 but they are not actually equal!", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["3bd138cc-26bc-54fb-a5e7-e4d24e050a01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is equation 20 is wrong? I believe it s wrong because it is calculated by solving Eq 12 and Eq 19. The authors cancel out the normalization terms from Eq 12 and Eq 19 but they are not actually equal!", "reference_answer": "A: Based on the paper, Eq. (12) and Eq. (19) are not equal in general, because Eq. (19) (which comes from from Eq. (17)) is a Monte-Carlo **approximation** of the marginal distribution in Eq. (12)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4ab2e7fa-840a-5a7b-a908-4cbb7e8c018f", "question": "What’s the intuition behind sharing everything else besides the embedding layer for image and sentence encoders?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What’s the intuition behind sharing everything else besides the embedding layer for image and sentence encoders?", "reference_answer": "A: The proposed intuition is *inspired by Lu et al. [1] which shows the ability of Transformers to transfer knowledge between text and other modalities (e.g., they show that a Transformer model pre-trained on text can be fine-tuned on downstream visual tasks).*  Note that Lu et al. [1]  investigate knowledge sharing between text and other modalities through *transfer learning*, i.e They do not introduce additional parameters and freeze most parameters learned from language and fine-tune a few layers (e.g., normalization) on downstream vision tasks. The proposed framework shows that knowledge can be shared between text and other modalities in a *multi-task setting*. Other than practical implications, these findings have conceptual significance as they support the hypothesis that Transformer models can share knowledge between different modalities.\n[1] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines, AAAI-2022"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "10dd5c24-6435-5ab2-8c62-9279847cd1f4", "question": "Is the ablation study and GAN inversion on other backbones important?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the ablation study and GAN inversion on other backbones important?", "reference_answer": "A: No. Inversion is just a simple application to verify that the proposed approach could learn a moderate 3D underlying shape from 2D images. This is not the proposed major focus."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "eff5cae0-42d1-5eb6-b9aa-34270a4ed6f3", "question": "In equation 7, the second term requires computing $\\max_{a′ \\in A} f_w(s+[a],a′)$. Does this mean that at each generation step the model needs to run a forward path for every action, i.e., word in the vocabulary? If so it might render the method slow and hard to scale.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In equation 7, the second term requires computing $\\max_{a′ \\in A} f_w(s+[a],a′)$. Does this mean that at each generation step the model needs to run a forward path for every action, i.e., word in the vocabulary? If so it might render the method slow and hard to scale.", "reference_answer": "A: No, the authors do not need multiple forward passes for a sample.\n\nOur RL training assumes a trajectory is sampled, so $[a]$ is already determined when the authors calculate Eq (7). Moreover, $f_w(s+[a], \\cdot)$ is implemented as the logit of a softmax layer; computing $\\max$ over $a’$ is simply computing maximum of softmax logits. Therefore, only one forward pass is needed for a sample. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "148476e6-0eee-51f7-8f6f-af53b7cb53f2", "question": "Additional comment #1: What will the network performance be if all inputs exit from the same classifiers?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["12a16837-7683-5205-a1fe-20f98adf46a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Additional comment #1: What will the network performance be if all inputs exit from the same classifiers?", "reference_answer": "A: The appendix contains Figures 5, 10, 11, 12, where the accuracy scores for individual ICs are marked."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "542ce94f-276f-5b12-9a6a-26971d8a75fd", "question": "What is the optimization problem that the attacker is solving?  Intuitively, my question is how does the attacker account for the ADGP defense?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["56e0935b-6668-506f-918b-97e83c9c5fe2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the optimization problem that the attacker is solving?  Intuitively, my question is how does the attacker account for the ADGP defense?", "reference_answer": "A: In this paper, the attacker aims to solve the first optimization problem because the attacker wants to generate the images whose gradient is similar to the obtained gradient. Even for the second optimization problem, the authors can still provide theoretical proof of privacy guarantee. \nThe relevant analysis is as follows.\nTo simplify the expression, the authors use $x$ to denote the gradient and $||\\cdot||$ denotes $||\\cdot||_2$. [1] states the following property of topl(x) (i.e., retain the top $l$-ratio of $x$):\n$$\n||x - {top}l(x)|| \\leq \\sqrt{1-l} ||x|| \\tag{1}\n$$\nAccording to formula (1), it is easy to obtain formula (2):\n$$\n|{top}l(x)|| \\ge(1-\\sqrt{1-l} )||x|| \\tag{2}\n$$\nThen, the authors make a strong assumption that ADGP(x')=ADGP(x), because the attacker has no prior knowledge of the remaining parameters, he can only randomly generate the remaining parameters, that is, E(x'-ADGP(x')))=0. So the authors have:\n$$\n||x-E(x')||=||x-ADGP(x')||>||topk1(x)|||\\overset{(d)}{\\ge}(1-\\sqrt{1-k_1} )||x||\n$$\n\n\nwhere (d) is directly an application of (2).\nIt can be seen that the proposed removal of the parameters of top-k1 can provide a stable lower bound on privacy."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1890f083-4abc-52b3-b8f3-b529aaae0002", "question": "In Corollary 1 and 2, why the optimization error of GD for the convex problem is of $1 / \\sqrt{t}$ and of SGD for the convex problem is $1/t^{4}$? It contradicts with what is mentioned in this paper from reference [1,2]. Which ones are correct? Cor 1 & seems to be far from optimal. \nReference:\n[1] S. Bubeck. Convex optimization: Algorithms and complexity. Preprint arXiv:1405.4980, 2014\n[2] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In Conference on Learning Theory, 2009.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["973b558e-915b-58ba-9d2a-9f46e25abc94"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Corollary 1 and 2, why the optimization error of GD for the convex problem is of $1 / \\sqrt{t}$ and of SGD for the convex problem is $1/t^{4}$? It contradicts with what is mentioned in this paper from reference [1,2]. Which ones are correct? Cor 1 & seems to be far from optimal. \nReference:\n[1] S. Bubeck. Convex optimization: Algorithms and complexity. Preprint arXiv:1405.4980, 2014\n[2] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In Conference on Learning Theory, 2009.", "reference_answer": "A: Yes, the convergence rate of GD and SGD under smooth convex problem is respectively $O(1 / t)$ and $O(1 / \\sqrt{t})$. However, the bound in this paper, of excess risk in equation (7) is of order $\\tilde{O}(\\sqrt{\\epsilon(t)} + 1 / n)$, which matches the results in Corollary 1 and 2. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bcf7ec1b-5f53-5934-8dc0-acbdf50f4f83", "question": "What are the results of comparing the baselines with the same backbone architecture?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results of comparing the baselines with the same backbone architecture?", "reference_answer": "A: The authors retrained BicycleGAN and MSGAN on super-resolution (SR) and colourization (Col) using the same generator architecture used by their method. Furthermore, the authors also retrained two other baselines, cIMLE and MoNCE, with the same architecture. The authors observed that the GAN-based baselines failed to converge when trained from scratch with the proposed architecture, so the authors pretrained their generator using the proposed method (which gave them an advantage over the vanilla randomly initialized versions). The authors show the FID results in the table below.\n\n|            | Super-Resolution (SR) | Colourization (Col) |\n|------------|-----------------------|---------------------|\n| BicycleGAN + the proposed architecture | $53.30$               | $66.32$             |\n| MSGAN + the proposed architecture      | $57.94$               | $81.86$             |\n| MoNCE + the proposed architecture      | $31.72$               | $\\underline{27.85}$             |\n| cIMLE + the proposed architecture      | $\\underline{21.13}$               | $42.67$             |\n| CHIMLE     | $\\textbf{16.01}$      | $\\textbf{24.33}$    |\n\nAs shown above, their method consistently outperforms the baselines with the same network architecture, thereby validating the effectiveness of their method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d2546760-67fb-586e-bf6c-5824f003a53d", "question": "What is the point of reporting numbers on the pretrained models and few-shot etc? There are some contradictions between pre-training and few-shot learning. However, since almost all of the state-of-the-art methods are based on pre-trained weights instead of random initialization, can you report performances with pre-trained weights to ensure comparability among them?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b0d70329-7bd8-5c32-81c7-5743ec00d545"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the point of reporting numbers on the pretrained models and few-shot etc? There are some contradictions between pre-training and few-shot learning. However, since almost all of the state-of-the-art methods are based on pre-trained weights instead of random initialization, can you report performances with pre-trained weights to ensure comparability among them?", "reference_answer": "A: Because the backbone of the proposed model synchronously learns spatio-temporal features, it is a model of video understanding type. The authors can only use video datasets to train it. The authors report the performance of the proposed model after pretraining on large-scale Kinetics-400 dataset. Similarly, all models here use 8 frames as input. Through pretraining on Kinetics dataset, all models are compared under a fair pretraining condition. After that, the few-shot dataset UCF101 is used for fine-tuning and testing, and the results are shown in the following table. As shown in Table 12, the proposed model still has the best performance after pretraining on large-scale dataset. With only 1/3 FLOPs, the proposed method can surpass TRX by 2.5% and 10.8% on the UCF101 and HMDB51 dataset, respectively.\n\n|     Method             |     Pretraining     |             |     UCF101    |               |             |     HMDB51    |               |\n|------------------------|---------------------|-------------|---------------|---------------|-------------|---------------|---------------|\n|                        |                     |     Acc     |     Params    |     FLOPs     |     Acc     |     Params    |     FLOPs     |\n|     TimeSformer [2]    |     -               |     63.0    |     40.7M     |     73.35G    |     41.7    |     40.7M     |     73.35G    |\n|     TimeSformer [2]    |     Kinetics-400    |     80.5    |     40.7M     |     73.35G    |     54.2    |     40.7M     |     73.35G    |\n|     TRX [25]           |     -               |     67.0    |     25.6M     |     41.43G    |     46.4    |     25.6M     |     41.43G    |\n|     TRX [25]           |     Kinetics-400    |     85.1    |     25.6M     |     41.43G    |     60.7    |     25.6M     |     41.43G    |\n|     Ours               |     -               |     69.7    |     8.84M     |     13.76G    |     60.4    |     8.91M     |     13.65G    |\n|     Ours               |     Kinetics-400    |     87.6    |     8.73M     |     13.61G    |     71.5    |     8.75M     |     13.52G    |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d1a8eabd-fb09-51cf-ae9e-8d11b1bcd965", "question": "In Table 3, it is observed that the hyperparameters vary with different scales according to the dataset. Is the performance sensitive with respect to the hyperparameter choice? Why is the scale of the hyperparameter different?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["368314b3-9259-5022-a0f3-fd17a4b571c2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 3, it is observed that the hyperparameters vary with different scales according to the dataset. Is the performance sensitive with respect to the hyperparameter choice? Why is the scale of the hyperparameter different?", "reference_answer": "A: Yes, $\\delta$ has different scales according to the dataset. As shown in Equation 16 (on page 6) and Algorithm 1 (on page 15), $\\delta$ is utilized to control the stopping condition. The authors observed that experiments on datasets of undirected graphs converged faster than on datasets of directed graphs. So the authors set the value of $\\delta$ on these datasets to be one scale larger. The authors leveraged $\\sigma$ be the variance terms of $\\mathcal{L}_p$, and the value of it differed a bit between datasets of undirected graphs and directed ones.\n$\\eta$ was leveraged to control the rounds of training before the first iterative process. The authors reported the values as the ones to produce the best experimental results from a search space of $80$ to $200$. The remaining hyperparameters are the ones in hybrid loss. The authors set the value of which to match the corresponding terms in the loss function to the same scale. \nThe authors tested the sensitivity of iSIDG against the choice of hyperparameters, and only the choice of $\\eta$ mattered a little bit. (This issue can be solved by performing a search on it.) The rest hyperparameters had no significant effect on the performance. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "601401e2-4014-5616-ad46-500d2e882241", "question": "Is the evaluation limited?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ca8797f9-dc18-5853-9021-f8fd915534e6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the evaluation limited?", "reference_answer": "A: The scores in Figure 6(c) are for different channels corresponding to different numbers of tokens. However, under the same compression ratio, a better rFID score indicates a better compact representation, leading to a better generation score."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b6a8a1e4-0392-5f89-91d0-5be48128fccd", "question": "Is it a fair conclusion that “learnable activation blocks and FLC generalize better to unseen attacks compared to AT”? How do Figures 4, 5, and 6 like on unseen attacks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9e06e7ed-2aaf-54c1-9fa8-d5bdc49ddf6f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is it a fair conclusion that “learnable activation blocks and FLC generalize better to unseen attacks compared to AT”? How do Figures 4, 5, and 6 like on unseen attacks?", "reference_answer": "A: From the results, it can not be concluded that learnable activation blocks or FLC generalize better than AT, because both models are additionally trained with AT. The authors can only conclude that FLC or learnable activations can have an additional positive impact. The authors used the black-box attack Squares to evaluate against unseen attacks. Specifically, none of the models has seen Squares samples during training. Further, the FLC pooling is trained with simple FGSM thus the PGD samples are also unseen for this model. However, the model including learned activation functions is trained with PGD and thus has seen PGD samples already during training. Squares samples are out-of-domain."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "41afdbe4-f7c3-5550-a624-1f2aec1b1764", "question": "The performance of QMIX is surprisingly low, why is that? It would have been useful to compare in another MARL where QMIX is not so bad e.g. starcraft.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["fb775f36-5ad4-5b12-9a52-f3e4a657b8d7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The performance of QMIX is surprisingly low, why is that? It would have been useful to compare in another MARL where QMIX is not so bad e.g. starcraft.", "reference_answer": "A: MACO benchmark features tasks that require sophisticate coordination among agents. Not only QMIX, most fully decomposed value function methods (e.g., DICG in Figure 5) cannot perform well on these tasks. On a super-hard scenario, MMM2, from the SMAC benchmark, the proposed method still outperforms QMIX by a large margin."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3b1241f4-7f3d-5473-8ac5-64f52bfd9234", "question": "Should the approach be demonstrated on object recognition on ImageNet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["f000c838-1733-5b65-b8ff-3ec832a960a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Should the approach be demonstrated on object recognition on ImageNet?", "reference_answer": "A: The authors have conducted the experiments on ImageNet to demonstrate the performance of the proposed model. The authors choose R18, p4-R18 and p4-$E^4$R18 which are described in section 5.2, except that the last fully connected layer are replaced to deal with classification of 1000 category. In the experiments, the authors adopt commonly used data augmentation as in [10] and train all these models for 120 epochs utilizing the Stochastic Gradient Descent (SGD) optimizer with the momentum of 0.9 and the weight decay of 0.0001. The learning rate initiates from 0.3 and gradually approaches zero following a half-cosine function shaped schedule. No training tricks are adopted. The results are listed in Table II. The proposed model significantly outperforms G-CNNs with smaller model size on the ImageNet which is consistent with results on CIFAR."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "101dd381-87d6-56b5-843a-2198ba90dc14", "question": "Explain the scale used for Fig 8. How is [1] used to report the results? Can data augmentation be used to improve the attack as demonstrated in [1]?\nReference:\n[1] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership Inference Attacks From First Principles. arXiv preprint arXiv:2112.03570, 2021.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["617d7416-f163-5ae1-a9e8-dbcf27170aa5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain the scale used for Fig 8. How is [1] used to report the results? Can data augmentation be used to improve the attack as demonstrated in [1]?\nReference:\n[1] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership Inference Attacks From First Principles. arXiv preprint arXiv:2112.03570, 2021.", "reference_answer": "A: Following the suggestions from Carlini et al. [1], authors report the true positive rate and false positive rate in the evaluation of membership inference attack. Authors present ROC with log-scale in the paper (as shown in Figure 2 in the Supplementary Materials) in Figure 8. \nData augmentation can be used to improve the attack. In the feature-based method, the authors trained the multimodal feature extractor (MFE) with data augmentation [r1]. The average attack success rate of data augmented MFE is 72.69% (in all scenarios), while the feature-based attack without data augmentation training achieves 69.51% on average (as shown in Figure 6). \n\n[r1] A survey on image data augmentation for deep learning[J]. Journal of big data, 2019"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4ec69edb-c4a6-5dc9-81a5-f0565802b71c", "question": "Why not use the same model architecture as COS-CVAE (LSTM)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0d97bc45-58ea-5a2c-9d6a-18b36254f00a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why not use the same model architecture as COS-CVAE (LSTM)?", "reference_answer": "A: The authors run experiments using the UpDown model (a two-layer LSTM with a visual attention module) for the proposed MIC branch, which is also the language generation model used by COS-CVAE. The oracle performance of this model is 1.688 and 1.942 in terms of CIDEr for 20 and 100 samples, respectively, still outperforms the COS-CVAE by a large margin. In fact, UpDown is a strong model that achieves compatible performance with a 6-layer Transformer model in a general image captioning setting (1.099 CIDEr vs. 1.114 CIDEr on Karpathy’s test split), which means that two-layer LSTMs may already have enough capacity for the COCO dataset. The authors will give more discussions on this in the revision.\nMoreover, considering that COS-CVAE requires a pre-processing step to construct pseudo supervisions with the help of a pretrained joint vision-language embedding model, the proposed end-to-end learning method could be more convenient to use than COS-CVAE."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "035548f5-b94d-5725-9574-0c13c4c0fe72", "question": "Each layer of CSC-layer of SDNet-18 and SDNet34 needs unrolling two iterations of FISTA and more iterations will only slightly improve the performance. As SDNet-18 and SDNet-34 have only one CSC-layer for the input images, I’m curious whether it is this low dimension (3 channels) of input that make two iterations sufficient. On SDNet-18-All and SDNet-34-All, could you list the dimension of the input and output of each CSC-layers and their corresponding iterations used?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["823a2e85-8548-57a2-8256-5fdb0159c2af"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Each layer of CSC-layer of SDNet-18 and SDNet34 needs unrolling two iterations of FISTA and more iterations will only slightly improve the performance. As SDNet-18 and SDNet-34 have only one CSC-layer for the input images, I’m curious whether it is this low dimension (3 channels) of input that make two iterations sufficient. On SDNet-18-All and SDNet-34-All, could you list the dimension of the input and output of each CSC-layers and their corresponding iterations used?", "reference_answer": "A: In SDNet18/34-All, the dimensions of the input and output of each CSC-layers are precisely the same as the one corresponded in ResNet 18/34, which are 3 -> 64 -> 128 -> 256 -> 512. And 2 FISTA iterations are used in all CSC-layers. The authors have conducted the ablation study on ImageNet, and the authors find that SDNet-18 with 2, 4, and 8 iterations of FISTA obtains 69.47%, 69.51%, and 69.79% Top-1 accuracy, respectively. While using more iterations slightly increases the model performance, it comes at the cost of increasing the training time and memory requirement as a result of unrolling of the FISTA algorithm. Hence, in all our"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "85e2f028-eab8-543b-902f-59a90166df1d", "question": "Describe a comparison of the proposed method with [A] and [B].\nReferences:\n[A] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055–4064. PMLR, 2018.\n[B] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. arXiv preprint arXiv:2012.09841, 2020.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Describe a comparison of the proposed method with [A] and [B].\nReferences:\n[A] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pages 4055–4064. PMLR, 2018.\n[B] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. arXiv preprint arXiv:2012.09841, 2020.", "reference_answer": "A: [A] is a fundamental and pioneer work that explores the possibility of transformers in the image domain. \n(1) The authors summarize all generation methods into autoencoder (AE), autoregressive (AR), and the proposed local autoregressive (LAR) in sec.1 and Fig.1(B). Since Taming [B] can be seen as an AR model, the authors have compared it with their method in L32-38, and Fig.1(B). Moreover, the authors have discussed the difference between [B] and their method in Line 102-104 of sec.2.\n(2) Model proposed in [A] can be seen as an AR model with local receptive fields. Thus, differences between AR and the proposed LAR have been discussed in Line 32-38, and Fig.1(B).\n(3) Built upon the success of [A], the authors found that Taming [B] proposed in CVPR2021 may potentially have better performance in the task concerned in this paper. Thus Taming could serve as a stronger baseline (than [A]). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ee16a442-5956-5eb2-8332-ca8ba9e7fe15", "question": "Infinite layers?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2e38f52d-4a7a-51bc-82cd-127893495219"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Infinite layers?", "reference_answer": "A: Removal at 25000 does appear to be somewhat more private than the original model/smaller removal counts, but is still very nonprivate (e.g. ~95% precision @fpr=10^-3). In an updated version of the paper the authors will include a figure showing this."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3dfd78d7-68af-5eab-8af6-1bc3061bbac1", "question": "How does the proposed stimulative training strategy compared with providing supervision (the class label) directly to each layer?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed stimulative training strategy compared with providing supervision (the class label) directly to each layer?", "reference_answer": "A: The authors compare the proposed stimulative training with providing supervision (the class label) directly to each layer or each stage: (1) The comprehensive comparisons are shown in Table r2. As the authors can see, layer supervision can improve both the performance of the main network and the average performance of all subnetworks, stage supervision can only improve the performance of the main network, while the proposed stimulative training can achieve the best performance of main network and the best average performance of all subnetworks. (2) As shown in Fig. r8 (c) and (d) of the revised supplementary, the proposed stimulative training can better relieve the network loafing problem than layer supervision and stage supervision. (3) As shown in Fig. r6 and Fig. r7 of the revised supplementary, the proposed stimulative training can provide stronger robustness in resisting various network destruction operations than layer supervision and stage supervision. Moreover, the proposed stimulative training is actually complementary to layer supervision and stage supervision and can be seamlessly combined.\n\n**Table r2: Comparisons.**\n|Method|Time|Memory|Main(%)|All(%)|\n|:--------------|:----------- |:------ |:-----------|:----|\n| CT | 16.91h | 3291MiB| 77.39|55.26±13.37|\n| CT + layer supervision | 23.3h |7193MiB |78.77 |59.18±11.12|\n| CT + stage supervision | 19.3h | 5197MiB |78.59 |54.82±13.31|\n| ST | 24.08h | 3291MiB | 81.07 | 80.01±0.59 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f9a7862d-0ba1-5642-8b4f-bb396956c2f0", "question": "In Figure 3 what is “Generator setpoint”?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["01ec1818-9fe6-516b-ab44-46377c67a0cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 3 what is “Generator setpoint”?", "reference_answer": "A: \"Generator setpoint\" is a term used by power systems engineers, with whom the authors collaborate, and it indicates the physical quantities required to operate a generator. These are the active power associated with the generator and the voltage magnitude associated with the bus in which the generator resides. It can be considered as a \"value assignment\" for that generator."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3d4ae024-7359-5f09-90f2-ccc96b6caecd", "question": "It is unclear that the proposed algorithm (section 3.2) is optimized for the objective function in equation (9). And it is possible to theoretically guarantee that the algorithm finds a spectrally optimized graph?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7e4b61e8-a189-5ae1-b073-cf7dfe313a98"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is unclear that the proposed algorithm (section 3.2) is optimized for the objective function in equation (9). And it is possible to theoretically guarantee that the algorithm finds a spectrally optimized graph?", "reference_answer": "A: In the paper, the authors have included a description of the connection between the proposed algorithm and the optimization objective in (2). The original optimization objective function (9) includes three components: (a) log (det L) that corresponds to the sum of the Laplacian eigenvalues, (b) - \\alpha* X^T L X that corresponds to the smoothness of signals across the graph, and (c) - \\beta* |L|_0 that corresponds to graph sparsity. The proposed algorithm flow aims to iteratively identify and include the most spectrally-critical edges into the latest graph so that the first few Laplacian eigenvalues & eigenvectors can be most significantly perturbed with the minimum amount of edges. Since the inclusion of spectrally-critical edges will immediately improve distortion in the embedding space, the overall smoothness of graph signals will thus be significantly improved. In other words, the spectrally-critical edges will only impact the first few Laplacian eigenvalues and eigenvectors key to graph spectral properties, but not the largest few eigenvalues and eigenvectors-which will require adding much more edges to influence. It can be easily shown that including any additional edge into the graph will monotonically increase (a), but monotonically decrease (b) and (c). Specifically, when the spectra of the learned graph is not stable, adding spectrally-critical edges will dramatically increase  (a), while decreasing (b) and (c) at a much lower rate since the improved graph signal smoothness will only result in a slight change (increase) to  Tr(X^T L x). Consequently, the objective function in (2) will be effectively maximized by including only a small amount of spectrally-critical edges until the first few eigenvalues become sufficiently stable; when adding extra edges can no longer significantly perturb the first few eigenvalues, (b) and (c) will start to dominate the objective function value, indicating that the iterations should be terminated. The stopping condition can be controlled by properly setting an embedding distortion threshold for $\\eta $ or parameters $\\alpha$  and $\\beta$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "113005ef-d04b-5697-868f-d5b8685f2be0", "question": "What do the images generated with z_s from one input and z_n from another input look like (in the proposed method)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["03bf6e60-2b39-5aa9-8f57-184eb3e9b901"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What do the images generated with z_s from one input and z_n from another input look like (in the proposed method)?", "reference_answer": "A: Those images (the metameric samples) are already shown in the last row in the top block of figure 7, the authors have adapted the figure and added some more description to it, to make everything more clear.\nIn the baseline the metameric samples are adversarial examples, meaning one can turn any image into any class without changing the logits at all. With the proposed objective (shown on the right side), this is not possible anymore as keeping z_s fixed and exchanging z_n only affects the style of the image, not its class-specific content. The objective has achieved its goal and successfully defended against the metameric sampling attack."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "911ce0fc-03b8-5cc7-a9b5-e96ded1ebdc1", "question": "Would Higher Sampling Rate help and how about learning to mimic the attention instead?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e070ad3b-180d-5e8b-a978-d27d1aeaec36"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would Higher Sampling Rate help and how about learning to mimic the attention instead?", "reference_answer": "A: Yes, a higher sampling rate through overlapping blocks or smaller token sizes can provide more fine-grained attention maps, as well as more tokens. Anti-aliasing downsampling on higher-res attention maps generally brings performance gain. However, there always exists a trade-off between the \"sampling-rate\" and computation costs, especially from the quadratic complexity of self attention.\n\nA: Yes, the authors think Table 3's finding (only applying early layers matters) is generic for various vision transformers. We’ve experimented with DeiT, Swin, and T2T-ViT architectures, which all exhibit similar observations. By applying the ARM module to early layers, e.g. 1/4 , these models receive consistent improvements."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "382642d2-5bc6-591a-8520-099569179046", "question": "Compare this work to the state-of-the-art BNN methods such as real2bin [1] and reactnet [2].\nReferences:\n[1] Martinez, Brais, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. \"Training binary neural networks with real-to-binary convolutions.\" arXiv preprint arXiv:2003.11535 (2020).\n[2] Liu, Zechun, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. \"Reactnet: Towards precise binary neural network with generalized activation functions.\" In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pp. 143-159. Springer International Publishing, 2020.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f48ec53c-2311-5560-884e-f1448e2a4e1c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare this work to the state-of-the-art BNN methods such as real2bin [1] and reactnet [2].\nReferences:\n[1] Martinez, Brais, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. \"Training binary neural networks with real-to-binary convolutions.\" arXiv preprint arXiv:2003.11535 (2020).\n[2] Liu, Zechun, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. \"Reactnet: Towards precise binary neural network with generalized activation functions.\" In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16, pp. 143-159. Springer International Publishing, 2020.", "reference_answer": "A: The authors use the same experimental setting as ReactNet [2] do, and only change the way of computing gradient of sign function with the proposed method. The experimental results of ResNet-18 on ImageNet dataset are shown below:\n\n|model| Top1 acc(%)| Top5 acc(%)|\n|-|-|-|\n|ReactNet| 65.5| 86.1|\n|Real2Bin| 65.4| 86.2|\n|ours| 66.0| 86.4|\n\nThe results show that the authors improve the ReactNet top-1 performance by 0.5% and top-5 performance by 0.3%, and improve the Real2Bin top-1 performance by 0.6% and top-5 performance by 0.2%. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d103e1cd-07b6-5106-a069-802d0347e40f", "question": "The method is quite hefty in computing as it requires identifying k-neighbors of support samples, where k in practice can be in the range 10,000. Fig.7 suggests the algorithm underperforms on mini-imagenet if k <100. In such a case, what is the advantage of using this method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["48f84a36-4fdb-55ab-85d5-545e4cb455b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The method is quite hefty in computing as it requires identifying k-neighbors of support samples, where k in practice can be in the range 10,000. Fig.7 suggests the algorithm underperforms on mini-imagenet if k <100. In such a case, what is the advantage of using this method?", "reference_answer": "A: It is not true. The complexity of calculating the similarity matrix is $\\mathcal O(N_{base}\\times d)$, and selecting the top-k feature is $\\mathcal O(N_{base}\\times log(k))$. Here $N_{base}$ denotes the number of features in base data and $d$ denotes the dimension of extracted features.  For empirical verification, the authors report latency (ms) of one meta-testing task for 5-way 1-shot classification on mini-Imagenet as below. For fair comparison, the latency is averaged over $2000$ tasks. As the authors see from this table, the increase of latency is really small ($11$ms) compared to the original time of training the classifier ($298$ms).  To further reduce the computational cost, the authors can sample a subspace from the whole base data. By randomly sampling p(\\%) features in each base class in the subspace, the authors show the computational latency and the classification performance below. The improvements are still significant when using only 1\\% features in base classes on Meta-dataset with 1\\% storage space (only 0.98M).\n\n| Selection Ratio| Latency| mini-test| CUB| Fungi| Omini| Sign| QDraw| Flower| DTD|\n|-|-|-|-|-|-|-|-|-|-|\n| baseline| 298| $64.63$| $47.75$| $42.36$| $77.28 $| $53.50$ | $51.60$| $70.33$| $50.47$         \n|TCPR| + 11| $68.06$ | $51.87$ | $44.38$ | $78.51 $ | $54.83$ | $54.62$ | $72.55$ | $52.50$ |\n| TCPR(p=1\\%)    | +4| $67.79$| $51.55$| $44.29$| $78.18$| $54.03$| $53.62$| $71.82$| $52.69$|\n\nThe authors also want to emphasize that the proposed TCPR still achieves better performance on most test sets except mini-test and coco if k<100, as shown in Figure 7.  In summary, the computational cost of identifying k-neighbors of support samples is small and can be further reduced with a large margin of improvement over baseline methods.   "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "73519ee9-6100-5c85-a99e-bd0486942efd", "question": "Cite the related works which are compared with the proposed work in Table 1.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Cite the related works which are compared with the proposed work in Table 1.", "reference_answer": "A: [r1] Menghan Xia and Tien-Tsin Wong. Deep inverse halftoning via progressively residual learning. In Asian Conference on Computer Vision, pages 523–539, 2018.\n[r2] Prafulla Dhariwal, and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\n[r3] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162–8171, 2021.\n[r4] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020a.\n[r5] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the European Conference on Computer Vision Workshops, pages 63–79, 2018.\n[r6] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for large-factor image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 14245–14254, 2021.\n[r7] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In International Conference on Computer Vision Workshops, pages 1905–1914, 2021."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "989f71a2-fa95-5ec7-9cb4-05656810d249", "question": "Line 16 of the appendix: Can clarify the relationship between Gaussian curvature and the first fundamental form in the proof in the appendix?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["1821bd6b-db14-5a63-b02c-4257dd291b97"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Line 16 of the appendix: Can clarify the relationship between Gaussian curvature and the first fundamental form in the proof in the appendix?", "reference_answer": "A: The first fundamental form is the expression of how the surface $S$ inherits the natural inner product of $\\mathbb{R}^{3}$. Geometrically, the first fundamental form allows us to make measurements on the surface (e.g., lengths of curves, angles of tangent vectors, areas of regions) without referring back to the ambient space $\\mathbb{R}^{3}$ where the surface lies. The second fundamental form describes the shape of the surface in the ambient space $\\mathbb{R}^{3}$. The Gaussian curvature can be defined by the coefficients of the first fundamental form and the coefficients of the second fundamental form. The Gauss formula and the Mainardi-Codazzi equations reveal the relations between the first and second fundamental forms of a surface. Gauss formula expresses the Gaussian curvature as a function of the coefficients of the first fundamental form and its derivatives, i.e., Eq. (A.1), which is also known as Gauss’ Theorema Egregium [2]. In the revision, the authors clarify this in Remark 2 in Appendix A."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "621518f3-f17c-5176-8273-41b073cd3f4d", "question": "How is the cost volume computed?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["052b92f6-35e8-59ed-bb9e-0ab6269d29da"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is the cost volume computed?", "reference_answer": "A: As mentioned in Ln-249, the authors construct cost volume by computing feature correlation by following PatchmatchNet [26] and TransMVSNet [3] as this formula is more consistent with feature matching. The authors will add more details of this part in the proposed final version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a75d93e4-ab65-5875-ac22-204a74e79192", "question": "it is not clear on the details of the proposed model and difficult to evaluate the performance improvement. Can you explain?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["1ba9b611-9a76-5d01-8476-dade426b3784"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "it is not clear on the details of the proposed model and difficult to evaluate the performance improvement. Can you explain?", "reference_answer": "A: The proposed model is based on VRCNet [16] since it is the current SOTA method on the MVP dataset. The effect of the proposed balanced design and how it improves the performance from the baseline model is studied in the ablation study (Sec. 5.3, Table 3, and Figure 6)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8c023b29-1832-5c6d-8fc7-fb1e8bc62949", "question": "I am confused what is the fixed reference in Figure 6. It is not explained in the main paper. Is it a baseline with the best hyperprameters in hindsight?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2a675de9-1ae1-5c11-9721-336a61a6191b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am confused what is the fixed reference in Figure 6. It is not explained in the main paper. Is it a baseline with the best hyperprameters in hindsight?", "reference_answer": "A: The “fixed reference” is described in Appendix C, and corresponds to the most commonly used settings in the literature. The authors made this clear in the main body of the text."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "308a2c13-50fa-5f96-92ef-2825f038d33b", "question": "I suspect the GAIfO failure mode was due to mode collapse, which often happens with GAIL (and GANs more generally). Should we expect the proposed algorithm to systematically avoid these problems, or will similar problems occur given enough task complexity?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a1d44a36-ee0d-593c-b65d-3937420a8bab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I suspect the GAIfO failure mode was due to mode collapse, which often happens with GAIL (and GANs more generally). Should we expect the proposed algorithm to systematically avoid these problems, or will similar problems occur given enough task complexity?", "reference_answer": "A: The authors present experimental results for the grid world in order to clarify this problem. The figures of the results are also shown in the above link.\n- the authors first let $k=1$. From the results, the authors can see that BCO and DPO share similar asymptotical performance (KLD), but DPO achieves a significantly faster convergence rate. On the contrary, GAIfO still fails to find the second path, indicating the mode collapse problem.\n- the authors complement additional figures (figure 1 & 2 in the external link)  to demonstrate the properties of both decoupled policy modules. The authors can see that the state transition predictor exactly aligns with the expert state transition (figure 2). More importantly, the action distribution plot indicates that the learned inverse dynamics is different from the expert's, and (almost) equally distributed on ambiguous actions (figure 1). This supports the proposed claim that any inverse dynamics valid on the expert transition support can be used to construct the expert hyper policy, and the proposed algorithm does not exhibit any preference on a particular inverse dynamics. \n- For complex tasks, it is hard to analyze whether DPO also suffers from the mode collapse, but in toy experiments, the authors can see that additional supervision is possible to ease such problems, and both the final performance and the training efficiency benefit from it."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "27fb5f9c-61ca-55b4-aaa5-1d2f805cf941", "question": "What is the capacity of the attacker? Authors mention that they have an l-infinity bound on the perturbations they are allowed to make, but are they allowed to perturb each client's update? If so, isn't it a very strict assumption?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["3c651ae8-35d2-563f-951d-2aef286897da"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the capacity of the attacker? Authors mention that they have an l-infinity bound on the perturbations they are allowed to make, but are they allowed to perturb each client's update? If so, isn't it a very strict assumption?", "reference_answer": "A: First, the authors want to clarify that the ratios of poisoned data have different meanings in online/real-time and offline settings. Namely, in real-time settings, the authors only poison data during the accumulative phase. If the authors ask the ratio of poisoned data points that are fed into the model, the formula should be $$\\frac{\\textrm{Per-batch poisoning ratio}\\times\\textrm{Accumulative epochs}}{\\textrm{Burin-in epochs}+\\textrm{Accumulative epochs}}\\textrm{.}$$\nSo even if the authors use $100\\\\%$ per-batch poisoning ratio during the accumulative phase, the ratio of poisoned data points fed into the model is only $100\\\\% \\times 2 / (40 + 2)\\approx 4.76\\\\%$ in the proposed settings. In contrast, if the authors poison $10\\\\%$ data in an offline dataset, then the expected ratio of poisoned data points fed into the model is also $10\\\\%$.\nNevertheless, keeping a high poisoning ratio during the accumulative phase could still be challenging in practice. To this end, the authors constrain the poisoning operations to be imperceptible (e.g., less than $8/255$ under $\\ell_\\infty$-norm), while some previous works allow arbitrary modification on the poisoned data. Besides, the proposed ablation studies in Table 2 also show that the proposed method is still effective even if the authors use a $10\\\\%$ per-batch poisoning ratio.\nAs to the case of federated learning, the authors propose a simple trick of recovered offset in Eq. (14), such that the authors can only manipulate one client to achieve any poisoned aggregated gradient. Namely, if the authors want to feed the model with a poisoned aggregated gradient ${\\color{blue}\\mathcal{A}}(G)$, and the aggregated clean gradients of other clients is $G'$, then the authors can manipulate a single client to contribute a gradient of ${\\color{blue}\\mathcal{A}}(G)-G'$, such that the total gradient is ${\\color{blue}\\mathcal{A}}(G)-G'+G'={\\color{blue}\\mathcal{A}}(G)$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "53abffdc-06d3-57e9-85f8-96fa85c7ec10", "question": "Why is the significance of DGRL emphasized in this result comparison?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3c8c8518-08a4-546e-98e5-a844ae34bf9c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the significance of DGRL emphasized in this result comparison?", "reference_answer": "A: The results the authors show in the paper were being transparent about the effects of the different groups of factors; which is why in the proposed plots, the authors included comparisons with all groups $G$. Previously, the authors reported results without fine-tuning the group factor G for a fair comparison. In this figure, the authors show the best performing factor $G$ for each environment, that the authors can specifically fine-tune for the task; and as shown, the authors see that by simply adding DGRL on top of the existing HRAC DGRL can become more apparent."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aa05868f-d9e7-5a25-80ad-c75147be4e65", "question": "What are shape, rotation, translation and category mean in Figure 10?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are shape, rotation, translation and category mean in Figure 10?", "reference_answer": "A: Sorry for the confusion. The authors have fixed this in the new version of the proposed draft.\n\nA: *Shape* stands for the Shape Tanimoto [2], which measures the shape similarity between the input shape and generated molecules. *Rotation* stands for the accuracy of the model in predicting the correct rotation bin. *Translation* stands for the accuracy of the model in predicting the correct translation bin. *Category* stands for the accuracy of the model in selecting the correct fragment. All of them can be treated as metrics reflecting how well the model fits the data, which shows that the proposed model builds up a strong mapping from shapes to molecules."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b337eb8c-d96a-5e75-913c-79987b7b3cd9", "question": "Is the fact that the temporal derivatives are calculated analytically with autograd using equation (15) automatically solving the issue?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["c6d7e4a3-b095-59e2-a507-911ed052756b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the fact that the temporal derivatives are calculated analytically with autograd using equation (15) automatically solving the issue?", "reference_answer": "A: This should be easily doable in the context of DT-PINNs also."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8d75234f-3480-5402-8e15-3e0f1f813882", "question": "In L217, how many augmented classes (i.e. m) are added? Any data sampling technique is used to balance original new classes and augmented new classes? How many images are generated for each augmented class?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4dab9e04-b50a-53b7-a04b-d87fc3c041b3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In L217, how many augmented classes (i.e. m) are added? Any data sampling technique is used to balance original new classes and augmented new classes? How many images are generated for each augmented class?", "reference_answer": "A: The number of augmented classes (i.e. m) depends on the number of (original) classes at current incremental step. Taking CIFAR-100 as an example, the m is 45 for 5 phases setting where each incremental step has 10 classes; and m is 10 for 10 phases setting where each incremental step has 5 classes. The authors generally use random sampling to keep the same quantity of samples for each class including original and novel classes, for the purpose of balanced training. Considering the authors will release source codes, those details were not included in the submission. Following your suggestion, the authors will add them to the manuscript."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a77c0945-dd89-5005-b4e4-93e33c0e8ae9", "question": "Why have the discounted and/or long run behavior not been investigated?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d3246049-be97-50c3-9459-d4bcc0a7a4a3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why have the discounted and/or long run behavior not been investigated?", "reference_answer": "A: Good question. The proposed primal-dual algorithm can be generalized to handle the infinite horizon setting by changing the subroutine for solving the unconstrained Markov game. To explain, first, the strong duality result can be straightforwardly generalized to the infinite-horizon setting, as the structure of the linear program is highly similar to that in the finite horizon setting. Then, in each primal update, the authors will need to solve an infinite-horizon unconstrained Markov game similar to the one shown in eq.(8). To do this, the authors can extend the existing V-learning algorithm for finite-horizon Markov games to the infinite horizon setting. Specifically, the existing V-learning algorithm is based on the finite-horizon Bellman equation, and the authors can adapt it to the discounted infinite-horizon Bellman equation, leading to the update rule $V^{(m)}(s_t)\\leftarrow (1-\\alpha_t)V^{(m)}(s_t)+\\alpha_t\\big(r_t^{(m)}+\\gamma V^{(m)}(s_{t+1})+\\beta_t\\big)$ (for agent $m$ at time $t$), where $\\beta_t>0$ is the bonus to promote optimism and exploration. \nThe convergence analysis of this primal-dual algorithm in the infinite-horizon setting follows the same logic as that of the proposed current analysis. The only difference is that the authors will need to develop finite-time convergence analysis for the modified V-learning subroutine, which the authors think can be inspired from the existing analysis in the finite-horizon setting."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e6362638-5799-5f21-898f-418517e59dbe", "question": "Comparison to ordinary PEARL using the dense rewards.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["31c65a87-0806-55e4-a9a9-5c121d67b6c3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comparison to ordinary PEARL using the dense rewards.", "reference_answer": "A: The authors updated Figure 1 in the paper content to include a bar that indicates the performance of dense-reward PEARL. In the next revision, the authors will add more descriptions and discussions regarding the comparison to dense-reward PEARL."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ad3c5996-1305-5aa9-aa5c-d2a8baeb33ca", "question": "If one has extracted, say, ten images per class and hopes to add five images per class for better performance, does the algorithm need to start from the very beginning (to extract 15 images per class), or can it start from the current point? In other words, what is the incremental performance of the proposed approach?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "If one has extracted, say, ten images per class and hopes to add five images per class for better performance, does the algorithm need to start from the very beginning (to extract 15 images per class), or can it start from the current point? In other words, what is the incremental performance of the proposed approach?", "reference_answer": "A: It is an interesting question to see the incremental performance as it is essential for practitioners. However, the authors do not think the proposed method has the desired adaptivity the authors hope. There are two ad hoc solutions. One is to ignore the previous distilled data and distill five images per class using a different random seed. However, since the target dataset is kept the same, it is expected to see that there will be a huge information overlap among the two sets of distilled images. Therefore, the improvement provided by the second distillation will be very marginal. The other solution is to consider the previously distilled images when the authors distill another set of images by including them in the meta-gradient computation. This way, the authors can minimize the information overlap between the two distillation phases. However, the authors do not expect to see any training efficiency improvement compared to distilling 15 images per class from scratch because the size of Gram matrices are the same, and it is likely to take a similar amount of steps for all synthetic images to converge. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "870a48e0-6f28-5813-8665-8ce7d96882ab", "question": "How is the Imagenet dataset split?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["d8e1ccac-63e9-5f49-8da3-8ae15b8035b6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is the Imagenet dataset split?", "reference_answer": "A: 700 for train, 300 for test."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ec8b0c77-6243-522d-ad94-cca2cfd77b72", "question": "Shouldn't rearrangement be used instead of arrangement to be inline with what the community is calling this task? Prior work by Batra et. al. 2020. does that.\nDhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d66a5cd9-35b7-5db9-816b-47aaae8ae114"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Shouldn't rearrangement be used instead of arrangement to be inline with what the community is calling this task? Prior work by Batra et. al. 2020. does that.\nDhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.", "reference_answer": "A: The problem setting the authors studied is different from *Batra et. al. 2020* in motivation, formulation, and challenges. As described above, the room rearrangement focus on the planning and exploration problems in embodied AI, where the goal is given and deterministic. Instead, the core challenge of object arrangement is **learning to control with examples and without reward**. So the authors do not agree that rearrangement should be used in this work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "77666e7c-6757-536b-9bad-1454002bbe0b", "question": "Why and how does language information help with this task", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9cd6253e-8a5a-55f2-8bdc-77afa61d48e4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why and how does language information help with this task", "reference_answer": "A: Why does language information help with this task? Existing methods are easy to overfit and usually attain unsatisfactory performance as the learned rank concepts are mainly derived from the vision training set. Since learning the rank concept from the image domain alone is prone to overfitting, the authors can leverage multimodal information to alleviate this issue. The human language contains rich semantic information and prior knowledge. The authors consider simultaneously borrowing the rank concept from the language domain. Specifically, each rank label is not only regarded as a class category but also linked to a sentence describing the corresponding rank, such as \"this person is 23 years old\". In this way, the proposed model not only learns the concept of ranks defined on the vision dataset but also exploits the common knowledge of rank in the language domain.\n\nHow does language information help with this task? In practice, the authors employ the pre-trained giant text encoder in CLIP to extract language prototypes for all ranks. Since the prototypes are obtained from a fixed language model, the authors are somehow distilling the language knowledge from the CLIP model. Moreover, the prototypes are constrained in the well-learned language latent space, which is also a kind of regularization leading to stronger generalization.\n\nAny experiments? To see the benefits of language priors, the authors first consider the zero-shot setting. The authors conducted two experiments: 1) without Language Priors (w/o LP), the classifier is a random initialized FC layer, 2) with Language Priors (w/ LP), the classifier is language initialized prototypes with the CLIP text encoder. Neither experiment involves model training. The results in Table R1-6 show that the w/ LP solution significantly outperforms the w/o LP across four datasets, which indicates that the CLIP model does contain a meaningful representation of rank numbers to some extent, and language information can help with this task. \n\nThe authors agree that CLIP may not be able to give a perfect representation of some arbitrary number simply using raw text input. Therefore the authors propose to learn rank prompts. Here the authors consider the full-training setting, where the full model is trained. w/ P refers to the proposed OrdinalCLIP and w/o LP means that the language prototypes are replaced with an FC layer. The results show the effectiveness of language priors again."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9d7fabff-5a4a-5e0a-8579-ef3e0d6088c6", "question": "Provide the claims for the following from the paper: Superiority of a dot product similarity metric ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["36305be5-629e-5348-9c0a-d8033c5e875f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide the claims for the following from the paper: Superiority of a dot product similarity metric ", "reference_answer": "A: The authors found that the dot product reached its optimal point 10+ epochs earlier than cosine similarity and euclidean distance, and reached a much higher harmonic mean. The ablations are provided in table 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ffdae88e-365b-5df8-8246-b03f929c44d3", "question": "What is the role of the language-guided attention block?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26e4a8b4-db7b-5cc4-b019-33b932836d74"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the role of the language-guided attention block?", "reference_answer": "A: To validate the effectiveness of the language-guided attention, the authors replaced this with a simple MLP that concatenates the Image and Text embeddings. Results on TV Sum and SumMe in a supervised setting are below. Without the attention block there’s a ~4% drop in performance\n\n| Method |  | SumMe |  |  | TVSum | | \n| --- | :---: | :---: | :---: | :---: | :---: | :---:\n|  | Standard | Augment | Transfer | Standard | Augment | Transfer\n| CLIP Image+Video Caption(MLP)+Transformer | 50.6 | 51.08 | 48.1 | 63.0 | 65.8 | 61.4\n| CLIP-It: CLIP-Image+Video Caption(Attn)+Transformer  | 54.2 | 56.4 | 51.9 | 66.3 | 69.0 | 65.5"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0757d452-7bb4-5b2d-b475-1ae3da086c61", "question": "As stated by the authors themselves there are several previous works like GREAT, Zugner et. al 2021, etc. that have already done this. Then what are the main contributions of this paper that are novel?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ba17e297-e254-519a-b329-6abf2c38dbe2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As stated by the authors themselves there are several previous works like GREAT, Zugner et. al 2021, etc. that have already done this. Then what are the main contributions of this paper that are novel?", "reference_answer": "A: The sequential information here means the encoding for a vector sequence. For example, The $Path(x_i\\rightarrow x_j)$ in Eq.5 is a vector list consisted with [$n_0$,$n_1$,$n_2$,...,$n_m$], where $m$ is the path length and each $n$ is the embedding vector for each node which is looked up from the embedding matrix table of node type. The authors use a bi-directional GRU to encode the input sequence and get the final state of the path. After that, the authors integrate this sequential information into Transformer as an inductive bias. \nUnlike the proposed model, the Great biases the attention vias manually designed structural edges and Code Transformer counts multiple distances to leverage code structure. The baselines convert one edge or distance into a embedding vector as a bias for the attention module. However, the path's different node combinations contain plenty of structure information, which is overlooked by encoding distances. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2c0a8f14-bc36-5572-8edb-ced1e2c2c606", "question": "In Sec. 7.2, the argument is made that since DPA is asymptotically optimal, then improving robustness \"reduces developing stronger defenses to finding more data-efficient learners.\" I understand the origin of this claim, but it seems overbroad. Could it not also be argued that a better/alternative approach is better ways to determine $\\ell_0$?  Does robustness of the individual models beyond the assumption that a single insertion/deletion can arbitrarily change the prediction?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["86b0a7a9-c402-5373-bc10-3ce3edc5e031"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Sec. 7.2, the argument is made that since DPA is asymptotically optimal, then improving robustness \"reduces developing stronger defenses to finding more data-efficient learners.\" I understand the origin of this claim, but it seems overbroad. Could it not also be argued that a better/alternative approach is better ways to determine $\\ell_0$?  Does robustness of the individual models beyond the assumption that a single insertion/deletion can arbitrarily change the prediction?", "reference_answer": "A: The rationale behind that argument is that the authors want to simplify the defense problem through reduction. In particular, to defend against data poisoning, the authors are trying to design algorithms/models with $\\ell_0$ robustness (with respect to the training set) overall. Here, Lethal Dose Conjecture implies that DPA is a nearly optimal reduction from designing $\\ell_0$ robust models to designing data-efficient models, **simplifying** a problem with robustness requirements to a problem with none. \nThis is desirable as now the authors can focus on a simpler task. Meanwhile, in formulations, making base models $\\ell_0$ robust is not easier than making the whole model $\\ell_0$ robust. \nHere is another way of looking at this: When the base models are already robust against data poisoning, it implies that one can also increase robustness by using more base models with less training data each. In fact, in some sense, an example of this is presented in Section 7.3 of the proposed work, where the authors show that a base learner for DPA can be derived from nearest neighbor, an approach with intrinsic robustness. DPA using the derived base learner offers similar robustness as the nearest neighbor method.\n"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a13ff424-f795-5b23-8cf6-8e7d8aa70cb7", "question": "Discuss the results of ablation experiments using a 0-tensor or a visual pooling feature for inference.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4fd78cd3-88da-5b98-b746-1d7a884fe23f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss the results of ablation experiments using a 0-tensor or a visual pooling feature for inference.", "reference_answer": "A: In order to validate the effectiveness of modal fusion, it is crucial to conduct the ablation experiments of using a 0-tensor or a visual pooling feature for inference (as discussed in Tab. 5 (c) of the manuscript). The authors conduct such an ablation as shown in Tab. #3. From Tab. #3, the authors can see that, when removing language from the tracking inference, the performance of VLT_SCAR heavily drops from 65.2%/48.3% to 50.8%/39.5% in SUC on LaSOT/TNL2K under 0-tensor setting, and 63.9%/49.8% to 53.4%/41.1 under template (i.e., visual pooling feature) setting. Likewise, without language for tracking, the performance of VLT_TT drops from 66.3%/52.2% to 60.7%/48.2% in SUC on LaSOT/TNL2K under 0-tensor setting, and 67.3%/53.1% to 61.0%/49.1% under template setting. All this reveals the importance of linguistic cues for tracking and shows that the learned representations are indeed multi-modal representations. \n\n**Table #3**: Ablation experiments of using a 0-tensor or a visual pooling feature (i.e., template in the table) for tracking.\n| # | Method | Setting | Language | LaSOT | LaSOT | TNL2K | TNL2K |\n| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n|  |  |  |  | SUC (%) | P (%) | SUC (%) | P (%) |\n| 1 | VLT_SCAR | 0-tensor | w/o. language (i.e., inference with 0-tensor only) | 50.8 | 52.6 | 39.5 | 41.2 |\n| 2 | VLT_SCAR | 0-tensor | w/. language | 65.2 | 69.1 | 48.3 | 46.6 |\n| 3 | VLT_SCAR | template | w/o. language (i.e., inference with template only) | 53.4 | 54.6 | 41.1 | 42.9 |\n| 4 | VLT_SCAR | template | w/. language | 63.9 | 67.9 | 49.8 | 51.1 |\n| |\n| 5 | VLT_TT | 0-tensor | w/o. language (i.e., inference with 0-tensor only) | 60.7 | 63.1 | 48.2 | 46.8 |\n| 6 | VLT_TT | 0-tensor | w/. language | 66.3 | 70.5 | 52.2 | 52.1 |\n| 7 | VLT_TT | template | w/o. language (i.e., inference with template only) | 61.0 | 63.4 | 49.1 | 48.3 |\n| 8 | VLT_TT | template | w/. language | 67.3 | 72.1 | 53.1 | 53.3 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "045e5874-00da-55de-95f8-c8c135dbe32a", "question": "How does FRePo perform using InstanceNorm?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does FRePo perform using InstanceNorm?", "reference_answer": "A: See Appendix C.6 Table 16 and Table 17. The authors observe that instance normalization performs slightly worse than the default batch normalization. However, Table 15 suggests that the drawback of the instance norm is the transferability. The distilled data trained using instance normalization transfer less well to other architecture, especially those without normalization. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "09f9b88b-58f9-5c75-8419-113401473e40", "question": "Why did we drop the denominator in the equation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["25c2eb12-a2ce-59c8-b48b-438ef66d9bca"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why did we drop the denominator in the equation?", "reference_answer": "A: To make a point about the ratio between on and off-policy context posteriors."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "939408eb-22ab-5ade-949d-09e7fdd743a0", "question": "On the computation/inference time of other methods.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["55b8b220-92ee-568e-ae6f-add043bfce90"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "On the computation/inference time of other methods.", "reference_answer": "A: The authors acknowledge that inference time for supervised-based learning methods is faster than runtime optimization.\nThe learning-based methods that optimize during training have very fast inference time (in the orders of milliseconds). While the non-learning-based methods that optimize during runtime have slow inference time (in the orders of seconds). \nNon-rigid ICP has a similar inference time to the Graph prior method. Please refer to Table 2 for more details.\nAs the authors discussed before, the proposed method is not yet competitive to learning-based methods *w.r.t* inference time as it relies on iterative optimization during runtime."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c60ed24f-36fb-5c2b-b552-3bfcd867906f", "question": "How to select the hyper-parameter?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["05cbcce4-79c0-5b2c-bcf3-2e66b7352e9d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to select the hyper-parameter?", "reference_answer": "A: For the results in Table 1, the authors took the results for the $\\lambda$ value with highest union accuracy in Table 3 in the Appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2da93495-59ef-5825-8971-ee6910629cc0", "question": "Are there other relevant works?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb1ce27d-d4ca-5669-8dba-4302171c1a0e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there other relevant works?", "reference_answer": "A: Yes, there are a body of recent works on invariant feature learning.\n1) Kim, B., Kim, H., Kim, K., Kim, S., & Kim, J. \"Learning Not to Learn: Training Deep Neural Networks with Biased Data.\" CVPR 2019 ---- This paper uses adversarial training similar to Xie et al. NeurIPS 2017.\n2) Roy, P., & Boddeti, V. Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach. CVPR 2019 ----- This paper shows that gradient reversal based adversarial training is ill-posed with oscillatory behavior. The paper proposes a non-zero sum game instead that stabilizes the optimization in theory and to a large extent in practice. \n3) Madras, D., Creager, E., Pitassi, T., & Zemel, R. \"Learning adversarially fair and transferable representations,\" ICML 2019 ----- This paper considers loss functions for the adversary that are not cross-entropy or MSE. Instead it considers loss functions corresponding to group fairness notions including demographic parity, equalized odds and equal opportunity.\nThe next group of papers look at this problem from a mutual information perspective. \n1) Song, J., Kalluri, P., Grover, A., Zhao, S., & Ermon, S. \"Learning Controllable Fair Representations,\" AISTATS 2019\n2) Bertran, M., Martinez, N., Papadaki, A., Qiu, Q., Rodrigues, M., Reeves, G., & Sapiro, G. \"Adversarially Learned Representations for Information Obfuscation and Inference,\" ICML 2019\n3) Moyer, D., Gao, S., Brekelmans, R., Galstyan, A., & Ver Steeg, G. \"Invariant Representations without Adversarial Training,\" NeurIPS 2018 ---- This paper proposes a mutual information based solution without an explicit adversary.\nLastly, the Sadeghi et al, ICCV 2019 paper minimizes the minimum MSE between b and \\hat{b} with the adversary being a linear regressor. This is exactly the same as maximizing the Pearson Correlation between b and \\hat{b}. See Lemma 1 and Lemma 4 of the arxiv version.\n"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6c9ec2e4-fdc4-5399-978a-284873d621ac", "question": "The relationship between $p(y|x)$, $p(a|y)$, and $p(a|x)$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["bb4b14e7-457a-5b7b-bfb8-fff2970120c9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The relationship between $p(y|x)$, $p(a|y)$, and $p(a|x)$.", "reference_answer": "A: By definition, for a fixed $\\delta$ (omitted in the following equations),\nwe have\n$$\np(a|x) = \\sum_y p(a|y, x) p(y|x)\n$$\nand practically the authors approximate $p(a|y, x)$ with a $q(a|y)$ which\ndoes not depend on $x$. The authors will further clarify this point in the paper.\nHere, the authors are maximizing the likelihood of $p(a|x)$ to\nlearn $p(y|x)$ *using $q(a|y)$ as a bridge*,\nso $q(a|y)$ works like a proxy between $p(a|x)$ and $p(y|x)$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c5412ae7-96dc-5aba-b798-defbace78956", "question": "Why is the return used for training option-policies not bootstrapped across their corresponding terminations?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7fba56c2-b606-5b06-b190-1155af65c38b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the return used for training option-policies not bootstrapped across their corresponding terminations?", "reference_answer": "A: The authors want to discover option-policies where each of them individually achieve a subgoal. Thus, the return (G^o_t; see Equation 3) that is used to train the option-policies (which are computed using the discovered option-rewards and terminations; not the extrinsic rewards) did not bootstrap across their corresponding terminations. The authors believe that it is unlikely to discover option-policies where each of them achieve a subgoal if the return used for training option-policies bootstrapped across their corresponding terminations."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4afa210f-dee9-5f76-b231-c06660f2267d", "question": "Is the description for G/D architecture and G/D objectives entangled?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the description for G/D architecture and G/D objectives entangled?", "reference_answer": "A: No. The proposed method is described with the following order:\n- Sec. 3.1 introduces the preliminaries on 3D-aware generator and inverse rendering.\n- Sec. 3.2 introduces the geometry branch of the proposed discriminator (which is the proposed main contribution), as well as how it is learned on real images and used to supervise fake images.\n- Sec. 3.3 introduces the implementation details of how geometry is extracted by the discriminator and the generator."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bd4ae6ed-d331-5d28-b1da-04781cb2569a", "question": "In figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case? (why should they be different?)", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c5203227-69aa-500f-bc4f-d5e9a525536e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case? (why should they be different?)", "reference_answer": "A: The authors use the same setting as Keskar et. al, 2017, which compare the small/large-batch solutions using the linear interpolation method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b1b19fbf-1a39-5549-b1f5-79be335607ec", "question": "Why does a model making confident mistakes have the same invariance as a model making unconfident mistakes (Figure 1 (c) and (d))?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["65d774b3-6bb6-5475-926a-0dfa3a2e9100"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does a model making confident mistakes have the same invariance as a model making unconfident mistakes (Figure 1 (c) and (d))?", "reference_answer": "A: The authors intuitively define the EI score as 0 if the network gives different class predictions on the original and transformed image (the \"otherwise\" case in Eq.1). Inspired by the question, the authors further consider the consistency of the softmax outputs when defining the \"otherwise\" case in EI. Specifically, the authors use the *negative JS* in the “otherwise” case. Under this modification, the EI scores are -0.665 and -0.029 in case (c) and case (d), respectively. Using this modified EI, the authors report the correlation studies on a series of benchmarks below (using the ImageNet models). \n\n|Test Set|ImageNet-Val|ImageNet-R|ImageNet-S|ImageNet-A|ObjectNet|\n|:-|:-:|:-:|:-:|:-:|:-:|\n| EI | 0.927 | **0.846** | **0.897** | **0.778**|**0.975**|\n|Modified EI|**0.972**|0.764|0.422|0.575|0.937|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ab81e39b-086d-5716-9bb4-bcd1c9157d0d", "question": "How efficient is the sampling?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9878a6fb-a940-5349-b04f-23e77e824e2f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How efficient is the sampling?", "reference_answer": "A: The authors report the average runtime of generating one sample on the Counterfactual Story Rewriting data. The table below shows the results (on an NVIDIA Quadro GV100 GPU, batch size=32). The authors compare with Mix-and-Match [[Mireshghallah, et al., ACL 2022]](https://arxiv.org/abs/2203.13299), a latest energy-based decoding method with discrete MCMC sampling (Metropolis-Hastings, in particular). The authors can see that the proposed COLD with the gradient-based sampling, is faster than the gradient-free Mix-and-Match. COLD is 30% faster with the base LMs of similar sizes (GPT2-M and BERTLarge), and has roughly the same time cost when using a much larger LM, GPT2-XL (while achieving much better performance as shown in the proposed response to Reviewer zcYZ)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "403a71ad-f75e-5eed-9d2c-a3bcdd456915", "question": "Fig 4 and Fig 5 analyze the effect of search space shrinking from the aspect of supernet. I notice that the test loss in Fig 5 starts from 2.5 epochs. Why not show the results from the 0 epoch? Besides, I do not think there is a direct correspondence between the training, testing, and removing useless operations. The supernet may get better training because of the smaller search space rather than removing redundant ops.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b0d70329-7bd8-5c32-81c7-5743ec00d545"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Fig 4 and Fig 5 analyze the effect of search space shrinking from the aspect of supernet. I notice that the test loss in Fig 5 starts from 2.5 epochs. Why not show the results from the 0 epoch? Besides, I do not think there is a direct correspondence between the training, testing, and removing useless operations. The supernet may get better training because of the smaller search space rather than removing redundant ops.", "reference_answer": "A: This is because the loss changes dramatically in the initial stage of training. Within the initial few epochs, loss often drops several times rapidly, e.g., 8.7->1.5. However, this figure is to highlight the differences of the model after it gradually tends to be stabilized. So the authors omit the performance of the first two epochs to highlight the overall performance of the follow-up."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ed094aa7-7e92-5692-93f7-278ba9405536", "question": "The formulation of the formal statement of the conjecture in page 3 is not justified enough, in my opinion. It seems that the conjecture is formulated with respect to a specific given data point $x_0$. I guess that this what a \"specific task\" (as written in the introduction) means? However, a \"specific task\" might be understood as drawing the test point from a specific hidden marginal distribution over instances, as usually done in PAC learning. Also, isn't this suggested formulation might be better? For example, think of a point that can only suffer attacks of a very small size, but on the other hand is not likely to be drawn as a test point. Isn't it better to define the lethal dose to be higher, than what reflects in the conjecture, in this case? (because a wrong prediction on this point is not lethal).’", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["86b0a7a9-c402-5373-bc10-3ce3edc5e031"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The formulation of the formal statement of the conjecture in page 3 is not justified enough, in my opinion. It seems that the conjecture is formulated with respect to a specific given data point $x_0$. I guess that this what a \"specific task\" (as written in the introduction) means? However, a \"specific task\" might be understood as drawing the test point from a specific hidden marginal distribution over instances, as usually done in PAC learning. Also, isn't this suggested formulation might be better? For example, think of a point that can only suffer attacks of a very small size, but on the other hand is not likely to be drawn as a test point. Isn't it better to define the lethal dose to be higher, than what reflects in the conjecture, in this case? (because a wrong prediction on this point is not lethal).’", "reference_answer": "A: Indeed, a ‘task’ is more often interpreted as a distributional argument rather than the pointwise one the authors present. However, the pointwise formulation is in fact **more desirable and more powerful**. \nFirstly, a pointwise argument can be easily converted into a distributional one, but the reverse is difficult. Given a distribution of $x_0$ and the (pointwise) ‘lethal dose’ for each $x_0$, one can define the distribution of the ‘lethal dose’ and its statistics as the distributional ‘lethal dose’. However, it is hard to uncover the ‘lethal dose’ for each $x_0$ from distributional arguments. Secondly, samples are not equally difficult in most if not all applications of machine learning: To achieve the same level of accuracy on different test samples, the number of training samples required can also be very different.\nFor example, on MNIST, which is a task to recognize handwritten digits, samples of digits ‘1’ are usually easier for models to learn and predict accurately, while those of digits ‘6’, ‘8’ and ‘9 are harder as they can look more alike. In consequence, the authors do not expect them to be equally vulnerable to data poisoning attacks. Compared to a distributional one, the pointwise argument better incorporates such observations.\n"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "27522902-3d8e-51cb-a2c2-b99625d0eb45", "question": "In Figure 4A, a vertical like at x = 10^3 intersects different lines representing different fraction of original dataset. Will all those points be iso-size?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7de55424-0d66-5dfd-9b68-f611c6c28c8c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 4A, a vertical like at x = 10^3 intersects different lines representing different fraction of original dataset. Will all those points be iso-size?", "reference_answer": "A: Yes, all points intersected by a vertical line correspond to iso-size pruned training sets. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b64330a9-f05b-5c1c-b4e9-90552e9f8862", "question": "Provide the claims for the following from the paper: Superiority of Gaussian Kernel over Linear Kernel. ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["36305be5-629e-5348-9c0a-d8033c5e875f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide the claims for the following from the paper: Superiority of Gaussian Kernel over Linear Kernel. ", "reference_answer": "A: In the proposed early experiments the authors have indeed considered both linear and gaussian kernels. The gaussian kernels always significantly outperformed the linear kernels. From that point on, the authors switched to the gaussian kernel exclusively. E.g., in one setting, a linear kernel reached a harmonic mean of 58.6, where the gaussian kernel reached a harmonic mean of 75.6, with all components and other parameters being equal. The ablations are provided in table 2.The authors found that the dot product reached its optimal point 10+ epochs earlier than cosine similarity and euclidean distance, and reached a much higher harmonic mean. The ablations are provided in table 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "012e3fd1-0c03-5231-b92d-9152c8ee1ae8", "question": "1& Limitation1: Are the results from tables 2, 3, and 4 also run 5 times and averaged as well?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8c7b8e19-117d-518a-a322-70b42ff36147"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "1& Limitation1: Are the results from tables 2, 3, and 4 also run 5 times and averaged as well?", "reference_answer": "A: Yes. As details, the authors run 5 times for Table1&Table4. For Table2&Table3, limited by the huge resources that adversarial training consumes, the authors run the attack to test adversarial robustness 5 times. The authors have highlighted this in the table caption. For reproducibility, the authors provide the source code and scripts with fixed random seeds in SupplementaryMaterial."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c5c64229-d01a-5ca9-a384-748f12d44449", "question": "What is the overhead of recompute step compared to the end-to-end training runtime?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e9212c2d-3733-5423-97bf-6bc024f6c4a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the overhead of recompute step compared to the end-to-end training runtime?", "reference_answer": "A: Recomputation is fast, since the inputs are already in SRAM (attention is bottlenecked by memory reads/writes), and it is done as part of the backward kernel. Figure 2 Left shows a comparison with Pytorch attention implementation where FlashAttention incurs more FLOPs (13% more) due to recomputation but reduces IOs by 9.1X, leading to 5.7X speedup."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6cd97a84-133a-5af4-9563-d57b95cd3087", "question": "Do you observe clear differences between tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["5005e9e9-0730-5b09-a003-12e0805b04ea"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do you observe clear differences between tasks?", "reference_answer": "A: Yes, the authors do observe clear differences between tasks (which typically consist of multiple datasets, see Table S1 in the appendix)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "400049b8-7d47-5571-92d7-9923d2e23ee8", "question": "Table 1 (b): According \"Exploring Diverse Expressions for Paraphrase Generation\", the quora paraphrase test set contains 30k, while the authors report 20k on test set. Why such difference?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Table 1 (b): According \"Exploring Diverse Expressions for Paraphrase Generation\", the quora paraphrase test set contains 30k, while the authors report 20k on test set. Why such difference?", "reference_answer": "A: The paraphrase generation dataset is constructed from the QQP dataset and does not have a standard train/valid/test split. There are two common ways to split the dataset (see Quora-S and Quora-U settings in [3]). The 30K test split is common for the purely supervised setting, and a 20K test split is usually used in the unsupervised setting [4,5]. \n\nthe authors followed the second split, since the proposed semi-supervised setting also incorporates the non-parallel dataset. All the semi-supervised competing methods adopt the same split and thus are directly comparable. This means that the comparison is fair between the proposed method and semi-supervised baselines. \n\n[3] Ding et al., Learning to Selectively Learn for Weakly-supervised Paraphrase Generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2021\n\n[4] Liu et al., Unsupervised paraphrasing by simulated annealing. In Proceedings of Association for Computational Linguistics, 2020\n\n[5] Li et al., Unsupervised Text Generation by Learning from Search, In Proceedings of Advances in Neural Information Processing Systems, 2020"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6533d132-84b2-50ad-aec6-4e790909f251", "question": "The authors mentioned it is hard to improve their worse dependence on $\\lambda$. Could you explain intuitively why it is hard?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["973b558e-915b-58ba-9d2a-9f46e25abc94"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors mentioned it is hard to improve their worse dependence on $\\lambda$. Could you explain intuitively why it is hard?", "reference_answer": "A: As described in Appendix B.1, the bound involves a term that estimates the probability of “good events”, i.e., equation (29). The probability is estimated via Markov’s inequality in equation (39). The dependence of $\\lambda$ can be weakened to $1/\\lambda^{2}$ via applying Markov’s inequality to $||\\nabla R_{S}(w^{*})||$  instead of its square as done in this paper. However, if doing so, the probability bound will be of order $O(\\sqrt{1/\\lambda^{2}n})$ which has a worse dependence on $n$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c47eb987-ddb0-5906-bfb1-2d8bf5ea34fd", "question": "Any insights on why DistilBERT performs the same as CLIP in Table 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d2802613-5fe3-5bcf-86d0-1840a39204e9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Any insights on why DistilBERT performs the same as CLIP in Table 1?", "reference_answer": "A: Both DistillBERT and CLIP are pre-trained with large-scale data, so they both have strong language modeling capabilities and can generate **good semantic targets**. Although the good semantic targets generated by DistillBERT are not aligned with the visual features of CLIP, it is easy to fit them with trainable visual encoders. The proposed observations in the experiment can also validate this, the loss of DistillBERT will be higher than CLIP in the early stage, but it will quickly decrease to the same level."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2cf940b8-66b4-5db3-ba7f-e85aa9b01347", "question": "In the paper, it is mentioned that \"For example, if it is extended recursively using $T$ steepest-descent sequential updates at each level, the problem size can be increased with $T^{n}$ variables. On the other hand, our formulation has polynomial-size $n T$ variables.\" Could you explain how the proposed method differs from \"$T$ steepest-descent sequential updates at each level\" that uses $T^{n}$ variables? How do we decrease to $n T$ variables?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d1767e5b-a16e-56b6-ad77-dfb263bd2c06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the paper, it is mentioned that \"For example, if it is extended recursively using $T$ steepest-descent sequential updates at each level, the problem size can be increased with $T^{n}$ variables. On the other hand, our formulation has polynomial-size $n T$ variables.\" Could you explain how the proposed method differs from \"$T$ steepest-descent sequential updates at each level\" that uses $T^{n}$ variables? How do we decrease to $n T$ variables?", "reference_answer": "A: The authors add some explanation on an approximated problem to be solved by the proposed algorithm (i.e., Problem (5) for the trilevel case) by assuming a simple setting, where the authors apply the steepest descent method for the lower-level problems with the same iteration number $T$ and stepsize $\\alpha$ for all levels as follows:\n\\begin{equation}\n\\begin{alignedat}{2}\n\\min_{x_{1} \\in S_{1}, \\\\{x_{2}^{(t)}\\\\}, \\\\{x_{3}^{(t)}\\\\}}{}&\n\tf_{1}(x_{1}, x_{2}^{(T)}, x_{3}^{(T)})\\\\\\\\\n\\text{s.t. }&\n\tx_{2}^{(t)} = x_{2}^{(t - 1)} - \\alpha\n\t\\nabla_{x_{2}} \\tilde{F}\\_{2}(x_{1}, x_{2}^{(t - 1)}, x_{3}^{(T)})&\n\t    \\quad& (t = 1, \\dots, T),\\\\\\\\\n& \tx_{3}^{(t)} = x_{3}^{(t - 1)} - \\alpha\n\t\\nabla_{x_{3}} \\tilde{F}\\_{3}(x_{1}, x_{2}^{(T)}, x_{3}^{(t - 1)})&\n\t    \\quad& (t = 1, \\dots, T).\n\\end{alignedat}\n\\end{equation}\nThe authors explain more on \"if  it  is  extended  recursively using $T$ steepest-descent sequential updates at each level, the problem size can be increased with $T^{n}$ variables\"\nby showing the corresponding formulation:\n\\begin{equation}\n\\begin{alignedat}{2}\n\\min_{x_{1} \\in S_{1}, \\\\{x_{2}^{(t)}\\\\}, \\\\{x_{3}^{(t_{1}, t_{2})}\\\\}}{}&\n\tf_{1}(x_{1}, x_{2}^{(T)}, x_{3}^{(T, T)})\\\\\\\\\n\\text{s.t. }&\n\tx_{2}^{(t)} = x_{2}^{(t - 1)} - \\alpha \\nabla_{x_{2}} \\tilde{F}\\_{2}(x_{1}, x_{2}^{(t - 1)}, x_{3}^{(T)})&\n\t    \\quad& (t = 1, \\dots, T),\\\\\\\\\n&\tx_{3}^{(t_{1}, t_{2})} = x_{3}^{(t_{1}, t_{2} - 1)} - \\alpha \\nabla_{x_{3}} \\tilde{F}\\_{3}(x_{1}, x_{2}^{(t_{1})}, x_{3}^{(t_{1}, t_{2} - 1)})&\n        \\quad& (t_{1} = 1, \\dots, T; t_{2} = 1, \\dots, T).\n\\end{alignedat}\n\\end{equation}"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "755cd0de-9da0-5f3d-b107-c5d21e0f4e24", "question": "How the performance is influenced by the warm-restarting learning rates?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["eae34c82-8e9c-5151-a7df-33fe27ccc3ce"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How the performance is influenced by the warm-restarting learning rates?", "reference_answer": "A: The authors have a section in Results called “Ablation Studies” and included a new figure for CIFAR-100. The figure highlights that the warm-restart method does not provide a significant accuracy benefit for the full-precision case but does in the single-bit-weights case. The figure also shows a comparison of learning and not learning the batch-norm offsets and gains."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7892cd46-f8f7-5127-bcb7-63727ba58a6b", "question": "Could you explain more about the theoretical background of the AS loss function? Is the AS stabilizer \"safe\" to use for complex physical systems?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you explain more about the theoretical background of the AS loss function? Is the AS stabilizer \"safe\" to use for complex physical systems?", "reference_answer": "A: The key formula $\\|x\\|^2(2\\langle x,F(x)\\rangle+\\|G(x)\\|_{\\rm F}^2  )-(2-\\alpha)\\|{x}^{\\top}G(x)\\|^2\\triangleq q(x)$ in AS loss is derived from $\\mathrm{d}||x||^\\alpha=\\mathrm{d}(||x||^2)^{\\alpha/2}$ using Ito's formula, a standard tool for stochastic analytics.  Then, the authors have $\\mathrm{d}\\Vert x\\Vert^\\alpha=\\alpha/2\\Vert x\\Vert^{\\alpha-4}q(x)\\mathrm{d}t+\\alpha\\Vert x\\Vert^{\\alpha-2}\\Vert x^\\top G(x)\\Vert\\mathrm{d}B_t$. Hence, the term $q(x)\\le0$ can drive the solution to zero due to the negative drift.  A specific explanation is provided in Appendix 3.5. Notice that the asymptotic stability in Theorem 2.3 stands almost surely (physically) instead of the stability only with a probability $1-\\varepsilon$ for some small number $\\varepsilon$. So, the AS stabilizer can be physically achieved and thus safely used for complex systems."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "834c5d4c-bd2f-57c9-879e-b86e53b49ceb", "question": "Discuss on the accuracy of the resulting clusters (super-classes).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8a4dd3c3-5cc9-5eca-8030-eebb477b53c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss on the accuracy of the resulting clusters (super-classes).", "reference_answer": "A: As the samples do not have a fixed super-class attribute, the authors first counted the distribution of each category of sample to assess the reliability of the proposed clustering. The Table R2-d shows the clustering distribution at the 50,000th iteration (10 labels on CIFAR-10), where each row represents the percentage of samples in one category that were assigned to each super-class. The \"MAX\" column indicates *up to how many samples that belong to the same ground-truth class are assigned to the same super-class.* If the authors count samples of the same ground-truth class and being in the same super-class as the correctly clustered samples, the authors can obtain the clustering average accuracy at the 50,000th iteration above as 90.34%. More importantly, as suggested, the authors compare the accuracy of pseudo-labels generated by FixMatch and the proposed methods over a larger number of iterations. (the authors set the random seeds to 1 and 2 respectively, as shown in the following two tables)\n\n|#iterations|3w|5w|10w|20w|30w|40w|50w|100w|test-best-acc\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|FixMatch  |17.1%|12.0%|10.2%|11.7%|9.1% |13.6%|8.6%|9.8%|19.15%|\n|ours      |34.3%|37.1%|63.6%|74.8%|79.1%|80.7%|82.6%|85.0%|81.28%\n|clustering|77.0%|90.3%|87.5%|90.8%|91.7%|85.2%|85.6%|87.3%|-\n\n|#iterations|3w|5w|10w|20w|30w|40w|50w|100w|test-best-acc\n|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|FixMatch  |22.8%|26.9%|33.1%|46.1%|55.4%|79.5%|80.1%|86.3%|85.11%|\n|ours      |37.1%|47.0%|59.1%|64.3%|80.1%|82.4%|87.4%|88.9%|86.12%|\n|clustering|76.0%|92.4%|90.4%|91.6%|91.5%|89.6%|90.8%|90.8%|-\n\nTable R2-e.  The rows \"FixMatch\" and \"ours\" indicate the accuracy of the pseudo-labels at different iterations of FixMatch and the proposed method, respectively. The line \"clutering\" indicates the accuracy of the proposed model in clustering (K=3 in 3w,5w iteration, K=5 in 10w,20w,30w iteration, K=10 in 40w,50w,100w iteration). It is clear to see that the super-classes the authors have implemented are more reliable compared to pseudo-labelling. Especially at the early stage of training, the accuracy of pseudo-labels is very low, so the authors perform clustering of super-classes, by which simple super-classes can learn more accurate information, thus helping model training.\n\n## More discussion\n### few-shot learning and BSL\nA: In few-shot (K-shot) setting，there are a large training set of base classes, a small support set of novel classes and a query set. When K=1, there is only one label per novel class, which seems very similar to BSL. However, the authors have to emphasize that the model does not learn only on the support set, but also on the large training set. Thus"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "46754cbc-d821-5c7d-9eb7-2ca32a154df9", "question": "Eq. 5 is a query-expensive gradient estimate. Will it make training extremely expensive?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f3b69b8e-d608-59ba-974f-c481c50789c8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Eq. 5 is a query-expensive gradient estimate. Will it make training extremely expensive?", "reference_answer": "A: For problems of high dimensions, the coordinatewise ZO gradient estimator in equation (5) does require function queries linearly scale with the problem dimension. But this estimator can be computed in parallel, the computational overhead would be released a lot. The authors have experimented with the MNIST attack task (the problem dimension is 784) to estimate the computation overhead. The authors compare two methods:\n1) use equation (5) to approximate the optimizee gradient. \n2) assume the gradient of the optimizee model is available at training time and use traditional backpropagation (note that this assumption is made in Chen et al. (2017b) but is usually not the case, so the authors only use this method as the baseline for comparison). \nThe authors find that the training time of 1) is about twice that of 2), which is acceptable. Moreover, potentially there could be several approaches to further reduce training time, such as sampling d'<d dimensions to estimate the gradient in equation (5) at each iteration."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fa4e544e-9f2f-578d-bb30-d42f9afd1fef", "question": "Figure 4: why do some curves not start at 0 on the x-axis?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["54be9a09-b764-596d-ac07-57ee007c5fdd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 4: why do some curves not start at 0 on the x-axis?", "reference_answer": "A: Some curves do not start at 0 because the authors start plotting the curves after obtaining the initial point which in some case requires more algebraic operations (e.g. kmeans methods)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b69d9e57-3afc-5d53-82ad-6b593f2e0155", "question": "I'm curious about the importance of assigning/tuning separate temperatures for each task (L89). Is this a crucial aspect of PaCo's success over baselines?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I'm curious about the importance of assigning/tuning separate temperatures for each task (L89). Is this a crucial aspect of PaCo's success over baselines?", "reference_answer": "A: The authors apologize that the authors did not make it clear in the paper. Assigning and (auto)-tuning of separate temperatures for each task is a standard setup used in previous MTRL methods (e.g. SoftModule, CARE). For example, in CARE work, this is applied to all the baselines as well (appendix of CARE paper Table 9-15, temperature is “learned and disentangled with tasks” for all methods). It is typically used because different tasks may have different learning dynamics along the training process.\nthe authors follow this setting in CARE to apply it to *all methods*. The authors will make this clear in the revised paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "77ce4979-42fb-5fa6-89bf-bb052df788dc", "question": "How is knowledge transfer used in the multi-modality model?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2cb98599-ddd1-5ea4-ba9d-a65c36c82757"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is knowledge transfer used in the multi-modality model?", "reference_answer": "A: Knowledge transfer and modality fusion are separate parts in the cross-modality interaction of Section 3.2. In Table 5 of the main paper, the authors only perform knowledge transfer from knowledge-rich settings to knowledge-poor settings, like LiDAR-based to camera-based models or multi-modality to single-modality models. For multi-modality inputs, to keep the simplicity, the authors optimize the whole framework in an end-to-end manner without cascade training. That means in a multi-modality setting, the authors do not perform knowledge transfer in the training stage. Of course, applying it in a cascade training manner may bring extra improvements. The authors do not use it to avoid making the pipeline complex. The authors will add more training details in the supplementary material to make it clear."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ef333b27-f97c-5469-8ea9-d26a6b0f103c", "question": "From Table 2, it seems that the impact of SSL (self-supervised learning) and AT (adversarial training) are not complementary.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "From Table 2, it seems that the impact of SSL (self-supervised learning) and AT (adversarial training) are not complementary.", "reference_answer": "A: There are indeed some misunderstandings. There are two types of adversarial training in the proposed adversarial pre-training for fine-tuning (APF) study: (1) **adversarial pre-training** for the SSL task and (2) **adversarial fine-tuning** for the downstream classification task. “Adversarial X” means that the authors additionally use **adversarially pre-training** for the self-supervised learning (SSL) task X, and “X” means that the authors only use **clean pre-training** for SSL task X. All the results in Table 2 are applied with  **adversarial fine-tuning** for the ultimate classification task. In fact,  all of the proposed results, including “Adversarial X”  and “X” in Table 2, actually reflect the performance of SSL + adversarial training. Both “Adversarial X”  and “X” show better performance than baseline (adversarial training) so that the proposed observations and insights that SSL is essential to the robustness improvements still hold."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f5901b40-96d7-5c14-9460-dbb628ac53cc", "question": "Quantitatvely analyze the model state prediction: how well does the model predict the state when integrated over time? It's hard to interpret the quality of the estimates without knowing how it fares over longer horizons than a single step, which is essentially what's quantified in tables 2 and 3.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["95771315-90ab-58db-a1fd-77e62b9a63d6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Quantitatvely analyze the model state prediction: how well does the model predict the state when integrated over time? It's hard to interpret the quality of the estimates without knowing how it fares over longer horizons than a single step, which is essentially what's quantified in tables 2 and 3.", "reference_answer": "A: Sorry for the confusion, but the authors had investigated long-term behaviors for the double pendulum test in Supplemental Materials (see Figure 11 for example). The predicted state by NODE vanished or diverged, while the proposed model sustained the oscillation. Moreover, the authors newly investigated long-term behaviors for the Lotka-Volterra test. Similarly to the double pendulum case, the prediction by NODE failed in the sense that the height of the peaks gradually increase or decrease, while the proposed"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "caddf6c4-113e-5e7c-9b74-9ecc3562f9c5", "question": "The negative keys far outweigh positive key, how is this imbalance handled?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5a46923f-6db9-5f38-95a2-9f528d88e2b0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The negative keys far outweigh positive key, how is this imbalance handled?", "reference_answer": "A: The authors did not specially handle this imbalance as the larger number of negative samples (negative keys far outweigh positive key) will not degrade the contrastive learning performance. Specifically, most existing contrastive learning methods employ much more negative samples than positive samples. For example, [21] uses $65536$ negative samples versus $1$ positive sample for each query and [9] (as well as the proposed method) uses $4096$ negative samples versus $1$ positive sample. This won’t degrade the contrastive learning performance as the infoNCE loss in contrastive learning is the widely adopted multi-class cross-entropy loss. In fact, prior studies [21,9] showed that a larger dictionary (with more negative keys/samples) leads to better contrastive learning outcome as a larger dictionary with more keys can better represent the dataset distribution. Please refer to the Figure 3 in [21] for more details."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b6671ebd-3492-56fd-8356-8a5ab47812cc", "question": "Why does end-to-end quantization enlarge optimization space?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f09e532-bc3b-56a1-96ac-6dc6c9cf7072"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does end-to-end quantization enlarge optimization space?", "reference_answer": "A: Because the authors only have 1024 images."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c4d08682-f6ee-50c2-b2a2-f27c27755c94", "question": "Is the formulation of the kernels with neighborhoods $\\mathcal N(g)$ vague?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f000c838-1733-5b65-b8ff-3ec832a960a4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the formulation of the kernels with neighborhoods $\\mathcal N(g)$ vague?", "reference_answer": "A: In lines 172-175 of the paper, the authors have defined $\\mathcal{N}(g)=\\\\{gg^{\\prime}|g^{\\prime}\\in\\mathcal{N}(e)\\\\}$ mathematically. Intuitively, the authors require the neighborhood of each group element to share the same relative position, just as the sliding windows used in the convolution operation. From the proposed perspective, this exactly characterizes the “some form of transformability of neighborhood”. To make it clearer, in Eqn.12, there are two types of neighborhood $\\mathcal{N}_1$ and $\\mathcal{N}_2$. The $\\mathcal{N}_1$ is used in the aggregation step of Eqn.12. As the summation does not rely on the ordering, the ordering of the elements in $\\mathcal{N}_1$ is not necessary. The $\\mathcal{N}_2$ is used to concatenate the features. The ordering here is important which is decided by the relative position of each neighborhood element, for example, $i\\in \\mathcal{N}_2(g)$ is decided by $g^{-1}i$. In practice, the authors just need to align the features to the right ordering (just like the method in Section7.1 of [5] which precomputes an indices permutation), before feeding them into MLPs."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "622d051c-91d3-5060-b2a0-194464cfac81", "question": "Are there meaningful comparison studies with other approaches in the experiment section? If yes, what is their conclusion? If no, what baselines could be included?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there meaningful comparison studies with other approaches in the experiment section? If yes, what is their conclusion? If no, what baselines could be included?", "reference_answer": "A: The authors supplement a numerical comparison with existing methods in Figure 6 to improve the validity of their proposed method. It can be observed that their method ourperforms the existing HDSCLF, BALSA, and LQR methods, establishing that it is the state-of-the-art method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "870b811a-5317-574e-823d-dc1f4e9a4f57", "question": "Compare the improved Precision and Recall Metric of the proposed work with baselines.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare the improved Precision and Recall Metric of the proposed work with baselines.", "reference_answer": "A: The computed Improved Precision and Recall metric [a] is shown, i.e. the results compared to baselines in the table below.\n\n|            | Night-to-day        | Night-to-day     | SR                  | SR               | Col                 | Col              | DC                  | DC               |\n|------------|---------------------|------------------|---------------------|------------------|---------------------|------------------|---------------------|------------------|\n|            | Precision$\\uparrow$ | Recall$\\uparrow$ | Precision$\\uparrow$ | Recall$\\uparrow$ | Precision$\\uparrow$ | Recall$\\uparrow$ | Precision$\\uparrow$ | Recall$\\uparrow$ |\n| BicycleGAN | $0.522$             | $0.041$          | $0.615$             | $0.159$          | $0.744$             | $0.518$          | $\\underline{0.869}$             | $\\underline{0.486}$          |\n| MSGAN      | $0.479$             | $0.003$          | $0.545$             | $0.156$          | $0.694$             | $0.578$          | $0.766$             | $0.346$          |\n| DivCo      | $0.611$             | $0.007$          | $0.561$             | $0.153$          | $0.759$             | $0.484$          | $0.845$             | $0.310$          |\n| MoNCE      | $\\textbf{0.818}$    | $0.008$          | $0.699$             | $0.120$          | $\\textbf{0.787}$    | $\\underline{0.624}$          | $0.830$             | $0.244$          |\n| cIMLE      | $0.578$             | $\\underline{0.054}$          | $\\underline{0.827}$             | $\\underline{0.278}$          | $0.638$             | $0.423$          | $0.853$             | $0.441$          |\n| CHIMLE     | $\\underline{0.785}$ | $\\textbf{0.352}$ | $\\textbf{0.934}$    | $\\textbf{0.697}$ | $\\underline{0.761}$ | $\\textbf{0.757}$ | $\\textbf{0.941}$    | $\\textbf{0.717}$ |\n\nAs shown in the table above, the proposed method outperforms all baselines by a significant margin across all tasks in recall, and in precision in most cases. In the few remaining cases, only one baseline outperforms the proposed method, and it does so at the expense of a lower recall."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "647068e8-bf61-5ec9-86a2-30db6c8c71be", "question": "Will the method run faster than DFSPH, given that the timestep is much larger than the timestep used by DFSPH, 0.02 ms vs. 0.001 ms? Will the learning-based physics engine have the potential to outperform the physics-based physics engine in terms of efficiency?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7566656d-1495-5b20-92c3-535a544492ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Will the method run faster than DFSPH, given that the timestep is much larger than the timestep used by DFSPH, 0.02 ms vs. 0.001 ms? Will the learning-based physics engine have the potential to outperform the physics-based physics engine in terms of efficiency?", "reference_answer": "A: Yes, the method runs faster. For instance the scene shown in Figure 3 runs in real-time with the proposed method while DFSPH needs about 9 minutes to simulate a sequence of 16 seconds. The proposed method is therefore more efficient with respect to the runtime. Following this direction, potential applications would be to approximate even more sophisticated SPH codes targeting CFD applications."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4d60814e-987e-513a-8771-40f94f0ba84b", "question": "The authors claim that the proposed framework for deep set prediction alleviates the need for hand-crafted distance metrics, and is efficient for a set of sequence predictions with teacher forcing requiring only $O(N)$ predictions which is an improvement over the usual $O(N^2)$. However, would $O(N^2)$ be a problem if the authors design the distance metric and do not work with the latent space?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors claim that the proposed framework for deep set prediction alleviates the need for hand-crafted distance metrics, and is efficient for a set of sequence predictions with teacher forcing requiring only $O(N)$ predictions which is an improvement over the usual $O(N^2)$. However, would $O(N^2)$ be a problem if the authors design the distance metric and do not work with the latent space?", "reference_answer": "A: Actually, the $O(N^2)$ problem has nothing to do with the problem of designing a distance metric. Teacher forcing is in conflict with Hungarian assignment because teacher forcing requires knowing the assigned ground truth for each element, while this information remains unknown until the assignment, which requires teacher forcing itself, is performed. There are two typical solutions: 1) do the teacher forcing with respect to all possible ground truths - this is the $O(N^2)$ problem, or 2) design a surrogate distance metric that does not require generation so that the assignment can be performed regardless of the teacher forcing. The former approach is impractical due to heavy resource requirements while the latter approach is far from trivial, especially in the general domain. LSP provides a third option for solving this problem. This scenario can be experimented in the image captioning experiments, but an $O(N^2)$ baseline would require heavy computing resources."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2128d77d-79e1-579e-aeaa-d9d62504014d", "question": "What are the advantages of the algorithm proposed in the paper?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["d175e1ea-79bf-5e85-99c7-c796fc5a34d8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the advantages of the algorithm proposed in the paper?", "reference_answer": "A: (1) Flexible budgets. The authors can handle various target budgets, such as difficult-to-balance ones (e.g., 150 images over 100 classes), or float budgets (e.g., 3.5 I/C).\n(2) Not linearly grow with the number of classes.\n(3) Addressable memories open the directions to other tasks and data modalities."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c4c4a345-d678-567f-83d8-196a36b423d9", "question": "Discuss the quantitative measure for the NTK eigenvalue \"closeness\" discussed in the paper.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0728fbcf-6f26-5777-be36-f4bbaf659ea2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss the quantitative measure for the NTK eigenvalue \"closeness\" discussed in the paper.", "reference_answer": "A: To provide a quantitative measure for the NTK eigenvalue \"closeness,\" the authors have discussed the spectral norm errors between $K_{CK}$ and $\\tilde K_{CK}$ (as has been established in Theorem 1 in the $n,p \\to \\infty$ limit). Specifically, in Figure 3 top, the authors have $\\parallel K_{\\rm CK} - \\tilde K_{\\rm CK} \\parallel = 0.15$ (**left** for GMM data) and $\\parallel K_{\\rm CK} - \\tilde K_{\\rm CK} \\parallel = 6.86$ (**right** for MNIST data).\nBesides, the authors have measured the similarity between the eigenvalues of $K_{\\rm CK}$ and $\\tilde K_{\\rm CK}$ using three different (histogram similarity) metrics: the cosine similarity [7], the correlation and the intersection [8]. The similarity estimates based on these three approaches are all close to one (in fact all greater than 0.99), indicating an extremely close match between the two histograms."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "42093cad-50f8-55b6-889e-b6790062d8c2", "question": "What happens if you remove the explanation regularization from the objective for all of the static and learned models?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["8d16e235-59ef-5739-a5c7-e6e59273973c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What happens if you remove the explanation regularization from the objective for all of the static and learned models?", "reference_answer": "A: If the authors remove explanations from the objective, then $\\mathcal{L}\\_{student}$ reduces to \n$\\mathcal{L}\\_{sim}$ and consequently $\\phi\\_S$ and $\\phi\\_T$ can be dropped from Equations 4 and 5,  arriving at a hard-label knowledge distillation, which is the proposed baseline."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b538224e-652a-5c4f-bca9-053429f8f55d", "question": "Is $\\Sigma_\\theta$ diagonal? If so, does that mean each feature/pixel of the generated sample is independent?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["695d1dcb-2e5c-57e5-bc1a-f8e84a82ad6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is $\\Sigma_\\theta$ diagonal? If so, does that mean each feature/pixel of the generated sample is independent?", "reference_answer": "A: The diffusion model the authors use is from “Diffusion Models Beat GANs on Image Synthesis” and uses diagonal covariances. Note that this is theoretically justified, as [48] have shown that for a Gaussian diffusion process, the reverse transitions $q(x_{t-1} | x_t)$ approach *diagonal* Gaussian distributions as the number of total diffusion steps $T$ approaches infinity. \nHowever, this does not yield independent pixels in the final image. At every timestep $t$ given $x_{t}$, the next sample $x_{t-1}$ is sampled from $\\mathcal{N}(\\mu_\\theta(x_t,t), \\Sigma_\\theta(x_{t},t))$ (this argument works for diffusion processes with or without a guidance).  But both $\\mu$ and and $\\Sigma$ are parameterized using an image-to-image DNN which connects each input to each output pixel. This means that every feature in $x_{t-1}$ is dependent on every pixel in $x_{t}$, therefore the pixels in the generated sample $x_{0}$ are not independent.   "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b8208096-033b-5ebf-8eef-60618ee3d7aa", "question": "In line 522, I am confused with the notation $u=\\frac{1}{T}\\sum_{t=1}^Tu_t$. In the equation between line 262 and 263, you had $\\bar u$. So both $u$ and $\\bar u$ denote average?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["befa2892-2ca9-59fb-bd49-724e13203e60"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In line 522, I am confused with the notation $u=\\frac{1}{T}\\sum_{t=1}^Tu_t$. In the equation between line 262 and 263, you had $\\bar u$. So both $u$ and $\\bar u$ denote average?", "reference_answer": "A: Yes, both are the same."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "843a192d-6d46-5cf8-9baf-750c2cc1ea7f", "question": "The performance drops when complete instructions are used in Table 2 (model #5 vs model #6), but using complete instructions provides improvement in Table 3 (line 266). What is the explanation for that?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The performance drops when complete instructions are used in Table 2 (model #5 vs model #6), but using complete instructions provides improvement in Table 3 (line 266). What is the explanation for that?", "reference_answer": "A: This is a good point. It is mainly because of the domain difference which is first found in [1]. Compared with R2R whose trajectories are short and instructions are simple, the much longer paths and richer instructions from en-RxR are out-of-domain. So when the complete instruction and trajectory pairs (complete pairs) from en-RxR are used as training data, it biases the model away from the domain of R2R and leads to the inferior performance on R2R. The performance on en-RxR and Landmark-RxR reported in Table3 is significantly improved by complete pairs, because the training data and validation data are both based on en-RxR and share the same domain. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c6fd4959-2102-5d90-8505-5c216976ea3d", "question": "Point addition (PA) seems to be just adding random points near the surface, so the attack looks weak.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Point addition (PA) seems to be just adding random points near the surface, so the attack looks weak.", "reference_answer": "A: The authors appreciate the reviewer for carefully reviewing the proposed formulations. However, point addition (PA) is NOT just adding random points near the surface. The authors need to run the adversarial attacks after initiating the added points. In the proposed study, the authors follow two principles to set up the threat model. First, the perturbation will not affect human perception much. Second, the perturbation is more than effective to break the clean trained models."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0705a365-8e75-504e-ade5-11f3467dc456", "question": "What is the purpose of the table of time per epoch?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["46b47e0a-c219-5d2c-a609-f04a9b617ca4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the purpose of the table of time per epoch?", "reference_answer": "A: To show that the proposed parameteric version in the worst case takes less than two times of the training time of vanilla KD and is even more efficient than the relation-based KD method FSP."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e732f78f-8233-54a5-bc89-3bf78346e23c", "question": "What are the references?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the references?", "reference_answer": "A: [1] Very deep convolutional networks for large-scale image recognition. Simonyan et al. ICLR 2015.\n\n[2] Rethinking the inception architecture for computer vision.  Szegedy  et al. CVPR 2016.\n\n[3] Deep residual learning for image recognition. He et al. CVPR 2016.\n\n[4] Densely connected convolutional networks. Huang et al. CVPR 2017.\n\n[5] Squeeze-and-excitation networks. Hu  et al.  CVPR 2018.\n\n[6] Wide residual networks. Zagoruyko  et al.   BMVC  2016. \n\n[7] Mobilenetv2: Inverted residuals and linear bottlenecks. Sandler et al.  CVPR 2018.\n\n[8]  Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.  Croce  et al. ICML 2022."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "816d1f10-6073-56a2-a49a-70c673aad1dd", "question": "How long does this model have to be trained when compared with the vanilla model? Is the presented model more noisy during training? Perhaps showing convergence plots will be instructive?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How long does this model have to be trained when compared with the vanilla model? Is the presented model more noisy during training? Perhaps showing convergence plots will be instructive?", "reference_answer": "A: The authors agree that including convergence plots will be helpful. Based on the proposed experiments, the authors did not observe any instabilities during the training of LSP given a reasonable encoder. For suboptimal encoders, the training was still stable but resulted in suboptimal performances. \n\nIn the CLEVR task, although it should be noted that it is not a perfect comparison because the baselines are not set prediction methods, [(click to see image)](https://i.ibb.co/r4Sk29T/lsp-clevr.png) are the validation performance curves for LSP and the two baselines.\n\nThe authors observed that Ordered Set and Concat converged faster than LSP but to worse solutions. Since these methods did not converge to solutions of the same quality, it was unfair to compare the convergence time directly. However, at any point in time, LSP was either on par or better with the other methods. \n\nIn the object detection task, the best performing batch size was 32 for DETR and 8 for LSP. The training progresses are shown in [(click to see image, same batch size 8)](https://i.ibb.co/TtbG8Z7/lsp-detr-bs8.png) and [(click to see image, best batch sizes)](https://i.ibb.co/Y8nSfWr/lsp-detr-best-bs.png).\n\nNo instability nor slowness in convergence was observed for LSP and DETR. It should be noted that AP (large) score is not shown because there was no large object. DETR (with batch size of 32) and LSP (with batch size of 8) were trained for the same number of iterations to not give LSP an advantage due to the smaller batch size. The authors will include the training progression curves in the appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5dc681e6-9c8d-5f67-8138-34e47f423292", "question": "What is the accuracy of ResNet-50 ResNet-18 KD on ImageNet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8ab99797-77c2-571c-8536-d5df28e7f652"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the accuracy of ResNet-50 ResNet-18 KD on ImageNet?", "reference_answer": "A: 71.425%"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b5abde76-2b58-52bc-a9e5-f99802b0b1b8", "question": "Explain if we need RL in this paper, compared with some adversarial manipulation on the gradients, e.g., directly maximizing the “reward” function w.r.t. gradients. Additionally, why is the proposed method better than myopic methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cade849-4388-5f47-8988-12a9668a2e49"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain if we need RL in this paper, compared with some adversarial manipulation on the gradients, e.g., directly maximizing the “reward” function w.r.t. gradients. Additionally, why is the proposed method better than myopic methods?", "reference_answer": "A: In this work, the authors consider online model-poisoning attacks against federated learning, which is a sequential decision making problem under uncertainty (from the attacker’s perspective) and RL is a natural framework for it. Compared with previous one-shot methods (e.g., LMP, IPM, and EB), the goal of an RL attack is non-myopic, that is, maximizing the attackers’ long-term expected rewards. A one-step optimizing goal is usually sub-optimal, especially when a robust aggregation rule is adopted.  It is observed in the experiments that the FL system can often recover quickly from a bad model under myopic attacks while RL can slow down the process (see Figures 3(c) and 3(d) in the paper). With potential strong defenses, it is crucial to attack in a “low-and-slow” way so that the attack effects will accumulate even if the one step attack ability is limited by the defense. In an FL system, since the next global model depends on the current one and the clients’ actions, it is natural to model it as a Markov decision process, which captures the evolution of the global model during FL training. \nThe authors choose deep-RL with dimension reduction (see Section 3.3 and Appendix D.1) to solve the attacker’s MDP, since it is typically more efficient than traditional dynamic programming and linear programming based methods, at the cost of being sample-inefficient and unstable as the reviewer points out. To solve the first problem, they have considered a model-based approach by building a simulator using the learned data distribution. To solve the second problem, they can set up a separate testing environment to identify the best trained policies as the authors briefly mentioned in the experiment section. On the other hand, they observed in the experiments that a sufficiently trained RL policy can typically obtain strong attack performance despite the instability. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2dddc23a-34bc-5033-97ae-dcf0f397dfab", "question": "Optimization problem: could you comment on the difficulty of optimizing this objective (in terms of robustness, sensitivity to hyperparameters of the optimization algorithms etc.) and perhaps describe/show some optimization traces.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4899724f-eebf-5cd2-8f14-5e4cad622d8a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Optimization problem: could you comment on the difficulty of optimizing this objective (in terms of robustness, sensitivity to hyperparameters of the optimization algorithms etc.) and perhaps describe/show some optimization traces.", "reference_answer": "A: The method, with objective function at line 238, is robust to hyperparameter tuning. In Table 3 below, authors show the empirical results of making changes to the proposed default setting when $n=1,000$; this table shows low errors across a range of hyperparameter values. The optimization trace can be a little counterintuitive because, when the authors heat up the parameter $\\tau$ for the contrastive loss, the loss function is changing with epoch. However, authors keep $\\tau$ fixed for the first 500 epochs, and display the optimization trace for that phase of training in Table 4.\n|   | $F\\downarrow$ | $h\\downarrow$ | $c\\downarrow$ | $b\\downarrow$ |\n|------------|------------|------------|------------|------------|\n| Default | **5.23(2.27)** | 2.77(2.04) | 7.96(3.52) | 4.02(2.15) |\n|Fixed $\\tau = 0.1$ | 5.60(2.62) |  3.15(2.11) | 7.40(4.30) | 4.03(2.32) |\n|Memory bank size $= 1,000$ | 5.99(2.37) | **2.46(1.77)** | 7.69(3.42) | 4.16(2.36) |\n|Initial learning rate = 0.001 | 5.71(2.77) | 2.95(2.02) | **6.84**(3.89) | 4.83(2.67) |\n|Embedding dimension $p = 48$ | 6.19(2.33) | 2.76(1.78) | 8.04(**3.04**) | **3.71(2.04)**|\nTable 3: Hyperparameters sensitivity analysis: Average MAPE (MdAPE, median absolute percentage error) evaluated on 200 test samples when $n = 1,000$. The default setting is: heat up $\\tau$ from $0.1$ to $0.5$, set memory bank of size $2,000$, initialize the learning rate at $0.01$ and set the embedding dimension $p = 64$. At each row, the authors make one change to the default setting. This experiment show performance of EnKI with Embed \\& Emulate is robust to hyperparameter tunning.\n\n|   | 0 | 50 | 100 | 150 | 200 | 250 | 300 | 350 | 400 | 450 | 500 |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|$\\ell$ | 27.41| 22.28 | 18.92 | 17.65 | 15.86 | 15.01 | 14.65 | 13.73 | 14.08 | 12.63 | 12.28 |\nTable 4: Optimization trace for first 500 epochs when $n = 1,000$ with $\\tau = \\tau' = 0.1$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6661e9be-ba94-5ab2-a730-ee7671d17422", "question": "Can the proposed method generalize beyond triplet training pairs? E.g. modality-specific setting or bi-modal modality-agnostic setting", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb9a3e42-f027-5472-bfc8-5b94af5294a9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can the proposed method generalize beyond triplet training pairs? E.g. modality-specific setting or bi-modal modality-agnostic setting", "reference_answer": "A: Great question. The proposed method can also generalize to modality-specific settings, where the authors only need to harmonize the gradient conflicts between $g_{va}$ and $g_{vt}$ in the Video Head (Figure 2). The authors are running experiments on modality-specific settings, and the authors will update the results in the rolling rebuttal period as soon as it’s ready.\n- The proposed method also works for bi-modal modality-agnostic settings such as MS-CLIP[r1], Since in their settings, the gradients from image ($g_{img}$) and text ($g_{txt}$) go through a modality-shared encoder, creating potential gradient conflicts. The authors will cite MS-CLIP [r1] in the related work of modality-agnostic settings, the authors will also apply the proposed method to MS-CLIP [r1] as soon as they have released the training code.\n\n[r1] You et al., MS-CLIP: Towards Modality-Shared Contrastive Language-Image Pre-training. ECCV 2022."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "224fba91-10ac-57b1-a463-48a6908a6c3a", "question": "Compare the results of this work with the method proposed in Deep Image Prior, Ulyanov et al. ?\nReference:\ňDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. CVPR, 2018.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare the results of this work with the method proposed in Deep Image Prior, Ulyanov et al. ?\nReference:\ňDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. CVPR, 2018.", "reference_answer": "A: Method | Variants | PSNR | SSIM\n-|-|-|-\n(Ulyanov et al., 2018) | --- | 9.31 | 0.568\nCDH (Ours) | --- | __24.24__ | __0.727__\n\nThe proposed method CDH achieves better results compared to deep image prior based methods (Ulyanov et al., 2018). This shows that simply using traditional image restoration approaches is not suitable for the inverse halftoning task, since they do not take into account the diverse pixel dithering patterns unique to halftone images."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6f413dc2-e3ef-5d3f-a18c-23c87382c90d", "question": "What should the reader understand about SGD trajectory?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["cf4cbc93-c79d-5784-a1f6-4485f4e5e84f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What should the reader understand about SGD trajectory?", "reference_answer": "A: The authors used the toy linear NN problem to illustrate that the training consists of two phases for all 4 algorithms -- SGD, Adam, AdamW, NovoGrad: \n1. phase 1 \"Training loss minimization\":  the trajectory goes from the initial point to some solution on the minima manifold, given by equation $w1*w2=1$  \n2. phase 2 \"Regularization\":  trajectory goes from that solution to the good solution along the minima manifold. \nPoints $(1,1)$ and $(-1,-1)$ are better from the regularization point of view since Hessian at any minima has 2 eigen values: $0$ and $(w_1^2 + w_2^2)$. The authors want the solution with minimal largest eigen value, and there are two such minima: $(-1,-1)$ and $(1,1)$.\nNote that the second phase \"regularization\" requires explicit weight decay / L2 regularization. For example, if the authors remove weight decay the trajectory stays at the point where it meets the minima manifold. This is true for all algorithms (SGD , NovoGrad...). The trajectory follows the hyperbola (minima manifold) since weight decay pulls is toward the origin, but loss gradient doesn't let trajectory to go too far from the minima manifold. Note that for NovoGrad the penalty for leaving the minima manifold is much higher than for SGD, and the trajectory stays more close to the hyperbola."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "73f149f6-2658-532f-aa90-885b8fcd4021", "question": "Constructing the parameters of the collage function F may be a lot to ask the encoder to do. If I understand it correctly, this means compressing the entire object's information into a single vector [...] which is used to parameterize F?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8c872a12-f8c8-5947-8865-90659c926e57"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Constructing the parameters of the collage function F may be a lot to ask the encoder to do. If I understand it correctly, this means compressing the entire object's information into a single vector [...] which is used to parameterize F?", "reference_answer": "A: That's spot on! The encoder is compressing each image into the corresponding vector of Collage iteration parameters $\\omega$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b51f3a40-bd04-54e8-86cb-95eab49537c7", "question": "Are the results in Table 3a surprising?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the results in Table 3a surprising?", "reference_answer": "A: No, since both too small or too large K will decrease the performance (at 2M), although due to different reasons. More concretely, a too small K decreases the performance due to the over-constrained policy parameter space. On the other hand, a too large K will reduce sample efficiency since it encourages less on parameter-sharing. Therefore, under the standard MTRL setting, where the authors evaluate the models after being trained on a fixed number of environmental steps, the model will typically have lower performance because of its lower sample efficiency."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "79704cb1-4e5a-5c47-8404-83cb99b42a6c", "question": "How does the approach affect the learned representations?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the approach affect the learned representations?", "reference_answer": "A: For the proposed task i.e. image generation, the generator learns how to map Gaussian noise to natural images. It does not involve learning representations (in common sense) of images. But if \"representations\" more generally refer to the outputs of the model i.e. generated results, then qualitative and quantitative results have shown the proposed promising generation performance gain. This is achieved by the proposed efficient latent space transform and a more powerful generative mapping trained by the proposed algorithms (i.e. twofold efforts on $z^*$ and $G$), as claimed in the contributions."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "74b8ffd8-c984-5edb-8f67-31e1c861b282", "question": "Why the compared methods are usually different. For example, compared methods are Adam, SGD, and NovoGrad in table 4 and compared methods are Adam, AdamW, and NovoGrad in table 6.  Why not compare all these methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["cf4cbc93-c79d-5784-a1f6-4485f4e5e84f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why the compared methods are usually different. For example, compared methods are Adam, SGD, and NovoGrad in table 4 and compared methods are Adam, AdamW, and NovoGrad in table 6.  Why not compare all these methods?", "reference_answer": "A: The choice of baseline algorithms for each particular problem was based on the best performing optimizers from the literature. The authors tried to solve several tasks with “non-traditional” optimizers but did not succeed. For example, the authors could not make Adam converge on ResNet-50 to reasonable accuracy and the authors could not make SGD converge on Transformer NMT."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c15981a5-3c9f-59df-b935-ab3c84cdb0ab", "question": "How to pick disentangled dimensions:", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c33416f2-1be4-5cb4-89e6-e85f0a27c0ab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to pick disentangled dimensions:", "reference_answer": "A: In this paper, the authors used an approach from the existing literature to identify disentangled dimensions. It involves \n   identifying the dimensions whose change can result in localized changes in the generated image. In summary it's a two step process:\n      1. Vary each dimension and compute the resulting gradient map over the entire image. Dimensions that result in localized changes are chosen for the next step.\n      2. Pick a pretrained classifier that is trained on a large corpus of data such as Imagenet or other big datasets. For each dimension chosen in Step (1), vary the value of the dimension and identify the change in classifier score. This dimension is chosen as disentangled if it affects the classifier score by a large margin.\nThere are other ways to identify disentangled dimensions such as training a separate set of controls or applying regularization during training. The proposed central goal is to construct confidence intervals, not to get better disentanglement."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ebf97dda-642b-54a2-8576-4d3a9191b5bd", "question": "Would improving T further improve the performance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ef4e07a2-e75c-5136-adc9-87fd8b029c3c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would improving T further improve the performance?", "reference_answer": "A: Because of the training cost, the authors did not test different $T$ on ImageNet. Instead, the authors have tested different $T$ on the CIFAR-10 dataset.  The authors aim to explore the effect of $T$ instead of achieving SOTA accuracy. Hence, the authors use a lightweight network, whose structure is *c32k3s1-BN-IF-{{SEW Block(c32k3s1)}\\*2-MPk2s2}\\*5-FC10*. The accuracy changes with respect to different $T$ ($1 \\leq T \\leq 32$) is shown in Tab.R8.\n\n| $T$  | 1     | 2      | 3      | 4      | 5      | 6     | 7      | 8     |\n| ---- | ----- | ------ | ------ | ------ | ------ | ----- | ------ | ----- |\n| Acc  | 0.737 | 0.7939 | 0.8133 | 0.8263 | 0.8334 | 0.846 | 0.8535 | 0.857 |\n\n| $T$  | 9      | 10     | 11     | 12     | 13     | 14    | 15         | 16     |\n| ---- | ------ | ------ | ------ | ------ | ------ | ----- | ---------- | ------ |\n| Acc  | 0.8598 | 0.8615 | 0.8634 | 0.8618 | 0.8627 | 0.861 | **0.8666** | 0.8634 |\n\n| $T$  | 17     | 18     | 19    | 20     | 21     | 22     | 23     | 24     |\n| ---- | ------ | ------ | ----- | ------ | ------ | ------ | ------ | ------ |\n| Acc  | 0.8663 | 0.8615 | 0.862 | 0.8587 | 0.8649 | 0.8566 | 0.8579 | 0.8587 |\n\n| $T$  | 25     | 26     | 27     | 28     | 29     | 30     | 31   | 32     |\n| ---- | ------ | ------ | ------ | ------ | ------ | ------ | ---- | ------ |\n| Acc  | 0.8594 | 0.8532 | 0.8589 | 0.8585 | 0.8559 | 0.8452 | 0.85 | 0.8473 |\n**Table R8: Test accuracy of SEW ADD ResNet on CIFAR-10 with different $T$.** \n\nIt can be found that the accuracy firstly increases and then decreases slowly with the increase of $T$. This result of SEW ResNet is consistent with the proposed previous experimental results on SNNs. A larger $T$ can increase the SNN's fitting ability, but too large $T$ does not guarantee better performance. The authors think the reasons are as follows:\n1) A larger $T$ may cause over-fitting. 2) Gradients are prone to vanish (the long-term dependency problem of RNNs)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0c976343-d460-5ddf-891c-7dde7324297f", "question": "How does sampling efficiency connect to the proposed strategy?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does sampling efficiency connect to the proposed strategy?", "reference_answer": "A: The proposed divide-and-conquer strategy reduces the search space for the latent code to a more promising region. Because the region is smaller, there are more samples generated within a given area within the region than outside of it. This makes it more likely to find a sample that is close to the observed image, which leads to better sampling efficiency."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "11ea47a7-8ef0-5ca2-b2b0-72c1e28923f6", "question": "For the motivation of this method, why would the graph be constructed within each class?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e46ab308-c360-57b7-b04c-cc84c9fce9cc"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For the motivation of this method, why would the graph be constructed within each class?", "reference_answer": "A: The most general graph would be constructed based on image and text similarities combined. Here, the authors pre-filter with text similarity, i.e., label names, and then build the graph based on visual similarities. This permits (a) to significantly reduce the size of the graph and hence the complexity and (b) to reduce the noise during the cleaning task. The authors agree that operating on the more complex graph could be the subject of future research, but a significantly different method would be required and the gain of the correlation is not granted."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5274daa3-ba7a-5017-9221-b39e4a0f7ee2", "question": "What are the two specific design choices?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["615a5caa-14fa-5c4e-a066-1d3d8c093a39"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the two specific design choices?", "reference_answer": "A: During ImageNav training the proposed agent learns to stop near the goal. This means it stops at viewpoints that are visually similar, but not exactly the same as the goal image. Learning this approximation in training transfers to the downstream ObjectNav task."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9f016be6-e9c0-5aef-911d-4e5ef19addf1", "question": "What if supports from two classes lie in the upper sphere while supports for the other two lie in the other sphere? Then, in this case, initially we could use the direction along the centroid for differentiation, and yet after projection, we might not be able to?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["48f84a36-4fdb-55ab-85d5-545e4cb455b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What if supports from two classes lie in the upper sphere while supports for the other two lie in the other sphere? Then, in this case, initially we could use the direction along the centroid for differentiation, and yet after projection, we might not be able to?", "reference_answer": "A: If the authors understand the question correctly, the questions assumes it is possible that the line connecting two classes is parallel to the direction of the task centroid, thus removing the direction of task centroid may lead to loss of discriminative power. However, the authors found that it is impossible in real-world high-dimensional data. To see this, the authors conduct an additional experiment. The authors randomly sample 5 classes in the test-time dataset (i.e., 5-way task). Then the authors calculate the cosine similarity between the task centroid and those lines connecting all possible pairs of classes. Thus for such a task the authors calculate 10 similarities (i.e., 10 possible combinations of classes). For each test-time dataset, the authors sample 2000 such 5-way tasks and report the average similarity as well as the maximum similarity among all $20000$ (i.e. $2000\n\\times10$ calculated similarities. The results are shown in the table below.\n\n|      | mini  | Cub   | Dtd   | Fungi | Omini | Sign  | Qdraw | Flowers |\n| ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ------- |\n| Avg  | 0.039 | 0.023 | 0.029 | 0.020 | 0.012 | 0.017 | 0.006 | 0.021   |\n| Max  | 0.191 | 0.197 | 0.162 | 0.205 | 0.087 | 0.108 | 0.065 | 0.141   |\n\n\nAs seen from the table, in each dataset, even the maximum similarity among 20000 possibilities is far less than 1, and the average similarity is extremely low. This indicates that the line connecting two classes is very likely to be orthogonal to the direction of task centroid. The authors conjecture that this is due to the nature of the high dimension of feature space: in a high-dimensional space, two randomly sampled vectors are very likely to be orthogonal to each other."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "01daa7d2-e010-5ed8-9f14-173a657dd2b2", "question": "It seems like evaluation is performed under the case where unlabeled samples include categories that are not presented in the target set?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8174ac9b-7bbd-50a2-b584-5598894ad25d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It seems like evaluation is performed under the case where unlabeled samples include categories that are not presented in the target set?", "reference_answer": "A: The authors did evaluate the proposed method and other baselines under the scenario where unlabeled samples include unseen categories, i.e., on the STL-10 dataset. As written in the official introduction of this dataset, *‘These examples (unlabeled samples) are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.’* This is just the case where unlabeled samples include unseen classes as reviewer posted. Therefore, the proposed experiments on the STL-10 dataset are under this case, where the proposed FlexMatch **outperforms** other baselines."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "32d454d0-73eb-5ba1-9bd5-a3e8c8bb7b75", "question": "Summarize the results from the study on the effectiveness of diffusion model over latent as a solution to the prior hole problem in VAEs.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9b2e64f8-a989-5565-91f2-58280f79add6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Summarize the results from the study on the effectiveness of diffusion model over latent as a solution to the prior hole problem in VAEs.", "reference_answer": "A: The authors performed an additional ablation study on this topic, where the authors train a D2 model (without the contrastive learning component), which applies a diffusion model over the latent variables.\n\nThis table shows the FID score of the generated images with a different number of diffusion steps.\n\n|        |       | CIFAR10 |       |       | CIFAR100 |        |\n|-------|-------|-------|-------|-------|-------|-------|\n| Steps | 10    | 50  | 100   | 10    | 50       | 100   |\n| D2      | 22.3 | 15.8 | 15.1  | 28.35 | 19.81    | 19.85 |\n| D2C   | 17.71 | 10.11| 10.15 | 23.16 | 14.62    | 14.46 |\n\nCompared with the performance of NVAE (36.4 on CIFAR10 and 42.5 on CIFAR100), even D2 is significantly better. Additionally, D2C is even better than D2 in terms of unconditional generation performance.\n\nThis table shows the MSE, FID and latent representation accuracy comparisons between D2, D2C and NVAE.\n\n|        |       | CIFAR10 |       |       | CIFAR100 |       |\n|-------|-------|---------|-------|-------|----------|-------|\n|        | FID   | MSE     | Acc   | FID   | MSE      | Acc   |\n| D2    | 15.1  | 0.24    | 40.6  | 19.85 | 0.48     | 17.89 |\n| D2C   | 10.15 | 0.76    | 76.02 | 14.62 | 0.44     | 42.75 |\n| NVAE  | 36.4  | 0.25    | 18.8  | 42.5  | 0.53     | 4.1   |\n\nHere, the D2 has worse latent representation accuracy than D2C but better than NVAE. These tables suggest that while adding a diffusion model over the latent space is beneficial (since D2 outperforms NVAE), adding the contrastive component may further improve performance. \n\nIn Theorem 2, the authors also present an argument as to why diffusion models are fundamentally superior to other types of latent priors in terms of generative modeling."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d2eb408c-bd55-5700-9017-8447365597f8", "question": "How would the proposed method far at inferring object-specific latent parameters which cannot be inferred from images alone?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0c5d282a-1541-5166-bc1b-300a30619e73"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How would the proposed method far at inferring object-specific latent parameters which cannot be inferred from images alone?", "reference_answer": "A: The proposed approach is able to learn object-specific latent parameters like mass, gravity, or spring constants that are not immediately detectable from images alone."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6ec76ef5-9f13-5887-a47e-97edaf2a82c6", "question": "Is the evaluation in Table 1 meaningful or fair?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["40c48abe-f035-5a03-9f8b-e12839c936f1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the evaluation in Table 1 meaningful or fair?", "reference_answer": "A: Evaluation metrics fidelity/inverse fidelity/sparsity are adopted from [1,2,3]. For real datasets without the ground truth, fidelity and inverse fidelity provide meaningful evaluation. However, the difficulty is that the fidelity of explanation with different sparsity is not directly comparable. For example, using the whole graph as an explanation will most likely result in much higher fidelity than using any single node in a graph. However, the whole-graph explanation is not sparse at all, and directly comparing these two fidelities is not fair. SubgraphX chooses to compare methods using fidelity vs. sparsity plots for a range of sparsity. The authors have both the fidelity vs. sparsity plot and the inverse fidelity vs. sparsity plot for each dataset in Figure 4 in Appendix A.4. These plots show that the proposed method outperforms baselines in most cases. The numbers in Table 1 are a normalized summary of the curves shown in Figure 4, it summarizes the fidelity and inverse fidelity analogously to the F1-score summarizes precision and recall, so the authors can quickly compare methods using a single number, which saves space for showing all the plots and save time for interpreting all the curves in the plots. Nevertheless, the plots are shown in Figure 4 in Appendix A.4 for a closer comparison.\n[1] Pope, P. E., Kolouri, S., Rostami, M., Martin, C. E., and Hoffmann, H. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10772–10781, 2019.\n[2] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12241–12252. PMLR, 18–24 Jul 2021\n[3] Yuan, H., Yu, H., Gui, S., and Ji, S. Explainability in graph neural networks: A taxonomic survey. arXiv preprint arXiv:2012.15445, 2020c."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "da845cf8-2fc8-5682-8134-ac7184b07e49", "question": "Does the proposed dataset contain mostly indoor scenes? How will the method perform on images of glasses outdoor?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0e6ba89c-14a5-5176-aa62-ec09a52d9717"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the proposed dataset contain mostly indoor scenes? How will the method perform on images of glasses outdoor?", "reference_answer": "A: The authors analyze the proposed dataset and find 4013 indoor images (88.8%) and 506 outdoor images (11.2%)  from the proposed GSD-S. The authors indiviually evaluate images outdoor in the test set and find the proposed method still performs well in outdoor scenes. (IoU: 0.744, $F_\\beta$: 0.836, MAE: 0.039 BER:8.88)"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "928917a9-3ebc-5175-9d31-928739824e79", "question": "Since the number of inverted lists to visit depends on the query, how much variation is there in the query time?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["1d3fbc62-9a2b-564c-bbdc-716203954b0c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Since the number of inverted lists to visit depends on the query, how much variation is there in the query time?", "reference_answer": "A: The authors report the query latency (query time) distribution at 50%, 90%, 95% and 99% percentile for the two datasets to reach 90% recall, which is more useful in real systems, to characterize the variation. The results are as the following:\n\nSPACEV1B|Average latency (ms)|50% percentile latency (ms)|90% percentile latency (ms)|95% percentile latency (ms)|99% percentile latency (ms)\n:--:|:--:|:--:|:--:|:--:|:--:\nrecall@1|1.117|1.059|1.407|1.519|2.315\nrecall@10|1.109|1.066|1.352|1.454|2.067\n\nSIFT1B|Average latency (ms)|50% percentile latency (ms)|90% percentile latency (ms)|95% percentile latency (ms)|99% percentile latency (ms)\n:--:|:--:|:--:|:--:|:--:|:--:\nrecall@1|0.714|0.716|0.787|0.808|0.854\nrecall@10|1.029|1.001|1.214|1.255|1.328\n\nAs we can observe from the two tables that the query latency at 99% percentile is only about 1.2 to 2.0 times larger than the average query latency."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6069f47a-5188-572f-bbdc-47a44c05ee9d", "question": "How do the other four geometries from the D are supervised?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do the other four geometries from the D are supervised?", "reference_answer": "A: Recovering the geometry information from 2D images is the goal of inverse rendering. The other four geometries can be *unsupervisedly* learned with the *renderer* (*e.g.*, the Lambertian shading model in L138-142) as the 3D prior, and the reconstruction error as the loss function (*i.e.*, Eq. (8))."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a34fbdbc-a9ec-5efe-82ca-b0f00f5dc195", "question": "Why are these design choices useful?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["615a5caa-14fa-5c4e-a066-1d3d8c093a39"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are these design choices useful?", "reference_answer": "A: Conceptually, proximity in physical space translates to similarity in the CLIP embedding space. Thus, the agent learns to stop close to goal embeddings (image or text), but does not require an exact match. Exact matching would be problematic, because as the reviewer points out, CLIP text and image embeddings are different."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7f19579c-6970-5664-bf68-643c94b2142c", "question": "In 7.1, is it possible to measure the reconstruction error of D2C without L_D2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9b2e64f8-a989-5565-91f2-58280f79add6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In 7.1, is it possible to measure the reconstruction error of D2C without L_D2?", "reference_answer": "A: The authors used a pre-trained MoCo-v2 model and trained a NVAE decoder to reconstruct the image. The reconstruction MSE per image was 58.20, significantly worse than NVAE (0.25) and D2C (0.76). The FID of the reconstructed images is 49, which is much higher than the proposed methods (which has an reconstruction FID of around 1). \n\nThe authors believe this could be relevant to the downsampling layers in ResNet; in the paper, the authors mentioned that the authors have tried ResNet encoders in D2C, which also led to much higher reconstruction errors (and worse FID)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5bc42e99-ef3d-566b-b6ef-9faf41fee2ba", "question": "Provide a clarification of the claim that \"Unlike Eq. (8), we estimate $w$ by formulating $Q_{{w}}(x_2|x_1)$ using raw features $f$, instead of the projected features ${h}$, for more accurate estimation\".  Can you show any quantitative evaluation of this claim?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table", "formula"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide a clarification of the claim that \"Unlike Eq. (8), we estimate $w$ by formulating $Q_{{w}}(x_2|x_1)$ using raw features $f$, instead of the projected features ${h}$, for more accurate estimation\".  Can you show any quantitative evaluation of this claim?", "reference_answer": "A: The authors conducted an experiment to verify this claim. In this experiment, the authors compared the accuracy of the estimated regional importance obtained by the following three different estimation methods.\n(1) The first method is to estimate the regional importance based on the raw features $f$, i.e. The authors directly use the method in Line 210-217 to estimate $w^{(r)}$ as the regional importance.\n(2) The second method is to estimate the regional importance based on the projected features $h$. This estimation is the similar as the above estimation of $w^{(r)}$. The only exception is that the authors replace the sample-wise similarity $Q_w(x_2|x_1)\\propto{\\prod}_r Q_w (f_2^{(r)}|f_1)^{w_2^{(r)}}$ in Line 216 with $Q_w (x_2|x_1)\\propto{\\prod}_r Q_w (h_2^{(r)}|h_1)^{w_2^{(r)}}$. The authors use $\\hat{w}^{(r)}$ to denote the regional importance estimated using this method.\n(3) The third method is to directly use the classification confidence of a projected regional feature $h^{(r)}$ to its ground-truth category as its regional importance, i.e. $\\tilde{w}^{(r)}=\\log\\frac{p(y\\ =\\ c^{\\text{t r u t h}}\\ |\\ h^{(r)})}{1\\ -\\ p(y\\ =\\ c^{\\text{t r u t h}}\\ |\\ h^{(r)})}$.\nIn this way, to verify the claim, the authors aim to prove that the regional importance estimated by the first method is more accurate than the regional importance estimated by the second or the third method. To this end, the accuracy of each regional importance is measured by the Pearson correlation coefficient between the estimated regional importance and Shapley values $\\phi^{(r)}$. A higher value of the correlation indicates higher accuracy of the estimated region importance. Given a certain input image, the authors measured the Pearson correlation coefficient between $w^{(r)}$ and $\\phi^{(r)}$, the Pearson correlation coefficient between $\\hat{w}^{(r)}$ and $\\phi^{(r)}$, and the Pearson correlation coefficient between $\\tilde{w}^{(r)}$ and $\\phi^{(r)}$, over different regional features. For each estimation method, the authors averaged the correlation through all input images. The table below shows that the correlation between $w^{(r)}$ and $\\phi^{(r)}$ was higher than the correlation between $\\hat{w}^{(r)}$ and $\\phi^{(r)}$, and the correlation between  $\\tilde{w}^{(r)}$ and $\\phi^{(r)}$. This showed the high accuracy of estimating regional importance based on raw features $f$. This verified the proposed claim that estimating regional importance based on raw features $f$ was more accurate than estimating regional importance based on projected features $h$.\n\n| Dataset       | DNN          | The correlation between $w^{(r)}$ and $\\phi^{(r)}$ | The correlation between $\\hat{w}^{(r)}$ and $\\phi^{(r)}$ | The correlation between $\\tilde{w}^{(r)}$ and $\\phi^{(r)}$ |\n| ------------- | ------------ | -------------------------------------------------- | -------------------------------------------------------- | ---------------------------------------------------------- |\n| Tiny ImageNet | ResNet-34    | **0.8943**                                         | 0.3638                                                   | 0.6538                                                     |\n| Tiny ImageNet | VGG-16       | **0.6307**                                         | 0.2836                                                   | 0.5428                                                     |\n| Tiny ImageNet | MobileNet-V2 | **0.8658**                                         | 0.4361                                                   | 0.7301                                                     |\n| COCO 2014     | ResNet-50    | **0.8814**                                         | 0.3881                                                   | 0.4406                                                     |\n| CUB-200-2011  | ResNet-34    | **0.8561**                                         | 0.3835                                                   | 0.3846                                                     |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0b1b178e-bfec-5fd5-8c7e-3808e0f12594", "question": "Table 1 (a): The authors cite results from \"DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances\". Why are the numbers from this paper not be found in the source? Same for AdaLabel.\nReference:\nGu, Xiaodong, Kang Min Yoo, and Jung-Woo Ha. \"Dialogbert: Discourse-aware response generation via learning to recover and rank utterances.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 14, pp. 12911-12919. 2021.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Table 1 (a): The authors cite results from \"DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances\". Why are the numbers from this paper not be found in the source? Same for AdaLabel.\nReference:\nGu, Xiaodong, Kang Min Yoo, and Jung-Woo Ha. \"Dialogbert: Discourse-aware response generation via learning to recover and rank utterances.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 14, pp. 12911-12919. 2021.", "reference_answer": "A: The previous paper worked on an overlapping (thus wrong) dataset. The authors instead worked on deduplicated datasets [1], as the authors presume scientific research should follow the correct setting. \n[1] Wen et al., An empirical study on the overlapping problem of open-domain dialogue datasets. In Proceedings of the Language Resources and Evaluation Conference, 2022."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "275aa701-6ac6-562e-8281-3e5f6c43ea8b", "question": "Is there any data augmentation in standard training (ST) and adversarial training (AT) baseline?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9e75cdd1-d017-57eb-b94e-0617bf1b16b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there any data augmentation in standard training (ST) and adversarial training (AT) baseline?", "reference_answer": "A: The authors leverage Gaussian jitter to augment the ST and AT baseline. The authors follow the reviewer’s suggestions to add rotation and jigsaw augmentation to test the performance on ModelNet40. \n\n|     Rotation Augmentation        || ModelNet40            |           |\n|:----:|:----:|:----:|:----:|\n| CA/RA (%)   | PointNet   | DGCNN     | PCT       |\n| ST Baseline | 85.2/0     | 90.9/0    | 90.1/0    |\n| AT Baseline | 82.2/17.5  | 87.2/49.2 | 85.1/38.1 |\n\n|      Jigsaw Augmentation       || ModelNet40         |        |\n|:----:|:----:|:----:|:----:|\n| CA/RA (%)   | PointNet   | DGCNN  | PCT    |\n| ST Baseline | 2.41/0     | 2.66/0 | 2.42/0 |\n| AT Baseline | 2.43/0     | 2.56/0 | 2.49/0 |\n\nThe authors find that both augmentations will hurt the performance, and there are some reasons.\n\n1. Although rotated point clouds preserve the global shape with the original point clouds, the designs of the point cloud recognition models are not rotation-invariant. Therefore, rotation augmentation will hurt the performance, since the model has limited ability to adapt the rotated data.\n\n2. Jigsaw itself does not fit as an augmentation method since it will displace the point cloud to different locations which will make the data totally different from the original distribution, as shown in Figure 1. Therefore, in the test time, it is not possible for the model to predict the original point cloud’s class. It explains that with jigsaw augmentation, the model’s decision becomes like a random guess.\n\n**It is worth mentioning that self-supervised learning (SSL) is fundamentally different from data augmentation.** The proposed SSL is to make the model predict the transformation itself to learn robust context information. However, data augmentation is to make the model generalize to different types of data. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d2f737d7-d99e-5ed5-8758-fc7e2a75d4ca", "question": "How does random voxel dropout work compared with magnitude-based pruning ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does random voxel dropout work compared with magnitude-based pruning ?", "reference_answer": "A: **Experiments on random voxel dropout:** The authors carried out the experiment of random drop ablation on both KITTI and nuScenes datasets. The ratio of random drop is set the same as the proposed magnitude-based pruning:   for KITTI, the authors set pruning ratios in SPSS-Conv and SPRS-Conv as 0.5 and 0.5 respectively; as for nuScenes, they are set as 0.3 and 0.5.  The table below shows the performance comparison of random drop and magnitude as indicators.\n\n| Method (nuScenes)     | mAP   | NDS   |\n| --------------------- | ----- | ----- |\n| SPSS-Conv             | 58.48 | 66.11 |\n| SPSS-Conv inverse     | 55.84 | 64.72 |\n| SPSS-Conv random drop | 56.12 | 64.49 |\n| SPRS-Conv             | 58.59 | 66.23 |\n| SPRS-Conv inverse     | 16.72 | 39.29 |\n| SPRS-Conv random drop | 55.58 | 64.34 |\n\n| Method (KITTI)        | Easy  | Moderate | Hard  |\n| --------------------- | ----- | -------- | ----- |\n| SPSS-Conv             | 89.22 | 84.36    | 78.83 |\n| SPSS-Conv inverse     | 89.15 | 79.13    | 78.47 |\n| SPSS-Conv random drop | 89.14 | 83.21    | 78.57 |\n| SPRS-Conv             | 89.64 | 84.26    | 78.91 |\n| SPRS-Conv inverse     | 70.36 | 49.81    | 44.06 |\n| SPRS-Conv random drop | 89.32 | 78.81    | 78.28 |\n\n**(1) Magnitude-base pruning vs Random drop**: compared to magnitude-based pruning, the authors observe using random drop as an indicator will lead to a certain loss in performance (around 2%)。This is caused by the randomness, part of the foreground is discarded, resulting performance degradation. However, the important part still has a 50% chance of being selected, which also guarantees performance to a certain extent.\n**(2) Analysis on random drop**: Randomly dropping points obtain reasonable results on both datasets. This further confirms the proposed observation about the extreme imbalance of foreground and background. Even randomly dropping points, the authors still have a reasonable chance of dropping useless points.  \n**(3) Drawback of random drop**: Despite of its degraded performance, the random drop method also has a certain degree of randomness. This is not desirable in practical applications as it may have the chance to lose some safety-critical areas which will cause problems in safety-critical applications. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "410a70fd-6851-5dd0-b2aa-3deb51f55ae9", "question": "What is the impact of meta-halftone guided network and its motivation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4ebd7c16-d0c9-5223-9e1e-0e2cd34afbf0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the impact of meta-halftone guided network and its motivation?", "reference_answer": "A: The motivation for designing the meta-halftone guided network is to introduce good blue noise properties during halftone dithering. Blue noise is essential for generating high-quality halftones, which avoids noticeable low frequency visual artifacts in the generated halftones by forcing random pixel dithering. To achieve this goal, the meta-halftone guided network constructs a meta-halftone set with the help of the diffusion state vectors of $k$ constant grayscale images, and guides the generation of new halftone dithering patterns through these meta-halftone vectors. \n\nTo verify the effectiveness of the proposed meta-halftone guided network, the authors conduct ablation experiments to evaluate the quality of generated halftones. Specifically, the authors evaluate the generated halftones in terms of tone consistency and structure consistency (Pang et al., 2008; Xia et al., 2021). Tone consistency is calculated by the peak signal-to-noise ratio between the generated halftones and the input continuous-tone images, where the halftones are smoothed by a Gaussian filter kernel. Structure consistency is computed by SSIM metric between the generated halftones and the input continuous-tone images. The authors compare the halftone quality with and without meta-halftone guided networks in quantitative experiments, and the experimental results are shown below.\n\nMethod | Structure Consistency | Tone Consistency\n- | - | -\nMeta-halftone Guided Network | 0.1283 | 24.51\nMeta-halftone Guided Network | 0.1550 | 26.56\n\nIt can be observed that when using the meta-halftone guided network, the generated halftones have higher tone consistency and structure consistency with original continuous-tone images, which verifies the effectiveness of the proposed meta-halftone guided network.\n\nThe pretrained VGG network is used for feature extraction of constant grayscale images, and the authors also perform feature mapping (Eq. 15) on the VGG features to learn task-related feature representations. The reason for using pre-training is to save the consumption of computing resources and training time, making the meta-halftone guided network more lightweight (only contains 81K training parameters). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "32641493-3518-54c4-aee9-0ae25cc01d94", "question": "How do you go from equation 8 to equation 9? and how does q lower bound p?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d93d3d23-a98b-5dbc-9827-e95bb0c84095"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do you go from equation 8 to equation 9? and how does q lower bound p?", "reference_answer": "A: Lower bounding q has the effect of going from equation 8 to equation 9. The derivation was initially omitted since it is merely a special case of prior work, but in hindsight this unduly hinders clarity. As such, we’ve added it to the appendix (with references to the work it’s based on), and referenced it near equations 8 and 9."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bea4efba-1246-5325-af22-2f2b65964152", "question": "Is the only difference between SENTRY and PACMAC the data-augmentation scheme and the loss function?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["71c0db56-6716-5fe2-8b69-ed7a4b489f99"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the only difference between SENTRY and PACMAC the data-augmentation scheme and the loss function?", "reference_answer": "A: No, the crucial distinction between SENTRY and PACMAC is a novel selection strategy used for self-training based on predictive consistency across partial images generated via an attention-conditioned masking strategy."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "06885e99-758e-5a9b-a3fa-6421ed257c72", "question": "Would the added regularizers affect the performance on some applications other than image classification, for example image-to-text retrieval?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would the added regularizers affect the performance on some applications other than image classification, for example image-to-text retrieval?", "reference_answer": "A: Yes, the added regularizers affect the performance on image-to-text retrieval."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ac157942-1002-5691-8580-9b77cd8ef397", "question": "Whether the improved architecture or the method contributes more to the SOTA performance?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9a39f69b-8e46-5cdd-85cf-4a47b2cca194"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Whether the improved architecture or the method contributes more to the SOTA performance?", "reference_answer": "A: The authors performed the suggested ablation study and trained cIMLE using the same architecture the proposed method uses on two tasks (Super-resolution and Colourization) to disentangle the effect of the sampling strategy and network architecture. The authors find that the proposed method still outperforms cIMLE by 33.6% on average with the same network architecture, which validates the effectiveness of the proposed method.\n\nIn addition, the authors retrained various GAN-based baselines (BicycleGAN, MSGAN and MoNCE) with the proposed architecture to further validate the proposed method’s effectiveness. The authors observed that the GAN-based baselines failed to converge when trained from scratch with the proposed architecture, so the authors pretrained their generator using the proposed method which gave them an advantage. The authors show the FID results in the table below.\n\n|                               | Super-Resolution (SR) | Colourization (Col) |\n|-------------------------------|-----------------------|---------------------|\n| BicycleGAN + the proposed architecture | $53.30$               | $66.32$             |\n| MSGAN + the proposed architecture      | $57.94$               | $81.86$             |\n| MoNCE + the proposed architecture      | $31.72$               | $\\underline{27.85}$             |\n| cIMLE + the proposed architecture      | $\\underline{21.13}$               | $42.67$             |\n| CHIMLE                        | $\\textbf{16.01}$      | $\\textbf{24.33}$    |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "32905eaa-3cbb-57ae-ad39-fd13961a7cfa", "question": "Why does MM take an action that deviates from the demonstration in Figure 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a497ab1-d8b9-5733-8bed-60767147adf6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does MM take an action that deviates from the demonstration in Figure 2?", "reference_answer": "A: Because MM is trying to match the expert’s state distribution, it might take an action that deviates from what it saw in the demonstration (the red self-loop). In contrast, in Figure 3, the expert never visits s_x so MM has no incentive to take an action that leads to it."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "89040e9a-4905-5548-a4c6-0b450f9a96d9", "question": "What is the model complexity here? I would like to see the duration of model training compared with baselines. Can the proposed method can be scaled up to large complex systems?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["368314b3-9259-5022-a0f3-fd17a4b571c2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the model complexity here? I would like to see the duration of model training compared with baselines. Can the proposed method can be scaled up to large complex systems?", "reference_answer": "A: The authors summarize the duration of model training (in hours) in the following table (averaged results of 10 rounds):\n\n| Methods | LI   | LL   | CY   | BF   | TF   | BF-CV | NetSim1 | NetSim2 | NetSim3 | Springs | Particles | Kuramoto |\n| ------- | ---- | ---- | ---- | ---- | ---- | ----- | ------- | ------- | ------- | ------- | --------- | -------- |\n| iSIDG   | 48.2 | 50.6 | 40.8 | 44.7 | 40.3 | 44.0  | 20.7    | 36.9    | 50.8    | 42.2    | 36.0      | 39.2     |\n| NRI     | 14.3 | 18.2 | 13.0 | 15.5 | 13.6 | 16.9  | 8.8     | 16.0    | 21.5    | 20.1    | 20.3      | 19.8     |\n| fNRI    | 15.5 | 21.9 | 14.9 | 18.6 | 13.7 | 18.0  | 9.0     | 17.8    | 25.6    | 22.8    | 20.7      | 19.0     |\n| MPRI    | 5.0  | 14.4 | 3.6  | 8.0  | 5.5  | 7.9   | 2.1     | 5.6     | 9.5     | 7.9     | 6.6       | 6.3      |\n| ACD     | 40.5 | 42.8 | 39.6 | 44.0 | 41.7 | 43.2  | 20.5    | 35.8    | 45.7    | 39.8    | 36.4      | 38.0     |\n\nAs for larger datasets, the authors tested \"Springs100\", \"ESC\", and the \"HSC\" mentioned in the response letter. The authors summarize the duration of model training (in hours) in the following table (averaged results of 10 rounds for \"Springs100\" and \"ESC\"; averaged results of 3 rounds for \"HSC\").\n\n| Methods | Springs100 | ESC  | HSC  |\n| ------- | ---------- | ---- | ---- |\n| iSIDG   | 106.5      | 96.8 | 50.3 |\n| NRI     | 40.6       | 39.4 | 30.4 |\n| fNRI    | 49.0       | 42.0 | 31.8 |\n| MPRI    | 20.7       | 19.5 | 12.0 |\n| ACD     | 82.4       | 80.4 | 51.8 |\n\nAccording to the results presented above, iSIDG seems to suffer from the problem of scalability, which is also the case for other VAE-based methods (NRI, fNRI and ACD). The authors mentioned this as the limitation of iSIDG in the conclusion section of the paper. The iterative process of iSIDG leads to more time consumption during model training (still comparable with ACD). The main goal of this work is to design an **effective** method for structural inference: iSIDG achieves more accurate inference results than baseline methods on most of the datasets; and for larger networks the inference accuracy of iSIDG gets even better than any of the baseline methods."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8fbaf438-b7dd-5709-83b9-1291877c9e77", "question": "As far as I can see, GD in deterministic settings is slightly different from GD in stochastic settings. How do you guarantee that SGD+teleport can accelerate convergence over SGD?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["580aabad-80be-5a4f-9f9b-03c23404e02b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As far as I can see, GD in deterministic settings is slightly different from GD in stochastic settings. How do you guarantee that SGD+teleport can accelerate convergence over SGD?", "reference_answer": "A: Whether teleportation accelerates the convergence of SGD depends on the data the authors use. \nThe expected change in loss and convergence is related to the variance in the data as well as minibatch sizes. In the proposed experiments, the authors observe that even teleporting using a small number of data points (e.g. 80 images in MNIST) is able to improve the convergence for the objective function trained on the entire dataset. This suggests that the loss landscape created by samples of data is similar to the landscape created using all available data. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2de986fb-3566-5bde-9a14-dc7a40b0f2a1", "question": "Explanation for the additional equations $\\boldsymbol{p}=\\nabla \\boldsymbol{u}$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f0e462e0-334e-5028-b737-be0499cbe403"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explanation for the additional equations $\\boldsymbol{p}=\\nabla \\boldsymbol{u}$.", "reference_answer": "A: With the proposed hard-constraint framework, the authors can enforce the BCs (see Eq.(9)) with the price of $nd$ additional \"soft constraints\" which are only enforced by the loss function as you pointed out. The change in the number of \"soft constraints\" is $nd$ minus the number of BCs. As the authors mentioned in **the last paragraph in Section 3.3**, the number of BCs is far larger than $nd$ in geometrically complex systems. Therefore, the framework can reduce the total number of \"soft constraints\" in such systems.\n\nBesides, even if the number of BCs is lower than $nd$, the authors empirically find that the proposed framework can still significantly improve the accuracy in the experiment of high-dimensional heat equation (see **Section 5.4**, where $n=1, d=10$ and the number of BCs is 2). The authors speculate that it may be due to the fact that competition between PDEs and BCs is greater than that between PDEs. Specifically, as the authors discussed in **the last paragraph in Section 2.1**, the convergence speed of PDE losses is quite different from that of BC losses. Since here the authors \"replace\" the BCs with additional PDEs $\\boldsymbol{p}_j = \\nabla u_j$, the proposed framework may be beneficial to reducing the unbalanced competition."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "78051fab-32ef-5479-9139-32c8b210ec3b", "question": "Why does the paper combine two dynamic thresholds that exhibit positive and negative correlations with the average membrane potential?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["b73ceeab-4bcd-5261-8f93-2810b1cbdbab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does the paper combine two dynamic thresholds that exhibit positive and negative correlations with the average membrane potential?", "reference_answer": "A: The positive and negative correlations in the proposed method are motivated by Fontaine et al.[16] who found that the spike threshold was positively correlated with the average membrane potential preceding spikes and negatively correlated with the rate of depolarization. The authors emphasize that DET leverages the _magnitude of the membrane potential_ to estimate a threshold, while the DTT is based on the _preceding rate of depolarization_. Eqs. 2-4 provide mathematical formulations for DET, also illustrated in Figure 1b. Eqs. 5-6 formalize DTT along with illustrations in Figure 1c."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d23feda8-2059-5339-b721-acc3c2230984", "question": "In the equation right below eq. (5), I think $\\Phi_{i}^{t_{i}}$, $t_{i}$, $T_{i}$ should be $\\Phi_{j}^{t_{j}}$, $t_{j}$, $T_{j}$ respectively, yes?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d1767e5b-a16e-56b6-ad77-dfb263bd2c06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the equation right below eq. (5), I think $\\Phi_{i}^{t_{i}}$, $t_{i}$, $T_{i}$ should be $\\Phi_{j}^{t_{j}}$, $t_{j}$, $T_{j}$ respectively, yes?", "reference_answer": "A: Following the suggestion from Reviewer RnFj,\nthe authors will add some explanation on an approximated problem to be solved by the proposed algorithm\n(i.e., Problem (5) for the trilevel case) by assuming a simple setting,\nwhere the authors apply the steepest descent method for the lower-level problems with the same iteration number $T$ and stepsize $\\alpha$ for all levels as follows:\n\\begin{equation}\n\\begin{alignedat}{2}\n\\min_{x_{1} \\in S_{1}, \\\\{x_{2}^{(t)}\\\\}, \\\\{x_{3}^{(t)}\\\\}}{}&\n        f_{1}(x_{1}, x_{2}^{(T)}, x_{3}^{(T)})\\\\\\\\\n\\text{s.t. }&\n        x_{2}^{(t)} = x_{2}^{(t - 1)} - \\alpha\n        \\nabla_{x_{2}} \\tilde{F}\\_{2}(x_{1}, x_{2}^{(t - 1)}, x_{3}^{(T)})&\n            \\quad& (t = 1, \\dots, T),\\\\\\\\\n&         x_{3}^{(t)} = x_{3}^{(t - 1)} - \\alpha\n        \\nabla_{x_{3}} \\tilde{F}\\_{3}(x_{1}, x_{2}^{(T)}, x_{3}^{(t - 1)})&\n            \\quad& (t = 1, \\dots, T).\n\\end{alignedat}\n\\end{equation}\nthe authors will explain more on\n\"if  it  is  extended  recursively using $T$ steepest-descent sequential updates at each level,\nthe problem size can be increased with $T^{n}$ variables\"\nby showing the corresponding formulation:\n\\begin{equation}\n\\begin{alignedat}{2}\n\\min_{x_{1} \\in S_{1}, \\\\{x_{2}^{(t)}\\\\}, \\\\{x_{3}^{(t_{1}, t_{2})}\\\\}}{}&\n        f_{1}(x_{1}, x_{2}^{(T)}, x_{3}^{(T, T)})\\\\\\\\\n\\text{s.t. }&\n        x_{2}^{(t)} = x_{2}^{(t - 1)} - \\alpha \\nabla_{x_{2}} \\tilde{F}\\_{2}(x_{1}, x_{2}^{(t - 1)}, x_{3}^{(T)})&\n            \\quad& (t = 1, \\dots, T),\\\\\\\\\n&        x_{3}^{(t_{1}, t_{2})} = x_{3}^{(t_{1}, t_{2} - 1)} - \\alpha \\nabla_{x_{3}} \\tilde{F}\\_{3}(x_{1}, x_{2}^{(t_{1})}, x_{3}^{(t_{1}, t_{2} - 1)})&\n        \\quad& (t_{1} = 1, \\dots, T; t_{2} = 1, \\dots, T)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "814f5f04-d5a1-55a2-bd30-14c22b7786fc", "question": "What does FR for Task 1 indicate? Is there a Task 0 then? The comparison is not clear.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["36883baf-d01c-533a-b3b4-376fb580e945"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does FR for Task 1 indicate? Is there a Task 0 then? The comparison is not clear.", "reference_answer": "A: The authors have defined the Forgetting Rate (FR) in Lines 209--212 of the proposed main paper. Note that the results in Table 1 are obtained by the final model $M_5$. Therefore, according to the proposed definition, the FR for Task 1 is the performance degradation on Task 1 when the model is trained after all 5 tasks (i.e., $A_1^1 - A_1^5$)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f6961db8-dc2f-5fe0-a062-c92abb889da3", "question": "Could you elaborate on the heuristic part of the search space design? The prior work mentioned in Line 129 is not evaluated by Transformer architectures, so I doubt inductive bias from this work is valid to apply.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you elaborate on the heuristic part of the search space design? The prior work mentioned in Line 129 is not evaluated by Transformer architectures, so I doubt inductive bias from this work is valid to apply.", "reference_answer": "A: The details of the search space is summarized in Table 1. In general, the authors partition the whole Transformer search space into $K=3$ sub-spaces such that each sub-space covers different sizes of student models (by number of parameters) depicting Tiny, Small and Base model sizes. Given a BERT-sized teacher model (109$M$ params), the authors roughly set the partition thresholds for Tiny, Small and Base sizes at 10$M$, 40$M$ and 80$M$ params. From Table 2 (\\#Para column), the authors observe that each partition contains compressed models from prior work -- allowing us to fairly compare the models in each partition on accuracy vs. params/FLOPs. \nFor the proposed search space, each partition still contains thousands of candidate subnetworks not all of which are useful. Now, the authors leverage two primary heuristics (lines 129-136): (i) the authors constrain all layers in a sampled student subnetwork to be homogeneous i.e., the same number of attention heads, hidden dimension, etc. This not only reduces the search space, it is also more friendly to hardware and software frameworks. (ii) Motivated by previous work [1-2] showing that thinner and deeper neural networks have better representation capabilities and perform better than wider and shallower neural networks, the authors designed sub-spaces with deeper layers (e.g., $4-7$ for Tiny, $9-12$ for Small and Base) and compute the range of hidden dimensions to meet the overall model parameter budget in each partition. Additional constraints arise from Transformer design principles, for instance, hidden size is always a multiple of the number of attention heads. While the above steps require enumeration of different subnetwork architectures, this is typically fast given an algebraic expression to compute model parameters as a function of layers, heads, hidden size, etc. (included in submitted source code), does not require any training, and a one-time process depending only on the teacher model architecture. The authors will add this discussion in paper.\nA more relevant reference for the Transformer architecture is given in [2]. Please refer to Figure 2(b) in [2] that shows the impact of Transformer depth on MNLI accuracy given similar overall model parameters.\n\n[1] Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., \\& Bengio, Y. (2015). Fitnets: Hints for thin deep nets. In ICLR.\n[2] Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., \\& Gonzalez, J. (2020). Train big, then compress: Rethinking model size for efficient training and inference of transformers. In ICML."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "83a49aad-8e0f-5aca-a388-2209e398fa31", "question": "Would it not be better if the combinations of modality discrepancy reduction losses and id loss, as the combination of losses sometimes can have larger impact than single ones?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["17e87869-0815-5e88-9ed2-3cb8faaf55ec"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Would it not be better if the combinations of modality discrepancy reduction losses and id loss, as the combination of losses sometimes can have larger impact than single ones?", "reference_answer": "A: Yes, as stated in Eq. (7) and Section 4.2 (Line 255) in the paper, the authors employ the combination of modality discrepancy reduction losses and id loss during training. Model performances in Table 4 prove \"the combination is better than single ones\"."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6a6e81ec-f8ec-5301-8ede-81507dcc9d0a", "question": "What is the accuracy of ResNet-18 training from scratch on ImageNet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8ab99797-77c2-571c-8536-d5df28e7f652"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the accuracy of ResNet-18 training from scratch on ImageNet?", "reference_answer": "A: 69.758%"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4e0c064e-f239-52ce-b780-0e8e7add9ce5", "question": "How did the authors extract the training data for CEN?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["426ea220-62b5-5f9a-b6c3-7a6e2c2814ca"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How did the authors extract the training data for CEN?", "reference_answer": "A: The authors extract meta-commands from game replay authorized by the game provider, which are consist of high-level (top 1%) license data without identity information. The input features of CEN are shown in Table 4 (Appendix). The game replay consists of multiple frames, and the information of each frame is shown in Figure1 (Appendix). The detailed training data extraction process is as follows:\n- First, the authors extract the trajectory $(s_0, s_1, ..., s_N)$ from the game replay, where $N$ is the total number of frames.\n- Second, the authors randomly sample some frames {$t | t \\in ${ $0,1,\\dots,N$}} from the trajectory $\\tau$.\n- Third, for each frame $t$, the authors extract feature $o_t$ from state $s_t$.\n- Fourth, the authors extract the label $m_t$ from the state $s_{t+T^{mc} }$ in frame $t+T^{mc}$, i.e. describe the state using the meta-command space $M$.\n- Finally, $<o_t, m_t>$ is formed into a training pair as a sample in the training data.\n\nSince meta-commands are generic to MOBA games, the above rules can easily be extended to new MOBA games."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "198b6e15-68d9-5629-b447-7b9dcf1954c4", "question": "The surrogate model is trained in a contrastive manner. Can other self-supervision tasks like rotation be used to train it?(ref. Unsupervised Representation Learning by Predicting Image Rotations)", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The surrogate model is trained in a contrastive manner. Can other self-supervision tasks like rotation be used to train it?(ref. Unsupervised Representation Learning by Predicting Image Rotations)", "reference_answer": "A: The authors agree that exploring different strategies to train the shallow model is exciting for further improvement of the performance of lightweight black-box attacks, as shallow layers play an important role in lightweight black-box attacks. Thus, the authors generate adversarial examples using EFT with shallow layers trained with a rotation prediction task [2] and report the results in [TABLE 2-3]. The authors can see that shallow layers trained with the rotation prediction task is slightly worse than using the contrastive strategy, but the performance can also reduce the model accuracy significantly."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9c515fe6-c152-51b7-a0ed-f5ff9f3ab12c", "question": "What is the major difference between AutoDistil and [r1] ?\nReference:\n[r1] Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, and Tian Guo. Few-shot neural architecture search. In International Conference on Machine Learning, pages 12707–12718. PMLR, 2021.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the major difference between AutoDistil and [r1] ?\nReference:\n[r1] Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, and Tian Guo. Few-shot neural architecture search. In International Conference on Machine Learning, pages 12707–12718. PMLR, 2021.", "reference_answer": "A: The main differences as also highlighted under **novelty** consist of the following:\n\n- **Fully task-agnostic SuperNet training**. {AutoDistil} training is fully task-agnostic in contrast to Zhao et al. [r1] that uses task-specific NAS. Task-agnostic NAS is challenging since the authors do not have access to task labels during training and the authors want to show generalization on evaluating diverse downstream NLU tasks in the GLUE benchmark. AutoDistil leverages self-attention distillation that is an unsupervised training objective. Incorporating self-attention loss for training and distillation in NAS is non-trivial as it requires aligning attention states of diverse student subnetworks and the large teacher model. The authors develop an extraction and alignment strategy (Section 3.2) to address this challenge.\n\n- **NLP vs. CV domain**. AutoDistil works on the NLP domain with the Transformer architecture (see Figure 2) under the pre-training and fine-tuning paradigm, while reference [1] works on the CV domain with a CNN architecture with different design and search spaces. Different from CV domain, NLP tasks have different objectives and evaluation metrics for classification (e.g., MNLI), regression (e.g., STS-B) and correlation (e.g., CoLA). Overall, the search space design (Section 3.1), SuperNet training with distillation and sub-network extraction strategy (Section 3.2) and search strategy (Section 3.3) are all quite different. While the authors briefly discuss these differences (lines 53-70), the authors will add a more elaborate discussion."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "51c9858e-31b2-58d7-82d7-6cf50dcc8fa0", "question": "Why are the generated images blurry and have unreasonable structures (we can observe clear unreasonable structures for the human hands or faces)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9614a57f-96f6-5e08-8978-97dc94a17579"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are the generated images blurry and have unreasonable structures (we can observe clear unreasonable structures for the human hands or faces)?", "reference_answer": "A: The area is indeed developing very fast, and the recent DALL-E2, Imagen (after submission) and Parti (after submission) show better quality. However, The current text-to-image model is a large project, the final performance depends on many things, e.g. data, framework, resolution, parameters, et al. the proposed work gives a concrete solution for a certain aspect -- the generation of high-resolution autoregressive models. In the proposed opinion, this should also be encouraged. The authors discussed the way to improve the proposed model in section 6, and the lack of deep text understanding revealed in Imagen might be the main reason of the gap, which is orthogonal to the contribution in this paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0aecf278-2667-5c9b-9e9e-207a8a73fb5d", "question": "Authors mentioned that the attributes are not independent of each other, which is a major challenge. Could you discussion or provide experimental data on how this affects the process and how did the current algorithm address it.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["c498bea5-f9e3-5959-be77-1c0caaf515c9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Authors mentioned that the attributes are not independent of each other, which is a major challenge. Could you discussion or provide experimental data on how this affects the process and how did the current algorithm address it.", "reference_answer": "A: A typical example of conflicting attributes are any color attributes that are rendered next to each other in the image. To address this issue in the refinement loop the authors use a simple heuristic which ensures that all attributes have different values when used as input to the refinement loop."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1d1046e9-f6b3-5253-a582-2e30eaf0163c", "question": "Insights on part localization ability and quantitative evaluation.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b450247e-0111-5e71-9a4f-e759f075be1b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Insights on part localization ability and quantitative evaluation.", "reference_answer": "A:  DPPN actually aggregates attribute-related clues from the visual features of a targeted image into attribute prototypes, so that the updated attribute prototypes can be corrected. This makes the prototypes adapted to the target image and produce better attribute localization.\n\nFollowing the evaluation settings in APN [Xu2020], the authors test the proposed DPPN with the evaluation metric PCP on CUB. Mean PCP of DPPN is $86.4\\%$, which is better than previous methods ($61.5\\%$ of SGMA [Zhu2019] and $84.7\\%$ of APN [Xu2020]).\n\n "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a73f6975-88ac-59ca-b036-bbcf2435226b", "question": "The impact of ARM on bigger image sizes.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e070ad3b-180d-5e8b-a978-d27d1aeaec36"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The impact of ARM on bigger image sizes.", "reference_answer": "A: To study that, the authors train another SwinT with $384 \\times 384$ input on ImageNet-1k, as well as its ARM variant. The validation accuracy on ImageNet-1k is Swin-T: $82.1$ VS Swin-T+Ours: $82.9$. The authors can see that bigger input sizes still suffer from aliasing. According to the Nyquist-Shannon sampling theorem, the aliasing issue occurs when the sampling rate is lower than 1/2. From this perspective, the problem still exists for larger image sizes due to the downsampling.\n\n| Model           | Input size |Top-1 Acc |\n| --------------- | ------------------ |------------------ |\n| Swin-T    | $384 \\times 384$ |     82.1        |\n| **Swin-T w Ours**   | $384 \\times384$ |   **82.8**          |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "06817d46-9bc7-59ef-a6ed-1c5bcbc7c9b7", "question": "Can you clarify if $z^*$ is a gradient update function on z? Does $z^*$ have any parameters?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you clarify if $z^*$ is a gradient update function on z? Does $z^*$ have any parameters?", "reference_answer": "A: Indeed $z^*$ is an I-FGSM updating function on $\\mathbf{z}$ i.e. an implicit function achieved by several I-FGSM updates. Please refer to Eq. 6 for the formulas of one single update. $z^*$ has no parameters as it does not involve any additional network."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d0f7b963-65f7-5ecd-9f20-c3b126002486", "question": "Does the dimensionality of the dataset affect the bounds?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["7acfb245-9b0c-55fc-87bf-eaa4d724876e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the dimensionality of the dataset affect the bounds?", "reference_answer": "A: The dimensionality of the dataset does not enter at any point in the proposed equations. Please see table 1 in Appendix A for sizes and dimensionalities of the datasets that the authors considered in the experiments and Appendix B.3 for the behavior of the bounds in these datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b1ca9886-910e-5f07-a09b-9e0d50e0687b", "question": "What magnitudes do levels 0-6 in Figure 2 correspond to for each type of noise? E.g. for Gaussian noise, what levels of noise are considered? Same for Tables 2 and 3.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["823a2e85-8548-57a2-8256-5fdb0159c2af"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What magnitudes do levels 0-6 in Figure 2 correspond to for each type of noise? E.g. for Gaussian noise, what levels of noise are considered? Same for Tables 2 and 3.", "reference_answer": "A: In the proposed experiments, the authors use the CIFAR-C and ImageNet-C data. The noises are added to the clean data with pixel values in the range of [0, 1]. The specific noise parameters from severity level 1-5 are as follows. For the gaussian noise, the standard deviation is 0.08, 0.12, 0.18, 0.26, 0.38. For the shot noise, the value of parameters are 60, 25, 12, 5, 3. For the impulse noise, the amount of s&p impulses are 0.03, 0.06,"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "901c2315-4a72-5c6b-a399-c036b4299337", "question": "I am confused with the last column of Table 1. Does it denote the ratio between $B_1$ and $B_2$. For BSGD and BSpiderBoost, there are two Big O notations, but for other methods, there is only one Big O notation. What is the exact meaning?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["12c1f7a8-59c1-5c99-963b-2113f33ce216"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am confused with the last column of Table 1. Does it denote the ratio between $B_1$ and $B_2$. For BSGD and BSpiderBoost, there are two Big O notations, but for other methods, there is only one Big O notation. What is the exact meaning?", "reference_answer": "A: $B_1$/$B_2$ means $B_1$ and $B_2$. When $B_1$ and $B_2$ are on the same order, e.g. $\\mathcal{O}(1)$ for SOX and the proposed method, the authors only give one Big O notation. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a3c17e1a-600f-5850-a549-31dd13031252", "question": "What is the number of FS terms used when conducting experiments on ImageNet? Is it the same with that used for CIFAR-10?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["f48ec53c-2311-5560-884e-f1448e2a4e1c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the number of FS terms used when conducting experiments on ImageNet? Is it the same with that used for CIFAR-10?", "reference_answer": "A: The authors use the same setting as in CIFAR-10 (as shown in Line 276-277) but a different start number $n_s$. When conducting experiments on ImageNet, the authors use $n_s=6$. This will be added in the final version of the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "cdd58ba3-ff04-5662-afb0-a58a8e726a23", "question": "According to you, what is the ideal benchmark that the authors should want to test the relevant questions?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a645ccfb-54fe-565b-a61f-7352b502b757"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "According to you, what is the ideal benchmark that the authors should want to test the relevant questions?", "reference_answer": "A: Different time lengths of a task: whereas playing one PHYRE task usually takes 10-15 seconds, an ideal benchmark can contain different tasks of multiple time lengths, which will lead to further discussion of how LfD performs under different numbers of the predicted frames.\nA: While PHYRE is based on 2D dynamics, the ideal benchmark can expand to 3D or even be based on the real-world scene while reserving the complexity and variety of PHYRE.\nA: The reward of PHYRE is binary, and there is only one winning condition per task, while an ideal benchmark can return a continuous number from 0 to 1, or return different rewards according to different conditions, which may help to figure out the internal mechanism of LfI.\nA: The authors also expect novel evaluation metrics; AUCCESS might encourage extensive sampling and evaluation, while the ideal one is measured on how many actions are needed until the problem is solved. Online learning might get involved in this process."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "178ea258-68be-5e47-beec-7d84d912fa74", "question": "In Figure 1A, if one draws a vertical line for a fixed alpha_prune, does that represent accuracy/test error at iso-dataset size for various experiments? If yes, how is that possible given that alpha_tot is fixed. If no,...", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7de55424-0d66-5dfd-9b68-f611c6c28c8c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Figure 1A, if one draws a vertical line for a fixed alpha_prune, does that represent accuracy/test error at iso-dataset size for various experiments? If yes, how is that possible given that alpha_tot is fixed. If no,...", "reference_answer": "A: Correct, as the authors explained above in the proposed answer about parsing Fig. 1, a vertical line represents a fixed pruned dataset size. This is possible because when using synthetic data, the authors have the luxury of generating as much data as the authors want (varying \\alpha_{tot}) to investigate how data pruning scales to ever-larger dataset sizes. For fixed \\alpha_{tot} see Fig. 3A, where the solid lines represent a fixed dataset size (in analogy to a fixed dataset like CIFAR-10 or ImageNet) while the dotted lines extrapolate the behavior to larger dataset sizes (indicating how the authors would expect scaling to behave if the authors could draw more samples from CIFAR-10 or ImageNet)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e0aa4215-9d75-54f3-bddb-5318c0ebf2be", "question": "Are there any difficulties applying the method to other vision problems, such as detection and segmentation? What are the major difficulties of generalizing the method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there any difficulties applying the method to other vision problems, such as detection and segmentation? What are the major difficulties of generalizing the method?", "reference_answer": "A: Generalizing the proposed method to other vision problems or to other domains like text or video seems to be an interesting future direction. The authors do not see significant difficulties when applying to other problems as long as the authors are in a supervised learning setting and the authors can formulate a regression problem. For example, for the image segmentation task where both the inputs and labels are images, the authors can still compute the Gram Matrix of inputs using the neural network feature and use KRR to compute the target labels (image) based on the training labels (images)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "94af7219-bfa1-57dd-a36f-1aaaec37a611", "question": "Provide the claims for the following from the paper: benefit of cluster and separation loss. ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["36305be5-629e-5348-9c0a-d8033c5e875f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide the claims for the following from the paper: benefit of cluster and separation loss. ", "reference_answer": "A: The losses are directly adopted from Chen et al. [r1], who provide evidence for their ability to improve prototypical representations.\n[r1] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su, \"This looks like that: Deep learning for interpretable image recognition.\" NeurIPS 2019"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6fceb99f-9bd7-54ad-9f36-2f14263617ab", "question": "Comparisons. How does the method differ in performance to previous methods with similar ideas?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Comparisons. How does the method differ in performance to previous methods with similar ideas?", "reference_answer": "A: Following the reviewer’s suggestion, the authors further compare the proposed stimulative training with different methods including layer/stage supervision, Self-Distillation [7] and Stochastic Depth [8]: (1) The comprehensive comparisons are shown in Table r1. As the authors can see, layer supervision and stochastic depth can improve both the performance of the main network and the average performance of all subnetworks, stage supervision and self-distillation can only improve the performance of the main network, while the proposed stimulative training can achieve the highest performance of main network and the highest average performance of all subnetworks. (2) As shown in Fig. r8 (a), (b), (c) and (d) of the revised supplementary, the proposed stimulative training can better relieve the network loafing problem than all the other methods. (3) As shown in Fig. r4, r5, r6 and Fig. r7 of the revised supplementary, the proposed stimulative training can provide stronger robustness in resisting various network destruction operations than all the other methods. \nBesides these experimental results, the authors find that: 1) The improved performance of Stochastic Depth can be also interpreted as relieving the loafing problem defined in this work; 2) the proposed stimulative training is actually complementary to layer/stage supervision and Self-Distillation, and their combinations can be a worthy research direction in the future.\n\n[7] Zhang L, Song J, Gao A, et al. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation[J]. arXiv preprint arXiv: 1905.08094, 2019.\n[8] Huang G, Sun Y, Liu Z, et al. Deep networks with stochastic depth[C]//European conference on computer vision. Springer, Cham, 2016: 646-661.\n\n**Table r1: Comparisons**\n|Method|Time|Memory|Main(%)|All(%)|\n|:--------------|:----------- |:------ |:-----------|:----|\n| CT|        16.91h|        3291MiB        |77.39        |55.26±13.37|\n| CT + layer supervision |        23.3h|        7193MiB        |78.77|        59.18±11.12|\n| CT + stage supervision |        19.3h|        5197MiB        |78.59|        54.82±13.31|\n| Self-Distillation        |26.8h|        3887MiB        |79.59        |50.39±14.22 |\n|Stochastic Depth|        13.6h|        3291MiB        |78.43|        70.72±3.76|\n|ST        |24.08h        |3291MiB|        81.07|        80.01±0.59|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8e16c00b-3468-524d-b69a-68ccf7641436", "question": "What does the term \"generative expression\" stand for? Why is it necessary for justifying the perturbation-based method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does the term \"generative expression\" stand for? Why is it necessary for justifying the perturbation-based method?", "reference_answer": "A: The generative expression stands for the main content of the generated image. The authors model the $\\mathbf{z}$ transform process as making perturbations to the original sampling since $z^*(\\mathbf{z})$ shall not depart much from $\\mathbf{z}$ as the authors hope the main content of the generated image remains the same. The hope that small perturbations can achieve considerable positive quality variation leads us to the adversarial sample mining methods. There can be other designs of $z^*$, but the authors believe adversarial sample mining methods are one of the effective solutions (we have also tried raw gradient updating, but it performs much worse), and note that qualitative/quantitative results in Table 4 and Figure 4 have verified its effectiveness."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aee3240a-369f-5c62-b5c5-f406b21da6b6", "question": "Is there a combination of attack types or step sizes that are clearly selected a vast majority of the time? If so, does provide a considerable improvement over just using this combination of attack parameters? Or are there classes or datasets that are more vulnerable to certain attack combinations?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8c7b8e19-117d-518a-a322-70b42ff36147"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there a combination of attack types or step sizes that are clearly selected a vast majority of the time? If so, does provide a considerable improvement over just using this combination of attack parameters? Or are there classes or datasets that are more vulnerable to certain attack combinations?", "reference_answer": "A: The authors analyze the selected attacks from the perspective of blocks with different steps and datasets.\nThe first and final perturbation blocks of 10-step $A^2$ in CIFAR-10 are chosen for analysis.\nFigures in Appendix B.5 show the distribution of selected attacks of different perturbation blocks.\n- **Perturbation Block 1:** $A^2$ tends to choose FGM, FGSM, and partially random methods as initialization in the first step.\nThe momentum-based attack methods are quickly discarded as the gradient of the previous step is absent.\nFGSM is chosen more frequently due to its stronger attack on both foreground and background.\n- **Perturbation Block 10:** The optimization of the victim model leads to changes in the distribution of selected attacks in the last block.\nIn the early stage of training, the victim model is vulnerable.\n$A^2$ retains the diversity and plays the role of friendly attackers like FAT[5].\nAt the end of the training, $A^2$ prefers the momentum-based attacks (i.e., FGSMM and FGMM).\nFrom the perspective of datasets, SVHN and CIFAR-10 prefer different attack methods.\nSVHN discards FGSMM, which is most frequently used in CIFAR-10, and pays more attention to FGMM.\nIn summary, $A^2$'s preference for selecting attacks in blocks varies according to the block step, dataset, and victim model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3155b62e-e751-5462-8b86-d87b863c6cb0", "question": "Why is no augmentation applied during training? Shouldn't this prevent overfitting too?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is no augmentation applied during training? Shouldn't this prevent overfitting too?", "reference_answer": "A: Yes, the authors agree that data augmentation during training can alleviate the overfitting problem, but finding the correct data augmentation can be complex. The authors not only need to consider the data augmentation for two different stages (i.e., meta-gradient computation and online model update), but the authors also need to consider the data augmentation for two different data types (i.e., distilled data and real data). In the proposed experiments, the same kind of data augmentation, cutout (https://arxiv.org/abs/1708.04552), for example, can improve the performance if applied to the online model update but hurt the performance if it is applied to the meta-gradient computation. Besides, different datasets may require different data augmentations. For example, the authors may want to use image flipping for datasets involving natural images but not for datasets containing digits like MNIST. Moreover, the optimal hyperparameters for different transformations (e.g., color jittering and scaling) are different and need to be tuned separately for each dataset. \n\nIn the proposed experiments, the authors found that applying the correct data augmentation could improve the final test performance by around 1–3%, especially when the authors distilled more data points. In Appendix D, the authors discuss the training time data augmentation and several other tricks that can improve the model's performance but are not included in the current method. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d555454b-6467-5cb4-86ec-9fe1bb412ef0", "question": "What is the statistical relevance of the results in Figure 6?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the statistical relevance of the results in Figure 6?", "reference_answer": "A: The statistics in Figure 6(left) shows the ratio of reliable knowledge points $r_{\\text{reliable}}=\\frac{\\verb|#|\\text{ of reliable knowledge points}}{\\verb|#|\\text{ of all knowledge points}}$ and the ratio of unreliable knowledge points $\\frac{\\verb|#|\\text{ of unreliable knowledge points}}{\\verb|#|\\text{ of all knowledge points}}$ in each layer of the trained DNN. The authors found that the ratio of reliable knowledge points increased through the forward propagation. This demonstrated the increasing quality of visual patterns through the forward propagation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9e784d1c-3fc2-576c-9bfb-88886ffb98bb", "question": "What are the zero-shot Top-1 classification accuracy results on the series of challenging datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the zero-shot Top-1 classification accuracy results on the series of challenging datasets?", "reference_answer": "A: | Top-1       | IN-1K | IN-V2 | IN-Sk | IN-A | IN-R |\n|-------------|-------|-------|-------|------|------|\n| CLIP (4M)   | 22.0  | 18.3  | 13.0  | 4.8  | 27.4 |\n| CyCLIP (4M) | 24.4  | 20.6  | 14.8  | 5.4  | 30.4 |\n| Gain (%)       | **11.1**  | **12.7**  | **13.6**  | **10.8** | **11.0** |\n\n*Abbreviations: IN-1K - ImageNet-1K, IN-V2 - ImageNet V2, IN-Sk - ImageNet-Sketch, IN-A - ImageNet-A, IN-R - ImageNet-R*"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "459b43a7-df3f-5567-9173-82de20cba4cf", "question": "Table 1 is really confusing: what do NPD, WT, i_dec stand for?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2a7b14b6-33c1-505a-8a3a-d9580ad76ca7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Table 1 is really confusing: what do NPD, WT, i_dec stand for?", "reference_answer": "A: - NPD: noisy parallel decoding used for beam searching in previous NATs work, such as Gu et al.[1] \n- WT: weak teacher version of Transformer which has a similar performance to the Transformer used in Gu et al.\n- The i_dec is the number of iterations used for iterative refinements.(Lee et al. [2])"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ab9d3ff-7658-5a3f-87d0-d6719554a1a7", "question": "Can the adder attention be applied on NLP tasks?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["07d1c15e-8753-53a5-bd7a-d26f69ac512b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can the adder attention be applied on NLP tasks?", "reference_answer": "A: For NLP tasks, the authors conduct experiments on machine translation task: WMT’14 En-De, consisting of 4.5M pairs of training sentences, respectively. The authors apply 32K source-target BPE vocabulary, train on WMT’16, validate on newstest2013 and test on newstest2014, replicating [1]. The proposed baseline models are Transformer [2] with the [3] implementation. For evaluation, the authors use beam four and length penalty 0.6. All BLEUs are calculated with case-sensitive tokenization. The authors test the model with the lowest validation set loss for the task. \nOur training settings are in line with [1]. The authors apply Adam optimizer and a cosine learning rate (LR) scheduler, where the LR is linearly warmed up from $10^{-7}$ to $10^{-3}$, and then cosine annealed. \nIn the following Table the authors compare various aspects of Adder Transformer with Transformer baselines. \n\n|      Method       |  #Mul  |  #Add  | Energy(pJ) | BLEU |\n| :---------------: | :----: | :----: | :--------: | :--: |\n|  Transformer[2]   | 0.338B | 0.338B |   1.55B    | 25.1 |\n| Adder Transformer | 0.112B | 0.563B |   0.92B    | 24.3 |\nAdder Transformer can obtain an about 1.7× reduction on energy consumption of the Transformer model from 1.55BpJ to 0.92BpJ at the cost of little performance loss on WMT’14 En-De task, compared with the baseline with massive multiplications.\n[1] Felix Wu, Angela Fan, Alexei Baevski, Y ann Dauphin, and Michael Auli. 2019b. Pay less attention with lightweight and dynamic convolutions. In International Conference on Learning Representations.\n[2] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Conference on Neural Information Processing Systems.\n[3] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ec8d52fd-4073-5058-8ba6-d80a1b335961", "question": "Can focal loss work on KD for 3D detection?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["d5ad3544-6364-57a9-b777-ab5e2cf224b7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can focal loss work on KD for 3D detection?", "reference_answer": "A: Focal loss is a defacto selection in 3D object detection to solve foreground/background imbalance and is already equipped in the supervised training objective in all the proposed trained models.\n\nA: As far as the authors know, focal loss is not widely employed as a distillation loss for 2D object detection as shown in Mimicking [22], FG [42], FGD [G], etc. Still, the authors implement a focal distillation loss similar to the supervised loss. The experimental results are shown in the following table. The proposed PP logit KD is around 0.7\\% higher than focal loss on CP-Voxel-XS. As for CP-Pillar-v0.64, since the capability difference between teacher and student are large, focal loss even suffers performance degradation compared to vanilla KD, while the proposed PP logit KD consistently brings performance boost."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c00a33be-d58b-53f7-96ef-7526b11ab4ff", "question": "How does the work position itself  with respect to previous work?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the work position itself  with respect to previous work?", "reference_answer": "A: In summary, the existing relevant works on benchmarking unsupervised object-centric learning focus on characterizing and analyzing architectural design of different models, and their experiments are still limited to synthetic datasets. By comparison, the proposed work targets at real-world datasets. Since all mentioned models fail on real-world datasets, architectural analysis is barely enough. Instead, the authors summarize and quantify inductive biases across different datasets. From the proposed experiments, the authors find that different models present different sensitivity to different dataset properties/biases, which also validates the findings of other study. More importantly, with the study of objectness biases in datasets, it is expected that better formulations of object-centric learning can be inspired in the future especially in the context of real-world applications."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b4475393-d299-55d7-9182-4ef39465717e", "question": "I wonder whether the few-shot task-agnostic NAS method is applicable to other domains, including computer vision.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I wonder whether the few-shot task-agnostic NAS method is applicable to other domains, including computer vision.", "reference_answer": "A: Most NAS works in computer vision (CV) (e.g., Once-for-all, One-Shot NAS) leverage hard class labels from a given task (e.g., image classification). They often use similar training recipes for SuperNets as in ImageNet-trained models (e.g., MobileNet, RegNet) for task-specific optimization with accuracy as an evaluation metric. In contrast, the few-shot task-agnostic NAS strategy used in AutoDistil training is fully task-agnostic and does not access task labels during SuperNet training.\n\nA potential method to adopt this strategy for the CV domain is to consider a self-supervised learning framework like SimCLR [1] that leverages data augmentation for consistency learning. This requires both a self-supervised teacher like SimCLR and a self-supervised training objective (e.g., self-attention relation distillation for Transformers or architecture-agnostic consistency learning). This would be a very interesting direction for future work.\n\n[1] Chen, T., Kornblith, S., Norouzi, M., \\& Hinton, G. (2020). A simple framework for contrastive learning of visual representations. In ICML."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ffa65386-3c66-5af2-97e8-da1bcbb73674", "question": "What are the weaknesses of NeuralPull's formulation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["e05a33f4-2946-599c-84d1-8e554c22e021"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the weaknesses of NeuralPull's formulation?", "reference_answer": "A: NeuralPull is not guaranteed to find a surface point $p$ such that $\\nabla f(q)$ is the direction vector between $p$ and $q$, and is susceptible to accumulated errors from this procedure."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5a9f0a62-0d2c-5395-ac3f-0ecfb0b5d1d3", "question": "Paper mentions one intermediate state that has to be temporary buffered after original task learning was executed, until augmented task learning has been performed. Is this state local to the synapses? Where is it in the update equations (is it $\\phi_{\\theta,0, \\tau}$)? What is the biological plausible way to perform this short-term storage?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["adc6a822-2be7-5581-b042-ecd991ff13bd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Paper mentions one intermediate state that has to be temporary buffered after original task learning was executed, until augmented task learning has been performed. Is this state local to the synapses? Where is it in the update equations (is it $\\phi_{\\theta,0, \\tau}$)? What is the biological plausible way to perform this short-term storage?", "reference_answer": "A: The authors are referring to $\\hat{\\phi}_{\\theta,0, \\tau}$. Indeed this state is local to the synapse for the synaptic model and could potentially be supported by short-term synaptic processes. Finding evidence for precise mechanisms supporting such storage is an important open question the proposed method shares with other contrastive learning theories, e.g. the short-term storage of synapse-specific products of pre- and post-synaptic activity over phases required by classical contrastive Hebbian learning.\n\nThis problem has for example been studied in the context of learning neural networks with equilibrium propagation. Ernoult et al. [7] have shown that learning can be made local in time by continuously updating the synaptic weights during the nudged phase ($\\beta > 0$). The very same algorithm can be applied at the meta-level and would remove the need to store $\\hat{\\phi}_{\\theta, 0, \\tau}$.\n\n[7] Ernoult, M., Grollier, J., Querlioz, D., Bengio, Y., & Scellier, B. (2020). Equilibrium propagation with continual weight updates. arXiv preprint arXiv:2005.04168."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "31289f89-3db6-56b1-b767-b87321fd26bf", "question": "Is the description of the geometry branch of the discriminator redundant?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2f1ab17e-3ad9-597f-a4e7-a303cb656a6d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the description of the geometry branch of the discriminator redundant?", "reference_answer": "A: No. Both descriptions are essential, because the authors would like to use the geometries extracted by the discriminator from the synthesized image to supervise the generator."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fbe861e7-a10d-52ec-85a8-a3e7a695acbd", "question": "Describe the limitations of the method as possible, such as the need for additional semantic annotations, inference speed, complex training process, etc.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6e611069-c4c2-578c-83e3-3279cc5b3dd0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Describe the limitations of the method as possible, such as the need for additional semantic annotations, inference speed, complex training process, etc.", "reference_answer": "A: The proposed SPoVT requires ground truth semantic labels for the point cloud data during training, which might not be practically available. As discussed in Q1, the authors are able to alleviate this limitation by utilizing pre-trained segmentors for assigning point cloud labels. \nAs for the concern about inference time, the authors do expect its increase when producing completion results with higher resolution. As stated in L195-201, since the authors produce such results by repeating the inference process multiple times, the inference time only grows linearly with the point cloud resolution (but not the memory usage). Please see the table below, in which the authors present the inference time and memory usage under different point cloud resolutions.\n\n| Output point cloud resolution | Inference time (ms) | Memory usage (GB) |\n|:-----------------------------:|:-------------------:|:-----------------:|\n| 2048 points                   |         50.0        |       1.923       |\n| 8192 points                   |        145.6        |       1.923       |\n| 16384 points                  |        27"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a63efc76-1bae-549e-852b-58e8a3adcbc1", "question": "What is the diversity of LTL formulas?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["798e13ba-b66c-5269-b4c4-65cc40a104b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the diversity of LTL formulas?", "reference_answer": "A: The instructions found in the TextWorld domains that form the basis of the GATA work and the proposed analysis only required a subset of LTL: conjunctive formulae involving the Next and Eventually temporal modalities."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5a7e520a-4569-5238-b5d8-6b599c92bde9", "question": "How does the proposed soft and hard focal-oriented reward relate to the fidelity-based reward from Jain et al, 2019?: ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed soft and hard focal-oriented reward relate to the fidelity-based reward from Jain et al, 2019?: ", "reference_answer": "A: The proposed soft and hard focal-oriented rewards have no direct relationship with the fidelity-based reward [3]. Because the CLS [3] metric is order-invariant, the authors only choose nDTW [4] as the fidelity metric to design the fidelity-oriented reward (model#16 in Table 3) in this paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "da5e63ee-b5a0-527c-89ef-a29843e78d75", "question": "In figure 1, can we encode the original text along with the appended sentence into one vector? Then, how do we guarantee that the perturbation only applies to the appended sentence but not the original text for the ADVCodec(sent)? Or will the original text be reproduced due to the autoencoder?”", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ace1319b-98b0-54d1-a428-a427fdd547e2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In figure 1, can we encode the original text along with the appended sentence into one vector? Then, how do we guarantee that the perturbation only applies to the appended sentence but not the original text for the ADVCodec(sent)? Or will the original text be reproduced due to the autoencoder?”", "reference_answer": "A: The authors will not encode the original text. Original text will not be perturbed or modified under any circumstances: the authors only add perturbation to the appended sentence for the concat attack; and the authors only manipulate on the scattered words for scatter attack while keeping original tokens unperturbed by masking out the perturbation on them. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b5b6322d-0376-56e4-9240-61612481e552", "question": "Provide a comparison between gradient oracle complexity of two-time scale SGDA versus stochastic OGDA/EG for NC-SC setting.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["5c82eb89-b12e-5a4b-884a-105abda1008d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide a comparison between gradient oracle complexity of two-time scale SGDA versus stochastic OGDA/EG for NC-SC setting.", "reference_answer": "A: In Equation 49 (appendix), the authors derived the final upper bound of stochastic OGDA in terms of $M_x$ and $M_y$, the batch size for primal and dual variables in computing stochastic gradients, respectively. The authors can conclude from Equation 49 that for reaching an $\\epsilon$-stationary point, $M_x = O (\\frac{1}{\\epsilon^2}) $, and $M_y = O (\\frac{ \\kappa}{\\epsilon^2})$. However, for the analysis of  SGDA in Theorem 4.5 of [23], the batch size for both primal and dual variables is the same and equal to $O(\\frac{\\kappa}{\\epsilon^2})$. Since the number of iterations has shown to be $T = O(\\frac{\\kappa^2}{\\epsilon^2}) $ in both OGDA/EG (Theorem 4.4 in the proposed work)  and GDA (Theorem 4.5 in [23]), the proposed analysis for stochastic OGDA shows an improvement in terms of primal gradient complexity. However, the authors agree that this paragraph needs more clarification as Theorem 4.4 in the proposed work is written in the case of $M_x = M_y$ and does not show the improvement the authors achieved in Equation 49 (appendix). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "194231e1-dedf-5818-97ac-7ae1f83b46ac", "question": "Why is the latency of BlkSConv-ResNet18 much larger than the original ResNet18?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["588efecf-df6f-5784-9ca4-c8dadbf1dc00"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the latency of BlkSConv-ResNet18 much larger than the original ResNet18?", "reference_answer": "A: The latency results shown in the table depend on the hardware optimization. In particular, in the papers [1,2], it is shown that the flops are not positively related to the latency because of the hardware optimization of the specific operations. So using different devices might result in different latency.\n\n[1] Dai, Xiaoliang, et al. \"Chamnet: Towards efficient network design through platform-aware model adaptation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n[2] Xiong, Yunyang, et al. \"Mobiledets: Searching for object detection architectures for mobile accelerators.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "371d6f3f-61b2-5c64-a24b-ae109cba4072", "question": "If shifting bits in Figure 1(a) is linear mapping, why the alignment module in Figure 1(b) which also shift bits is non-linear?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["70d23f47-63f0-5df1-88ba-b148cfca9c6a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "If shifting bits in Figure 1(a) is linear mapping, why the alignment module in Figure 1(b) which also shift bits is non-linear?", "reference_answer": "A: This is indeed a very interesting question. In the linear mapping, all the elements of the integer tensor are shifted/rounded to 8-bit integer. Note that at this stage, all the shifts are ***right shift***. In the second stage or non-linear inverse mapping, the authors have integer values that are undergone some computations, some of them might become zero and some might overflow. The non-linear inverse mapping module take care of each element of tensor and shift it left or right according the status of that element. In this case, some elements of tensor might be shifted to right, some might be shifted to left and some might remain untouched. This is why the authors call it non-linear inverse mapping since the shift is not in a uniform direction for the whole tensor."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dd9c3290-57fe-523f-b5e7-22d511502a98", "question": "Given some optimal student architecture derived by NAS, how would the traditional task-agnostic knowledge distillation method (e.g., MiniLMv2) perform?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Given some optimal student architecture derived by NAS, how would the traditional task-agnostic knowledge distillation method (e.g., MiniLMv2) perform?", "reference_answer": "A: In practice, additional or continued training of the optimal student architecture has demonstrated increased task performance with increased computational cost as in AutoTinyBERT (cost comparison in Table 3). The major advantage of AutoDistil is a single stage training scheme without additional training. The authors do perform an ablation in Table 4 where the authors continue training the searched model with self-attention distillation for additional steps referred as `KD$_\\text{att}$+Cont.' (similar to MiniLM). But the authors did not observe any significant gains on a subset of the tasks."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bd96dbb9-5749-5048-acf4-325efb87fd0c", "question": "“In figure 3, the model's rollout is a bit slower than the ground truth. The authors explained the phenomenon using the \"differences in the integration of positions and the much larger timestep.\" I do not quite get the point. Could you elaborate more on this? Also, it might be better to include labels for the two columns in figure 3 to make it more clear.”", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7566656d-1495-5b20-92c3-535a544492ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "“In figure 3, the model's rollout is a bit slower than the ground truth. The authors explained the phenomenon using the \"differences in the integration of positions and the much larger timestep.\" I do not quite get the point. Could you elaborate more on this? Also, it might be better to include labels for the two columns in figure 3 to make it more clear.”", "reference_answer": "A: Since DFSPH uses a much smaller time step it updates the particle velocities and positions more often resulting in slightly faster falling particles. Additionally, the time integration scheme is different. The authors use the midpoint method for computing the position, which is not used by DFPSH. Instead DFSPH corrects the density before updating the positions.\nthe authors added labels to figure 3."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "57953776-c977-5708-b6f1-a1d6a59e1fa6", "question": "in theorem 1, how many ωjs? it seems that there is only one ωk show in eq. 9. How about others?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["957b66a3-1bd0-534c-9ef1-c3837dbdf921"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "in theorem 1, how many ωjs? it seems that there is only one ωk show in eq. 9. How about others?", "reference_answer": "A: In Equation (9), each iteration (k) has its own $\\omega$. Hence, the authors require a set of $\\omega$s which the authors denote by $\\{\\omega_j\\}_{j\\geq0}$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5017877c-fae3-519d-8da6-5e48c3810c08", "question": "What is the APG version reported in Table 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["42636461-55ba-5d99-a3d8-e7949e4cef52"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the APG version reported in Table 2?", "reference_answer": "A: It is V5 in Table 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "782d835d-4c22-5189-b9dd-8f44167698d1", "question": "For the unrestricted setting, does the shadow training dataset overlap with the target training dataset? Are there any differences in the attack success rate if the shadow training data does or does not overlap with the target training dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["617d7416-f163-5ae1-a9e8-dbcf27170aa5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For the unrestricted setting, does the shadow training dataset overlap with the target training dataset? Are there any differences in the attack success rate if the shadow training data does or does not overlap with the target training dataset?", "reference_answer": "A: In unrestricted scenarios, where the shadow training dataset can be overlapped with the target training dataset, the attack performance is better than that in constrained scenarios where no overlap exists, as shown in Figure 4 and Figure 6. The reason is that more overlaps between the shadow and target training datasets may lead to a better mimicking of the target model by the shadow model. Then the thresholds learned from the shadow models could be more suitable for the target model. Therefore, if more shadow training data overlaps with the target training dataset, the attack success rate can be increased."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "fe9484db-345d-543d-883f-cde9ea939a48", "question": "State the major training difference of the proposed method with respect to other previous works?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["17e87869-0815-5e88-9ed2-3cb8faaf55ec"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "State the major training difference of the proposed method with respect to other previous works?", "reference_answer": "A: Yes, the generation of NIR-VIS images and the training of NIR-VIS face recognition network do not require any existing NIR-VIS face recognition datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "80d2e860-275f-54f4-848a-407599de89d9", "question": "In lines 65-66 you mention that you use separate temperature for each task with different skills. Does that mean that you use 10 different values of alpha in MT10-rand? Why is this necessary? Is the performance sensitive to the choice of these parameters and would it degrade with single value?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In lines 65-66 you mention that you use separate temperature for each task with different skills. Does that mean that you use 10 different values of alpha in MT10-rand? Why is this necessary? Is the performance sensitive to the choice of these parameters and would it degrade with single value?", "reference_answer": "A: Your understanding is correct. Different temperatures are used (and auto-adjusted as in SAC) for different tasks and the performance will degrade with a single value. This is actually a standard setup established in previous MTRL methods (e.g., SoftModule) and also adopted in later work such as CARE (appendix of CARE paper Table 9-15, temperature is “learned and disentangled with tasks” for all methods). It is used because different tasks may have different learning dynamics along the training process. In this work, the authors simply follow the setting established in previous work and also apply it to all methods. The authors apologize that the authors did not make it clear in the paper. The authors will revise the paper and clarify this point."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2fce8fd9-7a0f-57af-a15c-8ac93db1dc3e", "question": "An open question: authors state in their conclusion that it would be challenging to incorporate optimistic online learning within this work. But could not we incorporate, at each time, the optimistic information directly within the base algorithm and see where it goes?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d3122ff6-c400-59db-a094-ccd4c942a874"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "An open question: authors state in their conclusion that it would be challenging to incorporate optimistic online learning within this work. But could not we incorporate, at each time, the optimistic information directly within the base algorithm and see where it goes?", "reference_answer": "A: Incorporating optimism looks easy at an initial thought, but it is, unfortunately, nontrivial to achieve (please correct me if I am wrong, which I will be definitely happy to see). The reason is due to the surrogate loss technique used in the reduction scheme. Specifically, given an optimism $M_t$ that approximates the true gradient $\\nabla f_t(x_t)$, the authors need to come up with a \"surrogate optimism\" $\\tilde{M}_t$ to approximate the gradient of the surrogate loss $\\nabla g_t(y_t)$, in which the authors hope that $\\Vert\\tilde{M}_t - \\nabla g_t(y_t)\\Vert$ can be upper bounded by $\\Vert M_t - \\nabla f_t(x_t)\\Vert$. But such a construction of $\\tilde{M}_t$ is not easy to attain because the natural construction of $\\tilde{M}_t$ will depend on $x_t$, while $x_t$ also depends on $\\tilde{M}_t$ (recall the update step in optimistic online learning). So one needs to solve an equation to derive an appropriate $\\tilde{M}_t$, which is actually non-trivial, especially given the fact that $x_t$ is in a meta-base aggregation form and also requires to be projected back to the constrained feasible set."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "da5eebbd-a7db-5948-93bd-542050e774d6", "question": "Necessity of adding $\\boldsymbol{p}=\\nabla \\boldsymbol{u}$ into the loss function.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["f0e462e0-334e-5028-b737-be0499cbe403"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Necessity of adding $\\boldsymbol{p}=\\nabla \\boldsymbol{u}$ into the loss function.", "reference_answer": "A: The authors introduce the extra fields to reformulate the BCs as linear equations (see Eq. (9)). However, Eq. (7) and (9) are equivalent if $\\boldsymbol{p}_j=\\nabla u_j$ holds. So the authors have to add additional loss terms to achieve this. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b23512e9-a2e5-5634-ac14-98fd49f174fc", "question": "Besides objects (or semantic parts), the background is an important and large part contained in images. This paper does not discuss how to deal with the background. If we regard the semantic parts as the visual analog of words, what is the background for? How should we treat it?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["b375585e-1dea-57c3-9d6e-18d008b842a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Besides objects (or semantic parts), the background is an important and large part contained in images. This paper does not discuss how to deal with the background. If we regard the semantic parts as the visual analog of words, what is the background for? How should we treat it?", "reference_answer": "A: In this paper, the authors adopt two ways to deal with the background, i.e., 1) random masking 75% patches of the background and 2) ignoring the background to calculate the loss. Both of these settings show the effectiveness of the proposed SemMAE (Line 206-221 and Table 2). While the authors agree that it is still an open problem to find the visual analog of words for the background, and the authors will further study this problem in future work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e788ed35-318d-5655-aaf1-7a3dbbcec5aa", "question": "Explain the influence of the hyperparameters of Equation 21.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["368314b3-9259-5022-a0f3-fd17a4b571c2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain the influence of the hyperparameters of Equation 21.", "reference_answer": "A: Recall the Equation 21 is: $\\mathcal{L} = \\mathcal{L}_p + \\mu \\cdot\\mathcal{L}_K + \\alpha \\cdot \\Omega(\\mathbf{Z},V) + \\beta \\cdot \\mathcal{L}_d + \\gamma \\cdot \\mathcal{L}_s$. (The authors denote $\\mathcal{L}_K$ as the terms of KL-divergence in the original equation.)\n\n- $\\mathcal{L}_p$ and $\\mathcal{L}_K$ are common terms in ELBO for VAE, where the former is used to calculate the prediction error between the output of the encoder and the target, and the latter is used to regularize the pattern in latent space. The authors used $\\mu$ to match the two terms to the same scale.\n- The authors adopt the assumption of graph signals that values change smoothly across adjacent nodes. So the Dirichlet energy $\\Omega(\\mathbf{A},\\mathbf{X})$ (Equation 10) maps the graph representation $\\mathbf{G} = (\\mathbf{A}, \\mathbf{X})$ to a line, so that the connected points stay as close together as possible [3]. The authors used $\\alpha$ to match the $\\Omega$ to the same scale of $\\mathcal{L}_p$.\n- The connectiveness term $\\mathcal{L}_d$ has the purpose to ensure that each node has at least one edge with another node. (The result of $\\mathbf{Z}\\mathbf{1}$ is the node degree vector.) The authors use the logarithmic barrier to force the degrees to be positive, but not prevent edges from becoming zero. The authors used $\\beta$ to match the $\\mathcal{L}_d$ to the same scale of $\\mathcal{L}_p$.\n- However, adding the logarithmic term in $\\mathcal{L}_d$ leads to very sparse graphs, and changing its weight in the loss ($\\beta$) only changes the scale of the solution and not the sparsity pattern. For this reason, the authors added the third term $\\mathcal{L}_s$. Yet it was mentioned in [18] and observed by us that adding an $\\ell$-1 norm to control sparsity was not very useful. So the authors chose the Frobenius norm, which penalized the big values but not the smaller ones. This leads to a more dense adjacency matrix for a bigger value of $\\gamma$.\n\nThe authors also showed the count of indirect connections with different path lengths in Figure 4. During experiments, they firstly scaled the various terms in the hybrid loss to the same scale. Then they found the different values of the weights of the regularization terms in hybrid loss had a minor effect, and the number of indirect connections in the learned adjacency matrices had only a minor difference. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8bb140a2-3391-50f4-823a-703eef33db94", "question": "How does the model perform on cross-model retrieval?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the model perform on cross-model retrieval?", "reference_answer": "A: While CLIP and CyCLIP are comparable on the easier text retrieval tasks, the authors observe that CyCLIP outperforms CLIP across both the datasets on the Image retrieval task in both the zero-shot and fine-tune cases."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c126ff40-8ca4-5a61-bde5-47881bee13aa", "question": "Do the scaling ablations hold true when models are finetuned? Does the lack of consistent scaling for HM3D-Semantic ObjectNav reflect poorly on the ability to use ProcThor to benefit real-world robotics?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["604b5dbe-a5d2-52ec-a301-aa7a4215c44b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do the scaling ablations hold true when models are finetuned? Does the lack of consistent scaling for HM3D-Semantic ObjectNav reflect poorly on the ability to use ProcThor to benefit real-world robotics?", "reference_answer": "A: Table 3 presents ablation results in a 0-shot setting in order to avoid having to fine-tune 16 different models, which would be computationally very expensive. However, this is a valid research question, and hence the authors present numbers for 10 and 10k ProcTHOR pre-trained models when fine-tuned on RoboTHOR for the task of ObjectNav. As seen, jumping from 10 to 10k provides a huge improvement not just for 0-shot but also for fine-tuning."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3f0a3b58-786d-5f22-af3e-8d5ee58b4392", "question": "In equation (6), why are the expressions for m_i and m_v identical?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7b442b61-4007-52b6-ac88-2307f92e7c20"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In equation (6), why are the expressions for m_i and m_v identical?", "reference_answer": "A: In the main paper, Eq. (6) is \n$$\\mathbf{m}_{I}=\\operatorname{Softmax}\\left(\\frac{\\mathbf{q} \\cdot \\mathbf{k}^{T}}{\\sqrt{d}}\\right) $$\n$$\\mathbf{m}_{V}=\\operatorname{Softmax}\\left(-\\frac{\\mathbf{q} \\cdot \\mathbf{k}^{T}}{\\sqrt{d}}\\right)$$\n, where it should be noticed that $\\mathbf{m}_V$ and $\\mathbf{m}_I$ differ in a minus sign in the Softmax function. The proposed design objective is to let dynamic neighbors with higher attention scores be in the invariant patterns, and let those with lower attention scores be in variant ones. Therefore, the invariant and variant patterns have a negative correlation and capture complementary information. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a2a1f174-edae-59de-aca9-35b45c2d057d", "question": "How does the proposed SPS-Conv work on Waymo with much denser point cloud?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed SPS-Conv work on Waymo with much denser point cloud?", "reference_answer": "A: The authors show the results of the proposed method on the Waymo dataset in Q2 of the Common question. As shown in the table, the proposed method is also able to maintain competitive performance on various metrics on this dataset while saving 63% GFLOPs. This further illustrates the generality of the proposed method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a0736085-50af-519d-bac9-4a30b9725491", "question": "Could the second term of Eqn. 7 be further simplified?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d87b5532-5c5f-5940-aa2b-f17ffad843cd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could the second term of Eqn. 7 be further simplified?", "reference_answer": "A: Ideally, the second term in Eqn. 7 in the proposed work and $\\beta\\frac{1}{|\\mathcal{G}|}\\sum_{(G,y)\\in\\mathcal{G}}-\\log q_\\theta(y|G)$ are both mathematically equivalent to the simplified form $\\beta\\mathbb{E}_{(G,y)}[-\\log q_\\theta(y|G)]$. \n\nYet for implementation, the two formulas are slightly different. The expectation $\\mathbb{E}_{(G,y)}[-\\log q_\\theta(y|G)]$ is hard for direct computation, thus the authors use Monte Carlo estimation for approximation. The proposed implementation first uses the samples under each specific environemt for approximating the environment-specific risk and then calculate the average across different enviroments. The second term in Eqn. 7 is exactly what the authors have done in the proposed implementation. Hence, the authors kept this form in the paper instead of using the simplified one to stay consistent with the proposed implementation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dab6a756-2b1d-5102-a9d8-39365f9da9f4", "question": "Provide quantitative metrics, such as Boundary-IoU [1], and compare with recent high-quality mask prediction models, such as PointRend [2] and BMask R-CNN [3].\nReferences:\n[1] Cheng, Bowen, Ross Girshick, Piotr Dollár, Alexander C. Berg, and Alexander Kirillov. \"Boundary IoU: Improving object-centric image segmentation evaluation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334-15342. 2021.\n[2] Kirillov, Alexander, Yuxin Wu, Kaiming He, and Ross B. Girshick. \"Pointrend: Image segmentation as rendering. 2020 IEEE.\" In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9796-9805. 2019.\n[3] Wang, Yuqing, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. \"End-to-end video instance segmentation with transformers.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8741-8750. 2021.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["a31b1f08-c3ff-54a1-bb1e-e4d1f878a710"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide quantitative metrics, such as Boundary-IoU [1], and compare with recent high-quality mask prediction models, such as PointRend [2] and BMask R-CNN [3].\nReferences:\n[1] Cheng, Bowen, Ross Girshick, Piotr Dollár, Alexander C. Berg, and Alexander Kirillov. \"Boundary IoU: Improving object-centric image segmentation evaluation.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15334-15342. 2021.\n[2] Kirillov, Alexander, Yuxin Wu, Kaiming He, and Ross B. Girshick. \"Pointrend: Image segmentation as rendering. 2020 IEEE.\" In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9796-9805. 2019.\n[3] Wang, Yuqing, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. \"End-to-end video instance segmentation with transformers.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8741-8750. 2021.", "reference_answer": "A: The authors evaluate SOLQ under the Boundary-IoU as suggested and perform the comparison with PointRend and BMask R-CNN in the Table below. Similar to the results on MS COCO, SOLQ shows much better performance on the small and medium objects and is relatively inferior on large objects. In SOLQ, the authors use fixed coding length (e.g., 256) so the authors feel the results are reasonable. For fair comparison, the authors should also compare the results with Mask-RCNN (23.9% vs. 22.2% on APl), which is also based on fixed length representation. The authors believe that longer/dynamic coding length, may make up the information losses for large objects. The authors will cite these three papers, add performance comparison and discussion under the Boundary-IoU metric. Please also refer to the R3-Q2 for more analysis on the performance variances on different objects.\n\n|     Methods          |     AP      |     APs     |     APm     |     APl     |\n|----------------------|-------------|-------------|-------------|-------------|\n|     Mask-RCNN        |     23.1    |     18.6    |     33.4    |     22.2    |\n|     PointRend        |     25.4    |     19.1    |     34.8    |     **26.4**    |\n|     BMask   R-CNN    |     25.4    |     19.5    |     35.2    |     26.3    |\n|     SOLQ             |     25.2    |     **22.8**    |     **37.5**    |     23.9    |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ebe986b1-185a-50be-be5e-ab3ee8a1ab8d", "question": "In Table 1, why was a t-test performed (e.g., under what normality assumptions?)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1, why was a t-test performed (e.g., under what normality assumptions?)?", "reference_answer": "A: The authors add a t-test to verify the improvement since the margins between brainnetTF and baselines are not large enough for observation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7ed14df1-a805-5d88-b6db-c541d13c8990", "question": "What does the generative expression stand for? Why is it necessary for justifying the perturbation-based method?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does the generative expression stand for? Why is it necessary for justifying the perturbation-based method?", "reference_answer": "A: The generative expression stands for the main content of the generated image. The authors have rephrased the sentences to fix the ambiguity and emphasize why the authors choose the perturbation-based method: \"The authors model the $\\mathbf{z}$ transform process as making perturbations to the original sampling since $z^*(\\mathbf{z})$ shall not depart much from $\\mathbf{z}$ as the authors hope the main content of the generated image remains the same. The hope that small perturbations can achieve considerable positive quality variation leads us to the adversarial sample mining methods.\" There can be other designs of $z^*$, but the authors believe adversarial sample mining methods are one of the effective solutions (we have also tried raw gradient updating, but it performs much worse), and note that qualitative/quantitative results in Table 4 and Figure 4 have verified its effectiveness."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1f6be49d-7563-5e3c-bc08-d71ac62084c3", "question": "Present results for the analysis on the role of diffusion modeling in the latent space vs contrastive representation learning methods aka the two main modifications over a standard VAE. E.g. how good is a NVAE model when trained with an auxiliary contrastive representation learning objective? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9b2e64f8-a989-5565-91f2-58280f79add6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Present results for the analysis on the role of diffusion modeling in the latent space vs contrastive representation learning methods aka the two main modifications over a standard VAE. E.g. how good is a NVAE model when trained with an auxiliary contrastive representation learning objective? ", "reference_answer": "A: The authors train a diffusion-denoising (D2) model (without the contrastive learning component). This table shows the FID score of the generated images with a different number of diffusion steps.\n\n|        |       | CIFAR10 |       |       | CIFAR100 |        |\n|-------|-------|-------|-------|-------|-------|-------|\n| Steps | 10    | 50  | 100   | 10    | 50       | 100   |\n| D2      | 22.3 | 15.8 | 15.1  | 28.35 | 19.81    | 19.85 |\n| D2C   | 17.71 | 10.11| 10.15 | 23.16 | 14.62    | 14.46 |\n\nCompared with the performance of NVAE (36.4 on CIFAR10 and 42.5 on CIFAR100), D2 is significantly better. Moreover, D2C is better than D2 in terms of unconditional generation performance. This table shows the MSE, FID and latent representation accuracy comparisons between D2, D2C, and NVAE.\n|        |       | CIFAR10 |       |       | CIFAR100 |       |\n|-------|-------|---------|-------|-------|----------|-------|\n|        | FID   | MSE     | Acc   | FID   | MSE      | Acc   |\n| D2    | 15.1  | 0.24    | 40.6  | 19.85 | 0.48     | 17.89 |\n| D2C   | 10.15 | 0.76    | 76.02 | 14.62 | 0.44     | 42.75 |\n| NVAE  | 36.4  | 0.25    | 18.8  | 42.5  | 0.53     | 4.1   |\nHere, the D2 has worse latent representation accuracy than D2C but better than NVAE.\nThe authors also attempted an experiment with NVAE + contrastive loss, but at the moment, the authors were unable to achieve satisfactory generation results (reconstruction MSE remains high). This is possibly due to the many regularizations needed for NVAE to work well, which could conflict with contrastive learning ([https://github.com/NVlabs/NVAE#known-issues](https://github.com/NVlabs/NVAE#known-issues)); D2 and D2C did not adopt these regularizations, just the NVAE architecture. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9ec9d515-69dc-5570-a002-76abf24de0d5", "question": "How imperceptible are the examples generated by the proposed technique?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How imperceptible are the examples generated by the proposed technique?", "reference_answer": "A: The authors find that the resultant perturbations are truly imperceptible for the proposed ETF. Please refer to Figure 2 for the visualization with deep*-PGD attack (using training images), deep-PGD attack (using test images), and lightweight black-box attack. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a8c1489a-f1b7-5da1-b701-df4d01676729", "question": "Did the authors try experiments with MSE instead of KL divergence for the stimulative training?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Did the authors try experiments with MSE instead of KL divergence for the stimulative training?", "reference_answer": "A: The authors further try experiments with MSE instead of KL divergence for the stimulative training. As shown in Table r6, using MSE for the stimulative training can still improve the performance of common training, while using KL divergence for the stimulative training can achieve the best performance on various models and datasets.\n\n**Table r6: Comparisons**\n|Method| MBV3_C10| MBV3_C100 |Res50_C100|\n|:--------------|:----------- |:------ |:-----|\n| CT|        95.72        |77.39        |76.53|\n| ST(MSE)        |96.47        |78.78|        78.12|\n|ST(KL)        |96.88|        81.07|        81.06|"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "630794c1-0330-5952-9af0-9e999ac3e5bb", "question": "Are there results for BERT-large?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6cfcaada-4caa-5df5-bc14-14bc2552e295"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there results for BERT-large?", "reference_answer": "A: Yes, the authors have obtained the results of BERT-large, which are summarized in the following table."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d0902f4f-dcf2-550e-b3b9-a89603d564aa", "question": "\"Why is there no standard deviation of IWRE and GAIL in Humanoid result (Figure6 (e))?\"", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["dca4c95e-012f-538e-8e5c-438dec5fc664"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "\"Why is there no standard deviation of IWRE and GAIL in Humanoid result (Figure6 (e))?\"", "reference_answer": "A: The standard deviation shadow of IWRE in figure 6(e) was lost because of the error in PDF generation. The updated revision has fixed this issue, and the conclusion remains the same. Meanwhile, actually there exist standard deviations of GAIL at the beginning of training steps (you can find them when zooming in the figure). However, under such a complex environment of Humanoid, the learner's performance will degenerate quickly without the calibration of the importance weights."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "21d189a6-91f7-5b8f-b0f6-17f3128ac820", "question": "Why six parts per image were selected in the paper? For different kinds of objects in the ImageNet dataset, what are the six parts corresponded to?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b375585e-1dea-57c3-9d6e-18d008b842a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why six parts per image were selected in the paper? For different kinds of objects in the ImageNet dataset, what are the six parts corresponded to?", "reference_answer": "A: The number of parts is a hyperparameter, which is experimentally set to be 6. Specifically, the authors tried more parts (e.g., 8 parts), while the authors found the segmentation results are kind of noisy; the authors also tried fewer parts (e.g., 4 parts), while the authors found the segmentation results are kind of coarse. Figure 4 shows several examples of the learned parts (best viewed in color). The authors will show more cases and list the corresponding parts in the revised version."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "50cba06b-ba78-5a68-b5bc-fb5639b06d62", "question": "What is the range of performance of the models across runs?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e8b1d1de-2bdd-5181-8edc-cfbbd059ea05"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the range of performance of the models across runs?", "reference_answer": "A: The table below shows the results and standard deviations.\n\n| dataset  | AIR | MONet | IODINE | SlotAtt |\n| ------------- | ------------- | ------------- | ------------- | ------------- |\n|   | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec | AP / PQ / Pre / Rec |\n| dSprites | 45.4 ***1.8*** / 38.2 ***3.0*** / 57.6 ***7.4*** / 58.1***7.5*** | 69.7 ***4.1*** / 61.6 ***6.0*** / 70.4 ***8.1*** / 73.9 ***1.9***  | 92.9 ***4.3*** / 71.3 ***6.1*** / 82.6 ***2.3*** / 96.0 ***5.2*** | 92.9 ***1.4*** / 82.8 ***1.6*** / 88.8 ***3.4*** / 92.9 ***1.6*** |\n| Tetris  | 25.2 ***13.9*** / 23.4 ***12.4*** / 36.8 ***20.9*** / 39.9 ***12.9*** | 85.9 ***13.0*** / 75.8 ***13.6*** / 85.1 ***16.4*** / 89.7 ***8.2***| 52.2 ***2.3*** / 37.9 ***4.6*** / 48.0 ***2.3*** / 61.8***1.7*** |  94.3 ***1.2*** / 79.9 ***6.4*** / 90.5 ***3.3*** / 94.4 ***1.3***|\n| CLEVR  |  46.4 ***14.0*** / 44.3 ***12.4*** / 67.4 ***9.9*** / 52.5 ***15.9*** | 39.0 ***8.5*** / 37.3 ***6.3*** / 65.6 ***11.8*** / 42.8 ***10.8***| 82.8 ***2.8*** / 73.0 ***5.7*** / 77.5 ***3.1*** / 87.4 ***2.0*** |  91.7 ***6.4*** / 82.9 ***10.9*** / 90.8 ***9.7*** / 92.7 ***5.3***|\n| YCB |   0.0 ***0.1*** / 0.6 ***0.3*** / 1.1 ***0.4*** / 0.8 ***0.2*** | 3.1 ***1.6*** / 7.0 ***2.6*** / 9.8 ***3.6*** / 1.2 ***0.8***| 1.8 ***0.2*** / 3.9  ***1.3*** / 6.2 ***2.0*** / 7.3 ***1.9*** |  9.2 ***0.4*** / 13.5 ***0.9*** / 20.0 ***1.3*** / 26.2 ***6.8***|\n| ScanNet  | 2.7 ***1.4*** / 6.3 ***1.7*** / 15.6 ***2.8*** / 7.3 ***1.6*** | 24.8 ***1.6*** / 24.6***1.6*** / 31.0 ***1.6*** / 40.7 ***1.8***| 10.1 ***2.9*** / 13.7 ***2.7*** / 18.6 ***4.2***"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "57f62f5b-142a-5c08-b99c-caffb54ec419", "question": "Can you explain why in Figure 2 (center) the training loss of blue one is so high, compared to its test loss? It seems the training process can indeed improve the test performance even in case of Standard Labels. Does it mean that some patterns do exit in the training set, although they are not obvious to human but can be learned?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["01ec1818-9fe6-516b-ab44-46377c67a0cb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you explain why in Figure 2 (center) the training loss of blue one is so high, compared to its test loss? It seems the training process can indeed improve the test performance even in case of Standard Labels. Does it mean that some patterns do exit in the training set, although they are not obvious to human but can be learned?", "reference_answer": "A: Note that Figure 2 does not report the _training losses_.\nFigure 2 (center) compares the **test loss** of the standard label generation (blue curve) against the proposed OD label generation (orange curve). \nThe test losses improve in that average errors are indeed minimized during the learning process, albeit not significantly. In fact, they still result in predictions that are inaccurate and induce large constraint violations (see Figure 2 right). Additionally while not reported, the training losses are comparable, in magnitude and trends, with their respective test losses.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2083f70a-9b8b-501a-9ad4-c2a927b782f9", "question": "What is the novelty of the NTK-LC approach and how does it compare with state-of-the-art methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0728fbcf-6f26-5777-be36-f4bbaf659ea2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the novelty of the NTK-LC approach and how does it compare with state-of-the-art methods?", "reference_answer": "A: The proposed NTK-LC approach is novel in that it has a novel and sound theoretical foundation that depends on the *precise* CK ad NTK eigenspectra of fully-connected DNN models, which is derived for the first time under generic GMM data. In Figure 3, the authors compare the proposed NTK-LC approach to the magnitude-based pruning method, showing the advantageous performance of the proposed NTK-LC approach."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e84b7535-2509-51a0-8fad-81617bb0b36c", "question": "As the feature map of each level has to be resized to the original image size, is it necessary to apply FPN to generate multi-level prediction?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b81c4fdc-3bba-5940-a430-379bbd34ea5c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As the feature map of each level has to be resized to the original image size, is it necessary to apply FPN to generate multi-level prediction?", "reference_answer": "A: No, the authors do NOT resize the feature maps of all levels to the original image size. As noted in L216, only the first level of feature maps has the same size as the original image size, and other levels are down-sampled by powers of $2$, respectively, as in the standard FPN. Thus, FPN is still needed."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "598d1d29-6c4f-5303-bf02-4b618738e5d6", "question": "Page 7, footnote 3: If these baselines were implemented for a different setting, are there any hyperparameters that need to be re-tuned for MetaWorldV2? It would be helpful to know which hyperparameters from appendix table 4 were tuned for PaCo specifically and what values for each were included in the search.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Page 7, footnote 3: If these baselines were implemented for a different setting, are there any hyperparameters that need to be re-tuned for MetaWorldV2? It would be helpful to know which hyperparameters from appendix table 4 were tuned for PaCo specifically and what values for each were included in the search.", "reference_answer": "A: The hyper-parameter values in Table 4 are inherited from the standard setup used in previous MTRL work (e.g. Table 9 in appendix of CARE paper) and are not specifically tuned for PaCo. The same hyper-parameter values are used across all the methods."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "73f0fc15-6e90-5beb-afe1-893b04b2a5cf", "question": "How to compute the energy of multi-bit spikes? How to do convolution between multi-bit spikes and 32-bit weights?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ef4e07a2-e75c-5136-adc9-87fd8b029c3c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to compute the energy of multi-bit spikes? How to do convolution between multi-bit spikes and 32-bit weights?", "reference_answer": "A: The authors use the same calculation method from Ref.[15]. The authors find that there exists a problem with this method for multi-bit spikes.  The authors utilize 77fJ/SOP for SNN as the power consumption baseline, which is reported from the ROLLS neuromorphic processor [Qiao et al., 2015]. The authors find that this chip does not support multi-bit spikes. The Tianjic chip can support multi-bit spikes. However, they have not collected the power consumption data for multi-bit spikes (we have asked the authors of Tianjic chip for data).\nTo calculate the power consumption correctly, the authors trained the SEW IAND ResNet on ImageNet. The test accuracy is shown in Tab.R6, and the power consumption is shown in Tab.R7.\n\n| Network   | SEW ResNet(ADD) |          | SEW ResNet(IAND) |          | Spiking ResNet |          |\n| --------- | --------------- | -------- | ---------------- | -------- | -------------- | -------- |\n|           | Acc@1(%)        | Acc@5(%) | Acc@1(%)         | Acc@5(%) | Acc@1(%)       | Acc@5(%) |\n| ResNet-18 | 63.18           | 84.53    | 61.71            | 83.48    | 62.32          | 84.05    |\n| ResNet-34 | 67.04           | 87.25    | 64.76            | 85.95    | 61.86          | 83.69    |\n| ResNet-50 | 67.78           | 87.52    | 66.20            | 86.64    | 57.66          | 80.43    |\n**Table R6: Test accuracy of SEW ADD/IAND ResNet and Spiking ResNet on ImageNet.**\n\n| ResNet Structure | 18     | 34     | 50     |\n| ---------------- | ------ | ------ | ------ |\n| ANN OP (GFLOP)   | 1.82   | 3.68   | 4.14   |\n| SNN OP (GSOP)    | 1.61   | 3.15   | 3.24   |\n| ANN Power (mJ)   | 22.75  | 46     | 51.75  |\n| SNN Power (mJ)   | 0.12   | 0.24   | 0.25   |\n| A/S Power Ratio  | 183.51 | 189.65 | 207.43 |\n**Table R7: Comparison of power consumption of SEW IAND ResNet and ResNet.**\n\n[Qiao et al., 2015] Ning, Qiao, et al. A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses. Frontiers in neuroscience. 2015, 9: 141."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3b826a61-3c5b-5d34-b223-ec40d54144a8", "question": "It is important to explicitly provide the parameters required to achieve the rate in the theorems. For example, what is \"a suitable selection of stepsizes\" in Theorem 1 and \"there exist parameters of Algorithm 2...\" in Theorem 2? Do they depend on unknown parameters? While the linear rate is nice theoretically, it can be much slower than sublinear rates if the convergence factor is close to 1, unless the number of iterations tends to infinity. All these parameters are important for practitioners to implement the methods.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["851cca38-1ba1-5597-b98c-182d55d68a2d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is important to explicitly provide the parameters required to achieve the rate in the theorems. For example, what is \"a suitable selection of stepsizes\" in Theorem 1 and \"there exist parameters of Algorithm 2...\" in Theorem 2? Do they depend on unknown parameters? While the linear rate is nice theoretically, it can be much slower than sublinear rates if the convergence factor is close to 1, unless the number of iterations tends to infinity. All these parameters are important for practitioners to implement the methods.", "reference_answer": "A: All parameters of the proposed algorithms have simple explicit formulas that are provided in the full/detailed versions of the convergence theorems that can be found in the appendix. So, these details are already contained in the paper.  The resulting complexities (the complexities after the various stepsize and other parameters are substituted into the general formulas for rates) are also shown in the three tables of the paper. Having said that, the authors will make it all even more reader-friendly, and will  add a table with the formulas for the parameters of the algorithms in the final version of the paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9b31db47-c5a0-5c2e-9a8a-31eeff7cc99a", "question": "What is the novelty in the proposed VRL3 in comparison to related works such as MVP[1], PVR[2], R3M[3] which explore training encoders from out of domain data?\nReferences:\n[1] Xiao et al. Masked Visual Pre-training for Motor Control. arXiv 2022.\n[2] Parisi et al. The Unsurprising Effectiveness of Pre-Trained Vision Models for Control. ICML 2022.\n[3] Nair et al. R3M: A Universal Visual Representation for Robot Manipulation. arXiv 2022.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["856e4b21-2ff1-5184-879d-bb789283cda9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the novelty in the proposed VRL3 in comparison to related works such as MVP[1], PVR[2], R3M[3] which explore training encoders from out of domain data?\nReferences:\n[1] Xiao et al. Masked Visual Pre-training for Motor Control. arXiv 2022.\n[2] Parisi et al. The Unsurprising Effectiveness of Pre-Trained Vision Models for Control. ICML 2022.\n[3] Nair et al. R3M: A Universal Visual Representation for Robot Manipulation. arXiv 2022.", "reference_answer": "A: **MVP paper**: MVP uses self-supervised pretraining on images collected online and is tested on a new suite of control tasks proposed by the authors. The main differences are: \n- VRL3 considers the combination of **non-RL data, offline RL, and online RL data**. This paper only studies how non-RL data can be combined with online RL. \n- VRL3 work can be seen as **a more general** framework in the sense that if the authors remove stage 2 training of VRL3, set encoder learning rate to 0, and switch to a different dataset and pretraining method in stage 1, then the authors arrive at the MVP framework. \n- **Novel results** found in VRL3 paper are not covered by the MVP paper: VRL3 authors provide a comprehensive study on the effect of enabling/disabling encoder training in different stages, and discuss how it can be finetuned in a stable and effective manner, the authors discuss how self-supervised learning in stage 2 might be entirely unnecessary when offline RL updates are used, etc.\n- One of the proposed major contributions is a new SOTA performance on the popular and challenging Adroit benchmark. MVP **does not** study this benchmark. MVP also does not benchmark against prior SOTA algorithms in robotic control such as RRL and FERM (figure 5 of MVP paper). \n\n**PVR paper**: an interesting paper that combines pretrained encoders and imitation learning. The main differences are: \n- VRL3 paper is focused on combine encoder pretraining with offline and online **RL**, while PVR studies how pretraining is combined with **imitation learning**. \n- VRL3 work can be seen as **a more general** framework in the sense that if the authors disable stage 3 training of VRL3, set encoder learning rate to 0, enable BC training in stage 2, and switch to a different pretraining method in stage 1, then the authors arrive at the PVR framework. \n- **Novel results** found in VRL3 paper are not covered by the MVP paper: for example, VRL3 authors show that imitation learning (behavioral cloning) in stage 2 can be entirely removed when proper offline RL updates are applied, etc. \n- PVR has tested on the Adroit environment, they use more demonstrations than us (100 demos in PVR, while VRL3 authors use the standard 25 demos), and their performance is **lower than ours** (they achieve an average of 85% success rate, shown in Figure 1 of PVR paper, while the authors reach 95% or higher). \n- Pretraining excluded, PVR reports a wall-clock training time of 8-24 hours (appendix A.5 of PVR paper), while VRL3 authors report 1.6-13.3 hours (an average of 5.57 hours, see appendix B.3 (page 34) of the proposed work) training time to reach a stronger performance, which is **much faster**. \n\n**R3M paper**: similar to the PVR paper, they combine pretrained encoder (using a more sophisticated pretraining method) and imitation learning. The differences are similar to when the authors compare VRL3 and PVR. Performance-wise, R3M reports a <70% success rate on the Adroit benchmark, **weaker than** the proposed 95% success rate. In terms of computation efficiency, it is not reported in the R3M paper.\n\nHowever, the performance of above works on Adroit is weaker than the proposed method's results. Based on these results, it seems the proposed work is indeed the first successful framework that achieves a new SOTA performance on pixel-input Adroit while utilizing a combination of non-RL, offline RL and online RL data. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9169c885-3438-5155-8ad3-f6fd7553618c", "question": "The paragraph that comes after definition 2 is not clear to me. Why is this setting so much harder compared with the setting of definition 1? Also, definition 2 seems like a generalization of definition 1, and if that is indeed the case, perhaps it is good to mention that.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["86b0a7a9-c402-5373-bc10-3ce3edc5e031"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The paragraph that comes after definition 2 is not clear to me. Why is this setting so much harder compared with the setting of definition 1? Also, definition 2 seems like a generalization of definition 1, and if that is indeed the case, perhaps it is good to mention that.", "reference_answer": "A: Similar to the proposed answer for Question (5), this is a ‘difficult’ setting because the input space or the feature given is terrible in a sense that there is no correlation between labels corresponding to different inputs, so that one needs to see *all* samples in order to identify exactly *all* samples from a class.\nFor an extreme example, imagine you want to do classification based on only hash values of images. This is truly a poor choice of features as similar (but not identical) hash values may correspond to completely unrelated samples, and it is for sure a hard task, all because the feature extractor (in this case it is the hashing function) is so terrible."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "2d1bf706-ca68-5a0c-88a9-382c0a80b8d9", "question": "Are there any theoretical results on the convergence of the nonlinear ES/AS controllers?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there any theoretical results on the convergence of the nonlinear ES/AS controllers?", "reference_answer": "A: The authors provide two new theorems about the upper bound estimation of convergence time and energy cost for ES and AS, respectively, in __Theorem 4.2__ and __Theorem 4.3__. These two theorems significantly improve the proposed analytical results, the authors can further study the effect of the NN controller based on the formulation of the upper bound. The authors provide more analysis about these two theorems in Appendix."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e4d997e1-ef04-5348-a8a2-56af69fbef2d", "question": "Just wonder, this work is like Transformer + X, kind of work, but how do you think the Transformer will work better? in other words, the study lack interpretability. Could you highlight the origin of the idea?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["c0de57d0-1c93-5a02-86ea-b3e371ea1078"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Just wonder, this work is like Transformer + X, kind of work, but how do you think the Transformer will work better? in other words, the study lack interpretability. Could you highlight the origin of the idea?", "reference_answer": "A: The authors agree with the reviewer that the proposed work NIERT is a bit like \"_Transformer + X_\". However, it should be noted that NIERT has core differences from vanilla Transformer:\n  1. The main component of NIERT only has an encoder.\n  2. The authors modified self-attention to partial self-attention in this Transformer encoder to meet the inductive bias of interpolation.\n\nthe authors interpret the proposed NIERT via a tight connection with the classical RBF approach. Let's have a look first at the formalizations of these two approaches which are listed below:\n\n  - _RBF_: RBF interpolation formulates the interpolant as \n    $$f(x)= \\sum_j\\lambda_j\\phi(x,x_j)\\tag{1}$$\n    where $\\phi(x,x_j)$ is the radial basis function related to the observed point $x_j$ and $\\lambda_j$ is the coefficient.\n\n  - _NIERT_: In the core mechanism of NIERT, namely partial self-attention layer, a point $x_i$'s representation $\\tilde{v}_i$ is computed by\n    $$\\tilde{v}_i = \\sum_j\\alpha(q_i,k_j)v_j\\tag{2}$$\n    where $\\alpha(q_i,k_j)$ is the normalized attention weight function. $\\alpha(q_i,k_j)$ models the corelation between any query vector $q_i$ and key vector $k_j$ ($k_j$ is related to an observed point $x_j$).\n    \n  the authors can easily find that Eq.(2) is a general form of Eq.(1) by corresponding $\\alpha(\\cdot,\\cdot)$ to $\\phi(\\cdot,\\cdot)$ and $v_j$ to $\\lambda_j$. Thus, by enhancing with other mechanisms, such as layer normalization, skip connection and multi-head mechanism, and applying supervised training, it is promising to obtained a high-accuracy and generalizable neural interpolator.\n\nThe above deep connection is one of the origins of the proposed idea. Another origin of the proposed idea is that masked language model like BERT, which can predict missing tokens based on some given tokens in a sentence, which resembles interpolation problem.\n\nthe authors think the tight connection between the proposed NIERT and RBF interpolation is also instructive. Due to space limitations, the authors expand these explanations in the Supplementary material (Page 1, Line 6-18)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "734a2651-d7a9-578e-a1c6-df2e80956a86", "question": "Does the  unbounded optimization problem work for general, real-world images?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e76cdd40-14a4-5e64-b8b2-c6871f4cf676"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the  unbounded optimization problem work for general, real-world images?", "reference_answer": "A: Yes, if the training of NeRF can converge for real-world images."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "deeba128-24cf-558c-8c9a-05bc1003c658", "question": "Why are there some inconsistencies reported on the number for Imagenet-1k where FixMatch is reported as having a top-5 error of 19.55% but the prior work both the original FixMatch paper, and the reference [r1] paper report 10.87% for FixMatch. The conditions seem the same 10% of data annotated (= 100k samples annotated). \n[r1] Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning. AAAI 2021. February 2021. ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["8174ac9b-7bbd-50a2-b584-5598894ad25d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why are there some inconsistencies reported on the number for Imagenet-1k where FixMatch is reported as having a top-5 error of 19.55% but the prior work both the original FixMatch paper, and the reference [r1] paper report 10.87% for FixMatch. The conditions seem the same 10% of data annotated (= 100k samples annotated). \n[r1] Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised Learning. AAAI 2021. February 2021. ", "reference_answer": "A: The inconsistency of the ImageNet result reported in this work and two other works is because of the different experiment settings, in particular, **labeled data amount, learning rate, and batch size**. In the AAAI2021 and the FixMatch paper, they use **10%** labeled data whereas the authors use **100k**. Due to the fact that ImageNet contains more than 1M images, e.g. ImageNet2012 has over 1.28M thus 10% being **128k** labels, the proposed labeled data amount is **smaller** than theirs. Besides, they both use a learning rate of **0.1**, whereas the authors use the learning rate of **0.03** to make it consistent with the experiments on other datasets. While the learning rate being smaller, the number of training iterations remains the same, which also causes the proposed reported results lower than theirs. As for the batch size, FixMatch uses **1024** and the authors use **32** due to the computational resource limitation. All these factors together contribute to the inconsistency of the results. These parameter settings are introduced in Table 4 in the proposed appendix, as well as in their papers. However, despite the hyperparameter differences, **the comparison between FixMatch and FlexMatch within the proposed codebase under the same condition is still fair**, and the performance improvement is noticeable. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b856c1c6-8660-5d92-a362-2e432dbecc33", "question": "In Table 3, the MSE increases for some methods as a larger proportion of the data is observed. Why is that?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["c0de57d0-1c93-5a02-86ea-b3e371ea1078"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 3, the MSE increases for some methods as a larger proportion of the data is observed. Why is that?", "reference_answer": "A: Models evaluated on PhysioNet datasets in Table 3 are all trained to minimize prediction errors of both observed points and target points. As a larger proportion of the observed data, re-prediction error of observed points accounts for a larger proportion in the loss value. This may make the models more inclined to learn to re-predict the value of observed points more, which leads to a MSE increase of target points prediction."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a6ca68fc-645b-5aa8-87b4-ed24421e65a3", "question": "What was the architecture used for the hypernetwork? Is there any intuition or heuristics", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8c872a12-f8c8-5947-8865-90659c926e57"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What was the architecture used for the hypernetwork? Is there any intuition or heuristics", "reference_answer": "A: The size of collage parameter vector $\\omega$ does not depend on image resolution, but instead depends on source and range partitioning schemes. For tiling partitions, smaller tiles correspond to a larger vector $\\omega$, as each additional range tile requires two more elements in $\\omega$ for each source tile. Smaller tiles tend to yield more accurate results when decoding through a Collage (see for example `decoding_patch_4` and `decoding_patch_8` in this [anonymous gist](https://gist.github.com/anonymous-conf-sub/dd01870df49a5fdc65d3a99a41abed30). The authors decode the same test image using $4$ x $4$ and $8$ x $8$ range tiles, without using auxiliary domains.). The authors find that introducing auxiliary domains consistently improves performance in compression and generation tasks. Since real data is never perfectly self-similar, introducing the ability to \"extract\" common patterns across images as auxiliary domains for the Collage iterations makes the method more robust to a wider range of datasets compared to standard fractal compression. In the aerial image compression experiment, removing auxiliary patches reduces PSNR, partially closing the gap with vanilla fractal compresson (~ -1 PSNR at high bpp)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "72a2a537-6a73-50de-a144-30ce30679ee1", "question": "What about the diversity scores in Sec. 4.5?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0d97bc45-58ea-5a2c-9d6a-18b36254f00a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What about the diversity scores in Sec. 4.5?", "reference_answer": "A: The authors cannot directly compute the diversity scores under a fair setting for the models in Figure 5a (DML w/o NAT) and Figure 5b (DML w/o Hungarian assign) since they only have five and three effective modes respectively and cannot provide enough candidates for consensus reranking. Nevertheless, the authors still calculate the SelfCIDEr scores for the models in Figure 5 by skipping the consensus reranking step and calculating the score within three randomly sampled captions for each image. The diversity scores are 0.64, 0.73, and 0.86 for DML w/o NAT, DML w/o Hungarian assign, and the original DML."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "03d03ad2-d7e7-5b63-ac35-5b061dca82f6", "question": "Does the improvement in Knowledge-CLIP comes from the knowledge-based objective or just from more training data? How do the authors validate the effectiveness of various components in their architecture?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["5809db97-e501-54dc-b37f-05d442498011"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the improvement in Knowledge-CLIP comes from the knowledge-based objective or just from more training data? How do the authors validate the effectiveness of various components in their architecture?", "reference_answer": "A: The authors carefully design several settings, including:\n(1) CLIP+continuous learning: they train vanilla CLIP (pretrained weights as initialization) on knowledge datasets adopted in this work. \n(2) Knowledge-CLIP-(t1, t2, t3): they remove the training objectives respectively in their work to analyze the contribution of each loss. \nTable 1 Ablation studies of continuous learning / training objectives. The authors report results on the Flickr30K retrieval task and VQA task with ViT-B/32 as image encoder.\n| **Model**                | **KG** **datasets** | **E2E Loss** | **E2R Loss** | **G2E Loss** | **Flickr30K** Retrieval |       **VQA**       |\n| ------------------------ | :-----------------: | :----------: | :----------: | :----------: | :---------------------: | :-----------------: |\n|                          |                     |              |              |              |      Text / Image       | test-dev / test-std |\n| CLIP                     |          -          |      -       |      -       |      -       |       84.2 / 63.1       |     68.9 / 69.2     |\n| CLIP+Continuous Learning |        **√**        |      -       |      -       |      -       |       84.5 / 63.0       |     69.1 / 69.5     |\n| Knowledge-CLIP-t1        |        **√**        |      -       |    **√**     |    **√**     |       85.0 / 64.6       |     70.4 / 71.1     |\n| Knowledge-CLIP-t2        |        **√**        |    **√**     |      -       |    **√**     |       85.7 / 66.0       |     71.2 / 69.9     |\n| Knowledge-CLIP-t3        |        **√**        |    **√**     |    **√**     |      -       |       84.9 / 65.8       |     70.2 / 70.4     |\n| Knowledge-CLIP (Full)    |        **√**        |    **√**     |    **√**     |    **√**     |     **86.3 / 67.2**     |   **72.5 / 72.7**   |\n\nThe above comparison shows results on two representative tasks, including the image/text retrieval task on Flickr30K, and the visual question answering task in VQA. Several observations can be made from the ablation: \n(1) All three training objectives (E2E, E2R, G2E) contribute to improving the model performance. Training the model without any single objective leads to inferior performances on downstream tasks. The E2E, E2R, and G2E loss promote the model from different perspectives by focusing on semantic understanding of concepts, complicated relations between entities, and structural information. Therefore, all three objectives are necessary for the framework and contribute to the improvement respectively.\n(2) By comparing the first and second row, the authors can see that simply training the CLIP model with extra time and data fails to improve the generalization performance. It also demonstrates that the improvements mainly come from the injected knowledge information rather than the continuous learning scheme."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c249fd4a-a6de-5fd9-9f70-31ef8b505973", "question": "Do I understand the training process correctly: T5 base pretrained --> train it on task-parallel data with teacher forcing --> to add additional parallel/non-parallel data using RL? Which of that does line 3 Table 1(a) (T5-Base) report?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do I understand the training process correctly: T5 base pretrained --> train it on task-parallel data with teacher forcing --> to add additional parallel/non-parallel data using RL? Which of that does line 3 Table 1(a) (T5-Base) report?", "reference_answer": "A: Yes, you are correct. The procedure was explained in the paper. Table 1a reports the T5-Base model trained on the task-parallel data with teacher forcing."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4fd1d0fc-3303-5528-b0d5-ed100f9dd3d8", "question": "Which of the following datasets did the authors evaluate on, ImageNet-100, ImageNet-1k, CIFAR-10, CIFAR-100 datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["066d7d21-caf3-5d81-a520-23ebfe718566"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which of the following datasets did the authors evaluate on, ImageNet-100, ImageNet-1k, CIFAR-10, CIFAR-100 datasets?", "reference_answer": "A: Yes, the authors evaluate on all of these datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6da4e703-912d-54c5-ab0a-bf5038311efe", "question": "Selection of $\\gamma_1$ and $\\gamma_2$ values.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6640fe75-58e2-5b35-bf02-e03896a7b227"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Selection of $\\gamma_1$ and $\\gamma_2$ values.", "reference_answer": "A: An analysis of the effect of the parameters $\\gamma_1$ and $\\gamma_2$, at varying of the privacy loss $\\epsilon$, was reported in Figure 14, in the Appendix. Notice how small $\\gamma_1$ and $\\gamma_2$ values may weakly reduce unfairness and how large values could even exacerbate unfairness. \n\nIn the proposed experiments (for all datasets and benchmarks) the authors set $\\gamma_1 = 1$ and $\\gamma_2 = 1$. \nWhile beyond the scope of this work, the authors suggested (L361-L363) that the adoption of a Lagrangian Dual framework (e.g., as in [Ref. 18]) could be a useful tool to their automatic value selection, albeit at an extra privacy cost. The authors think this is an interesting direction for future work."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3c840b9e-4aaa-508c-bcb5-0599d63b8c5c", "question": "Does PACMAC match DINO's multi-crop augmentation strategy?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["71c0db56-6716-5fe2-8b69-ed7a4b489f99"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does PACMAC match DINO's multi-crop augmentation strategy?", "reference_answer": "A: Yes, the authors match DINO's local-global multi-crop augmentation strategy and measure predictive consistency across a random local image crop (of size 112x112) and global image crop (of size 192x192)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e1f3efc6-754f-5bf5-8172-6106f7a623d5", "question": "How are the experiments structured?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9b5e7170-c549-5fab-bef1-c02dc34dc582"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How are the experiments structured?", "reference_answer": "A: For each method reported in tables, the authors use 10 runs, in each of which the authors sample one calibration dataset $D_{cal}$ of size $n$, which the authors subsequently use to construct the conformal predictor $C$ for all test points in that run.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "96c792f5-d7cb-56e6-9b25-e43968a2115e", "question": "From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2a675de9-1ae1-5c11-9721-336a61a6191b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?", "reference_answer": "A: Yes, the authors show this in aggregate in Figure 6 (old Figure 5-right): it shows how the bandit is roughly on par with uniform when the modulation set is curated, but the bandit significantly outperforms uniform in the untuned (“extended”) setting. The authors clarified the caption for this too."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ae98b500-0c35-5565-ba94-d3f7f2b8128c", "question": "I am wondering why this paper chooses the masking training strategy to validate its idea (i.e., semantic parts of objects are the visual analogue of words). Are there any other tasks related to this idea? In addition, what is the relationship between the idea and the masking strategy (i.e., masking parts from intra-part patterns to inter-part)? These questions were not well explained?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b375585e-1dea-57c3-9d6e-18d008b842a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "I am wondering why this paper chooses the masking training strategy to validate its idea (i.e., semantic parts of objects are the visual analogue of words). Are there any other tasks related to this idea? In addition, what is the relationship between the idea and the masking strategy (i.e., masking parts from intra-part patterns to inter-part)? These questions were not well explained?", "reference_answer": "A: *The choice of the task:* the research on mask language modeling (MLM) inspires us to choose the task of mask image modeling (MIM) to explore the visual analogue of words. Specifically, the success of MLM shows that the high-level representations of texts can be learned by masking semantic words. Thus the authors explore the problem of masking semantic meaningful patches (i.e., visual analogue of words) in MIM. \n\n*The relationship between the idea and the masking strategy:* Once the authors obtained part segmentations, a most intuitive way to validate the proposed idea is to mask a portion of semantic parts. However, as the learned semantic parts are coarse-grained (e.g., 6 parts for each image), it is too hard to directly reconstruct the masked parts. To this end, the authors design a masking strategy that can gradually guide the network to learn from intra-part patterns to inter-part relations (i.e., parts reconstruction).\n\n*Other related tasks:* There are several tasks that can be used to explore the visual analogue of words, e.g., multimodal understanding tasks and multimodal generation tasks. And the authors are interested in further discussing this problem in multimodal tasks."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4d0c1c18-984d-516f-964e-3269a1bb84e9", "question": "Are the networks used for other baselines the same? Do all methods have same amount of hyper-parameters?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["23eeff0a-7c50-5517-95ed-a2a28744526f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the networks used for other baselines the same? Do all methods have same amount of hyper-parameters?", "reference_answer": "A: The network structures and hyper-parameters for the common components (e.g., SAC algorithm related, network structure related) are the same across all the methods (as summarized in Appendix Table 4). For soft-modularization and CARE which introduce extra networks and network structures, the authors use the hyperparameters reported in their paper."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "41bd7962-8def-5eee-8696-2fa999fbef69", "question": "Whether the conditional mutual information I(u_j;m_i|o_j,m_-j) given in formula (2) still conforms to the original theory.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["0144c51e-b5f9-5147-8d87-3719b6edc252"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Whether the conditional mutual information I(u_j;m_i|o_j,m_-j) given in formula (2) still conforms to the original theory.", "reference_answer": "A: Yes, it still conforms to the original theory. Following the introduction of the variational information bottleneck, the authors explain it as follows.\nConsider a Markov chain of  $o - \\hat{u*}-m$,  (substituting the $X-Y-Z$ in the original IB and VIB), regarding the hidden representation of encoding $\\hat{u*}$ of the input o, the goal of learning an encoding is to maximize the information about target m measured by the mutual information between encoding and the target $I(\\hat{u*};m)$. \nTo prevent the encoding of data from being $m = \\hat{u*}$, which is not a useful representation, a constraint on the complexity can be applied to the mutual information as $I( \\hat{u*}; m) \\leq I_{c}$, where $I_{c}$ is the information constraint. This is equivalent to using the Lagrange multiplier $\\beta$ to maximize the objective function $ I( \\hat{u*}; m) - \\beta  I( \\hat{u*}; o)$. Intuitively, as the first term is to encourage $m$ to be predictive of $\\hat{u*}$ while the second term is to encourage $m$ to forget $o$. Essentially $m$ is to act like a minimal sufficient statistic of $o$ for predicting $\\hat{u*}$ [1].\nThen specifically for each agent $i$, the authors intend to encourage assistive information$m_{-j}$  from other agents to agent $j$ to memorize its $\\hat{u_j*}$ when assistive information from agent $i$ is conditioned on observation $o_{j}$, \nwhile the authors encourage assistive information $m_i$ from agent $i$ to not depend directly on its own observation $o_i$. Then the authors have the definition of assistive information generation as eq(2). Following [2], the authors have a neural network to generate such assistive information with the objective as the evidence lower bound derived from it in appendix A1.1. \nNote in this specific task $o$ is not at a higher dimension of information compared to $\\hat{u*}$, thus a small $\\beta$ can be used as the authors described in the appendix when listing hyper-parameters used."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "bb2b530e-b627-56bf-922b-36d4a7f764ac", "question": "How to ensure the in-the-wild data contains neither the base nor novel classes information (L52)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["bb1f82ae-09df-5174-adbf-157ba87a8d01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How to ensure the in-the-wild data contains neither the base nor novel classes information (L52)?", "reference_answer": "A: In practice, the authors cannot guarantee that the in-the-wild datasets do not contain any images from the base and novel classes. Furthermore, it should be noted that images in the in-the-wild dataset that contain the base and novel classes (i.e. co-occurrence) are inherently helpful in improving the performance of incremental learning; albeit these images might only exist in small quantities. To demonstrate the effectiveness of the proposed algorithm, the authors show results on the extreme case where there is no co-occurrence of the base and novel classes in the in-the-wild dataset. The authors preprocess the in-the-wild dataset by removing images that contain the base and novel classes using the ground truth object labels.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "35af025a-c1bf-5e91-80cc-529e9262e63d", "question": "What is the performance, compression ratio, and speedup on Waymo dataset?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["b876da92-a3f3-5a21-8e0e-abef53853922"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the performance, compression ratio, and speedup on Waymo dataset?", "reference_answer": "A: The authors evaluate the proposed model on the Waymo dataset. Due to storage reasons, all experiments kept the batch size as 1 and tested on a single A100 GPU.\nThe authors report the performance (table in commen response Q2),  speed, and FLOPs on the Waymo dataset in the following Table.  The proposed method can effectively reduce GFLOPs (around 63%). Although, FLOPs cannot all translate into speed improvements. But the authors still have a nearly 20% speedup in latency reduction due to implementation optimization and hardware issues discussed in (Common respose Q1). The authors believe the authors still have a room for optimization to further improve the efficiency by implementing customized CUDA fuctions.\n\n| Method / speed(ms) | Waymo (VoxelResNet) | speed up | GFLOPs |\n| ------------------ | ------------------- | -------- | ------ |\n| baseline           | 37 ms               | None     | 76.7   |\n| spss               | 32 ms               | 13.5%    | 43.5   |\n| sprs               | 33 ms               | 11%      | 55.2   |\n| sprs+spss          | 30 ms               | 19%      | 28.8   |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4a9982c1-0579-5d2f-a8ee-0209de80cdef", "question": "What are the two steps of DiffPure?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["916943a1-d7b2-5f15-9fce-d4ccf3637a9c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the two steps of DiffPure?", "reference_answer": "A: Forward SDE adds noise to the image to decrease invariance. The model becomes more robust (Eq. 5) due to shifted input distribution. Reverse SDE removes noise from the image to recover invariance. The model becomes less robust (Eq. 6) due to recovered input distribution."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8fcd6f80-1afd-52f4-886d-2a5b0e22e273", "question": "What model was used for the image classification task?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b73ceeab-4bcd-5261-8f93-2810b1cbdbab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What model was used for the image classification task?", "reference_answer": "A: The authors adopted the SCNN model [R3-1] and trained on the MNIST dataset."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "15470417-27dd-5c12-87a8-310ec972f633", "question": "What does “condition” and “target” mean in “C2C, C2T, T2C, and T2T”?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What does “condition” and “target” mean in “C2C, C2T, T2C, and T2T”?", "reference_answer": "A: (1) condition: source image discrete codebooks (pose: source image tokens and pose landmarks vectors, face: sketches tokens)\n(2) target: target image discrete codebooks"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "0ddb40e5-886d-52ce-a81f-24a91055dff3", "question": "What is meant by the rigid accumulation of point clouds, shown in Figure 7?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["55b8b220-92ee-568e-ae6f-add043bfce90"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is meant by the rigid accumulation of point clouds, shown in Figure 7?", "reference_answer": "A: In short, rigid means that the point cloud accumulation was performed using a rigid registration method (*i.e*., ICP). Rigid 6-DoF poses are used for the registrations. The authors will clarify that in the paper.\n\nTo give you a full picture, imagine an autonomous vehicle is moving through the world and sampling point clouds using a lidar sensor. Lidar point clouds are generally sparse, and often one wants to accumulate/densify a set to draw more geometrical information. Therefore the authors can register/align consecutive point clouds into a single frame. If the world is rigid, the authors can use the ICP algorithm for the pair-wise rigid registration (*i.e*., find the rigid 6-DoF pose that best aligns one point cloud against another). However, if the scene is dynamic and there are a lot of movers (*i.e*., moving pedestrians and/or vehicles), if the authors were to apply ICP the authors would see a \"rolling shutter effect\" (a smearing effect) on the dynamic objects. This effect is demonstrated in the front car in the middle row of Figure 7, where the authors used simple rigid 6-DoF poses to register the point clouds rigidly.\nOn the other hand, scene flow gives a per-point transformation (*i.e*., a translational vector) that allows for non-rigid registration. Thus, reducing the smearing effect because the rigid and non-rigid parts of the scene are correctly registered/accumulated. \n\n> **Regarding minor comments.**\n1. Yes, it should be [15,27] in L110.\n2. The authors will consider adding a zoomed inset image in Figure 5 for clarity.\n\n**References:**\n1. Qi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. \"Pointnet++: Deep hierarchical feature learning on point sets in a metric space.\" arXiv preprint arXiv:1706.02413 (2017)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6035e4cf-2aa8-56e2-b3c8-178e57124895", "question": "Table 1 (b): \"Learning to Selectively Learn for Weakly-supervised Paraphrase Generation\" does not report SBLEU, how did the authors get the number then?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["fd4e4c89-3975-58e3-8017-abdebf4e8ac2"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Table 1 (b): \"Learning to Selectively Learn for Weakly-supervised Paraphrase Generation\" does not report SBLEU, how did the authors get the number then?", "reference_answer": "A: In this case, SBLEU = [(1-alpha)BLEU - iBLEU] / alpha. (Here, alpha is not 0.)"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1b304217-f572-5a43-82b6-0b3c796f93aa", "question": "The motivation of few-shot NAS is to alleviate conflicts in weight-sharing (Line 161) and mitigate gradient conflicts (Line 90). As this is one contribution highlighted by the authors, I believe some ablation analysis or discussion on not using the 3 subspaces design should be included.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The motivation of few-shot NAS is to alleviate conflicts in weight-sharing (Line 161) and mitigate gradient conflicts (Line 90). As this is one contribution highlighted by the authors, I believe some ablation analysis or discussion on not using the 3 subspaces design should be included.", "reference_answer": "A: The authors included this ablation analysis in Section 4.1.4 (lines 312-332). The authors compare the performance of a single space ($K=1$) corresponding to not using the 3 subspaces design against the proposed few-shot design with multiple sub-spaces ($K=3$) with results in Table 5. The authors extract subnetworks with the same architecture ($6$ layers, $768$ hidden, $12$ heads, MLP ratio $4$) from trained SuperLMs for each strategy for evaluation with results in Table 5. The authors observe that the proposed design strategy performs the best while containing lesser number of subnetworks demonstrating the benefit of few-shot NAS for language model distillation. The authors choose $K$=$3$ (i.e. 3 sub-spaces) for few-shot NAS for three reasons: (i) The 3 sub-spaces correspond to base, small and tiny model sizes. (ii) Searching over different values of $K$ is a very resource-extensive process since it requires training $K$ SuperLMs for each choice of $K$. (iii) As $K$ increases, the search process becomes similar to the undesirable brute-force discrete search that trains all models in search space individually."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "00a8c1f5-cf82-55d8-87e2-e1209f15b107", "question": "Are the obtained explanations actually sparse as the importance scores are continuous? For example, a method can lead to a uniform importance score distribution over the nodes, i.e., all nodes are almost equally important. What are the results using entropy based metric for sparsity as proposed in [R1] which show that indeed the importance score distribution is not very uniform.\n[R1] Funke, Thorben, Megha Khosla, and Avishek Anand. \"Zorro: Valid, sparse, and stable explanations in graph neural networks.\" arXiv preprint arXiv:2105.08621 (2021).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["40c48abe-f035-5a03-9f8b-e12839c936f1"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the obtained explanations actually sparse as the importance scores are continuous? For example, a method can lead to a uniform importance score distribution over the nodes, i.e., all nodes are almost equally important. What are the results using entropy based metric for sparsity as proposed in [R1] which show that indeed the importance score distribution is not very uniform.\n[R1] Funke, Thorben, Megha Khosla, and Avishek Anand. \"Zorro: Valid, sparse, and stable explanations in graph neural networks.\" arXiv preprint arXiv:2105.08621 (2021).", "reference_answer": "A: The authors followed Definition 2 in ZORRO and added evaluation results using the entropy-based sparsity. In particular, the authors computed the entropy of normalized scores GStarX output for each graph, and then averaged the entropy over each dataset. To understand how sparse these results are, the authors pick three distributions as references. 1) The entropy of uniform distribution over all n nodes in a graph, i.e., Uniform(n), which represents the least sparse output and is an upper bound of entropy-based sparsity as pointed out in the ZORRO paper. 2) The entropy of uniform distribution over the top 25% nodes in a graph, i.e., Uniform(0.25n), where probabilities of the bottom 75% nodes are set to zero. This case is very sparse since 75% of nodes are deterministically excluded, which can be treated as a practical lower bound of the entropy-based sparsity. 3) The entropy of Poisson distribution with mean 0.25n, i.e. Poisson(0.25n). This is a more realistic version of the sparse outputs in case 2). Instead of setting all 75% of nodes to have probability zero, the authors assume the probabilities for tail nodes decrease exponentially as a Poisson distribution while the mean is kept the same as in case 2). \n\nIn the table below, the authors show the average entropy-based sparsity of GStarX outputs vs. these three reference cases on each dataset. The authors see that the GStarX sparsity is between Uniform(0.25n) and Uniform(n) and is close to Poisson(0.25n), which justifies that GStarX outputs are indeed sparse. The authors have included the Table and corresponding discussion in Section 5.2 in the proposed revised PDF.\n\n|                    |   BA-2motifs  |   BACE    |   BBBP    |   SST     |   MUTAG   |   Twitter  |\n|--------------------|---------------|-----------|-----------|-----------|-----------|------------|\n|   GStarX scores    |   2.1352      |   2.4481  |   2.3290  |   2.3282  |   2.2434  |   2.2114   |\n|   Uniform(n)       |   3.2189      |   3.5080  |   3.0728  |   2.8698  |   2.8612  |   2.9833   |\n|   Uniform(0.25*n)  |   1.8326      |   2.1217  |   1.6893  |   1.4855  |   1.4749  |   1.5970   |\n|   Poisson(0.25*n)  |   2.3204      |   2.4686  |   2.2416  |   2.1336  |   2.1323  |   2.1945   |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6baf72d7-dfca-57ed-a14a-3c4e6382fb96", "question": "Why don't you need double sampling for $(s^\\prime, a^\\prime)$ in $\\phi$ and $\\delta$ in $g$?x", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["c5bf1cac-159b-569b-82b7-fbdb276fb010"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why don't you need double sampling for $(s^\\prime, a^\\prime)$ in $\\phi$ and $\\delta$ in $g$?x", "reference_answer": "A: First recall that the updates of the proposed GenTD and GTD are given as follows:\n\nGenTD: update= $E_\\pi[\\phi\\delta]$\n\nGTD: update = $E_D[(\\phi - \\gamma\\phi^\\prime)\\phi^\\top] E_D[\\phi\\phi^\\top]^{-1} E_D[\\phi\\delta]$\n\nFrom the above equations, it can be seen that GenTD does not have a double sampling issue, because its update expression does not have a product of two expectations, whereas the update of GTD does have the form of two expectations multiplied together and hence requires double sampling. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5736dc84-0e1e-5bf2-a680-7f2f5dc57c52", "question": "Does the number of pre-defined filters affect the final results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["e070ad3b-180d-5e8b-a978-d27d1aeaec36"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the number of pre-defined filters affect the final results?", "reference_answer": "A: We’ve discussed how the varying sizes influence the results in the paper, Table 5 and L291-L296. We’ve experimented with different sizes $n = 2, 8, 16, 24$ and found a trend of improvements from larger banks when the number is small. A larger dictionary with more than 8 filters doesn’t bring further improvement. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "937fa1c8-2c12-5d00-9b1f-1030cf7b6802", "question": "The idea of adding an additional network to transform z seems to be something could have been done within the generator. Is the improvement was actually achieved by the bi-level optimization? Are all the components in the proposed model really effective?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The idea of adding an additional network to transform z seems to be something could have been done within the generator. Is the improvement was actually achieved by the bi-level optimization? Are all the components in the proposed model really effective?", "reference_answer": "A: Please kindly note that the authors implement the latent space transform $z^*(\\cdot)$ by updating $\\mathbf{z}$ using I-FGSM rather than a neural network. Using an additional network to transform $\\mathbf{z}$ does not offer any help to address the quality discontinuity issue (and indeed could be done within the generator), because the task of $z^*(\\cdot)$ is to transform the continuous Gaussian distribution to $p_z^{op}$ (defined in Definition 3.3) which is supported on disconnected manifolds, thus $z^*(\\cdot)$ must be a discontinuous mapping. However, neural network naturally leads to a continuous mapping. As an implicit iterative updating transform implemented for $z^*(\\cdot)$ in the proposed work, it is capable of establishing a discontinuous mapping, and note the quantitative and qualitative results in Table 3, Figure 4 and Figure 9 have shown its significant effectiveness."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "757f36b7-c379-5a0f-8bcf-8b9bf157ac0d", "question": "It appears to be that the\"option\" is a sequence of actions? This can only happen in the deterministic environment. What will you do if applying pi does not give the same sequence of actions? For instance, from (s1,a1) -> (s2, a2), where s2 is generated from a random distribution, and a2 is based on s2.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["930b88ee-4750-594c-a7c4-35c7e3447d0f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It appears to be that the\"option\" is a sequence of actions? This can only happen in the deterministic environment. What will you do if applying pi does not give the same sequence of actions? For instance, from (s1,a1) -> (s2, a2), where s2 is generated from a random distribution, and a2 is based on s2.", "reference_answer": "A: The option in the proposed formulation is a latent representation of a sequence of actions. The learning happens in two phases. First, the encoder E and decoder D are trained on sequences of actions with other networks (P and F) as regularization. Second, during the HRL training, the policy pi learns to output an option, which is decoded by the decoder D. The algorithm is not restricted to deterministic environments, because the policy pi learns to output accordingly with the state."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3b2fdcb8-2dc9-5055-9d8d-75a825df739f", "question": "K (total number of modes in the codebook) seems to be an important hyper-parameter. How does the performance vary with respect to K?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["0d97bc45-58ea-5a2c-9d6a-18b36254f00a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "K (total number of modes in the codebook) seems to be an important hyper-parameter. How does the performance vary with respect to K?", "reference_answer": "A: In the proposed main submission, the authors have trained Transformer-DML and AoANet-DML with different codebook sizes (i.e., k = 20, 64, and 100, which can be found in Table 2 and Section 4.5), and evaluated the oracle results. Specifically, Transformer-DML achieves 1.704, 1.871 and 1.953 oracle CIDEr scores with k = 20, 64, and 100, respectively. Moreover, the numbers of effective modes for k = 20, 64, and 100 are 20, 29, and 34, respectively. A similar trend is also observed in the results of AoANet-DML, showing that the oracle performance"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d5bd28f2-3967-5938-bdb5-e718783bf9a0", "question": "Does the method extend to tabular data with a fixed set of features in matrix form?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["1fc786d5-cfc8-580e-b579-c2c56e8dba87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Does the method extend to tabular data with a fixed set of features in matrix form?", "reference_answer": "A: Yes, fairness reprogramming can be applied to tabular data. There are many ways to design triggers. As the tabular data have a fixed input size, the authors can directly apply the **additive trigger** to the input data to keep the input dimension unchanged (i.e., adding a perturbation on the original input), just as the authors adopted in image domains (Figure 1). The authors will include more discussion on trigger designs for different modalities of data in the revised version. To verify the proposed argument, the authors applied the proposed method to the tabular data and conducted additional experiments on the UCI Adult dataset with a two-layer MLP model, and the results are shown in this **[Figure](https://ibb.co/ssNyK7v)**. The results suggest that the proposed method could effectively improve model fairness for tabular data. The proposed method achieves comparable debiasing performance with the post-processing adversarial training method without modifying any model parameters."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1c265b19-ce60-58e6-9c88-6b18a62253c2", "question": "How are the MIWs normalized in Fig.2? It is surprising that the median of DualDICE can be so far away from 1 after normalization.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3959ec3f-8c34-50e6-8515-ca5c61e8020c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How are the MIWs normalized in Fig.2? It is surprising that the median of DualDICE can be so far away from 1 after normalization.", "reference_answer": "A: As discussed in Section 4.2 (b), for numerical stability, the estimated MIW from DualDICE is clipped into $(10^{-8}, 500)$. This is implemented by adding differentiable softplus activations onto the output layer of the MIW network. As discussed in the caption of Figure 2, authors plot the normalized MIWs whose mean on the entire dataset is 1. This is implemented by the code `w = w / w.mean(axis=0)`, where `w` is a tensor for the MIWs of the observations in the offline dataset. Figure 3 (Appendix G) shows the distribution plots of the $\\log(\\text{MIW})$ of the entire dataset generated by the proposed method, and by the variants with the three alternative MIW estimation methods in Section 4.2 (b) over the training process on the example at Figure 2. In particular, in Figure 3, it is observed that for the DualDICE variant, the distribution of MIW on the entire dataset gradually degenerates on very small and very large values. By contrast, the MIWs from this work's method are well-shaped and concentrate around the mean 1 over the entire training process. To conclude, these plots show that the proposed method can still perform well even when the current policy is far away from the behavior policy."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "de9d1e62-9de2-5f29-a453-fab1e82e8421", "question": "Why does Fig. 5 show that Adam converges faster in terms training perplexity but Adam's test perplexity is worse than the one of NovoGrad?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["cf4cbc93-c79d-5784-a1f6-4485f4e5e84f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why does Fig. 5 show that Adam converges faster in terms training perplexity but Adam's test perplexity is worse than the one of NovoGrad?", "reference_answer": "A: Training perplexity depicted in the figure with WikiText-103 learning curve is with dropout turned-on, validation perplexity is with dropout turned-off which makes it a priori lower."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dc083d4f-32a3-52f7-8df4-212b35de079c", "question": "What happens if there are no W's at all (just W=c*I)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9b3f2a55-d06a-540f-ba13-7f25701fa1e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What happens if there are no W's at all (just W=c*I)?", "reference_answer": "A: Actually, SGC is a simplified model by setting all weights $W$ to be identity matrices, which results in much poor performances than those of EGNN. To further clarify the confusion in terms of orthogonal weight controlling, the authors provide supplementary experiments by comparing EGNN and its variant with constant matrices $W=cI$ on all the four concerned datasets. Given the orthogonal diagonal initialization in EGNN, the following table shows the orthogonal regularization is necessary to learn the good trade-off between the energy constraint and the model's learning ability. By visualizing matrices $W$ in EGNN, the authors observe $W$ will be updated lightly around the initialized matrices to adapt to the downstream node classification tasks. Such weight optimization is important for the attributed graph with plenty of informative node features. \n\n| Datasets | Cora | Pubmed | Coauthors-Physics | Ogbn-arxiv |\n| :---:        |   :----: |    :---: |    :---: |  :---: | \n| # Layer | 64 | 64 | 32 | 32|\n| Constant $W$ | 82.5 | 79.9 | 92.8 | 71.7 |\n| Orthogonal regularization | 85.7 | 80.1 | 93.3 | 72.7 |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e37fa66c-7ceb-5d40-817e-066802913793", "question": "It would have been nice to see this model run on datasets other than MSCOCO. I wonder if this approach could still work even if there were only one reference at training time?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["0d97bc45-58ea-5a2c-9d6a-18b36254f00a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It would have been nice to see this model run on datasets other than MSCOCO. I wonder if this approach could still work even if there were only one reference at training time?", "reference_answer": "A: The authors run an experiment on a subsampled version of MSCOCO, where each image is paired with only one caption. The authors find that with careful tuning of the learning rates and batch sizes of the CdVAE branch and the MIC branch, the proposed method is still able to learn representative modes from the training corpus."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "43202dd3-c5b7-545f-88fb-f632e2dc81b7", "question": "As shown in Table 6, using AFeB and AMS together could signiﬁcantly improve the performance. However, using either AFeB or AMB alone slightly gains the performance over ResB. Why is that? Some clear explanations are needed for a better understanding.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["431c48ab-9371-5daf-b349-346879e446a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "As shown in Table 6, using AFeB and AMS together could signiﬁcantly improve the performance. However, using either AFeB or AMB alone slightly gains the performance over ResB. Why is that? Some clear explanations are needed for a better understanding.", "reference_answer": "A: AFeB and AMB together exploit the WSC. As one WSC of high-resolution features is the mixture of details and noises, AFeB is designed to exploit this characteristic for adaptively preserving the indispensable details and filtering unpleasant noises. As one WSC of low-resolution features is with rich contextual information while a too low-resolution destroys the image contents, AMB is designed for enriching the contextual information while keeping the resolution unchanged. Therefore, suboptimal results will be obtained if using either AFeB or AMB alone, i.e., the WSC of multi-scale features is partially neglected."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "adb6ace2-8d73-5460-9670-d40224d80ead", "question": "With similar performance, the proposed method is much faster than its baselines. In Table 1, the proposed SDNet only replaces the first convolutional layer with CSC-layer while SCN is a multilayer sparse coding network. Compare the time and memory consumption of a single sparse coding layer between those methods.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["823a2e85-8548-57a2-8256-5fdb0159c2af"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "With similar performance, the proposed method is much faster than its baselines. In Table 1, the proposed SDNet only replaces the first convolutional layer with CSC-layer while SCN is a multilayer sparse coding network. Compare the time and memory consumption of a single sparse coding layer between those methods.", "reference_answer": "A: Following the reviewer’s suggestion, the authors replace the first convolution layer of ResNet18 with the sparse code layer from SCN[1], and keep the parameters the same as ResNet18 such as channel, strides, kernel size, etc. The comparisons of model size, test accuracy, memory used during training, and training speed are shown as follows:\n\n| CIFAR10      |    Model Size    |      Top-1 Acc   |   Memory      |  Speed |\n|---------------|-------------------|------------------|----------------|---------|\n|ResNet18     |   11.2M              |    95.54%       |   1.0GB         |1600 n/s |\n|SCN             |    0.7M               |   94.36%      |   10.0GB         |   39  n/s  |\n|SCN-first     |    11.2M             |     95.12%     |      3.5GB        |   158  n/s| \n|SDNet18      |    11.2M             |     95.20%     |      1.2GB        | 1500 n/s |\n\n\n\n| CIFAR100     |    Model Size    |      Top-1 Acc   |   Memory      |  Speed |\n|---------------|-------------------|------------------|----------------|---------|\n|ResNet18     |   11.2M              |    77.82%       |   1.0GB         |1600 n/s |\n|SCN             |    0.7M               |   80.07%      |   10.0GB         |   39  n/s  |\n|SCN-first     |    11.2M             |     78.59%     |      3.5GB        |   158  n/s| \n|SDNet18      |    11.2M             |     78.31%     |      1.2GB        | 1500 n/s |\n\nIt can be seen that SCN-first is still much slower than the proposed SDNet. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "076a5d8b-4873-5173-81cf-115d96e9cc4a", "question": "How does the proposed method compare to other state-of-the-art algorithms for k-medoids?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["c54123c6-251b-50b5-a775-396bdfee5734"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the proposed method compare to other state-of-the-art algorithms for k-medoids?", "reference_answer": "A: The authors compare the proposed method with the state-of-the-art algorithms for k-medoids, including K-Medoids++, PAM, and the heuristic method proposed in the proposed work. The results are shown in Table 1. The proposed method can always obtain the same or better objective value than K-Medoids++, PAM, and the heuristic method. Moreover, BB+LD (our global optimal algorithm) can further improve the objective value for several datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5f60e1b9-aa86-58ac-b9bd-20700a164283", "question": "This paper states that one of the limitations of Mixup is that it uses small $\\alpha$ for good generalization. And this paper uses $\\alpha=0.3$ for CiFAR-10 & CIFAR-100, and $\\alpha=0.1$ for ImageNet. But as I know in the case of image classification, e.g., CIFAR-10, ImageNet-1k, the commonly used $\\alpha$ is 1 which leads to a uniform distribution. Could you provide results of these datasets under the commonly used \\alpha and provide some explanation on why Mixup prefers small \\alpha in this case?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["689b981c-7783-576d-9e9d-62efd234639a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "This paper states that one of the limitations of Mixup is that it uses small $\\alpha$ for good generalization. And this paper uses $\\alpha=0.3$ for CiFAR-10 & CIFAR-100, and $\\alpha=0.1$ for ImageNet. But as I know in the case of image classification, e.g., CIFAR-10, ImageNet-1k, the commonly used $\\alpha$ is 1 which leads to a uniform distribution. Could you provide results of these datasets under the commonly used \\alpha and provide some explanation on why Mixup prefers small \\alpha in this case?", "reference_answer": "A: Authors report the cross-validation results below (on the validation splits taken from the training set for C10 and C100, and from the test set for ImageNet) that lead to the choice of the hyperparameters the authors used in the paper, both for CIFAR 10, CIFAR 100 and ImageNet on WideResNet28-10 and ResNet50.\n\n|          | WRN28-10 | WRN28-10 | ResNet50 | ResNet50 |\n|----------|----------|----------|----------|----------|\n| $\\alpha$ | CIFAR10  | CIFAR100 | CIFAR10  | CIFAR100 |\n| 0.1      | 96.06    | 81.04    | 95.35    | 79.60    |\n| 0.2      | 96.46    | 80.91    | 95.21    | 80.11    |\n| 0.3      | **96.77**    | **81.06**    | **95.36**    | **80.31**    |\n| 0.4      | 96.71    | 81.01    | 95.26    | 78.93    |\n| 0.5      | 96.70    | 80.99    | 95.28    | 78.931   |\n| 1        | 96.74    | 80.66    | 94.96    | 78.79    |\n| 5        | 96.62    | 79.84    | 94.98    | 77.74    |\n| 10       | 96.54    | 79.24    | 94.94    | 75.76    |\n| 20       | 96.26    | 78.40    | 95.16    | 75.56    |\n\nDue to the cost of training on ImageNet, authors considered a restricted set of hyperparameters for ImageNet as presented below\n\n|          | ResNet50 |\n|----------|----------|\n| $\\alpha$ | ImageNet |\n| 0.1      | **77.10**    |\n| 0.2      | 77.02    |\n| 1        | 76.19    |\n| 10       | 72.17    |\n| 20       | 71.51    |\n\nFurther cross-validation of $\\alpha$ for CutMix and YOCO architectures: For completeness, authors performed a thorough cross-validation of the hyperparameters for two architectures shown in the CutMix [3] (PyramidNet200) and YOCO [4] (DenseNet121) papers on CIFAR-10 and CIFAR-100:\n\n|          | DN-121  | DN-121   | Pyr-200 | Pyr-200  |\n|----------|---------|----------|---------|----------|\n| $\\alpha$ | CIFAR10 | CIFAR100 | CIFAR10 | CIFAR100 |\n| 0.1      | 95.89   | 80.54    | 96.71   | 82.34    |\n| 0.2      | 96.10   | **80.80**    | 96.70   | 82.17    |\n| 0.3      | **96.21**   | **80.80**    | 96.67   | 81.70    |\n| 0.4      | 96.06   | 79.71    | 96.79   | **82.62**    |\n| 0.5      | 95.98   | 80.17    | **96.92**   | 81.90    |\n| 1        | 96.07   | 79.08    | 96.89   | 81.80    |\n| 10       | 95.93   | 75.76    | 96.69   | 79.50    |\n| 20       | 95.74   | 76.03    | 96.60   | 78.75    |\n\nConclusions:\n- on both C10 and C100 **increasing $\\alpha$ produces suboptimal performance** (and reduced with respect to low $\\alpha$ ). \n- While on C10 the effect of increasing $\\alpha$ can be small, on C100 the differences are sharp.  \n- With respect to [3] authors find $\\alpha$ =0.5 and $\\alpha$ =1.0 to have very similar results on C10 (PyramidNet200), but $\\alpha$ =0.4 (_which was not considered in their cross-validation_) to significantly outperform $\\alpha$ = 0.5.\n- As for **ImageNet** experiments, the training setup (as described in Appendix A.2 and as reproducible using the timm library code) produces optimal value for very low $\\alpha$ (in agreement with [1,2]) and decreased performance for high $\\alpha$ (in agreement with [1]).\nHence, choosing lower $\\alpha$ is beneficial for Mixup and that $\\alpha$ >> 1 degrades the performance and is empirically validated by extensive experiments.\n[1] mixup: Beyond Empirical Risk Minimization, Zhang et al. ICLR 2018\n[2] Resnet strikes back: an improved training procedure in timm, Wightman et al. ImageNet PPF Workshop NeurIPS 2021\n[3]: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV 19.\n[4]: You Only Cut Once: Boosting Data Augmentation with a Single Cut, ICML 22."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4a51fec1-fdbc-5b89-828d-703cc37c813d", "question": "SOLQ gets higher AP on small objects, however, it suffers from large objects compared with SOLOv2 (Table 1). Could you explain why?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["a31b1f08-c3ff-54a1-bb1e-e4d1f878a710"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "SOLQ gets higher AP on small objects, however, it suffers from large objects compared with SOLOv2 (Table 1). Could you explain why?", "reference_answer": "A: Lower performance on large objects has two reasons: **sparse activation of object query** and **fixed coding length of query**. For DETR-based approaches, the authors observed that object queries tend to sparsely focus on specific local regions in the image, so it is relatively hard for object query to capture enough receptive field for large objects. As discussed in R1-Q5, the fixed coding length of object query also constraints the representation power for large objects. Therefore, longer/dynamic coding length of queries maybe developed to adapt various sized objects. The authors will add more discussion regarding to the performance gap between small and large sized objects."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b75866f5-4995-5ba9-9f6a-8e61167ef915", "question": "In DVG-Face, the evaluation metrics of generation quality are Mean Similarity, Mean Instance Similarity and Frechet Inception Distance. Why do the authors take different metrics in this work? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["17e87869-0815-5e88-9ed2-3cb8faaf55ec"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In DVG-Face, the evaluation metrics of generation quality are Mean Similarity, Mean Instance Similarity and Frechet Inception Distance. Why do the authors take different metrics in this work? ", "reference_answer": "A: Yes, the authors did not take the same metrics as DVG-Face due to the differences in the generation method and the training process.\n* Even though DVG-Face can generate multiple pairs of NIR-VIS images for a particular identity, it only generates one NIR-VIS pair per person. DVG-Face measures Mean Similarity (MS) between the pair to evaluate intra-identity consistency. However, the authors generate multiple NIR and VIS face images for a given identity. To obtain the intra-identity consistency, the feature distances (similarity) across multiple images are calculated, namely Mean Identity feature Distance (MID) in the proposed work. In the revision, for better understanding, the authors compare with DVG-Face on LAMP-HQ in terms of MS between pairs and MS across multiple images, which are indicated by 1v1 and 1vN, respectively. The results have been added to Table 2 in the revision. The results show that the proposed method outperforms DVG-Face by achieving higher MS on both settings, which proves the proposed generation well preserves intra-identity consistency. Additionally, the 1vN MS of the proposed method is 0.411. Given the general identity verification threshold (around 0.3), the proposed generation preserves the faces diversity.\n* DVG-Face obtains identity representations for the face generation via random noise sampling. The evaluation of inter-identity diversity via Mean Instance Similarity (MIS) proves the low overlap between generated identities. However, the identity features the authors used for the face generation come from a benchmark VIS face recognition dataset (CelebA). There is no overlap between identities. Thus, the authors did not evaluate MIS in the proposed work. In the revision, the authors add the comparison results on MIS in Table 2. Following the settings in DVG-Face, the comparisons are conducted between VIS-VIS pairs and NIR-VIS pairs. The results suggest that the proposed generation achieves a higher inter-identity diversity than DVG-Face.\n* Frechet Inception Distance (FID) is widely used in GAN-based generation, but the authors use physical rendering based generation. Following DVG-Face, the authors also employed LightCNN for FID evaluation in the revision. The proposed method exhibits higher feature distribution consistency with real data than the GAN-based DVG-Face. Even though the proposed method has not rendered hair and torso, the proposed generation is more close to the feature of real data from the view of a face recognition network."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f30f777f-9f81-5f35-9ac7-b75731995f82", "question": "Explain details of why in table 4, `iBOT-initialized part' performs worse than the baseline model? How will the part learning influence the Semantic-MAE results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["b375585e-1dea-57c3-9d6e-18d008b842a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain details of why in table 4, `iBOT-initialized part' performs worse than the baseline model? How will the part learning influence the Semantic-MAE results?", "reference_answer": "A: Precise semantic parts can benefit SemMAE while imprecise parts would decrease the performance. For example, some parts may be mislocated in the background, and when the authors mask 75% \"parts\" of an image, the models would be required to predict foreground objects given only background information. Such a task would undermine the representation learning process. As a result, the authors think it is reasonable that in Table 4 (main submission), the \"iBOT-initialized part\" performs worse than the baseline model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6b998aff-14b7-5e8c-967f-8851802e17f6", "question": "How do the authors decide the part segmentation results of stage-I being good enough? According to my understanding, for a fair comparison, no labels should be introduced.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["b375585e-1dea-57c3-9d6e-18d008b842a7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How do the authors decide the part segmentation results of stage-I being good enough? According to my understanding, for a fair comparison, no labels should be introduced.", "reference_answer": "A: There are no part labels that are available to evaluate the part segmentation results. The authors analyze the part learning process from two aspects, i.e., 1) qualitative analyses as shown in Figure 4 (main submission), and 2) indirect quantitative evaluation of linear probing as shown in Table 4 (main submission).\nIt is hard to define \"good enough\", but it is clear that better part segmentation results are more beneficial to Sem-MAE."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "36722774-f5bc-5889-9035-d72ee8036cba", "question": "Are such comparisons being made?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["495b8fb4-8621-5d32-bed8-ab504ae61518"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are such comparisons being made?", "reference_answer": "A: The authors agree with this comment. The authors originally designed three variants of the ZFC-SHCN model and reported the results of ablation study on two datasets (i.e., PeMSD4 and COVID-19 in TX) in the paper (see Table 4 in Section 5.5). Now, to emphasize the importance of different components in ZFC-SHCN, the authors conducted additional experiments/ablation studies on PeMSD8, COVID-19 in CA. and COVID-19 in PA. As Tables 6, 7, and 8 show, the authors obtain similar conclusions compared to Table 4 in Section 5,  that is, the proposed ZFC-SHCN significantly outperforms its three variants on PeMSD8, COVID-19 in CA, and COVID-19 in PA. In view of these findings, the authors can safely conclude that all three components - zigzag filtration features, supra-Hodge convolution operation, and graph convolution operation are important components for spatio-temporal forecasting tasks."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "689e6539-2cc5-5b32-b907-4f3bcc7e82e2", "question": "How are individual genes pulled out in Table 3? Do all genes in a significant cluster get put in the table?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["acce441e-5c3a-57b0-affa-a006a47d8e06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How are individual genes pulled out in Table 3? Do all genes in a significant cluster get put in the table?", "reference_answer": "A: It is indeed the case here: the authors make inference on 1000 clusters, then all genes"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6dc7d195-1168-5e5b-8908-e91f000e72b0", "question": "What file format were the images written as before being compressed?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["50a93dcf-f0bd-571d-ba60-a03f6fd2e35d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What file format were the images written as before being compressed?", "reference_answer": "A: The PNG stream does, as pointed out, contain some meta-data, including (at least) the dimensions of and number of color channels in the images. The bz2 stream contains no metadata and was run directly on the raw bits in the (concatenated) images, with no shape or channel information. The authors thought this was the fairest way to compare bz2 to BB-ANS."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7d431523-3614-50de-96fc-d0b40f58fc26", "question": "Can you give comparison to similar works with masked modeling on point clouds?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["9f55a7d3-0879-5f3c-b3ec-cfc003e81c2f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you give comparison to similar works with masked modeling on point clouds?", "reference_answer": "A: \nFu et al. [1], Liu et al. [2], and Pang et al. [3] also conduct point cloud pre-training via masking, which are ***concurrent works*** to ours, but use different strategies for masked modeling.\nComparison to Fu et al. [1]:\nA: 1) **Different pre-training strategies.** Following Point-BERT, [1] utilizes BERT-style pre-training. It is not a masked autoencoder (MAE) and different from the proposed MAE-style pre-training. Such BERT style predicts the masked token encoded by an independently trained tokenizer, while the proposed MAE style directly reconstructs the masked points' raw 3D coordinates, which is simpler and more efficient. \n2) **Less self-supervisory signals.** [1] consists of two complicated losses, a masked modeling loss and a contrastive loss for different sub-sets of point clouds. The proposed Point-M2AE only requires the simple reconstruction loss and achieves better performances.\nComparison to Liu et al [2]\nA: 1) **Different pre-training strategies.** [2] proposed a masked discrimination (MD) pre-text task that conducts binary classification to judge if a point token is masked. It adopts binary focal loss for self-supervision and is different from the proposed MAE-style pre-training that reconstructs masked coordinates.\nComparison to Pang et al [3]\nA: 1) **Hierarchical architectures.** [3] also adopts MAE-style pre-training but utilizes a plain transformer-like 2D MAE without 3D specific modifications. The proposed Point-M2AE adopts a hierarchical encoder-decoder with skip connections and local attention to better capture local-to-global 3D geometries. \n2) **Multi-scale Masking strategy.** [3] adopts the vanilla random masking, but the authors introduce a multi-scale masking to generate consistent visible region across scales. It can largely boosts the performance as shown in Table 7 of the main paper (88.4 $\\rightarrow$ 92.9 for Linear SVM on ModelNet40).\nVisual interpretation of local spatial attention in ablation study.\nA: the authors visualize the attention weights with and without the local attention in ***Figure 5 of the newly-revised supplementary material.*** As shown in the figure, with the local attention, the query point (marked by star) only has large attention values within a local spatial range (marked by yellow dotted circles), other than scattering over the entire 3D shape (marked by yellow arrows). This enables each point to concentrate more on neighboring local features in early stages for capturing and encoding detailed structures.\n\nReferences\n[1] POS-BERT: Point Cloud One-Stage BERT Pre-Training. arXiv 2022.\n[2] Masked Discrimination for Self-Supervised Learning on Point Clouds. arXiv 2022.\n[3] Masked Autoencoders for Point Cloud Self-supervised Learning. ECCV 2022.\n[4] An Image-Based Deep Learning Workflow for 3D Heritage Point Cloud Semantic Segmentation. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences-ISPRS Archives 46.2/W1-2022 (2022): 429-434."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a7faee20-beec-52a0-bae4-c2d6361660e1", "question": "The QBR strategy looks heuristic, is it possible to fall into an oscillation process?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["14c1199a-b695-5cec-a11a-023781bfb0c5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The QBR strategy looks heuristic, is it possible to fall into an oscillation process?", "reference_answer": "A: The below table shows the detail of the QBR process, that the authors can see QBR strategy is not an oscillation process."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "efeef606-8da4-5c9b-955c-f957085c6f34", "question": "Can we apply the model selection using Equation (6) even when SRS is used to estimate $L_{\\mathcal{M}}$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["6fa2c2e1-33d4-5d11-a688-a6a8a5330b1f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can we apply the model selection using Equation (6) even when SRS is used to estimate $L_{\\mathcal{M}}$?", "reference_answer": "A: Yes, SRS can be combined with the proposed model selection procedure, hereafter called AutoMS-SRS. AutoMS-SRS can be regarded as a special case of AutoMS and also has the theoretical guarantees that the selected model yields asymptotically valid FDR control."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ebc07b0d-1ea9-5cc0-a801-1f0487b9c056", "question": "Why have the authors compared their method with many GAN-based methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9a15a16b-5f91-5052-aaf1-79bd0adde73e"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why have the authors compared their method with many GAN-based methods?", "reference_answer": "A: (1) GAN-based autoencoder (AE) methods are the most popular approaches to solve image editing problems. But these methods show limited performances in non-iconic views (unaligned data with complex scenes). Therefore, the authors want to indicate that transformer-based image editing is competitive."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "f35bb2d8-ea42-54f1-893c-9f0cf22eb02b", "question": "Which loss (Eq (1) vs (2)) is used for the actual training of the surrogate model?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Which loss (Eq (1) vs (2)) is used for the actual training of the surrogate model?", "reference_answer": "A: Eq. (1) is used in most experiments, i.e., Table 1, 2, and 3 (in the paper), as label information is usually available. Eq. (2) is a promising candidate, especially for the scenarios where the adversary cannot access the label information. Thus, the authors also report the results in Table 4 (in the paper, termed as Unsupervised) to show that the authors can generate powerful adversarial examples in the no-box threat model, even if the label information is unavailable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a8b39412-6c4e-5258-bcfa-c20f6a8dd49f", "question": "Is Lemma A.4 missing an absolute value sign on the right hand side of both equations in the displace below line 408?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["acce441e-5c3a-57b0-affa-a006a47d8e06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is Lemma A.4 missing an absolute value sign on the right hand side of both equations in the displace below line 408?", "reference_answer": "A: Yes, the absolute value sign is missing on the LHS of the lemma"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "37dd4ee6-7b81-514d-862d-ebb993ce42c2", "question": "Summarize the novelty of this work with respect to prior works in the literature.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Summarize the novelty of this work with respect to prior works in the literature.", "reference_answer": "A: A summary of the novelty and distinction over prior work is:\n**(i) Task-agnostic (AutoDistil) vs. Task-specific NAS.**\nNAS works in computer vision (CV) (e.g., Once-for-all, One-Shot NAS) leverage hard class labels from a given task (e.g., image classification). They often use similar training recipes for SuperNets as in ImageNet-trained models (e.g., MobileNet, RegNet) for task-specific optimization with accuracy as an evaluation metric. \nIn contrast, **AutoDistil training is fully task-agnostic** and does not access task labels during SuperNet training. Different from CV domain, NLP tasks have different objectives and evaluation metrics for classification (e.g., MNLI), regression (e.g., STS-B) and correlation (e.g., CoLA). \n\n**(ii) Fully task-agnostic training objective.**\nIn view of the above, the proposed SuperNet training objective leverages self-attention distillation which is unsupervised and does not require task labels or additional training. In contrast, for downstream task adaptation, existing NAS works in NLP (e.g., DynaBERT, NASBERT, AutoTinyBERT) use additional expensive step(s) of further pre-training / distillation of the optimal architecture with task labels for best performance.\nIncorporating self-attention loss for SuperNet training and distillation in NAS is non-trivial. It requires aligning the attention states (query, key, value) of varying size subnetworks to that of the large teacher. The authors develop an extraction and alignment strategy (Section 3.2) to address this challenge. During sampling, the authors employ Sandwich rule (lines 173-176) to improve the performance of all subnetworks by increasing the performance lower bound (smallest subnetwork) and upper bound (largest one) across all subnetworks.\n\n**(iii) Single-stage training for computational savings.**\nIn contrast to prior works, the authors do a single-stage training combining NAS and distillation with no further pre-training or augmentation and demonstrate the superior performance of the NAS process itself. Obtained subnetworks are simply fine-tuned on downstream tasks. Table 3 demonstrates a massive reduction in search and additional training cost over state-of-the-art NAS work (AutoTinyBERT) on NLP tasks.\n\n**(iv) One-shot vs. Few-shot NAS.** \nIn contrast to prior NAS works in the NLP domain (e.g., DynaBERT, AuotTinyBERT, NASBERT) that employ a single large search space (One-shot NAS), the authors demonstrate the value of sub-space partitioning to reduce gradient conflicts and optimization interference for improved performance with Few-shot NAS design and ablation analysis."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dceb92c0-b655-5967-9270-97551b8a4528", "question": "One major issue is that it is not clear if the improvements are provided by the landmark annotations or just by breaking the long trajectories. Two baseline experiments should be added to the experiment section: (1) divide the instructions into equal size segments (2) divide the instructions into segments of random size (ending at a period). Without these two experiments, it is hard to judge if the provided dataset is a useful contribution or not.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "One major issue is that it is not clear if the improvements are provided by the landmark annotations or just by breaking the long trajectories. Two baseline experiments should be added to the experiment section: (1) divide the instructions into equal size segments (2) divide the instructions into segments of random size (ending at a period). Without these two experiments, it is hard to judge if the provided dataset is a useful contribution or not.", "reference_answer": "A:|           | |en-RxR|||Lk-RxR| | \n| --------- | ------ | ------ | ------ |  ------ | ------ | ------ |\n| #         | nDTW↑  | sDTW↑ | SR↑  | SA↑ | SSA↑| LN↓ |\n| ours         | 21.8    | 10.2 | 18.6  | 49.4 | 44.1 | 7849 (40.2) |\n| baseline1 | 19.7    | 8.5   | 15.8  | 47.0 | 40.3 | 8923 (45.6) |\n| baseline2 | 19.0    | 8.6   | 17.0  | 47.8 | 42.8 | 8353 (42.7) |\n\n1. The authors divide the complete trajectory and instruction pairs from en-RxR into equal size and random size segments to train #baseline1 and #baseline2 models separately. The proposed model (#ours) is only trained on sub-instruction and sub-trajectory pairs from the Landmark-RxR, the same as #model7 in Table3. Three models are tested on the unseen validation split of en-RxR and Landmark-RxR (LK-RxR).   \nThe experimental results are reported in the table above. The proposed model outperforms both the #baseline1 and #baseline2 models on all metrics significantly, with 2.8% and 1.6% improved on SR and 4.4% and 2.5% dropped on Loss Number (LN) separately. The results indicate that the proposed Landmark-RxR has high-quality annotations and has a useful contribution to the community.\n2. Except as training data, the proposed Landmark-RxR is also useful in the validation phase to better evaluate the navigation model, for example, in the way of the proposed re-initialization mechanism. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "81ca7cf2-5d8d-5d62-bce5-12bfa88a166d", "question": "Figure 3 is a bit hard to follow. Could you help clarify this figure? Where does the dotted line come from? Why are there multiple fidelity values for every Δ?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["c0a86e23-300e-5ce6-ba2e-36d9cfb10177"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Figure 3 is a bit hard to follow. Could you help clarify this figure? Where does the dotted line come from? Why are there multiple fidelity values for every Δ?", "reference_answer": "A: The dotted line is the unfairness of the black-box model computed on the suing set data. Figure 3 displays the results of solving the constrained optimization problem in Equation 9. More precisely, the constraints in Equation 9 are related to the fidelity (defined based on loss) and $\\Delta$. For each value of $\\Delta$, the authors consider different values for fidelity because the proposed objective is to assess the evasion power of fairwashing attacks on the Rashomon Set of interpretable models. This is designed to characterize the damage an adversary can achieve given a constraint on $C_{KL}$. Therefore, the multiple fidelity values for every $\\Delta$ in Figure 3 show the performance of the detector when facing different high-fidelity interpretable models."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7507a4e3-577a-592a-9919-bad70fec2b2b", "question": "Provide the claims for the following from the paper: superiority of simple linear combinations.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["36305be5-629e-5348-9c0a-d8033c5e875f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Provide the claims for the following from the paper: superiority of simple linear combinations.", "reference_answer": "A: Similar to superiority of gaussian kernels over linear kernels, the authors switched once the authors found linear combinations performed better. The authors assume this is because the individual prototypes are already well-tuned to the visual domain, and don’t need complex nonlinear transformations such as with semantic embeddings. The ablations are provided in table 2."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a5306512-c90b-5ebd-8960-84cde395ea96", "question": "Are 3D objects generated by MaxOT more perceptible?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["1821bd6b-db14-5a63-b02c-4257dd291b97"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are 3D objects generated by MaxOT more perceptible?", "reference_answer": "A: As shown in Table 2, MaxOT and EOT achieve very similar results in naturalness of the generated adversarial objects.  Besides, as shown in Figure C.3 in Appendix C.4, the qualitative visualization results of MaxOT and EOT also show the same degree of naturalness and imperceptibility of the adversarial objects.  The proposed proposed MaxOT algorithm has better results because it actively searches for the most harmful physical transformations, rather than inducing more perceptible perturbations."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "70b539cf-5716-5352-a059-43ea480d36e1", "question": "Although the proposed method is effective, the major two components (i.e., few-shot NAS and task-agnostic knowledge distillation) are not novel. When compared with DynaBERT, it seems that AutoDistill is its task-agnostic version with a larger searching space and a few other modifications.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["32c48285-22ee-52e8-9f8e-b142616afefa"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Although the proposed method is effective, the major two components (i.e., few-shot NAS and task-agnostic knowledge distillation) are not novel. When compared with DynaBERT, it seems that AutoDistill is its task-agnostic version with a larger searching space and a few other modifications.", "reference_answer": "A: Prior work on few-shot NAS to obtain multiple compressed models of varying FLOPs are task-specific and developed for computer vision (CV) domain. Traditional works on task-agnostic knowledge distillation target a specific compressed model architecture. However, it is non-trivial to obtain a combination of the above. For instance, task-agnostic self-attention distillation for SuperNet training and distillation with NAS requires aligning the attention states (query, key, value) of varying size subnetworks to that of the large teacher. The authors develop an extraction and alignment strategy (Section 3.2) to address this challenge. During sampling, the authors employ Sandwich rule (lines 173-176) to improve the performance of all subnetworks by increasing the performance lower bound (smallest subnetwork) and upper bound (largest one) across all subnetworks.\n\nFurther, existing NAS works in NLP (e.g., DynaBERT, NASBERT, AutoTinyBERT) use additional expensive step(s) of further pre-training / distillation of the optimal architecture with task labels for best performance. In contrast, the proposed single-stage task-agnostic method without additional training offers massive reduction in computational cost for training and search (see Table 3). \n\nCompared with DynaBERT, (i) the proposed search space is more fine-grained. For instance, the authors independently search for width, depth, heads, MLP ratio etc. as opposed to searching for a constant depth ($m_d$) or width multiplier ($m_w$) in DynaBERT which only considers $12$ possible combinations of $m_d$ and $m_w$; (ii)  the proposed training objective does not require labels and is fully task-agnostic with subnetwork attention state alignment for self-attention relation distillation; (iii) further, AutoDistil uses few-shot NAS (Table 1) to mitigate gradient conflicts in SuperNet training, while DynaBERT applies one-shot NAS; (iv) DynaBERT uses additional tricks like data augmentation and teacher assistant also specific to each task, whereas AutoDistil uses a single-stage task-agnostic training resulting in reduced computational cost.\n\nTable 1 in the Appendix compares AutoDistil against all recent NLP works on NAS (e.g., DynaBERT, AutoTinyBERT, NASBERT) and Distillation (MiniLM, DistilBERT, MobileBERT, PKD). Key experimental comparisons are summarized in Figure 1 and Table 2 of the main paper and Figure 1 of the Appendix. The authors will add these discussions to the proposed revision."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "239407ff-11f6-5426-ae04-3aa14322b22c", "question": "In the novel class data subsets, how frequently do novel classes occur with no co-occurrence to base ones?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["bb1f82ae-09df-5174-adbf-157ba87a8d01"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the novel class data subsets, how frequently do novel classes occur with no co-occurrence to base ones?", "reference_answer": "A: The following table shows the co-occurrence frequency count of objects from the base and novel classes in the novel dataset from PASCAL VOC: [Number of objects on novel dataset](https://docs.google.com/presentation/d/1xE4oMZ_4wd9jLii-wQJxme3rqJUQQ6Xq/edit?usp=sharing&ouid=104968873642613184581&rtpof=true&sd=true). The authors can see that objects from the base classes still occur very frequently (even exceeding the frequency counts of novel objects in some cases) in the novel dataset, i.e. strong co-occurrences in all experimental settings. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "6088d687-409b-5d0d-ae81-37bc8a23d8ea", "question": "What kind of non-linguistic dataset is suitable for the paradigm? Does ImageNet fit for all language training?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What kind of non-linguistic dataset is suitable for the paradigm? Does ImageNet fit for all language training?", "reference_answer": "A: Because contrastive sentence embedding learning is about *clustering* and because the data needs to be encoded before being passed to the transformer model, the requirements for a non-linguistic dataset to be used for the paradigm are that it should be suitable for clustering (i.e., be labeled) and that a suitable embedding layer exists for that data. The authors used ImageNet and LibriSpeech for the proposed non-linguistic datasets as they are commonly used and easily accessible labeled datasets. For both datasets the authors used around 30K samples. It is possible that careful cherry picking of the non-linguistic dataset could lead to improved performance, but that was not the objective of this paper. The advantage of not cherry picking the non-linguistic datasets is that the proposed framework can be easily applied without much additional cost. As the authors show in Section 4.5, ImageNet was fit for training in all languages that the authors experimented with (including an experiment on Chinese, showing that the proposed framework can work on non-European languages as well). In brief, the proposed experiments show that ImageNet should suffice as the non-linguistic dataset for all language training."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "e17bebef-c740-5927-8d48-4dd795dfdf60", "question": "Is there any randomness (random input) involved during completion (generation)? Is this model possible to generate different faces from the same conditional input? &  What happens if the attributes are interpolated rather than zero or one?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["dd73f883-1f41-5458-940a-029226e7b278"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is there any randomness (random input) involved during completion (generation)? Is this model possible to generate different faces from the same conditional input? &  What happens if the attributes are interpolated rather than zero or one?", "reference_answer": "A: For the model in this paper, there is no randomness involved. But the authors made a video demo (not included in this submission) to show the synthesized faces from interpolated attributes (some snapshots of the demo video is added as Figure 15 in the appendix). All of the faces are very natural, and the transitions between attributes (e.g. from smile to not smile, from male to female) are very smooth. It means this model is able to produce a variety of images. The results of soft attribute control will be added into result figures. Randomness can be achieved by attaching a random noise vector to the attribute vector."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4662ec46-e046-5eb0-b80e-80019048c4ad", "question": "Discuss and compare this work against related work (Jain et al, 2019, Ilharco et al, 2019 as both used fidelity-oriented rewards with RCM).\nReferences:\n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019. \n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Discuss and compare this work against related work (Jain et al, 2019, Ilharco et al, 2019 as both used fidelity-oriented rewards with RCM).\nReferences:\n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019. \n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019", "reference_answer": "A: The major differences between the proposed work and you mentioned previous ones [1, 2] are as follows.  \nOur focal-oriented rewards focus on addressing the local cross-modal alignment problem with fine-grained supervision, while prior works only pay attention to the global cross-modal alignment, like the global goal points (goal-oriented reward [3]) and global trajectory similarity (fidelity-oriented reward [1, 2]).   \nIn addition, the authors have made the desired comparisons in Table 3 (model#16). For the fidelity-oriented reward, it can be decomposed as: fidelity metric + SR. In the proposed experiment using model#16, the authors choose the nDTW [2] as the fidelity metric to design the fidelity-oriented reward. The authors compare the proposed two kinds of focal-oriented rewards with the fidelity-oriented reward (model#16, nDTW+SR) in Table 3 and analyzed the results in Section 6.2. The proposed soft focal-oriented reward outperforms nDTW+SR with 0.7% dropped on Loss Number (LN), and the proposed hard focal-oriented reward outperforms nDTW+SR with 1.8% dropped on LN and 1.3% improved on SR. In addition, the authors did not consider the CLS [1] metric in the paper because it is order-invariant and not ideal in some scenarios as described in [2]. For your reference, the proposed results using CLS+SR as the fidelity-oriented reward on Landmark-RxR (Val Unseen) are SA (56.4), SSA(32.3), LN (5279) and on en-RxR (Val Unseen) are nDTW (39.6), sDTW (24.5), SR (32.9). The proposed soft focal-oriented reward outperforms CLS+SR with 0.5% dropped on LN and 0.8% improved on SR, and the proposed hard focal-oriented reward outperforms CLS+SR with 1.7% dropped on LN and 2% improved on SR. \n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[3] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7cb28c62-d44d-5e92-97c6-dc63a84b43a5", "question": "Why is the offline allocation problem a semi-discrete Optimal Transport problem?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["74f0301e-fe9c-5e13-a26b-958c7e7c374b"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the offline allocation problem a semi-discrete Optimal Transport problem?", "reference_answer": "A: Equation 2 is a convex optimization problem with n variables. The difficulty of computing Equation 2 is the evaluation of the objective function. However, efficient stochastic optimization methods have been proposed (see Aude et al 2016)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8a807cf2-8cd4-51ab-a722-31bb9841254d", "question": "Are there some states that are consistently discarded due to model inaccuracies? What proportion of states would be rejected with a 1-step rollout and double check?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["ae801009-35b9-58e7-941e-726cb84d7b54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there some states that are consistently discarded due to model inaccuracies? What proportion of states would be rejected with a 1-step rollout and double check?", "reference_answer": "A: In Figure 4 of the main text, the authors show that either forward imagination and backward imagination is unreliable, as many invalid states are generated. While CABI can consistently reject those states. Therefore, there are some imagined states that are consistently discarded due to the disagreement between forward model and backward model in CABI. In the proposed experiments, the authors keep $k$ unchanged. That is, even for 1-step rollout, the double check mechanism will reject 80\\% samples. The authors want to note here that the authors only adopt 1-step rollout for *pen-human*, *pen-cloned*, *pen-expert* and *hammer-expert*. Because the model disagreement is large for larger horizons on those datasets (fitting these complex high-dimensional datasets can be difficult, please see Table 5 in the appendix) and the authors find reject 80\\% samples for them is better. For simple tasks like MuJoCo, one ought not to reject 80\\% imagined transitions when 1-step rollout is adopted (we keep 80\\% transitions in MOPO (filtering) for 1-step rollout)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d9d5a5d7-7021-5c28-8fb4-49edd910cdf6", "question": "In Table 1, why do you mark Word2Vec as not being effective with small D? One of the persistent advantages of Word2Vec is its low dimensionality.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["266bb9ac-3f58-5a57-9c1b-862d7d3d6df4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1, why do you mark Word2Vec as not being effective with small D? One of the persistent advantages of Word2Vec is its low dimensionality.", "reference_answer": "A: In the proposed work, D refers to the dimensionality of the semantic space, not the number of parameters for each word. For Word2Vec, these are the same. The “small-D” property decides whether polysemy can be naturally represented with a low dimensionality to facilitate visualization. The authors clarified the meaning of “small D” in the new version. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b1c14ac8-9945-551a-ba8a-5251ceeb427a", "question": "What is the finding of the extended MDP formulation?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["376c551b-8193-53f7-a790-e2619229492a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the finding of the extended MDP formulation?", "reference_answer": "A: The number of states of the extended MDP grows exponentially with the horizon $H$, which means that solving the finite trials convex RL problem with the extended MDP is not tractable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "689467c0-7386-5697-bc0b-0abdc1a738a7", "question": "Why is scatter a useful performance proxy anywhere but the first classification layer?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9f7699aa-c177-51c2-9cd5-456febaa7dbb"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is scatter a useful performance proxy anywhere but the first classification layer?", "reference_answer": "A: Let me explain by giving an example of a network with 3 CLs, connected as shown in this figure: https://drive.google.com/file/d/1tTGtdeyAwPvzbQ2YWeTQicDzm1RPn38q/view?usp=sharing\nIf the authors compute all the scatter vector values, S_f and S_b will be good because every output neuron is connected to every input neuron, i.e. the input-to-output connectivity is good. But this is not a good network because 2 of the 3 hidden neurons are being wasted and can be removed. The problem with this network is captured by the other scatter values S_1f, S_1b, S_2f and S_2b, which will be poor. This is why all the values in the scatter vector need to be considered, since some low values may lead to performance degradation, as shown in Fig. 7.\nThis is a toy example used for demonstration, but the authors simulated a larger example using a similar approach and obtained inferior performance. The authors hope this serves to explain why intermediate hidden layer connectivity is important."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a8dfdd63-088c-5a6e-9157-ab547458ce2f", "question": "Where does the score function estimator come from?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d93d3d23-a98b-5dbc-9827-e95bb0c84095"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Where does the score function estimator come from?", "reference_answer": "A: The score function estimator is a straightforward application of the log-ratio or REINFORCE trick to the loss function in equation 9 with respect to the policy parameters, and this derivation has been added to the appendix. We’ve also added this alternative (equivalent) terminology to the main body of the text, as REINFORCE is the more common touchstone for parts of the community."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3981328c-8701-5bb9-bbbb-eff195de0167", "question": "How many samples are used in table 1 LPD? Or is it argmax decoding for each length?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["2a7b14b6-33c1-505a-8a3a-d9580ad76ca7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How many samples are used in table 1 LPD? Or is it argmax decoding for each length?", "reference_answer": "A: The authors set the \\Delta M=4, which means that there are 9 candidates for length parallel decoding (LPD). Yes, the authors follow the previous practice[1] and perform the argmax decoding both the position predictor and the decoder for each length."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c19074e5-5d8c-595c-bfe4-0ceb898e3711", "question": "Will the large-scale temporal networks limit the behavior of PINT?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["412b382c-197c-589c-864a-4e8a67c013e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Will the large-scale temporal networks limit the behavior of PINT?", "reference_answer": "A: As shown in Figures 6 and S6, PINT's computational overhead can be amortized during training."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dbc183e1-babb-52d2-98a9-0b96034ee82e", "question": "Can you please explain the setting difference between lines 306-307 and lines 133-135?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["13b7a9cd-5011-548f-8c52-2c24218ee25a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you please explain the setting difference between lines 306-307 and lines 133-135?", "reference_answer": "A: Line 306-307: This experimental setting is to show that the proposed negative data augmentation can effectively reduce the model's reliance on features preserved in small patches and perform similarly as humans.  the authors **train** the model on the **clean images** using the proposed negative data augmentation. The authors then test the model on the patch-transformed images and find that the model could not identify the semantic class of these patch-transformed images. \nLine 133-135: This experimental setting is to show that if the model only captures the features preserved in small patches, the robustness of the model degrades significantly. Specifically, the authors **train** the model **only on patch-based transformed images** with the original semantic class assigned as their ground-truth. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b6edd72b-7cac-5b14-bd31-4b70c226f134", "question": "According to Table 7, ASoftmax does not bring consistent improvement with 1024 images, and may even suffer from overfitting (L254). Is there a systematic way to determine the optimization space with the training size? Additionally, has the paper tried larger discrete optimization space given 10240 images?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["7f09e532-bc3b-56a1-96ac-6dc6c9cf7072"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "According to Table 7, ASoftmax does not bring consistent improvement with 1024 images, and may even suffer from overfitting (L254). Is there a systematic way to determine the optimization space with the training size? Additionally, has the paper tried larger discrete optimization space given 10240 images?", "reference_answer": "A: ASoftmax brings consistent improvement compared to RSeR, as show in Table 5(Table 4 in the proposed revised version). Table 7 further explores the potential of ASoftmax via extending the learnable range to a wider range. Let B denote the learnable range and P denote the model's weights, then the optimization space can be quantified as $|B|^{|P|}$. The basic rule is that given 1024 images, extending B for networks whose $|P|$ larger than 10M may raise the risk of over-fitting. Given 10240 images, extending B to wider range is very safe according to the proposed experiments. The authors have extended B to {-2, -1, 0, 1, 2, 3} but the improvement is marginal. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "1614f458-9a1b-5cc2-92c8-cf8a409c23bd", "question": "Given a statistical test was used in Table 1, why is there no statistical test in table 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["f7af899e-0bc4-5746-9d3e-9711afb910b9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Given a statistical test was used in Table 1, why is there no statistical test in table 2?", "reference_answer": "A: For table 2 the authors focus on observing the influence of readout functions for various Transformer architectures. Therefore the significant test is not performed between the best readout function with others."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a7b481d1-9c0e-539c-8a3d-336df0a84f62", "question": "Are the experimental results significant looking at the score differences of PINT with best model of the literature. For instance the differences are in the ranges ~0.5 scores of AP for many results in Table 1?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["412b382c-197c-589c-864a-4e8a67c013e3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are the experimental results significant looking at the score differences of PINT with best model of the literature. For instance the differences are in the ranges ~0.5 scores of AP for many results in Table 1?", "reference_answer": "A: The authors note that taking into account standard deviation is important to assess statistical significance. While mean AP differences between PINT and the second-best model do not seem high on attributed networks (Twitter/Reddit/Twitter), the authors highlight that the corresponding standard deviations are much smaller. For instance, in table 1 (transductive setting), on 4 out of 5 datasets, the mean AP of PINT is over 5 standard deviations away from the second-best model."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "df3561ea-80d0-5c35-842b-128a88b69525", "question": "Is a uniform distribution necessarily better?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["db8f6428-8da5-5415-8f58-ce4bc7a45df3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is a uniform distribution necessarily better?", "reference_answer": "A: Regarding the concern that a uniform distribution is not necessarily better, it is indeed not ideal for 1D data with the support being the entire $\\mathbb{R}$ to have a uniform distribution over the entire infinite $\\mathbb{R}$ space. However, for natural image datasets such as human faces, a uniform distribution over the manifold is reasonable, because all human face images fall on a manifold restricted to a bounded region $\\left[0,255\\right]^{C\\times H\\times W}$ rather than extending to the entire infinite $\\mathbb{R}^{C\\times H\\times W}$ space, where $\\left[0,255\\right]$ is the range of pixel values and $C\\times H\\times W$ is the dimensionality of the image. Therefore, it is reasonable to adopt a uniform distribution on a finite manifold. In addition, it is subjective to adopt which kind of distribution over the support set. Although one may prefer some samples to others, the authors adopt the uniform distribution over the manifold because the authors take into account that every sample on the manifold can be equally accepted as a real image, which should also be acceptable."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c248b985-b54a-54e9-8cf8-a93a3630921e", "question": "Why NovoGrad method works much better than Adam or AdamW in Table 6 with no weight decay?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["cf4cbc93-c79d-5784-a1f6-4485f4e5e84f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why NovoGrad method works much better than Adam or AdamW in Table 6 with no weight decay?", "reference_answer": "A: For language modeling with Transformer-XL, the authors used only Dropout for regularization, following the original paper [4]. The authors experimented with weight decay too, but did not manage to get better results for both NovoGrad and Adam (the scores of AdamW are comparable to those of Adam)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c9886a07-5b3b-5030-9e05-d14ae5413cc6", "question": "The performance of HCID without $r$ and HCCD without $h_{con}$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["5a46923f-6db9-5f38-95a2-9f528d88e2b0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The performance of HCID without $r$ and HCCD without $h_{con}$?", "reference_answer": "A: The authors conducted the suggested experiments (i.e. the HCID without $r$ and HCCD without $h_{con}$ in the above table). It can be seen that HCID w/o $r$ outperforms the recent contrastive learning methods by over 5\\% in mIoU, which is largely attributed to the proposed historical contrast design. Including the reliability score (in HCID w/ $r$) further improves the mIoU by 1.4\\%. In addition, HCCD w/o $h_{con}$ becomes a basic self-training method and further including the proposed historical consistency (HCCD w/ $h_{con}$) improves mIoU by 4.1\\%. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "956269d7-1cdb-5d31-be2b-2b3f4c4f36b0", "question": "why not use BCD as a training metric to replace CD or EMD?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["1ba9b611-9a76-5d01-8476-dade426b3784"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "why not use BCD as a training metric to replace CD or EMD?", "reference_answer": "A: The authors tried to use BCD a training loss in a PCN baseline model. The results are provided in the table."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "255569ac-85d0-513b-bc17-6e09414c1409", "question": "Limitations: In my opinion, the limitations of this work are two-fold. First, as the authors mention, they only tackle the CVML task, however, to fully address this task, the results of state-of-the-art approaches on particular datasets should also be included, showing that they indeed struggle with catastrophic forgetting. Otherwise, it would be beneficial to address other cross-modal tasks. Can you justify?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["36883baf-d01c-533a-b3b4-376fb580e945"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Limitations: In my opinion, the limitations of this work are two-fold. First, as the authors mention, they only tackle the CVML task, however, to fully address this task, the results of state-of-the-art approaches on particular datasets should also be included, showing that they indeed struggle with catastrophic forgetting. Otherwise, it would be beneficial to address other cross-modal tasks. Can you justify?", "reference_answer": "A: In this work, the authors choose to study the CVLM setting based on cross-modal MoCo, and the results in Figure 1 show that the catastrophic forgetting problem indeed exists. Since the state-of-the-art approaches to VLM including COTS [32] and HiT [30] have similar cross-modal MoCo architectures, they would also suffer from catastrophic forgetting. Therefore, the proposed study on the CVLM setting is vital for video-language modeling with streaming data. Additionally, the proposed BMU-MoCo is generalizable and can be transferred to other cross-modal tasks or other continual learning settings."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "095a89ba-afff-5d38-8569-dca435395f8b", "question": "Could you elaborate on the epochs used for training the audio and vision model, and will the quality/amount of image/audio affect the performance of the proposed methods?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["26d07b07-7e03-50d2-be32-6ffb8ae58d27"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you elaborate on the epochs used for training the audio and vision model, and will the quality/amount of image/audio affect the performance of the proposed methods?", "reference_answer": "A: unsup-SimCSE trains for one epoch, and converges in thousands of steps. Thus, the authors also simply match the training policy of SimCSE and train for only one epoch. The number of images used for training (30K) is chosen such that during the training until convergence the model will iterate through the image dataset a few times (when using a batch size of 48).\n\nthe authors further carry out experiments on substituting ImageNet with a lower quality CIFAR for VisualCSE. CIFAR images have a shape of 32x32 and the authors intentionally resize (enlarge) them to 224x224 to be encoded by the ViT embedding layer. This interpolation causes the CIFAR images to become blurry and lower quality. The experiments in the paper show that the proposed framework improves over SimCSE even with this lower quality dataset (results shown below).\n\nRoBERTa-base-uncased:\n| Model | Avg. |\n|:--------|:-----:|\n| SimCSE | 76.57 |\n| VisualCSE (CIFAR) | 77.71 |\n| VisualCSE (ImageNet) | 77.87 |\n \n\nBERT-base-uncased:\n| Model | Avg. |\n|:--------|:-----:|\n| SimCSE | 76.25 |\n| Visual"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "63e97153-3618-5368-b80b-f0d492fc2099", "question": "Equation 1: this formulation is interesting. Just the dependence of the policy isn't explicitly dependent on $w_k$, which is flawed. Perhaps you should define $\\pi_k$ from $Q_k$.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["7bb011ff-6526-5a93-9b59-878505dc29ac"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Equation 1: this formulation is interesting. Just the dependence of the policy isn't explicitly dependent on $w_k$, which is flawed. Perhaps you should define $\\pi_k$ from $Q_k$.", "reference_answer": "A: The proposed definition of $\\pi_k$ follows Eq.1: $\\pi_k(s)=\\arg\\max_a Q(s,a)$, so $\\pi_k$ is already dependent on $Q_k$. Probably you mean $\\pi_k$ should be defined with $w_k$? This is indeed the case and the authors will change the notation $\\pi_k$ to $\\pi_k^{w_k}$ and $Q_k$ to $Q_k^{w_k}$ to emphasize the dependence of $\\pi_k$ and $Q_k$ on $w_k$."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7028e121-d8bc-594d-a8f1-47e0255f81be", "question": "Failure on SVHN is associated to imbalanced classes, however can you provide a concrete analysis on SVHN results?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["8174ac9b-7bbd-50a2-b584-5598894ad25d"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Failure on SVHN is associated to imbalanced classes, however can you provide a concrete analysis on SVHN results?", "reference_answer": "A: The SVHN dataset has two unique properties compared to other datasets: first, samples of each class in SVHN are imbalanced; second, the task is simple and easy (i.e., classifying digits vs. natural images in CIFAR). Due to the data imbalance issue, CPL may generate low final thresholds for the tail classes according to equation 6. Such low thresholds allow noisy pseudo-labeled samples to be trusted and learned. This is known by observing the loss descent figure where the low-threshold classes have major fluctuations. FixMatch, on the other hand, fixes its threshold at 0.95 to filter out noisy samples. Such a fixed high threshold is not preferable with respect to both accuracies of hard-to-learn classes and overall convergence speed as explained in the proposed work, but since SVHN is an easy task, the model can easily learn the task and make high-confidence predictions, it becomes less problematic to set a high-fixed threshold. The two properties together contribute to the result that FixMatch performs slightly better than FlexMatch in several cases. And we’d like to mention again that the more challenging the task is, the more significant performance improvement the proposed method will bring, as reflected on the STL-10, CIFAR-100, and ImageNet datasets. In addition, it could be the proposed future work to do research on imbalanced classification problems with the proposed method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b176e6a4-a5ea-5401-ad25-d454ebcab774", "question": "In Table 1, why is the proposed method highlighted as having the smallest accuracy drop on SqueezeNet?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ea39d473-35e7-5674-852d-e5b7733801a6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In Table 1, why is the proposed method highlighted as having the smallest accuracy drop on SqueezeNet?", "reference_answer": "A: The proposed evaluation methodology is to configure the proposed method to have an energy that is *the same as or lower than the lowest energy of prior work*, and compare the accuracy drops."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "14ad0e35-dd17-58cc-b0f2-b39934eda856", "question": "The sensitivity analysis of sketch dimension k indicates that reducing dimensions can result in distinct performance patterns... will there be a recommendation for selecting k?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["5a61440f-7209-5976-84ba-2fccfdfd2b62"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The sensitivity analysis of sketch dimension k indicates that reducing dimensions can result in distinct performance patterns... will there be a recommendation for selecting k?", "reference_answer": "A: Reducing the sketch size certainly can result in distinct performance patterns. Loosely speaking, the proposed methods work similarly to regularization. Depending on the dataset, different values of the sketch size k may be optimal. For example, Figure 2 (in the main text) shows that k=1 is optimal for Random Projections on Dionis, but on SF-Crime or MoA, k=20 performs better. The positive side of the proposed experiments is that the proposed methods work well for a wide range of values of k, which means that one can take simply k=5. However, it is also possible to add k to hyperparameters that are tuned. In the proposed view, k will not play a significant role here taking into account how many hyperparameters boosting frameworks have and that hyperparameter optimization is usually done using the random search or Bayesian optimization. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "226bf7e4-1421-5a14-9e58-08cec674c518", "question": "What is the text consistency metric?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the text consistency metric?", "reference_answer": "A: The authors further create a text consistency metric that measures the proportion of the captions for which the authors retrieve the correct image and one of the four similar captions simultaneously over the whole dataset."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "88b5afc9-a423-5236-b43d-39f17d1e9276", "question": "What is the time and memory consumption for the ImageNet experiments, in particular, how long did the program run, and how many GPUs (or other devices) did it use? If one hopes to extract more training instances from ImageNet (e.g., 5, 10, 20, ... images per class), what is the computational cost w.r.t. the number of training instances?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the time and memory consumption for the ImageNet experiments, in particular, how long did the program run, and how many GPUs (or other devices) did it use? If one hopes to extract more training instances from ImageNet (e.g., 5, 10, 20, ... images per class), what is the computational cost w.r.t. the number of training instances?", "reference_answer": "The largest experiment the authors have run was the ImageNet-1K (distilling two images per class, 2000 in total). This experiment was run on an A100 GPU with 40GB memory on AWS for a week (2 million gradient updates, including various checkpoints evaluation). The scalability with respect to the number of training instances is shown in Appendix C.5. The authors cannot extract more than 10K training instances from ImageNet due to the proposed hardware constraints. The proposed current codebase does not support distributed training, so the number of distilled images the authors can extract is bounded by the GPU memory the authors can use. This engineering problem can be solved using the KIP paper's distributed kernel computation framework (https://arxiv.org/abs/2107.13034).\n\nUsing all the synthetic data points to compute the meta-gradient is a limitation of the current method since the matrix inversion dominates the time complexity in Equation 2, which is O(N^3). It is acceptable with a few thousand images but becomes extremely expensive with tens of thousands of images. An ad hoc solution to distill more data points is to split the whole dataset into groups, either by class or randomly, and perform independent distillation like in Section 5.1. This is also the standard practice for previous methods. However, the authors observe a performance drop when the authors use such a strategy since the independent distillation may generate redundant information or fail to capture the distinguishable features that can only be identified considering the whole dataset. This problem is caused by not optimizing all parameters jointly, which can be potentially addressed by better parameterizing the distilled data. For example, the authors can parameterize the distilled data using a neural network. In that case, the authors can use a subset of synthetic data to compute the meta-gradient and optimize all model parameters jointly. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b9c23a7b-bb01-5c17-bf23-4af846877a42", "question": "Compare against relevant prior work, including the close works of RCM with fidelity oriented reward [Jain et al, 2019, Ilharco et al, 2019], as well as other prior work using sub-instructions (such as BabyWalk [Zhu et al, 2020], [Hong et al 2020]).\n[1] Yicong Hong, Cristian Rodriguez-Opazo, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-language navigation. Empirical Methods in Natural Language Processing, 2020.\n[2] Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, and Fei Sha. Babywalk: Going farther in vision-and-language navigation by taking baby steps. Association for Computational Linguistics, 2020.\n[3] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[4] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[5] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["9cb6fc64-e649-54a7-af7b-45c9a68f529a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Compare against relevant prior work, including the close works of RCM with fidelity oriented reward [Jain et al, 2019, Ilharco et al, 2019], as well as other prior work using sub-instructions (such as BabyWalk [Zhu et al, 2020], [Hong et al 2020]).\n[1] Yicong Hong, Cristian Rodriguez-Opazo, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-language navigation. Empirical Methods in Natural Language Processing, 2020.\n[2] Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, and Fei Sha. Babywalk: Going farther in vision-and-language navigation by taking baby steps. Association for Computational Linguistics, 2020.\n[3] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[4] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[5] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019.", "reference_answer": "A: The authors propose the current largest-scale, human-annotated sub-instruction dataset. In mentioned related works, their used sub-instructions are automatically obtained by heuristic rules, which are not precise enough and limit the navigation performance. This is the first paper to create sub-goal level human annotations for sub-instructions for the instructions in RxR, and makes meaningful contributions in terms of adding more fine-grained data to RxR dataset. In addition, the proposed experiments about fine-grained data are designed to demonstrate that the supervision from fine-grained and coarse-grained data can complement each other to improve the cross-modal alignment ability of the model itself. By contrast, although Hong et al. [1] and Zhu et al. [2] also use fine-grained data, both of them use the fine-grained data mainly for a “one by one” strategy but not to investigate the relationship between different granularity data and improve the cross-modal alignment ability of the navigation model itself. During validation, they segment a given instruction into several sub-instructions that are much easier to navigate and mainly focus on how to navigate these easy sub-instructions one by one for better performance. And the major differences between the proposed work and your mentioned previous ones [3, 4] are as follows.  \nOur focal-oriented rewards focus on addressing the local cross-modal alignment problem with fine-grained supervision, while prior works only pay attention to the global cross-modal alignment, like the global goal points (goal-oriented reward [5]) and global trajectory similarity (fidelity-oriented reward [3, 4]). In addition, the authors have made the desired comparisons in Table 3 (model#16). For the fidelity-oriented reward, it can be decomposed as: fidelity metric + SR. In the proposed experiment using model#16, the authors choose the nDTW [4] as the fidelity metric to design the fidelity-oriented reward. The authors compare the proposed two kinds of focal-oriented rewards with the fidelity-oriented reward (model#16, nDTW+SR) in Table 3 and analyzed the results in Section 6.2. The proposed soft focal-oriented reward outperforms nDTW+SR with 0.7% dropped on Loss Number (LN), and the proposed hard focal-oriented reward outperforms nDTW+SR with 1.8% dropped on LN and 1.3% improved on SR. In addition, the authors did not consider the CLS [3] metric in the paper because it is order-invariant and not ideal in some scenarios as described in [4]. For your reference, the proposed results using CLS+SR as the fidelity-oriented reward on Landmark-RxR (Val Unseen) are SA (56.4), SSA(32.3), LN (5279) and on en-RxR (Val Unseen) are nDTW (39.6), sDTW (24.5), SR (32.9). The proposed soft focal-oriented reward outperforms CLS+SR with 0.5% dropped on LN and 0.8% improved on SR, and the proposed hard focal-oriented reward outperforms CLS+SR with 1.7% dropped on LN and 2% improved on SR. \n[1] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. Association for Computational Linguistics, 2019.\n[2] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. NeurIPS Visually Grounded Interaction and Language Workshop, 2019.\n[3] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6629–6638, 2019."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dbdc7660-cba2-5732-9774-13dc2c1ee0fd", "question": "Adding vs Removing attributes: Currently the utterances are additive, ie we add attributes to the chair, they might be conflicting such as \"stand legs\" and \"wheels\". But is it's possible to say mutually exclusive things, such as \"a char with four legs\", then \"a chair with one metal leg\" and then \"a chair with four legs\". OR \"a chair with armrests\" and then \"a chair without armrests\". I wonder if there is a metric to compute, when you do subtract attributes you should return to the exactly previous chair? Is it a good loss function?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["27b7f5a3-f467-576e-b6bf-531ed0e8bbc9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Adding vs Removing attributes: Currently the utterances are additive, ie we add attributes to the chair, they might be conflicting such as \"stand legs\" and \"wheels\". But is it's possible to say mutually exclusive things, such as \"a char with four legs\", then \"a chair with one metal leg\" and then \"a chair with four legs\". OR \"a chair with armrests\" and then \"a chair without armrests\". I wonder if there is a metric to compute, when you do subtract attributes you should return to the exactly previous chair? Is it a good loss function?", "reference_answer": "A: It is an interesting point about 'conflicts' in the input text prompt: either a conflicting attribute of the same part or conflicting existence of a part. While none of the previous text-conditioned shape generation papers report any editing with such 'conflicted' text prompts, the authors indeed tried editing with such texts during the proposed experiments. As a result, the model can sometimes generate a novel shape which is a hybrid of the conflicting attributes. For example, if the inputs are 'standard legs' and 'wheels', the model can generate a shape that looks like the last example in Figure 1 row 2, where the shape has wheels on standard legs. Most of the time, the model tends to choose one of the attributes. This could be due to the proposed model representing the shape as a probability distribution. Therefore, when the model is only conditioned on one attribute, the probability distribution is more narrow. When the model is conditioned on more than one attribute, the probability distribution widens out. The authors will add visualizations of editing with 'conflicted' text prompts in the supplementary material. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "78cd3e82-ab39-5f49-94ab-d3810b231fcf", "question": "What is the intuition about using multi-branch blocks and down-sampling block with multiple max-pooling branches?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["4f0119d8-a664-5be3-87ab-bf4c76d488cd"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the intuition about using multi-branch blocks and down-sampling block with multiple max-pooling branches?", "reference_answer": "A: The motivation comes from the architecture difference between MLP and CNN. The only difference between FC layer (basic element in MLP) and conv layer (basic element in CNN) is that FC layer can be treated as convolution with kernel size 1, while conv layer in CNN always have larger kernel size. As shown in Line 144 in original paper, the representation ability of binary FC and conv layer is related to the kernel size (N=C_in * K_h * K_w), and binary FC layer tend to have less representation ability due to the small kernel size and yields poor performance, as shown in the table below.\n\n|Network|Kernel Size|Performance drop|\n|-|-|-|\n|WaveMLP|1|22%|\n|ResNet-18|3|17%|\n|AlexNet|11 & 5|13%|\n\nNote that the larger the kernel size, the less performance drop between 1bit network and full-precision network. Thus, the authors need to increase the representation ability of MLP.\n\nIn order to make the representation ability of FC layer (1x1 conv) to be the same as conv layer (kxk conv), there are two different ways. The first is to increase the input channel. Note that output channels should also be scaled up in order to maintain the representation ability (RA) of the next FC layer (the number of output channel of current layer is the number of the input channel of next layer). Thus, the computational complexity (CC) will be drastically increased, as shown in the table below.\n\n||in_channel|out_channel|kernel_size|CC|RA|\n|-|-|-|-|-|-|\n|bi-FC layer|$C_{in}$|$C_{out}$|$1\\times1$|$1$|$1$|\n|bi-conv layer|$C_{in}$|$C_{out}$|$k\\times k$|$k^2$|$k^2$|\n|bi-FC layer with more channel|$k^2C_{in}$|$k^2C_{out}$|$1\\times1$|$k^4$|$k^2$|\n\nThus, the authors use multi-branch blocks to increase the representation ability while maintain the computational complexity, as shown below.\n\n||branch_num|in_channel|out_channel|kernel_size|CC|RA|\n|-|-|-|-|-|-|-|\n|bi-FC layer|$1$|$C_{in}$|$C_{out}$|$1\\times 1$|$1$|$1$|\n|bi-conv layer|$1$|$C_{in}$|$C_{out}$|$k\\times k$|$k^2$|$k^2$|\n|bi-FC layer with more channel|$1$|$k^2C_{in}$|$k^2C_{out}$|$1\\times1$|$k^4$|$k^2$|\n|bi-FC layer with more branches|$k^2$|$C_{in}$|$C_{out}$|$1\\times1$|$k^2$|$k^2$|\n\nThe intuition of downsampling block is simple. Original downsampling layers (Figure 3 left) occupy the OPs of the whole binary network, and directly binarize them yield severe performance drop. Thus, the authors separate the changing of spatial size and channel number with maxpooling and 1x1 conv, and reduce the OPs while keeping the performance."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aa8371a1-d193-5046-bdc6-9ed89a6698c7", "question": "What are the two degraded conditions that were used for the image classification task?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b73ceeab-4bcd-5261-8f93-2810b1cbdbab"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the two degraded conditions that were used for the image classification task?", "reference_answer": "A: Adversarial samples as degraded inputs and weight uncertainty."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aca5a37b-fc5d-5659-864e-c69b1c9f6e32", "question": "How is a model defined to be robust? \nReference:\nCroce, Francesco, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. \"Robustbench: a standardized adversarial robustness benchmark.\" arXiv preprint arXiv:2010.09670 (2020).", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9e06e7ed-2aaf-54c1-9fa8-d5bdc49ddf6f"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is a model defined to be robust? \nReference:\nCroce, Francesco, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. \"Robustbench: a standardized adversarial robustness benchmark.\" arXiv preprint arXiv:2010.09670 (2020).", "reference_answer": "A: A model is robust if it shows robust accuracy on RobustBench of more than 41.33 % accuracy on Cifar10, 18.95% on CIFAR100 and 25.32 on ImageNet (listed on RobustBench) accuracy.."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a95267f7-82c0-5c0e-bacf-5d724410b9f0", "question": "Why not use MSCOCO as the target dataset for image captioning?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["2940ef79-519b-599e-9fe7-d27dc8c949e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why not use MSCOCO as the target dataset for image captioning?", "reference_answer": "A: The authors prefer MIMIC-CXR over standard datasets like MS-COCO because each MS-COCO’s caption usually contains one to two sentences while medical reports are much longer. Furthermore, MIMIC-CXR is less subjective than MS-COCO because while all radiologists receive standardized training to spot the same abnormalities, general image captioning highly depends on the labellers. Most importantly, medical reports [R4, R5] and automatic chest x-ray assessments [R1, R2, R3] have seen great interest in recent years due to its applications in healthcare [R6]."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "49042287-69f3-5f0a-9a1a-043c77931025", "question": "Can you present results from the paper, which has a comparison to the SOTA just like what happened in Table 4, about how even lower bit-width, such as 4-bit, will affect the training accuracy?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["70d23f47-63f0-5df1-88ba-b148cfca9c6a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Can you present results from the paper, which has a comparison to the SOTA just like what happened in Table 4, about how even lower bit-width, such as 4-bit, will affect the training accuracy?", "reference_answer": "A: The **Table 5** as well as **lines 312-314** in the main article discuss this. \n\n| **Model** | **Dataset** | int8 | int7| int6 | int5 | int4 |\n|---                | ---              | --- | --- | --- | --- | ---\n| **ResNet18**  | **CIFAR10**    | 94.8 | 94.7    | 94.47    | 88.5    | Diverges    |\n\n*Low-bit integer training: Table 5 provides an ablation study of how lowering integer bit-width can affect the training accuracy. The experiments in the paper shows that training has a significant drop of accuracy with int5 and diverges using int4 number formats.* Also note that the authors were not able to do this experiments for all SOTA models because of the limited time that the authors had for this rebuttal. However, the authors can infer the extent to which this method works in low-bit regimes from the CIFAR10 experiment in Table 5. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "68e7ecb5-39b4-5b82-943c-b259456863ec", "question": "When using the loss from [1], you mention that you neglect the entropy term. What's the problem with keeping it? Would the results from [1] improve had they neglected it? \nReference:\n[1] Y. Du, S. Li, J. Tenenbaum, and I. Mordatch, “Improved contrastive divergence training of energy-based models,” in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021. [Online]. Available: https://proceedings.mlr.press/v139/du21b.html", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["7f1c2adf-3164-5df0-b6ba-b27f76ecd17c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "When using the loss from [1], you mention that you neglect the entropy term. What's the problem with keeping it? Would the results from [1] improve had they neglected it? \nReference:\n[1] Y. Du, S. Li, J. Tenenbaum, and I. Mordatch, “Improved contrastive divergence training of energy-based models,” in Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila and T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021. [Online]. Available: https://proceedings.mlr.press/v139/du21b.html", "reference_answer": "A: The entropy term [1] serves to increase the diversity of the generated examples. And the computation of entropy requires many examples. This is fine in [1] since the EBM there has the form of E(x) which only needs to generate images <em>unconditionally<em>, and the entropy can be estimated using all previous generated images x. In the proposed work, the proposed EBM are E(x,m,c) and E(x,m1,m2,c), and the authors need to generate the mask <em>conditionally<em>, e.g. generate mask m conditioned on the image x and label c. The entropy term would need to be a conditional entropy of m given x and c, where the pool of mask m should be different for each individual image x and label c. This requires, e.g. for each x, c, the authors generate over 100 masks to estimate the entropy which is computationally expensive, while currently the authors only need to sample 1 mask. Moreover, typically there are limited correct masks for a concept in an image, and encouraging diversity may not help the model identify the correct mask. In fact, the authors have tried empirically with keeping the entropy term and it results in a much worse accuracy, likely due to the above reason. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b31d9df0-806e-574e-a916-ea11727c816e", "question": "Do you have guidance on choosing the subgraph height?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["928d74ad-4c97-537a-b49e-b215f66676f0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Do you have guidance on choosing the subgraph height?", "reference_answer": "A: Yes, this parameter should be related to the dataset and graph size. Theoretically, to distinguish regular graphs, the proposed Theorem 1 provides a guidance (i.e., choose h = 0.5log(n)/log(r-1)). As shown in Appendix Figure 3, for 100-node 3-regular graphs, an h=3 is enough, and for 1000-node graphs, an h=4 is enough. For practical datasets, the authors can tune it like how the authors tune the number of layers in a standard GNN. Nevertheless, the authors find h=3 to be generally a good choice."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "5c74cf95-3873-570a-8b88-a21246131ccc", "question": "What are the results of the text consistency metric on the Flickr30K 1K and MSCOCO 5K test set?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6a0a1c4d-3b62-56cb-a6e9-48b790274828"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What are the results of the text consistency metric on the Flickr30K 1K and MSCOCO 5K test set?", "reference_answer": "A: The authors find that CyCLIP outperforms CLIP on fine tuning for both the datasets. The following table summarizes are results.\n\n|           | Flickr30K 1K | MSCOCO 5K |\n|-----------|--------------|-----------|\n| **Zero-shot** |              |           |\n| CLIP      | 20.9         | 2.8       |\n| CyCLIP    | **21.3**         | 2.8       |\n| **Fine-tuned** |              |           |\n| CLIP      | 35.4         | 3.8       |\n| CyCLIP    | **37.7**         | **4.1**       |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "51739490-53fa-5aec-a7cf-b0f8d8d4a729", "question": "About the relationship between knowledge distillation and adversarial attack. \"Since distillation can be used as adversarial defense [R1], what is the relation between the results of the student model and the adversarial attacks?\"\nReference:\n[R1] Papernot, Nicolas, et al. \"Distillation as a defense to adversarial perturbations against deep neural networks.\" 2016 IEEE symposium on security and privacy (SP). IEEE, 2016.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["67576ae0-7ff8-54d2-a065-58a3a4f1c70a"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "About the relationship between knowledge distillation and adversarial attack. \"Since distillation can be used as adversarial defense [R1], what is the relation between the results of the student model and the adversarial attacks?\"\nReference:\n[R1] Papernot, Nicolas, et al. \"Distillation as a defense to adversarial perturbations against deep neural networks.\" 2016 IEEE symposium on security and privacy (SP). IEEE, 2016.", "reference_answer": "A: The authors conducted the following experiment to compare the distilled student model with the normal model, in terms of the change of regional features in their reliability and importance after the attack. The student model was a VGG-16 net (termed the distilled VGG-16), which was learned by pushing the output feature of its conv_53 layer towards the corresponding feature in a normally trained VGG-16 (termed the original VGG-16, Line 235) for distillation. The authors used $\\Delta_{\\text{orientation}}=E_x[E_r(1-\\cos(h_{\\text{ori}}^{(r)},h_{\\text{adv}}^{(r)}))]$ to measure the utility of the attack to regional features' orientation, which reflects the change of regional features' reliability. Besides, the authors used  $\\tilde\\Delta_{\\text{strength}}=E_x\\left[E_r\\left(\\frac{|\\Vert {h_{\\text{ o r i}}}^{(r)}\\Vert_2-\\Vert {h_{\\text{ a d v}}}^{(r)}\\Vert_2|}{\\Vert {h_{\\text{ o r i}}}^{(r)}\\Vert_2}\\right)\\right]$to measure the utility of the attack to regional features' strength, which reflects the change of regional features' importance. Note that for fair comparison between different DNNs, the authors used the strength of regional features $\\Vert h_{\\text{ori}}^{(r)}\\Vert_2$ for normalization when computing the change in regional features' strength $\\tilde\\Delta_{\\text{strength}}$, instead of $\\Delta_{\\text{strength}}$ in Line 305. Larger values of $\\Delta_{\\text{orientation}}$ and $\\tilde\\Delta_{\\text{strength}}$ indicates more significant changes in reliability and importance. The following table shows $\\Delta_{\\text{orientation}}$ and $\\tilde\\Delta_{\\text{strength}}$ of regional features in the original VGG-16 and the distilled VGG-16. The authors found that the utilities of the attack on the original VGG-16 was higher than those on the distilled VGG-16. This indicated that the regional features were more robust in the distilled DNN than those in the normally trained DNN.\n\n|                      | $\\Delta_{\\text{orientation}}$ | $\\tilde\\Delta_{\\text{strength}}$ |\n| -------------------- | ----------------------------- | -------------------------------- |\n| the original VGG-16  | 0.3091                        | 0.6030                           |\n| the distilled VGG-16 | 0.1510                        | 0.2891                           |\n\n[R1] Papernot, Nicolas, et al. \"Distillation as a defense to adversarial perturbations against deep neural networks.\" 2016 IEEE symposium on security and privacy (SP). IEEE, 2016."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "155e41db-aa5a-5a0b-b78f-bdb8dc79988c", "question": "In the proof of Theorem 3.1, for the equation between line 494 and line 495, what do you mean by $P(w\\notin\\mathcal{W}_{u}^{K})$? The previous term involves taking sup over $w$, but here, it is for some arbitrary $w$? Also in the same line, what do you mean by $\\sup{S}$? What is the space of the supremum taken over? Can you explain in more detail how do you get the next inequality?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["befa2892-2ca9-59fb-bd49-724e13203e60"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "In the proof of Theorem 3.1, for the equation between line 494 and line 495, what do you mean by $P(w\\notin\\mathcal{W}_{u}^{K})$? The previous term involves taking sup over $w$, but here, it is for some arbitrary $w$? Also in the same line, what do you mean by $\\sup{S}$? What is the space of the supremum taken over? Can you explain in more detail how do you get the next inequality?", "reference_answer": "A: It is a typo, it should read $\\bar{w}^S$ instead of $w$. The supremum is taken over all possible samples (to be exact, and the authors will clarify, samples of $n$ norm bounded $x$’s and labels $y$). The inequality is given as follows: the first term of the LHS is bounded as stated (using Eq.(13) with $K$ being the RHS of Eq.(14))  and the second term is bounded by bounding the worst case norm of $w_S$ as described in the sentence below."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b85eb846-7bdd-5680-a47c-48e53e7f5e0d", "question": "Is Theorem 2.3 is tightly coupled with other theoretical results such as Theorem 2.2 or is it a way of approximation that can simplify the aforementioned framework?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is Theorem 2.3 is tightly coupled with other theoretical results such as Theorem 2.2 or is it a way of approximation that can simplify the aforementioned framework?", "reference_answer": "A: Theorem 2.3 utilizes Ito's formula of $\\mathrm{d}||x||^\\alpha$ for some $0<\\alpha<1$ and aims at steering  $||x||^\\alpha$ to zero with constraint used in AS loss, while Theorem 2.2 and other theoretical results consider the case of $\\mathrm{d}\\log V(||x||)$ or $\\mathrm{d}\\log||x||$. So the results in Theorem 2.2 can be seen as negative fractional polynomial growth, and the growth rate in Theorem 2.3 is negative exponential growth. Hence, these two theorems have no direct connection and the constraints in these theorems do not cover each other."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ebd4b6b5-8a6d-5322-9119-a06de2f6bfc6", "question": "Explain the motivation and the description of the tasks.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9cd6253e-8a5a-55f2-8bdc-77afa61d48e4"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Explain the motivation and the description of the tasks.", "reference_answer": "A: For a given image, the task of ordinal regression in computer vision is dedicated to predicting a rank number or continued value. For example, age estimation aims to estimate the age of a given face image while image aesthetic assessment predicts the aesthetic score for an image. As many popular methods adopt a classification framework, there are two main challenges. First, treating ranks as independent class categories fails to grasp the ordinal property. Second, as the learned concepts are mainly derived from the training set, these approaches are prone to overfit and usually attain unsatisfactory performance. Since learning the rank concept from the image domain alone is prone to overfitting, the authors can leverage multimodal information to alleviate this issue. The human language contains rich semantic information and prior knowledge. The authors consider simultaneously borrowing the rank concept from the language domain. Specifically, each rank label is not only regarded as a class category but also linked to a sentence describing the corresponding rank, such as \"this person is 23 years old\". In this way, the proposed model not only learns the concept of ranks defined on the vision dataset but also exploits the common knowledge of rank in the language domain. Therefore the authors propose a language-powered paradigm for ordinal regression to alleviate the overfitting issue by associating each rank category with its language concept. Moreover, the authors propose to learn rank prompts to model the ordinal property."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ecdb0f2f-50a4-5295-9557-aeb84367b4fd", "question": "Considerations about ARI and NMI results in Table 2?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table", "formula"], "anchor_pdf": ["c54123c6-251b-50b5-a775-396bdfee5734"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Considerations about ARI and NMI results in Table 2?", "reference_answer": "A: 1. On HF, HRTU2 and UK, an ARI value closer to 0, essentially means a random clustering. Therefore the results are very weak.\n2. ARI and NMI values from K-Means, K-Means++.\nARI and NMI evaluate how well the formulation matches the true labels. Hence, a formulation's objective value may not be consistent with ARI and NMI. As seen from this table, PAM obtains the best ARI and NMI among the five methods for the HCV dataset, while it has the worst KMedoids objective value. The proposed method focuses on obtaining the best KMedoids objective values within an acceptable running time. However, if the authors need to compare with respect to ARI and NMI, the proposed method always provides better or the same performance compared with K-means and K-means++."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "c0514d46-ffd8-5f11-b30d-22c78a11d555", "question": "How is sampling achieved for generating diverse molecules for a specific pocket? Is sampling only involved after generating molecules from the shape?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["aa04e4ab-fa31-5dcb-a790-029a0317dae9"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How is sampling achieved for generating diverse molecules for a specific pocket? Is sampling only involved after generating molecules from the shape?", "reference_answer": "A: The sampling is achieved in two steps: a) Sampling molecular shapes based on the given pocket. When sampling molecular shapes, the authors use different seed shapes and set the initial position of the seed shape randomly. Both of them contribute to the diversity of the generated molecules; 2) For each molecular shape, the authors further sample diverse molecules that fit it. Specifically, the authors employ the Nucleus decoding method to selectively combine different fragments in different decoding steps to achieve diversity. The sampling happens through the whole generation process. As reported in Table 1, as expected, the proposed method obtained high diversity."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "7168f8c0-45a1-54d7-8d2d-2b20d19e37f2", "question": "What would be the main modification to make the original KRR work? Is it that this paper is using a better model pool or models with higher feature dimensions or moving backbones?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["3caa27d0-1d46-5025-996c-ddd8b47eb705"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What would be the main modification to make the original KRR work? Is it that this paper is using a better model pool or models with higher feature dimensions or moving backbones?", "reference_answer": "A: The authors think the idea of KIP is excellent. The spirit of the proposed method and KIP are similar as the authors both want to find a good approximation to the inner loop optimization. If you think more along this line and are familiar with NTK theory, you would likely come up with a similar idea. Indeed, KIP works well in certain cases (e.g., given enough computation resources, low-resolution images). \n\nHowever, KIP has several drawbacks that make it less practical. The authors show them in the decreasing order of significance below.\n- High computation cost: It requires thousands of GPU hours to perform a single experiment on low-resolution image datasets like CIFAR10. You can see more details in Appendix B of https://arxiv.org/abs/2107.13034, and you can also learn more from the neural tangent library https://github.com/google/neural-tangents#cnn-with-pooling).\n- Poor Scalability: KIP can not scale well to high-resolution images (time complexity is O(d^2) if using convolutional neural networks with pooling layers, where d is the number of pixels) or many data points.\n- Discrepancy between finite-width neural networks: KIP uses analytical NTK to compute the meta-gradient, which assumes the neural network to be infinitely wide and operates in the kernel regime. However, a standard finite-width neural network does not satisfy these assumptions, and its training dynamic is quite complex. Training a finite-width neural network on the distilled data causes a significant performance drop in evaluation. \n- Architecture Constraints: Many modern architectures with a normalization layer (e.g., BatchNorm) do not have an analytical NTK expression. So, the set of model architectures that KIP can use is quite limited.\n\nA: There are three main components in the proposed method:\n- (1) Conjugate kernel using a wider neural network (higher feature dimension)\n- (2) Online model update (moving backbones)\n- (3) Model pool\n\nThe authors think all of them are important, but in decreasing order of significance. The authors also provide additional tricks to further improve the performance in Appendix D. \n\nA: The first component is the most crucial one, designed to approximate the inner loop optimization efficiently. Compared to NTK approximation in KIP, the proposed method is at least four orders of magnitude faster, which makes the proposed method practical. You can find more ablation studies regarding the significance of the architecture (e.g., width, depth, and normalization) in Appendix C.6. Note that KIP uses the infinite-wide neural network to compute the meta-gradient for the distilled data, which is indeed using a much wider neural network than ours.\n\nA: The second component (online model update) aims to take the neural network training dynamic into account, which can improve the performance on CIFAR100 1 Img/Cls by 2% (Appendix Figure 9(b)). Essentially, this component is designed to account for errors caused by linear approximation. The authors want to take the complex NN training dynamics into account and gather gradient information from all stages of training rather than just the initialization. (1) + (2) form the analogy of 1-step TBPTT. People generally think 1-step TBPTT is bad as it causes significant truncation errors. However, the kernel approximation elegantly addresses this problem as it is equivalent to training the last layer into convergence.\n\nA: The third component is a generalization of the iterative model reinitialization techniques, which can improve the performance on CIFAR100 1 Img/Cls by 1% (Appendix Figure 9(a)). A direct motivation of this design is observing the cyclic behavior of loss when using an iterative model reinitialization. It suggests that the iterative model reinitialization scheme wastes some computation when it overfits a particular training trajectory. Besides, from the perspective of meta-learning, it is always good to have a diverse meta-train task to learn a more generalized meta-parameter. Therefore, the authors use the \"model pool\" idea to provide diverse meta-training tasks. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "726393cd-9d8e-5343-b787-58761ee0315d", "question": "Isn't it bad in terms of diversity if $\\mathbf{z}$ is optimized by Eq.5?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "formula"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Isn't it bad in terms of diversity if $\\mathbf{z}$ is optimized by Eq.5?", "reference_answer": "A: Equation 5 may indeed cause a slight diversity drop compared to the generation results of raw Gaussian samples. However, please note that this diversity deviation is brought by the bad generations of raw Gaussian samples, which is meaningless for realistic generation. For example, natural images mixed with bad generation results will have a better diversity than pure natural images, as bad generations have a more different distribution from natural images. On the other hand, as the authors bound the perturbation of $\\mathbf{z}$, the diversity drop will only be slight, while the quality gain can be significant."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a9d7416f-0339-5899-a527-d3d2d7c533f0", "question": "For the harmonic linear oscillator, how would an optimal deterministic controller (u=K_{lqr}x) perform compared to the stochastic counterparts?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["9aa81fae-7ba1-54ea-90d3-ac09a63304f7"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "For the harmonic linear oscillator, how would an optimal deterministic controller (u=K_{lqr}x) perform compared to the stochastic counterparts?", "reference_answer": "A: The authors compare the performance of the proposed learning control with existing methods in Figure 6, it can be seen that their method outperforms the LQR method."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b39ad112-678c-5f9f-96ed-0c9f05e01fa1", "question": "It is true that x(t+1) = Ax(t) is both recurrent and linear, but the term \"RNNs of RNNs\" gives me an impression that x(t) is a RNN, is this true from the paper? If the x(t) is the output from another RNN, what is difference to stacked RNNs, or other ways of combining RNNs?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["04f187d0-d8b8-5235-8662-1907ca882b83"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "It is true that x(t+1) = Ax(t) is both recurrent and linear, but the term \"RNNs of RNNs\" gives me an impression that x(t) is a RNN, is this true from the paper? If the x(t) is the output from another RNN, what is difference to stacked RNNs, or other ways of combining RNNs?", "reference_answer": "A:In the proposed networks, the output from another RNN is passed through another set of weights before entering a downstream RNN. This is slightly different from the equation you have written above. For two linear RNNs, the proposed network equations would read:\nx' = A x(t) + C y(t)\n and \ny' = B y(t) - $C^T$ x(t)\nOur approach differs from other ways of combining RNNs by carefully constraining the connection matrices between RNNs (matrix C in the above example) to preserve the contractive stability of the individual RNNs. To the best of the proposed knowledge, this has not been done before in the machine learning and neuroscience literature."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4156fb8f-ca54-52a2-9cd7-085348a99dbe", "question": "Could you clarify what does it mean that the generation quality is not a continuous function?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["4eda77a0-ac45-597a-bf33-c54dc78919bf"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you clarify what does it mean that the generation quality is not a continuous function?", "reference_answer": "A: The generation quality is reflected by the matching degree to the natural image distribution in pixel space. As natural image distribution is supported on disconnected manifolds, the generation quality does not exhibit a continuous nature, e.g. two nearby images' quality can differ much (please see Figure 1 and Figure 3)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d61d8a56-218a-5163-a9ac-d73b8324f40f", "question": "What is so special about the plot?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["14c1199a-b695-5cec-a11a-023781bfb0c5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is so special about the plot?", "reference_answer": "A: As mentioned in Lines 156-163, the two insights are derived from Table 1, Table 2 and Figure 5: 1) Increasing the initial standard deviation $\\sigma$ will help us distinguish the disparity of entropy between different bits; 2) Quantization of activations and weights has different effects on accuracy. For example, A3W4 and A4W3 models have the same BitOps, but A4W3 models have about 2\\% accuracy higher than A3W4, which means that quantization on activations and weights has different effects on accuracy. A3W6 and A6W3 models have the same situation. Therefore, the authors set different values of $\\sigma_A$ and $\\sigma_W$ to show differences between activations and weights."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "b98285d4-85e3-5107-a8b2-165eaf699edb", "question": "What happens when one increases the block size too much?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e9212c2d-3733-5423-97bf-6bc024f6c4a0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What happens when one increases the block size too much?", "reference_answer": "A: In Figure 2 Middle, the authors show block size up to 512. Larger block sizes simply do not fit into the available SRAM on an A100 GPU. Your understanding is right in that block size 512 does about the same as block size 256, since other resources (e.g., compute) become the bottleneck."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "592c1ab6-64e5-5ff1-ae92-35766a434947", "question": "Is the performance on image synthesis good enough?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["ca8797f9-dc18-5853-9021-f8fd915534e6"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Is the performance on image synthesis good enough?", "reference_answer": "A: It is hard to give a fair comparison with MaskGIT and VIT-VQGAN due to the different training settings. However, the proposed model significantly improves the performance on image reconstruction and generation."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ce4e9e7a-d760-5595-8e95-ebeb30673bea", "question": "What is the proposed threat model for adversarial examples?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["cb076cee-4910-5d68-b558-9940c9b9eacc"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the proposed threat model for adversarial examples?", "reference_answer": "A: Any image that is differently classified as the original image."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "96c5d4cd-54e4-5be0-b236-c628634e550b", "question": "Clarification of the evaluations in Figure 4. Explain?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["48f9ec18-2cd0-51ff-ab26-65a64d18c681"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Clarification of the evaluations in Figure 4. Explain?", "reference_answer": "A: The yellow curves represent the performance of P2P-MPC, which minimizes the multi-step loss on the trajectories generated by active interactions between the model and the current policy. The blue curves show the results of an ablation version of MBPO where the original one-step loss is replaced by a multi-step loss computed over the trajectories sampled from the environment dataset. The lengths of these trajectories are set to the same in this comparison. This clarification have been added to the revised version (Section 4.3, Page 8, Figure 4). "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "a624f56a-15a3-5f7c-a874-ac307292e9c8", "question": "How novel is the tree MDP formulation? What, if any, relationship, does tMDP have to a recursively-optimal hierarchical RL algorithm like MAXQ [Dietterich99])?\nReferences:\nDietterich, T. G. (1999). Hierarchical reinforcement learning with the MAXQ value function decomposition. CoRR. arXiv preprint cs.LG/9905014", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["dd95752f-0336-50fe-a659-818f7fdb8358"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How novel is the tree MDP formulation? What, if any, relationship, does tMDP have to a recursively-optimal hierarchical RL algorithm like MAXQ [Dietterich99])?\nReferences:\nDietterich, T. G. (1999). Hierarchical reinforcement learning with the MAXQ value function decomposition. CoRR. arXiv preprint cs.LG/9905014", "reference_answer": "A: In MAXQ, the authors propose to decompose the main task into a set of simpler tasks which can be solved recursively, independently of the parent task (e.g., pick up and deliver a package from A to B decomposes to: move to A, pick up, move to B, drop). Both approaches have similarities, in the sense that they exploit a hierarchical decomposition of the task at hand in order to simplify the credit assignment problem in RL. However, the two methods also differ on several points. \n1) in MAXQ, the hierarchical sub-task decomposition must be given a priori by the user for each new task, and is set in stone with a limited depth, while in tree MDPs the decomposition holds by construction, and can be applied recursively for virtually infinite depths; \n2) in MAXQ, the subtasks are different (different reward and optimal policy), while in tree MDPs the reward remains the same; \n3) in MAXQ, each sub-task necessarily results in a series of consecutively processed states (e.g., AAABBBCC), while in tree MDPs the temporal processing order of states can vary, and switches between different sub-trees are allowed (e.g., AACBBAC); and \n4) in MAXQ, the resulting process is made Markovian by including the subtask stack K to the state S, while in tree MDPs the state S is sufficient to have the Markovian property. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "30bcd576-734b-5ff8-85d0-5d19164764bb", "question": "Are there any other details about the weighting coefficients $\\alpha$?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["4cbd7cbb-7b71-57e9-9b12-e7a19df3dff5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Are there any other details about the weighting coefficients $\\alpha$?", "reference_answer": "A: In the inline equation $f_\\alpha(\\mathbf{x}, \\mathbf{d})$ at line 180, $\\mathbf{x}$ refers to the position of any given point, and $\\mathbf{d}$ is the viewing direction. The weighting map of a given view is rendered similarly to Eq.(2), where $\\sigma_{t}^{(i)}$ being the $\\sigma$  in the equation, and the value of $\\alpha$ along the ray emitted from the camera is accumulated. This setting enables the network to increase its robustness with real-world cases in the proposed experiments."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "4022756a-a180-51da-a50d-4492586b778d", "question": "What is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["580c0752-1384-5c86-8c78-0002b0af7e87"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What is the dataset used for the evaluation of the first paragraph in section 5.1? How do you split the Pascal VOC data to exclusive sets?", "reference_answer": "A: The dataset used in the first paragraph of Section 5.1 is PASCAL VOC/SBD, as used in Xu et al., which the authors compare against. For few-shot semantic segmentation, the authors follow the experimental protocol of Shaban et al., which tests few-shot performance on held-out classes by dividing the 20 classes of PASCAL into 4 sets of 5, then reports the average performance across these sets for the 5 held-out classes after training on the remaining 15. Images that contain both held-out and training classes are placed in the held-out set."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "8cc788c5-c6f5-57fc-8ee2-cacfca34c6cb", "question": "How well do Newton's method-based second-order optimizers do here? Are there other promising results using other second-order optimizers? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["1eb3000c-95d6-590d-a4af-4fcbb2e5cd06"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How well do Newton's method-based second-order optimizers do here? Are there other promising results using other second-order optimizers? ", "reference_answer": "A: An inherent problem with classical second-order methods like L-BFGS is that they do not work well with mini-batches since they estimate the Hessian by accumulating multiple updates. However, the authors have run gradient descent and L-BFGS-B on single examples, e.g. for the heat equation in Figs. 10, 13 in the appendix. While the proposed domain-specific iterative optimizers perform very well, standard solvers take a large number of iterations to match the network prediction. L-BFGS-B matches the Adam network after 500 iterations but slows down significantly before reaching the SIP network prediction. The authors believe that, given an unlimited optimization budget, both iterative solvers will eventually reach the accuracy of Adam and SIP, but will take many orders of magnitude longer. AdaHessian is a prime example of a second-order optimizer for these problems."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "aa2cb804-d4e7-5948-94ca-8f3ff8e4ce69", "question": "Results of different down-project ratios of adapters?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["99754856-542e-5463-8e6a-6104b605cdef"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Results of different down-project ratios of adapters?", "reference_answer": "A: The authors vary the down-projection ratios (ρ)  of the adapters and report the results in the Table. The authors find that the semantic segmentation reaches the near-optimal performance when the small adapters are used (ρ = 32). However, for other dense prediction tasks, there exist obvious gaps when the smaller adapters are used, and averaged relative improvement shrinks when the adapter sizes are smaller.\nThis suggests that the required network capacity for semantic segmentation is sufficient when small adapters are used, while other dense prediction tasks require more trainable parameters. \n\nSuch a trend potentially comes from the usage of a backbone pretrained on image classification tasks with overlapping object categories (ImageNet). Such a backbone is expected to contain similar semantic information required by semantic segmentation, so that using a limited amount of trainable parameters can achieve near-optimal results. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "d5091a2f-93ed-55ae-bacc-b9e51c1c0d7a", "question": "The authors perform an evaluation on the setting of varying resolution, can you discuss the memory usage and runtime comparisons?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["6e611069-c4c2-578c-83e3-3279cc5b3dd0"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The authors perform an evaluation on the setting of varying resolution, can you discuss the memory usage and runtime comparisons?", "reference_answer": "A: Since the authors produce such results by repeating the inference process multiple times, the inference time only grows linearly with the point cloud resolution (but not the memory usage). Please see the table below, in which the authors conduct extra experiments on varying point cloud resolutions and list the required inference times and memory requirements.\n\n| Output point cloud resolution | Inference time (ms) | Memory usage (GB) |\n|:-----------------------------:|:-------------------:|:-----------------:|\n| 2048 points                   |         50.0        |       1.923       |\n| 8192 points                   |        145.6        |       1.923       |\n| 16384 points                  |        277.2        |       1.923       |"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "03eab1ff-9dd7-5389-9548-804177f3f316", "question": "The sampling from the distribution $Q(w, S)$ is not trivial (since it involves an exponent of the empirical loss). Can you elaborate on the sampling method used besides referencing SGLD and SVGD? ", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["97d7c10d-2bbc-589a-8fd0-e7af45e9cc08"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "The sampling from the distribution $Q(w, S)$ is not trivial (since it involves an exponent of the empirical loss). Can you elaborate on the sampling method used besides referencing SGLD and SVGD? ", "reference_answer": "A: The exponent of the empirical loss actually makes the inference easier, because $Q(w, S)$ become exponential families and SGLD and SVGD are based on the gradient of the logarithm of the unnormalized posterior (see [14, 27] or https://en.wikipedia.org/wiki/Stochastic_gradient_Langevin_dynamics). In order to fairly compare with MAML, the authors used only one sample to approximate $Q(w, S)$. The authors tried both SGLD and SVGD (which reduces to SGD in the log space for the one sample case) in the proposed image and natural language experiments, and they yielded similar results. Using 5 samples may further improve some results by 1%.  "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "ec4674f0-0b4a-576b-b898-893036a103e3", "question": "How many atoms are in each fragment? If the conformation of a fragment does not lie in a plain, how to determine the dihedral angles?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["b27aca20-af64-5b2b-b51d-4b3babcca320"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How many atoms are in each fragment? If the conformation of a fragment does not lie in a plain, how to determine the dihedral angles?", "reference_answer": "A: 1. The number of atoms of non-ring fragments: mean 3.42, std 1.76. \n\n2. The number of atoms of ring-containing fragments: mean 7.44, std 2.86. \n\n3. As shown in Figure 3 and Eq(7), the dihedral angle calculation between two fragments involves 4 atoms and 3 bonds (with 1 common bond connecting two fragments).  The dihedral angle only describes the angle between the two planes intersecting at the common bond, which has nothing to do with other fragment atoms. Therefore, as long as the authors have a pre-defined set of interface atoms for each fragment, the authors can always calculate the dihedral angle between fragments using their atomic coordinates (taking roto-translation invariance into consideration)."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "903cf628-211e-5ee1-9d4c-7b090ec1b0f0", "question": "Intuitively explain how connecting the parameter space and feature space makes it possible to mitigate the adverse impact caused by approximation error?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["8a90ed3c-3384-59c3-8be9-680b0f9329e5"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Intuitively explain how connecting the parameter space and feature space makes it possible to mitigate the adverse impact caused by approximation error?", "reference_answer": "A: The question makes us aware that exploring which kind of feature perturbations are more preferred is exciting and interesting, which can benefit the attack success rate of lightweight black-box attacks. \nTo alleviate the approximation error of shallow models, the authors propose transforming the parameter space's approximation error as the feature space's perturbation. The inspiration is borrowed from the feature space attack. Specifically, the authors have little knowledge to determine which perturbations can point (from the surrogate model) to the target model, making it challenging to alleviate the approximation error in the weight space. In contrast, the authors have the prior that samples with different labels should have distinguishable representations/features. Thus, the authors can leverage the prior knowledge to select preferred perturbations in the feature space, i.e., the authors prefer perturbations that can make representations/features of samples with different labels indistinguishable. Therefore, the authors design a min-max optimization to identify the \"worst'' model, and then make different image features obtained by the worst model indistinguishable. Consequently, the authors select a guide image for each source image and generate adversarial examples by perturbing the source image to make the guide and source images have the same/similar representation/features. \nInspired by the question, the authors are aware that how selecting a guide image is an exciting direction to improve the performance of lightweight black-box attacks further."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "67103fa1-8e81-5b8d-8c16-13b0d2bbedd0", "question": "what causes the success on Fold-2 when the novel classes are removed from the training set?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["eb805361-8dc3-514d-857b-40fb116fd5b8"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "what causes the success on Fold-2 when the novel classes are removed from the training set?", "reference_answer": "A: Below the authors count the number of images in each fold before and after using the dataset trick.\n\n| Pascal 5$^i$  | Fold-0  |  Fold-1  |   Fold-2 | Fold-3   |\n| ------------ | ------------ | ------------ | ------------ | ------------ |\n| w/o remove novel classes  | 4760        |4588        |4097        |5108 |\n| remove novel classes        |4208        |3726        |2752        |4510 |\n| reduction rate        |11.6%        |18.8%        |**32.8%**        |11.7% |\n\nThe statistical results show that the number of images containing novel classes in Fold-2 training set is 2-3 times that of other folds. The authors guess that the removed images negatively affect the results of Fold-2. Therefore, the performance improvement of Fold-2 is most obvious when removing images containing novel classes in training set."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "9f5c02da-7b78-5e38-b151-757fff6bef0e", "question": "What would be analogous to using translation, masking and scaling against an $\\ell_p$ defense in image classification?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["02363191-d394-5180-9f06-ef2890340930"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What would be analogous to using translation, masking and scaling against an $\\ell_p$ defense in image classification?", "reference_answer": "A: Having extra attack transformations in test-time."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "52df6a4b-272d-5e25-bc82-c4c70996346a", "question": "Why is the lighting different in Figure 4?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image"], "anchor_pdf": ["e76cdd40-14a4-5e64-b8b2-c6871f4cf676"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Why is the lighting different in Figure 4?", "reference_answer": "A: This is because the images are taken in different days."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "00967772-73af-5644-b40f-cb5ea47a9571", "question": "How does CARD compare with methods in paper \"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\" NIPS 2017, ( Equation 2 for regression uncertainty estimation)?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["066d7d21-caf3-5d81-a520-23ebfe718566"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does CARD compare with methods in paper \"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?\" NIPS 2017, ( Equation 2 for regression uncertainty estimation)?", "reference_answer": "A: The above-mentioned paper addresses Bayesian neural network (BNN) as an important class of methods for modeling uncertainty. CARD is related to BNNs in providing stochastic output. However, BNNs deliver such stochasticity by modeling *epistemic* uncertainty, the uncertainty over network parameters $W$ (by placing a prior distribution over $W$) – this type of uncertainty is a **property of the model**. On the other hand, CARD does not model epistemic uncertainty, as it applies a deterministic deep neural network as its functional form; it is designed to model *aleatoric* uncertainty instead, which is a **property intrinsic to the data**. In Eqn. 2 of the mentioned paper, such aleatoric uncertainty is captured by the last term as $\\sigma^2$, which is a constant w.r.t. the network parameters $\\theta$ for the variational distribution of model parameter $W$, thus ignored during the optimization of $\\theta$. The new method proposed in this paper aims to model the aleatoric uncertainty by making $\\sigma^2$ as part of the BNN output (Eqn. 7); however, note that it still explicitly assumes $p(y|x)$ to be a Gaussian distribution, as the objective function is the negative Gaussian log-likelihood, thus its effectiveness in capturing the actual aleatoric uncertainty depends on the validity of such parametric assumption for $p(y|x)$. "}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "dc0e1e9a-3aec-5ca3-8eb7-1e41ea3b0fc7", "question": "How does the performance change depending on different choices of the validation datasets?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["92992ac1-22b6-5961-976b-872a4f52e10c"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "How does the performance change depending on different choices of the validation datasets?", "reference_answer": "A: The authors have shown how the hyperparameters affects the performance of the proposed methods in Tables 17 - 28. The authors also demonstrate the detailed performance of the proposed methods on different validation datasets."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "3155e5dd-874e-5409-9806-d236660aac5a", "question": "What kind of applications for constrained Markov games? How can we formulate hard constrains into expectation form?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "formula"], "anchor_pdf": ["d3246049-be97-50c3-9459-d4bcc0a7a4a3"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "What kind of applications for constrained Markov games? How can we formulate hard constrains into expectation form?", "reference_answer": "A: Regarding the first question, for example, [2] formulates an anti-jamming wireless communication system as a constrained Markov game involving a legitimate transmitter and a jammer. Specifically, the state corresponds to how long has the transmitter get rid of the jammer. For the actions, the transmitter can select one of $K$ channels and a transmission rate to transmit message, and the jammer can select a channel and a power level to jam. \nMoreover, the jammer is subject to an average power constraint written as $\\mathbb{E} _ {\\pi}\\big[\\frac{1}{H}\\sum _ {h=1} ^ {H}C_h(s_h,a_h^{(1)},a_h^{(2)})\\big]\\le P_{avg}$, where $C_h$ denotes the power consumption of jammer at time slot $h$ (the authors made their notations consistent with ours). This constraint can be rewritten into the proposed standard form $\\mathbb{E} _ {\\pi}\\big[\\sum _ {h=1} ^ {H}r_{1,h}^{(2)}\\big]\\ge C_{\\max}-P_{avg}$ by defining a reward for the jammer as $r_{1,h}^{(2)}=C_{\\max}-C_h$, where $C_{\\max}$ is an upper bound of $C_h$.\nRegarding the second question, some additional hard constraints taking the deterministic form $r_{j,h}^{(m)}(s_h, a_h) \\ge c_j^{(m)}$ for some $h, m, s_h, a_h$, the proposed primal-dual algorithm can be extended to address them. To summarize the main idea, the above hard constraints can be translated into a set of linear equality constraints $\\pi_h^{(m)}(a_h|s_h)=0$ such that the violation $r_{j,h}^{(m)}(s_h,a_h)<c_j^{(m)}$ occurs for at least one $j$. These hard constraints can be rewritten into an expectation form by properly defining an auxiliary reward function. Moreover, the authors can adjust the proposed primal-dual algorithm to handle these additional equality constraints by introducing additional dual variables. \n[2] M. K. Hanawal, M. J. Abdel-Rahman, and M. Krunz. Joint adaptation of frequency hopping and transmission rate for anti-jamming wireless systems. IEEE Transactions on Mobile Computing, 15(9):2247–2259, 2015."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "567e32f1-04fc-53e0-8e08-0f7307c9ba2f", "question": "Learning redundant blocks. How can the objective of learning redundant residual blocks be combined with the understanding that CNNs extract feature hierarchies?", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "image", "table"], "anchor_pdf": ["6aba79aa-96a4-5499-a5d2-780ee5246148"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Learning redundant blocks. How can the objective of learning redundant residual blocks be combined with the understanding that CNNs extract feature hierarchies?", "reference_answer": "A: (1) It is widely accepted that modern CNNs such as MobileNet and ResNet have multi-stage structure where sequential stages (usually different resolutions between stages and the same resolution in one stage) generate hierarchical representations. From this view, whatever the proposed sampling method, deleting and shuffling strategy, they don’t break the multi-stage structure, which means that the hierarchical structure of representations is reserved.\n(2) Moreover, [5] and [6] discover that successive layers in the same stage of residual networks are in fact estimating the same optimal feature map so that the outputs of these layers stay relatively close to each other at convergence. From this view, the proposed method actually forces each layer in the same stage of residual networks to independently estimate the optimal feature map, so as to learn better feature representation in each stage. A more proper comprehension of the proposed method is to train better single blocks instead of training redundant blocks. \n(3) In fact, what the authors attempt to emphasize in the analysis is that the proposed method not only can improve the performance but also the robustness in resisting various network destruction operations. In the practical application, the authors believe it’s important to ensure the running network won’t collapse due to some layers’ damage. \n(4) The key point of the proposed method is to provide each member (i.e., subnetworks) with appropriate supervision and make them has the consistent overall goal with the group (i.e., main network), instead of forcing each member to do the same job. \n(5) Table 1 in the main text show that the proposed method can maintain excellent performance and robustness on different models (e.g., MobileNet and ResNet) and datasets (e.g., CIFAR10, CIFAR100 and ImageNet-1K).\n[5] Greff K, Srivastava R K, Schmidhuber J. Highway and residual networks learn unrolled iterative estimation[J]. arXiv preprint arXiv:1612.07771, 2016.\n[6] Veit A, Wilber M J, Belongie S. Residual networks behave like ensembles of relatively shallow networks[J]. Advances"}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
{"uuid": "07a543cc-bf4b-5e45-977c-8a43830a6766", "question": "Could you please justify why the improvement over the base offline RL algorithm is not large on some datasets.", "answer_format": "Your answer should be concise free-form text string, directly answering the question(s).", "tags": ["single", "text", "subjective", "table"], "anchor_pdf": ["ae801009-35b9-58e7-941e-726cb84d7b54"], "reference_pdf": [], "conference": [], "reasoning_steps": [], "evaluator": {"eval_func": "eval_scidqa", "eval_kwargs": {"question": "Could you please justify why the improvement over the base offline RL algorithm is not large on some datasets.", "reference_answer": "A: In the proposed experiments, the authors observe remarkable performance improvement over the base BCQ algorithm on many Adroit datasets, while on MuJoCo domain, the performance improvement upon TD3$\\\\\\_$BC is not that large. The generated reliable transitions by CABI are still similar to the raw samples in the static offline dataset. Therefore, combining CABI with TD3$\\\\_$BC (with a behavior cloning term) and IQL (that learns without querying OOD samples) does not bring much performance improvement. Nevertheless, the authors observe that CABI is still beneficial for these methods. To better show the effectiveness of CABI in MuJoCo domain, the authors combine CABI with CQL and conduct extensive empirical experiments on 15 datasets over 4 different random seeds. The authors summarize the results in Table 1. The authors find that CABI brings larger performance improvement for CQL."}}, "state": {"gui-gpt-4o-2024-11-20": false}, "annotator": "scidqa"}
